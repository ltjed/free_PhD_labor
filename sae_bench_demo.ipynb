{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Optional\n",
    "\n",
    "import evals.core.main as core\n",
    "import evals.scr_and_tpp.main as scr_and_tpp\n",
    "import evals.sparse_probing.main as sparse_probing\n",
    "import sae_bench_utils.general_utils as general_utils\n",
    "import custom_saes.custom_sae_config as custom_sae_config\n",
    "import custom_saes.vanilla_sae as vanilla_sae\n",
    "from sae_bench_utils.sae_selection_utils import get_saes_from_regex\n",
    "import custom_saes.run_all_evals_custom_saes as run_all_evals_custom_saes\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "output_folders = {\n",
    "    \"absorption\": \"eval_results/absorption\",\n",
    "    \"autointerp\": \"eval_results/autointerp\",\n",
    "    \"core\": \"eval_results/core\",\n",
    "    \"scr\": \"eval_results/scr\",\n",
    "    \"tpp\": \"eval_results/tpp\",\n",
    "    \"sparse_probing\": \"eval_results/sparse_probing\",\n",
    "    \"unlearning\": \"eval_results/unlearning\",\n",
    "}\n",
    "\n",
    "# Note: Unlearning is not recommended for models with < 2B parameters and we recommend an instruct tuned model\n",
    "# Unlearning will also require requesting permission for the WMDP dataset (see unlearning/README.md)\n",
    "# Absorption not recommended for models < 2B parameters\n",
    "# asyncio doesn't like notebooks, so autointerp must be ran using a python script\n",
    "\n",
    "# Select your eval types here.\n",
    "eval_types = [\n",
    "    \"absorption\",\n",
    "    # \"autointerp\",\n",
    "    \"core\",\n",
    "    \"scr\",\n",
    "    \"tpp\",\n",
    "    \"sparse_probing\",\n",
    "    # \"unlearning\",\n",
    "]\n",
    "\n",
    "if \"autointerp\" in eval_types:\n",
    "    raise ValueError(\"autointerp must be ran using a python script\")\n",
    "\n",
    "device = general_utils.setup_environment()\n",
    "\n",
    "model_name = \"pythia-70m-deduped\"\n",
    "llm_batch_size = 512\n",
    "dtype = \"float32\"\n",
    "\n",
    "\n",
    "# If evaluating multiple SAEs on the same layer, set save_activations to True\n",
    "# This will require at least 100GB of disk space\n",
    "save_activations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads your custom SAEs. If you just want to use existing SAE Lens SAEs, comment it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/radev/project/lafferty/tl784/AI-Scientist/custom_saes/vanilla_sae.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt_params = torch.load(path_to_params, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys in state_dict: odict_keys(['bias', 'encoder.weight', 'encoder.bias', 'decoder.weight'])\n",
      "Renamed keys in state_dict: dict_keys(['b_dec', 'W_enc', 'b_enc', 'W_dec'])\n",
      "d_in: 512\n",
      "d_sae: 4096\n",
      "sae dtype: torch.float32, device: cuda\n",
      "d_in: 512, d_sae: 4096\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"canrager/lm_sae\"\n",
    "baseline_filename = \"pythia70m_sweep_standard_ctx128_0712/resid_post_layer_4/trainer_8/ae.pt\"\n",
    "hook_layer = 4\n",
    "hook_name = f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "\n",
    "sae = vanilla_sae.load_vanilla_sae(repo_id, baseline_filename, hook_layer)\n",
    "sae = sae.to(device, dtype=general_utils.str_to_dtype(dtype))\n",
    "\n",
    "print(f\"sae dtype: {sae.dtype}, device: {sae.device}\")\n",
    "\n",
    "d_sae, d_in = sae.W_dec.data.shape\n",
    "\n",
    "assert d_sae >= d_in\n",
    "\n",
    "print(f\"d_in: {d_in}, d_sae: {d_sae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sae object we need to have a CustomSAEConfig. This contains some information which is used by the evals (hook_name, hook_layer, model_name, d_sae, etc). In addition, it contains information that is used by our plotting functions, like number of training tokens and architecture. For example, we should have the sae.cfg.architecture defined if we want to plot multiple SAE architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.cfg = custom_sae_config.CustomSAEConfig(\n",
    "    model_name, d_in=d_in, d_sae=d_sae, hook_name=hook_name, hook_layer=hook_layer\n",
    ")\n",
    "\n",
    "# Core evals require us to specify the dtype. This must be a string that can be converted to a torch dtype using general_utils.str_to_dtype.\n",
    "sae.cfg.dtype = dtype\n",
    "\n",
    "\n",
    "# The following contains our current defined SAE types and the shapes to plot for each. Add your custom SAE as new_sae_key\n",
    "new_sae_key = \"vanilla\"\n",
    "trainer_markers = {\n",
    "    \"standard\": \"o\",\n",
    "    \"jumprelu\": \"X\",\n",
    "    \"topk\": \"^\",\n",
    "    \"p_anneal\": \"*\",\n",
    "    \"gated\": \"d\",\n",
    "    new_sae_key: \"s\",  # New SAE\n",
    "}\n",
    "\n",
    "trainer_colors = {\n",
    "    \"standard\": \"blue\",\n",
    "    \"jumprelu\": \"orange\",\n",
    "    \"topk\": \"green\",\n",
    "    \"p_anneal\": \"red\",\n",
    "    \"gated\": \"purple\",\n",
    "    new_sae_key: \"black\",  # New SAE\n",
    "}\n",
    "\n",
    "sae.cfg.architecture = new_sae_key\n",
    "sae.cfg.training_tokens = 200_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`selected_saes` is a list of tuples of (unique_sae_id, sae object) OR (sae lens release, sae lens id). If it is a list of custom sae objects, then memory size will increase with the length of the list. This is especially important if the SAEs are large. If memory is a concern, I recommend calling the `run_eval()` function multiple times with lists of length 1, each list containing a new sae object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_id: pythia70m_sweep_standard_ctx128_0712_resid_post_layer_4_trainer_8_ae_pt\n"
     ]
    }
   ],
   "source": [
    "# Note: the custom_sae_id should be unique, as it is used for the intermediate results and final results file names\n",
    "\n",
    "unique_custom_sae_id = baseline_filename.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "print(f\"sae_id: {unique_custom_sae_id}\")\n",
    "\n",
    "# list of tuple of (sae_id, sae object)\n",
    "custom_saes = [(unique_custom_sae_id, sae)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your baseline SAEs here. Refer to `sae_regex_selection.ipynb` for more regex patterns. We are going to get a topk SAE from the same layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 34187.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_saes: [('sae_bench_pythia70m_sweep_topk_ctx128_0730', 'blocks.4.hook_resid_post__trainer_8')]\n",
      "baseline_sae_id: sae_bench_pythia70m_sweep_topk_ctx128_0730_blocks_4_hook_resid_post__trainer_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sae_regex_pattern = r\"(sae_bench_pythia70m_sweep_topk_ctx128_0730).*\"\n",
    "sae_block_pattern = r\".*blocks\\.([4])\\.hook_resid_post__trainer_(8)$\"\n",
    "\n",
    "baseline_saes = get_saes_from_regex(sae_regex_pattern, sae_block_pattern)\n",
    "print(f\"baseline_saes: {baseline_saes}\")\n",
    "baseline_sae_id = f\"{baseline_saes[0][0]}_{baseline_saes[0][1]}\".replace(\".\", \"_\")\n",
    "print(f\"baseline_sae_id: {baseline_sae_id}\")\n",
    "\n",
    "selected_saes = custom_saes + baseline_saes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run time for the next 2 functions is approximately 2 minutes on an RTX 3090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running SAE evaluation on all selected SAEs:   0%|          | 0/2 [00:00<?, ?it/s]/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m-deduped into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sae_lens/training/activations_store.py:285: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n",
      "Reconstruction Batches: 100%|██████████| 200/200 [00:14<00:00, 14.11it/s]\n",
      "Sparsity and Variance Batches: 100%|██████████| 200/200 [00:04<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation results to: eval_results/core/pythia70m_sweep_standard_ctx128_0712_resid_post_layer_4_trainer_8_ae_pt_custom_sae_eval_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running SAE evaluation on all selected SAEs:   0%|          | 0/2 [00:21<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdict clause runned.\n",
      "eval_results[-1] = defaultdict(<function nested_dict at 0x15151f96c9a0>, {'unique_id': 'pythia70m_sweep_standard_ctx128_0712_resid_post_layer_4_trainer_8_ae_pt_custom_sae', 'sae_set': 'pythia70m_sweep_standard_ctx128_0712_resid_post_layer_4_trainer_8_ae_pt', 'sae_id': 'custom_sae', 'eval_cfg': CoreEvalConfig(model_name='pythia-70m-deduped', llm_dtype='float32', batch_size_prompts=32, n_eval_reconstruction_batches=200, n_eval_sparsity_variance_batches=200, dataset='Skylion007/openwebtext', context_size=128, compute_kl=True, compute_ce_loss=True, compute_l2_norms=True, compute_sparsity_metrics=True, compute_variance_metrics=True, compute_featurewise_density_statistics=False, compute_featurewise_weight_based_metrics=False, exclude_special_tokens_from_reconstruction=True, verbose=False), 'metrics': {'model_behavior_preservation': {'kl_div_score': nan, 'kl_div_with_ablation': nan, 'kl_div_with_sae': nan}, 'model_performance_preservation': {'ce_loss_score': 0.92235079483651, 'ce_loss_with_ablation': 10.145527839660645, 'ce_loss_with_sae': 5.422423362731934, 'ce_loss_without_sae': 5.024803161621094}, 'reconstruction_quality': {'explained_variance': 0.7739413380622864, 'mse': 0.0077055529691278934, 'cossim': 0.9359137415885925}, 'shrinkage': {'l2_norm_in': 16.654922485351562, 'l2_norm_out': 14.748374938964844, 'l2_ratio': 0.877399206161499, 'relative_reconstruction_bias': 0.9631132483482361}, 'sparsity': {'l0': 83.5423583984375, 'l1': 28.97000503540039}, 'token_stats': {'total_tokens_eval_reconstruction': 819200, 'total_tokens_eval_sparsity_variance': 819200}}})\n",
      "eval_results[-1] = <class 'collections.defaultdict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type CoreEvalConfig is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Note: We typically run with n_eval_sparsity_variance_batches=2000, but I have reduced it here for a faster run\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m _ \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mmultiple_evals(\n\u001b[1;32m      4\u001b[0m     selected_saes\u001b[39m=\u001b[39;49mselected_saes,\n\u001b[1;32m      5\u001b[0m     n_eval_reconstruction_batches\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     n_eval_sparsity_variance_batches\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     eval_batch_size_prompts\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     compute_featurewise_density_statistics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     compute_featurewise_weight_based_metrics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     exclude_special_tokens_from_reconstruction\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSkylion007/openwebtext\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     context_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     output_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meval_results/core\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/tl784/AI-Scientist/evals/core/main.py:1011\u001b[0m, in \u001b[0;36mmultiple_evals\u001b[0;34m(selected_saes, n_eval_reconstruction_batches, n_eval_sparsity_variance_batches, eval_batch_size_prompts, compute_featurewise_density_statistics, compute_featurewise_weight_based_metrics, exclude_special_tokens_from_reconstruction, dataset, context_size, output_folder, verbose, dtype, force_rerun)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_results[-1] = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(eval_results[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(all_info_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m-> 1011\u001b[0m         json\u001b[39m.\u001b[39;49mdump(existing_data, indent\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, fp\u001b[39m=\u001b[39;49mf)   \n\u001b[1;32m   1012\u001b[0m \u001b[39mreturn\u001b[39;00m eval_results\n",
      "File \u001b[0;32m~/.conda/envs/ai_scientist/lib/python3.11/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(chunk)\n",
      "File \u001b[0;32m~/.conda/envs/ai_scientist/lib/python3.11/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ai_scientist/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ai_scientist/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ai_scientist/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type CoreEvalConfig is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Note: We typically run with n_eval_sparsity_variance_batches=2000, but I have reduced it here for a faster run\n",
    "\n",
    "_ = core.multiple_evals(\n",
    "    selected_saes=selected_saes,\n",
    "    n_eval_reconstruction_batches=200,\n",
    "    n_eval_sparsity_variance_batches=200,\n",
    "    eval_batch_size_prompts=32,\n",
    "    compute_featurewise_density_statistics=False,\n",
    "    compute_featurewise_weight_based_metrics=False,\n",
    "    exclude_special_tokens_from_reconstruction=True,\n",
    "    dataset=\"Skylion007/openwebtext\",\n",
    "    context_size=128,\n",
    "    output_folder=\"eval_results/core\",\n",
    "    verbose=True,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a subset of the sparse probing datasets here for shorter runtime\n",
    "dataset_names = [\"LabHC/bias_in_bios_class_set1\"]\n",
    "\n",
    "# TODO: Add a verbose flag\n",
    "\n",
    "_ = sparse_probing.run_eval(\n",
    "    sparse_probing.SparseProbingEvalConfig(\n",
    "        model_name=model_name,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        llm_batch_size=llm_batch_size,\n",
    "        llm_dtype=dtype,\n",
    "        dataset_names=dataset_names,\n",
    "    ),\n",
    "    selected_saes,\n",
    "    device,\n",
    "    \"eval_results/sparse_probing\",\n",
    "    force_rerun=False,\n",
    "    clean_up_activations=True,\n",
    "    save_activations=save_activations,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sae_bench_utils.graphing_utils as graphing_utils\n",
    "\n",
    "from sae_bench_utils.sae_selection_utils import select_saes_multiple_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = output_folders[\"sparse_probing\"]\n",
    "\n",
    "core_results_path = output_folders[\"core\"]\n",
    "image_path = \"./images\"\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sae_ids = []\n",
    "\n",
    "for sae_id, sae in custom_saes:\n",
    "    custom_sae_ids.append((sae_id, \"custom_sae\"))\n",
    "\n",
    "sae_lens_ids = []\n",
    "\n",
    "for sae_id, sae_release in baseline_saes:\n",
    "    sae_lens_ids.append((sae_id, sae_release))\n",
    "\n",
    "graphing_sae_ids = custom_sae_ids + sae_lens_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can view the raw results, and we see that both SAEs significantly outperform the residual stream baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results_dict = graphing_utils.get_results_dict(graphing_sae_ids, eval_path, core_results_path)\n",
    "\n",
    "print(raw_results_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sae_id = f\"{custom_sae_ids[0][0]}_{custom_sae_ids[0][1]}\".replace(\".\", \"_\")\n",
    "baseline_sae_id = f\"{sae_lens_ids[0][0]}_{sae_lens_ids[0][1]}\"\n",
    "\n",
    "\n",
    "baseline_filename = f\"{sae_lens_ids[0][0]}_{sae_lens_ids[0][1]}_eval_results.json\".replace(\"/\", \"_\")\n",
    "baseline_filepath = os.path.join(eval_path, baseline_filename)\n",
    "\n",
    "with open(baseline_filepath, \"r\") as f:\n",
    "    baseline_sae_eval_results = json.load(f)\n",
    "\n",
    "custom_filename = f\"{custom_sae_ids[0][0]}_{custom_sae_ids[0][1]}_eval_results.json\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "custom_filepath = os.path.join(eval_path, custom_filename)\n",
    "\n",
    "with open(custom_filepath, \"r\") as f:\n",
    "    custom_sae_eval_results = json.load(f)\n",
    "\n",
    "k = 1\n",
    "\n",
    "print(baseline_sae_eval_results.keys())\n",
    "\n",
    "print(\n",
    "    f\"Baseline SAE top {k} accuracy was:\",\n",
    "    baseline_sae_eval_results[\"eval_result_metrics\"][\"sae\"][f\"sae_top_{k}_test_accuracy\"],\n",
    ")\n",
    "print(\n",
    "    f\"Custom SAE top {k} accuracy was:\",\n",
    "    custom_sae_eval_results[\"eval_result_metrics\"][\"sae\"][f\"sae_top_{k}_test_accuracy\"],\n",
    ")\n",
    "print(\n",
    "    f\"LLM top {k} accuracy was:\",\n",
    "    baseline_sae_eval_results[\"eval_result_metrics\"][\"llm\"][f\"llm_top_{k}_test_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the metrics, plotting L0 vs Custom Metric or L0 vs Loss Recovered vs Custom metric. We can have different shapes for the SAE type or dictionary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base_name = os.path.join(image_path, \"sparse_probing\")\n",
    "\n",
    "graphing_utils.plot_results(\n",
    "    graphing_sae_ids,\n",
    "    eval_path,\n",
    "    core_results_path,\n",
    "    image_base_name,\n",
    "    k,\n",
    "    trainer_markers=trainer_markers,\n",
    "    trainer_colors=trainer_colors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run all of the evals, and create more plots. Be warned - this takes around an hour. Note that a significant amount of the costs are one time per layer - for example, with absorption, we have to train a bunch of probes on a given layer. So, if we have multiple SAEs per layer the cost should be much less than 30 minutes per SAE. In addition, to save disk space usage we currently are not saving activations for reuse by multiple SAEs.\n",
    "\n",
    "Additionally, we can make this faster by evaluating on a subset of the datasets. Sparse probing and Spurious Correlation Removal both have approximately 8 datasets each. We could lower the time by using fewer datasets at the cost of not having as strong of a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_all_evals_custom_saes.run_evals(\n",
    "    model_name,\n",
    "    selected_saes,\n",
    "    llm_batch_size,\n",
    "    dtype,\n",
    "    device,\n",
    "    eval_types,\n",
    "    api_key=None,\n",
    "    force_rerun=False,\n",
    "    save_activations=save_activations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_type in eval_types:\n",
    "    graphing_utils.plot_results(\n",
    "        graphing_sae_ids,\n",
    "        output_folders[eval_type],\n",
    "        core_results_path,\n",
    "        image_base_name,\n",
    "        k=10,\n",
    "        trainer_markers=trainer_markers,\n",
    "        trainer_colors=trainer_colors,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
