\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gaoScalingEvaluatingSparse}
\citation{bussmannBatchTopKSparseAutoencoders2024}
\citation{rajamanoharanJumpingAheadImproving2024}
\citation{chaninAbsorptionStudyingFeature2024}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{marksSparseFeatureCircuits2024}
\citation{farrellApplyingSparseAutoencoders2024}
\citation{gaoScalingEvaluatingSparse}
\citation{bussmannBatchTopKSparseAutoencoders2024}
\citation{rajamanoharanJumpingAheadImproving2024}
\citation{Liu2021ExactSO}
\citation{Ayonrinde2024InterpretabilityAC}
\citation{chaninAbsorptionStudyingFeature2024}
\citation{karvonenEvaluatingSparseAutoencoders2024}
\citation{pauloAutomaticallyInterpretingMillions2024}
\citation{ghilardiEfficientTrainingSparse2024a}
\citation{gurneeFindingNeuronsHaystack2023}
\citation{karvonenEvaluatingSparseAutoencoders2024}
\citation{farrellApplyingSparseAutoencoders2024}
\citation{goodfellow2016deep}
\citation{gaoScalingEvaluatingSparse}
\citation{bussmannBatchTopKSparseAutoencoders2024}
\citation{rajamanoharanJumpingAheadImproving2024}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{2}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Setting}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Sparsity-Guided Orthogonality}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Architecture and Training}{4}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of absorption scores across different model configurations. Run 4 (optimal dictionary size n=18432, $\alpha =0.075$) achieves the highest absorption score of 0.025, nearly 3x improvement over the baseline (0.009). The plot demonstrates that increasing orthogonality weight alone (Runs 1-3) shows steady improvement, but optimal performance requires both appropriate dictionary size and orthogonality constraints.}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:absorption}{{1}{4}{Comparison of absorption scores across different model configurations. Run 4 (optimal dictionary size n=18432, $\alpha =0.075$) achieves the highest absorption score of 0.025, nearly 3x improvement over the baseline (0.009). The plot demonstrates that increasing orthogonality weight alone (Runs 1-3) shows steady improvement, but optimal performance requires both appropriate dictionary size and orthogonality constraints}{figure.caption.1}{}}
\newlabel{fig:absorption@cref}{{[figure][1][]1}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training Configuration}{4}{subsection.5.1}\protected@file@percent }
\citation{gaoScalingEvaluatingSparse}
\citation{bussmannBatchTopKSparseAutoencoders2024}
\citation{rajamanoharanJumpingAheadImproving2024}
\citation{karvonenEvaluatingSparseAutoencoders2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Evaluation Protocol}{5}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{5}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Feature Separation and Reconstruction Quality}{5}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ablation Studies}{5}{subsection.6.2}\protected@file@percent }
\citation{de-arteagaBiasBiosCase2019}
\citation{hou2024bridging}
\citation{gurneeFindingNeuronsHaystack2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Interpretability Evaluation}{6}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Limitations}{6}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sparsity-Constrained Reconstruction (SCR) metrics at k=2 and k=20 thresholds across different model configurations. All orthogonal variants showed improved SCR scores over baseline, with Run 4's configuration achieving best metrics (0.172 at k=2). Higher orthogonality weights correlated with better SCR scores up to a point, while larger dictionary sizes didn't necessarily improve feature selectivity.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:scr_comparison}{{2}{6}{Sparsity-Constrained Reconstruction (SCR) metrics at k=2 and k=20 thresholds across different model configurations. All orthogonal variants showed improved SCR scores over baseline, with Run 4's configuration achieving best metrics (0.172 at k=2). Higher orthogonality weights correlated with better SCR scores up to a point, while larger dictionary sizes didn't necessarily improve feature selectivity}{figure.caption.2}{}}
\newlabel{fig:scr_comparison@cref}{{[figure][2][]2}{[1][6][]6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Reconstruction quality metrics (MSE and cosine similarity) across different model configurations. Despite stronger constraints, reconstruction quality remained remarkably stable, with MSE consistently around 1.41 and cosine similarity maintaining $\sim $0.93 even with highest orthogonality weight. This stability suggests orthogonality constraints don't compromise reconstruction ability.}}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:reconstruction_quality}{{3}{7}{Reconstruction quality metrics (MSE and cosine similarity) across different model configurations. Despite stronger constraints, reconstruction quality remained remarkably stable, with MSE consistently around 1.41 and cosine similarity maintaining $\sim $0.93 even with highest orthogonality weight. This stability suggests orthogonality constraints don't compromise reconstruction ability}{figure.caption.3}{}}
\newlabel{fig:reconstruction_quality@cref}{{[figure][3][]3}{[1][6][]7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Top-1 and top-20 accuracy for sparse probing tasks across model configurations. All orthogonal variants improved over baseline probing accuracy, with Run 4 achieving best balance of top-1 (0.961) and top-20 (0.959) accuracy. The small gap between metrics suggests high feature precision.}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:sparse_probing}{{4}{7}{Top-1 and top-20 accuracy for sparse probing tasks across model configurations. All orthogonal variants improved over baseline probing accuracy, with Run 4 achieving best balance of top-1 (0.961) and top-20 (0.959) accuracy. The small gap between metrics suggests high feature precision}{figure.caption.4}{}}
\newlabel{fig:sparse_probing@cref}{{[figure][4][]4}{[1][6][]7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{7}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{7}{Conclusions}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][6][]7}{}{}{}}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{Ayonrinde2024InterpretabilityAC}{{1}{2024}{{Ayonrinde et~al.}}{{Ayonrinde, Pearce, and Sharkey}}}
\bibcite{bussmannBatchTopKSparseAutoencoders2024}{{2}{2024}{{Bussmann et~al.}}{{Bussmann, Leask, and Nanda}}}
\bibcite{chaninAbsorptionStudyingFeature2024}{{3}{2024}{{Chanin et~al.}}{{Chanin, {Wilken-Smith}, Dulka, Bhatnagar, and Bloom}}}
\bibcite{de-arteagaBiasBiosCase2019}{{4}{2019}{{{De-Arteaga} et~al.}}{{{De-Arteaga}, Romanov, Wallach, Chayes, Borgs, Chouldechova, Geyik, Kenthapadi, and Kalai}}}
\bibcite{farrellApplyingSparseAutoencoders2024}{{5}{2024}{{Farrell et~al.}}{{Farrell, Lau, and Conmy}}}
\bibcite{gaoScalingEvaluatingSparse}{{6}{}{{Gao et~al.}}{{Gao, Goh, and Sutskever}}}
\bibcite{ghilardiEfficientTrainingSparse2024a}{{7}{2024}{{Ghilardi et~al.}}{{Ghilardi, Belotti, and Molinari}}}
\bibcite{goodfellow2016deep}{{8}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{gurneeFindingNeuronsHaystack2023}{{9}{2023}{{Gurnee et~al.}}{{Gurnee, Nanda, Pauly, Harvey, Troitskii, and Bertsimas}}}
\bibcite{hou2024bridging}{{10}{2024}{{Hou et~al.}}{{Hou, Li, He, Yan, Chen, and McAuley}}}
\bibcite{karvonenEvaluatingSparseAutoencoders2024}{{11}{2024}{{Karvonen et~al.}}{{Karvonen, Rager, Marks, and Nanda}}}
\bibcite{Liu2021ExactSO}{{12}{2021}{{Liu et~al.}}{{Liu, juan Zhao, and Wang}}}
\bibcite{marksSparseFeatureCircuits2024}{{13}{2024}{{Marks et~al.}}{{Marks, Rager, Michaud, Belinkov, Bau, and Mueller}}}
\bibcite{pauloAutomaticallyInterpretingMillions2024}{{14}{2024}{{Paulo et~al.}}{{Paulo, Mallen, Juang, and Belrose}}}
\bibcite{rajamanoharanJumpingAheadImproving2024}{{15}{2024}{{Rajamanoharan et~al.}}{{Rajamanoharan, Lieberum, Sonnerat, Conmy, Varma, Kram{\'a}r, and Nanda}}}
\ttl@finishall
\gdef \@abspage@last{8}
