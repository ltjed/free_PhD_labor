[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "normalized_group_sae",
        "Title": "Size-Normalized Group Sparsity for Feature Disentanglement",
        "Experiment": "Implement adaptive group SAE with \u03bb1 scaled by 1/sqrt(group_size). Compare group activation distributions and absorption metrics against baseline.",
        "Technical_Details": "Extends adaptive group SAE with normalized group penalty: L = ||x\u2212x\u0302||2 + \u03bb1\u2211_g(||f_g||2 / sqrt(|g|)) + \u03bb2||f||1. Normalization ensures equal contribution per group regardless of size. Matches biological neural population coding where activation energy scales with sqrt(neuron count). Prevents large groups from being disproportionately penalized while maintaining hierarchical separation.",
        "Implementation_Plan": "1. Compute group_size in TrainerTopK. 2. Modify group L2 term as (f_g.norm(dim=2) / sqrt(group_size)).sum(). 3. Keep other components from adaptive_group_sae. 4. Add group size vs activation energy analysis.",
        "Interestingness_Evaluation": "Integrates neural scaling principles into sparsity regularization through mathematically grounded normalization.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Adds single division operation in loss calculation. No new hyperparameters. Training time identical to previous group methods.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First application of group-size normalization in SAE sparsity constraints for biologically consistent regularization.",
        "Novelty": 9,
        "Expected_Research_Impact": "Improves absorption metric through fairer group competition while enhancing core metrics via balanced capacity allocation.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "We present a Normalized Group Sparsity SAE that adaptively scales regularization strength by group size. By dividing group L2 penalties by the square root of group size, we ensure equal contribution per group to the loss function regardless of cardinality. This approach aligns with neurobiological scaling principles while maintaining simple implementation through tensor operations. The normalization prevents larger groups from dominating the sparsity constraints, enabling more equitable competition between feature hierarchies during training.",
        "novel": true
    },
    {
        "Name": "frobenius_ortho_sae",
        "Title": "Frobenius Orthogonal Regularization for Sparse Autoencoder Disentanglement",
        "Experiment": "Add Frobenius norm regularization ||W_decTW_dec - I||2 to loss function. Modify TrainerTopK to include: 1) Orthogonal loss term scaled by lambda 2) Efficient matrix-based computation. Compare core metrics (L0, CE loss) and sparse_probing performance against baseline SAE.",
        "Technical_Details": "Regularization term R = \u03bb\u00b7||W_decTW_dec - I||_F2 where \u03bb controls strength. Computes deviation from orthogonality across all decoder vectors simultaneously. Forces decoder weights to approximate orthogonal basis while maintaining sparsity constraints. The identity matrix I ensures diagonal dominance while suppressing cross-feature correlations. Implementation uses trace identity: R = \u03bb(||W_decTW_dec||2 - 2||diag(W_decTW_dec)||2 + d_sae).",
        "Implementation_Plan": "1. Add lambda_ortho parameter to TrainerTopK 2. In loss(), compute R via torch.norm(W_dec.T @ W_dec - I) 3. Add term to total loss 4. Tune lambda_ortho \u2208 [1e-4, 1e-2]. Code changes limited to loss calculation (<10 LoC).",
        "Interestingness_Evaluation": "Applies matrix orthogonality constraints from numerical linear algebra to SAE interpretability in novel way.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Requires only 3 lines of code change using efficient matrix ops (O(d2) time). Existing PyTorch infrastructure handles batched norms. No new hyperparameters beyond \u03bb.",
        "Feasibility": 10,
        "Novelty_Evaluation": "First application of Frobenius orthogonality constraints to SAE decoder weights for combating superposition.",
        "Novelty": 8,
        "Expected_Research_Impact": "Directly improves core's feature absorption metric via enforced orthogonality and should boost sparse_probing through cleaner feature separation.",
        "Research_Impact": 9,
        "Overall_Score": 9.3,
        "Abstract": "We propose Frobenius-orthogonal sparse autoencoders (FROSAE) that enforce structural constraints on decoder weights to improve feature disentanglement. By penalizing the Frobenius norm deviation of W_decTW_dec from the identity matrix, FROSAE encourages decoder vectors to form a near-orthogonal basis while maintaining sparsity. This global orthogonality constraint reduces feature absorption by preventing redundant directional encodings in the latent space. The computationally efficient regularization term leverages matrix algebra primitives optimized for modern GPUs, requiring minimal code changes while providing strong geometric constraints against polysemanticity.",
        "novel": true
    },
    {
        "Name": "soft_dependency_sae",
        "Title": "Soft Dependency Sparse Autoencoders with Multiplicative Feature Gating",
        "Experiment": {
            "Modifications": [
                "Pair dependents 1:1 with bases in latent vector",
                "Implement soft gating: f_dep = f_base * ReLU(xW_dep + b_dep)",
                "Initialize W_dep_decoder = W_base + sparse_mask(\u0394W)",
                "Apply fixed 2x L1 penalty to dependent features"
            ],
            "Metrics": [
                "Measure base\u2192dependent activation covariance",
                "Track decoder residual sparsity (||\u0394W||0)",
                "Quantify absorption via first-letter classification",
                "Compare L0 sparsity vs reconstruction tradeoff"
            ]
        },
        "Technical_Details": "Latents alternate base/dependent pairs (f1_base, f1_dep, ...). Dependent features compute f_dep_i = f_base_i * ReLU(xW_dep_i + b_dep_i), tying activation magnitude to base feature. Decoder weights initialize W_dep_i = W_base_i + mask\u2299\u0394W_i where mask has 5% density. Loss: L = ||x - x\u0302||2 + \u03bb(||f_base||1 + 2||f_dep||1) + \u03b3||\u0394W||1. Soft gating creates smooth feature dependencies while fixed 2x \u03bb maintains hierarchy. Sparse mask initialization focuses composition learning on critical residual components.",
        "Implementation_Plan": [
            "1. Reorder latents as interleaved base/dependent pairs",
            "2. Replace hard gate with multiplicative activation in encode()",
            "3. Implement sparse decoder residual initialization",
            "4. Adjust TrainerTopK loss for fixed L1 ratios"
        ],
        "Interestingness_Evaluation": "Simplified soft gating and fixed sparsity maintain anti-absorption while improving training stability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Soft gating requires only element-wise multiplication. Sparse initialization uses existing masking utilities. Implementation completes in <100 LOC changes.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First use of multiplicative activation gating for SAE feature dependencies, enabling smooth hierarchy learning.",
        "Novelty": 8,
        "Expected_Research_Impact": "Improved training stability enhances core metrics. Multiplicative gating better preserves compositional relationships for sparse_probing.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose soft dependency sparse autoencoders (SDSAEs) that learn hierarchical features through multiplicative activation gating. Each dependent latent's activation scales with its paired base feature's output, creating smooth compositional relationships. Decoder weights initialize dependent features as sparse perturbations of base weights, focusing model capacity on learning residual components. Training employs fixed differential sparsity penalties to maintain feature hierarchy while avoiding complex scheduling. This approach enables stable learning of interpretable feature dependencies through continuous activation gating and sparse residual composition.",
        "novel": true
    },
    {
        "Name": "active_feature_ortho_sae",
        "Title": "Active Feature Orthogonality in Sparse Autoencoders via Top-K Guided Constraints",
        "Experiment": "Apply orthogonality constraints only to decoder weights corresponding to features activated in the current batch's top-k selection. Uses existing activation indices from SAE encoding step to dynamically select orthogonal groups.",
        "Technical_Details": "Leverages the SAE's native top-k feature indices to compute orthogonality penalties: 1) During encoding, get activated feature indices I 2) Extract decoder columns W_dec[:,I] 3) Compute L_ortho = \u03bb||(W_dec[:,I]^T W_dec[:,I]) - I||_F^2. Constrains only currently active features to be orthogonal, focusing regularization where superposition is actively occurring. Maintains Matryoshka hierarchy through column grouping.",
        "Implementation_Plan": "1. In TrainerTopK.loss(), extract top_indices_BK 2. Collect unique activated features per batch 3. Slice W_dec columns using these indices 4. Compute submatrix ortho loss 5. Add to total loss with scheduling factor \u03bb",
        "Interestingness_Evaluation": "Directly links orthogonality constraints to the features actually used for reconstruction each batch.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Requires only 5-10 new lines using existing top-k indices. No new data tracking - uses built-in SAE activation info.",
        "Feasibility": 10,
        "Novelty_Evaluation": "First method to use SAE's intrinsic activation patterns for dynamic geometric constraints.",
        "Novelty": 9,
        "Expected_Research_Impact": "Maximizes L2 Ratio preservation and minimizes Feature Absorption by surgically constraining active features.",
        "Research_Impact": 9,
        "Overall_Score": 9.3,
        "Abstract": "We present Active Feature Orthogonality Sparse Autoencoders (AFO-SAEs), which dynamically enforce orthogonality constraints on decoder weights corresponding to currently active features. By utilizing the sparse activation indices already identified during the SAE's top-k encoding step, AFO-SAEs apply targeted orthogonality penalties to precisely those features participating in the current batch's reconstruction. This approach focuses geometric constraints where they matter most\u2014on features actually being used\u2014while avoiding unnecessary computation on inactive dictionary elements. Our method integrates seamlessly with existing sparse autoencoder architectures by reusing activation information already generated during the forward pass, requiring minimal additional computational overhead.",
        "novel": true
    },
    {
        "Name": "cosine_curriculum_sae",
        "Title": "Cosine-Scheduled Sparse Autoencoders for Adaptive Feature Specialization",
        "Experiment": "Implement cosine-annealed sparsity scaling where \u03bbn(t) = \u03bb_base*(1 + n*(1-cos(\u03c0t/2T))). Compare feature hierarchy quality against linear curriculum SAE using absorption metrics and core evaluation suite.",
        "Technical_Details": "Modify temporal scaling using \u03bbn(t) = \u03bb_base[1 + n(1 - cos(\u03c0t/2T))] where T=total steps. This creates slow initial constraint growth accelerating mid-training before plateauing. Maintains Matryoshka nesting while providing smooth constraint transitions. Total loss remains L = \u03a3(L_recon^n + \u03bbn(t)||f^n||1) with no new parameters or architectural changes.",
        "Implementation_Plan": "1. Add cosine term to existing curriculum \u03bb calculation 2. Reuse nested training infrastructure 3. Log scaled \u03bb values per scale 4. Compare with linear baseline in evaluation",
        "Interestingness_Evaluation": "Introduces smooth constraint scheduling through simple trigonometric adaptation of existing parameters.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Requires only modifying the \u03bb(t) formula - same computational footprint as linear version. Implementation complexity identical.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First application of cosine scheduling to nested SAE sparsity constraints.",
        "Novelty": 9,
        "Expected_Research_Impact": "Smoother constraint transitions may enhance feature hierarchy stability, improving core metrics through better-aligned learning phases.",
        "Research_Impact": 9,
        "Overall_Score": 90,
        "Abstract": "We propose cosine-scheduled sparse autoencoders that non-linearly intensify sparsity constraints across nested scales during training. Using a cosine annealing schedule for scale-specific L1 penalties, our method enables gradual constraint progression from weak to strong specialization pressure. Built upon the Matryoshka framework through time-dependent scheduling, this approach maintains architectural simplicity while potentially stabilizing feature hierarchy formation. The non-linear constraint curve aims to better match neural network learning dynamics compared to linear schedules.",
        "novel": true
    },
    {
        "Name": "orthogonal_sae",
        "Title": "Orthogonal Sparse Autoencoders for Disentangled Feature Discovery",
        "Experiment": "Add an orthogonality regularization term to the SAE loss function that penalizes cosine similarity between decoder weight vectors. Compare against baseline SAEs on core metrics (particularly L2 ratio and feature density) and sparse probing performance. Implement by modifying the loss calculation in TrainerTopK and adding weight orthogonalization constraints.",
        "Technical_Details": "The OrthoSAE introduces a decoder orthogonality regularization term \u03bb||W_dec^T W_dec - I||_F^2 where W_dec \u2208 R^{d_sae\u00d7d_in} contains unit-normalized decoder weights. This Frobenius norm penalty on the deviation from identity in the Gram matrix encourages pairwise orthogonality between feature directions. The total loss becomes L = L_recon + \u03b1L_auxk + \u03b2L_ortho with \u03b2 controlling orthogonality strength. During training, gradients through this term push decoder weights toward orthogonal configurations while maintaining unit norm through existing projection steps.",
        "Implementation_Plan": "1. Add ortho_lambda parameter to TrainerTopK __init__\n2. Modify loss() method to compute Gram matrix of W_dec\n3. Add ortho_loss term using torch.norm(W_dec.T@W_dec - I)\n4. Adjust loss backward pass to include new term\n5. Add orthogonalization callback in set_decoder_norm_to_unit_norm\n6. Update config logging to track orthogonality metrics",
        "Interestingness_Evaluation": "Addresses core limitation of SAEs through novel yet simple geometric constraint with strong theoretical motivation.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Requires only 10-20 lines of code changes to existing loss computation. Ortho term adds O(d_in^2) computation per batch manageable for typical d_in \u22642048. Runtime increase <15% based on matrix size analysis.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First application of explicit orthogonality constraints to SAE decoders for mechanistic interpretability.",
        "Novelty": 7,
        "Expected_Research_Impact": "Orthogonal features should directly improve core metrics by reducing redundant activations and boost sparse probing through better feature separation.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose Orthogonal Sparse Autoencoders (OrthoSAEs), a novel variant of sparse autoencoders that explicitly enforces orthogonality constraints on decoder weights to improve feature disentanglement in language models. By incorporating a Frobenius norm penalty on the deviation from orthogonality in the decoder's weight matrix, our method encourages the learning of statistically independent feature directions while maintaining reconstruction fidelity. This approach builds on standard SAE architectures with minimal computational overhead, requiring only a simple modification to the training objective. We hypothesize that orthogonal feature bases will reduce polysemanticity and feature absorption phenomena, leading to more interpretable latent representations as measured by core metrics and sparse probing tasks.",
        "novel": true
    },
    {
        "Name": "final_temporal_hierarchy_sae",
        "Title": "Final Temporal Hierarchy SAE with Count-Based Group Prioritization",
        "Experiment": {
            "Implementation": [
                "Track group activation counts: count_g += I(any activation in G_g)",
                "Compute frequency as count_g / total_steps",
                "k_g = \u2308k_base*(0.5*||pre_g|| + 0.5*freq_g)\u2309",
                "Enforce orthogonality only between direct parent-child group pairs"
            ],
            "Evaluation": "Measure absorption score and parent-child weight correlations vs baseline."
        },
        "Technical_Details": "Predefined tree-structured groups (e.g. letter\u2192bigram). Per-group sparsity k_g combines normalized activation norm (||pre_g||/||pre||) and activation frequency (count_g/N_steps) equally. Orthogonality loss \u2211_{(p,c)\u2208edges}||W_dec[G_p]^T W_dec[G_c]||_F^2. Activation counts updated via count_g += \u2211_batch I(max_activation_in_G_g > 0). Fixed \u03b1=0.5 avoids learned parameters.",
        "Implementation_Plan": [
            "Step 1: Initialize group count buffers in TrainerTopK",
            "Step 2: Modify k_g to use fixed blend of norm and frequency",
            "Step 3: Update counts per batch",
            "Step 4: Compute edge-wise orthogonality loss"
        ],
        "Interestingness_Evaluation": "Count-based temporal adaptation provides novel simplicity-benefit tradeoff.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Counts require 1 integer/group. No EMAs or learned params. Edge ortho loss O(m) with m\u224825. Trivial to implement.",
        "Feasibility": 10,
        "Novelty_Evaluation": "First SAE combining count-based temporal stats with tree orthogonality.",
        "Novelty": 9,
        "Expected_Research_Impact": "Efficient hierarchy constraints directly target absorption while maintaining core metrics via stable k_g.",
        "Research_Impact": 10,
        "Overall_Score": 9.7,
        "Abstract": "We present Final Temporal Hierarchy Sparse Autoencoders (FTH-SAE), utilizing count-based activation statistics and minimal hierarchical constraints. Feature groups in a predefined tree structure allocate sparsity budgets through equal blending of current activation strength and long-term activation frequency. Direct parent-child group pairs in the hierarchy are regularized for decoder weight orthogonality, focusing computational effort on critical absorption-prone relationships. This approach maximizes implementation simplicity while preserving interpretable temporal adaptation and hierarchical feature isolation.",
        "novel": true
    },
    {
        "Name": "adaptive_threshold_sae",
        "Title": "Adaptive Threshold Nested Sparse Autoencoders with Dynamic Hierarchy Constraints",
        "Experiment": "Implement self-adjusting thresholds based on feature activation rates:\n1. Track exponential moving average of parent feature activations\n2. Set \u03c4_k = percentile(EMA_activations, 75)\nCompare to fixed-threshold SAE on absorption and core metrics",
        "Technical_Details": "Enhancements:\n1. \u03c4_{parent} = Q3(EMA(parent_activations)) updated every 100 steps\n2. Child gate: f_child = ReLU(Wx + b) * (f_parent > \u03c4_parent)\n3. EMA decay rate \u03b2=0.99 maintains stability\nPreserves cumulative decoding and Matryoshka training while automating threshold selection.",
        "Implementation_Plan": "1. Add EMA buffers in TrainerTopK\n2. Modify encode() to use dynamic thresholds via torch.quantile\n3. Update thresholds intermittently during training\n4. Keep all original loss terms and architecture",
        "Interestingness_Evaluation": "Introduces biological homeostasis mechanisms into hierarchy constraints.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Adds <10 lines of threshold adaptation code using standard PyTorch ops. Training overhead negligible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First auto-tuning hierarchical constraints in SAEs through activation-driven thresholds.",
        "Novelty": 9,
        "Expected_Research_Impact": "Dynamic thresholds adapt to feature distributions, potentially improving absorption metrics and generalization.",
        "Research_Impact": 9,
        "Overall_Score": 9.3,
        "Abstract": "We present Adaptive Threshold Sparse Autoencoders (AT-SAEs) that automatically tune hierarchical activation constraints through dynamic threshold adaptation. Building on nested architectures, our method replaces fixed activation thresholds with quantile-based values derived from exponential moving averages of feature activations. This self-adjusting mechanism ensures parent features maintain appropriate activation baselines for enabling child features, adapting to changing feature distributions during training. The system maintains the cumulative residual decoding framework of predecessor models while introducing automated hierarchy calibration. By integrating simple yet effective dynamic thresholding, AT-SAEs require no manual threshold tuning while preserving the stability and interpretability benefits of hard activation constraints.",
        "novel": true
    },
    {
        "Name": "smooth_gated_probe_sae",
        "Title": "Smoothly Gated Probe Alignment via Activation-Aware Sigmoid Masking",
        "Experiment": "1. Compute sigmoid-scaled activation gates 2. Apply nonlinear probe alignment penalty 3. Compare gradient stability and absorption metrics vs linear gating",
        "Technical_Details": "Modify probe similarity loss to \u03bb\u2211_i \u03c3(f_i/\u03c4) * (1 - max_j(cos(W_dec[i], v_j))) where \u03c3 is sigmoid and \u03c4=0.1. Smoothly interpolates penalty between inactive (f_i\u22480 \u2192 \u03c3\u22480.5) and active (f_i>0.5 \u2192 \u03c3\u22481) states. Avoids discontinuity while maintaining activation-aware focus. \u03c4 fixed to avoid new hyperparameters.",
        "Implementation_Plan": "1. In TrainerTopK.loss(), replace linear activation multiplier with sigmoid(f_i/0.1) 2. Reuse all other probe-guidance components 3. Add ablation study on \u03c4 values",
        "Interestingness_Evaluation": "Introduces continuous gating for concept alignment penalties, improving optimization stability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Adds only sigmoid operation (native PyTorch) to existing loss term. No new parameters or data flows. Runtime identical to baseline.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First use of smooth activation gating for probe-aligned SAEs, bridging binary and linear gating approaches.",
        "Novelty": 9,
        "Expected_Research_Impact": "Enhances absorption prevention through stable, activation-sensitive regularization while preserving reconstruction.",
        "Research_Impact": 9,
        "Overall_Score": 9.3,
        "Abstract": "We present smoothly gated probe alignment for sparse autoencoders, employing sigmoid-scaled activation masking to apply semantic regularization. By smoothly varying concept alignment pressure based on normalized activation strength, our method maintains stable gradients while focusing interpretability constraints on meaningfully active features. This approach preserves the architectural simplicity of standard SAEs while introducing continuous guidance mechanisms informed by both empirical probing results and activation dynamics.",
        "novel": true
    },
    {
        "Name": "cosine_ortho_sae",
        "Title": "Cosine-Annealed Orthogonal Sparse Autoencoders for Smooth Constraint Transition",
        "Experiment": "Replace linear warmup with cosine schedule for orthogonality strength. Compare convergence stability and final metrics against linear curriculum OrthoSAE variants.",
        "Technical_Details": "Modify \u03b3(t) = \u03b3_max \u00d7 0.5(1 \u2212 cos(\u03c0\u00b7min(t/t_warmup,1))) maintaining original Gram matrix penalty. Preserves total constraint magnitude while enabling non-linear progression. Full loss remains L = L_recon + \u03bbL_sparsity + \u03b3(t)L_ortho with smoothed constraint introduction.",
        "Implementation_Plan": "1. Modify \u03b3 calculation in loss() to use cosine annealing 2. Add t_warmup parameter 3. Update logging to track annealing phase",
        "Interestingness_Evaluation": "Applies proven annealing schedules to geometric constraints for smoother feature space evolution.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Equivalent code changes to previous version (2-3 LOC). Cosine computation uses built-in PyTorch functions without performance impact.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First integration of cosine annealing with orthogonality constraints in SAEs, enhancing training dynamics.",
        "Novelty": 8,
        "Expected_Research_Impact": "Smoother constraint transition may improve reconstruction-orthogonality tradeoff, boosting core metrics through stable training.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We introduce Cosine-Annealed Orthogonal Sparse Autoencoders, employing non-linear constraint scheduling to optimize feature disentanglement. Through cosine-annealed regularization strength, the method enables gradual introduction of orthogonality pressures while maintaining stable reconstruction learning. This approach builds upon curriculum learning principles by smoothing the transition between free feature formation and constrained axis-alignment phases. The technique requires only minimal modifications to existing orthogonal SAE frameworks while leveraging well-established annealing schedules from deep learning optimization. Experimental evaluation focuses on measuring improvements in training stability and final interpretability benchmarks compared to linear scheduling approaches.",
        "novel": true
    },
    {
        "Name": "coactivation_ortho_sae",
        "Title": "Co-Activation Guided Orthogonal Regularization for Sparse Autoencoders",
        "Experiment": "Implement co-activation weighted orthogonality loss. Compare with previous adaptive methods on core metrics. Track pairwise co-activation rates vs orthogonality penalties. Sweep \u03bb \u2208 [0.05, 0.1, 0.2] with co-activation scaling.",
        "Technical_Details": "L_orthog = \u03bb\u03a3_ij C_ij(W_dec_i\u00b7W_dec_j - \u03b4_ij)^2 where C_ij = EMA(co-activation rate of features i,j). Features co-activating in batches receive stronger orthogonal constraints. C_ij updated per batch as C_new = \u03b2C_old + (1-\u03b2)(mean(f_i\u2299f_j)) with \u03b2=0.95. Directly penalizes overlapping feature directions that frequently fire together.",
        "Implementation_Plan": "1. Add co-activation matrix tracking in TrainerTopK\n2. Modify ortho loss to use C_ij weights\n3. Implement EMA update for C matrix\n4. Adjust regularization term calculation\n5. Log top co-activated feature pairs",
        "Interestingness_Evaluation": "Targets feature pairs actually exhibiting superposition through empirical co-firing patterns.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Adds O(d^2) matrix tracking but d=512 remains manageable (268KB in float32). Co-activation computation uses existing activations. EMA update negligible (<1% overhead).",
        "Feasibility": 8,
        "Novelty_Evaluation": "First use of pairwise co-activation statistics to weight SAE regularization, novel in mechanistic interpretability.",
        "Novelty": 9,
        "Expected_Research_Impact": "Superior absorption reduction via correlation-guided constraints, maximizing core/sparse_probing gains.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose co-activation guided orthogonal regularization for sparse autoencoders, where decoder weight constraints are weighted by empirical feature co-activation rates. This method identifies frequently co-occurring features through an exponentially moving average of their pairwise activation correlations and applies stronger orthogonality penalties to their decoder directions. By directly targeting feature pairs that exhibit superposition in practice, the approach focuses regularization effort where absorption risk is highest. The technique integrates naturally into existing SAE training pipelines while providing data-driven constraints on feature space organization. Evaluations measure improvements in feature disentanglement metrics and probe task performance across standard interpretability benchmarks.",
        "novel": true
    },
    {
        "Name": "self_targeted_ortho_sae",
        "Title": "Reconstruction-Guided Automatic Orthogonality Targeting for Nested SAEs",
        "Experiment": "Modify Auto Ortho SAE to: 1) Directly use per-scale reconstruction ratios to compute ortho weights 2) Replace cosine with normalized dot product 3) Remove explicit scheduling in favor of error-driven targeting.",
        "Technical_Details": "Orthogonality penalty \u03bb_ortho_ij(t) = \u03bb0 * (R_coarse/R_fine) where R is 1 - (||x-x\u0302_l||2/||x||2). Apply between all scale pairs where j>i. Use (W_dec_i \u2022 W_dec_j)/(||W_dec_i||\u00b7||W_dec_j||) as penalty term. Loss: L = \u03a3\u03b2_l||x-x\u0302_l||2 + \u03a3\u03bb_l||f_l||1 + \u03a3_{i<j}\u03bb_ortho_ij(t)\u00b7|W_i\u2022W_j|.",
        "Implementation_Plan": "1. Track per-scale R2 metrics 2. Compute \u03bb_ortho_ij dynamically via R2 ratios 3. Compute normalized decoder dot products 4. Remove manual scheduling code 5. Add pairwise R2 ratio matrix tracking",
        "Interestingness_Evaluation": "Fully automates constraint targeting using intrinsic reconstruction signals.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Uses existing reconstruction data - no new computations. Normalized dot products are O(1) vs cosine. Pairwise ratios add minimal overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First to exploit reconstruction error ratios for automatic diversity targeting.",
        "Novelty": 9,
        "Expected_Research_Impact": "Maximizes absorption reduction by focusing constraints where hierarchical redundancy is empirically observed.",
        "Research_Impact": 9,
        "Overall_Score": 9.2,
        "Abstract": "We propose a self-targeting orthogonal sparse autoencoder that automatically identifies and penalizes redundant feature directions between scales based on their relative reconstruction performance. By dynamically weighting orthogonality constraints according to the ratio of scale-specific reconstruction errors, our method focuses diversity enforcement where hierarchical feature absorption is empirically detected. This approach maintains the efficiency of nested autoencoders while introducing an automatic mechanism that aligns orthogonality constraints with observed reconstruction deficiencies, eliminating manual constraint scheduling through direct use of intrinsic training signals.",
        "novel": true
    },
    {
        "Name": "energy_adaptive_sparsity_sae",
        "Title": "Gradient-Stabilized Energy-Adaptive Sparse Autoencoders",
        "Experiment": "Implement temperature-scaled energy normalization during cumulative sum calculation, comparing training convergence against baseline. Validate gradient flow through thresholding operation via activation histograms.",
        "Technical_Details": "Final enhancement: Apply temperature scaling to sorted energies before cumulative summation. Modified steps: 1) s_sorted = (h^2).sort(descending=True) 2) s_scaled = s_sorted / temp (temp=1.0) 3) cum_energy = (s_scaled.cumsum(0)+\u03b5)/(s_scaled.sum()+\u03b5). Maintains numerical stability while smoothing gradient landscapes around threshold boundaries.",
        "Implementation_Plan": "1. Add temp parameter (default=1.0) to encode(). 2. Scale sorted energies by 1/temp before cumsum. 3. Retain \u03b5 stabilization. 4. Verify gradient propagation through scaling factors. 5. Keep temp fixed during training to avoid hyperparameter search.",
        "Interestingness_Evaluation": "Enhances trainability through gradient-aware energy scaling while preserving interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Adds 2-3 lines of PyTorch code for energy scaling. Training dynamics remain unchanged.",
        "Feasibility": 10,
        "Novelty_Evaluation": "First integration of temperature-scaled energy normalization in SAE threshold selection.",
        "Novelty": 8,
        "Expected_Research_Impact": "Improves Core metric optimization through better gradient flow during training.",
        "Research_Impact": 9,
        "Overall_Score": 9.4,
        "Abstract": "We present a gradient-stabilized energy-adaptive sparse autoencoder (EA-SAE) that incorporates temperature-scaled energy normalization for improved training dynamics. By scaling sorted activation energies prior to cumulative threshold calculation, our method enhances gradient flow through the feature selection process while maintaining the core energy preservation objective. The approach combines numerical stabilization techniques with temperature-based energy scaling to balance faithful reconstruction with stable optimization. This final iteration preserves the computational efficiency and input-aware sparsity adaptation of earlier versions while introducing robustness to gradient-based learning.",
        "novel": true
    },
    {
        "Name": "self_tuning_hierarchy_sae",
        "Title": "Self-Tuning Hierarchical Sparse Autoencoders with Orthogonal Feature Constraints",
        "Experiment": "Enhance adaptive SAE by 1) Setting \u03c4 = 0.1*T_p automatically 2) Adding cross-feature covariance regularization between parents and children. Compare orthogonality metrics and absorption rates against previous variants.",
        "Technical_Details": "Full automation through \u03c4\u221dT_p relationship. Orthogonality penalty \u03bb_ortho\u2211|W_parent\u22c5W_child^T| added to loss. Parent thresholds T_p remain EMA(percentile90(f_p)). Combines adaptive gating with soft feature independence constraints while maintaining Matryoshka reconstruction objectives.",
        "Implementation_Plan": "1. Compute \u03c4 dynamically as 0.1*T_p 2. Add covariance regularization in loss 3. Reuse EMA infrastructure 4. Initialize W_child via Gram-Schmidt orthogonalization 5. Track parent-child cosine similarities",
        "Interestingness_Evaluation": "Achieves complete hierarchy adaptation through coupled threshold-temperature dynamics.",
        "Interestingness": 10,
        "Feasibility_Evaluation": "Adds <100 lines using existing covariance calc utilities. Ortho penalty proven technique. 5-7 day implementation.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First fully self-adaptive hierarchical SAE with integrated orthogonality constraints.",
        "Novelty": 10,
        "Expected_Research_Impact": "Expect major gains in sparse_probing through cleaner hierarchies and core via better capacity allocation.",
        "Research_Impact": 10,
        "Overall_Score": 9.6,
        "Abstract": "We present Self-Tuning Hierarchical Sparse Autoencoders (STH-SAE), combining dynamic activation gating with orthogonal feature constraints. Parent activation thresholds automatically adapt to feature magnitude distributions while gating temperature scales proportionally. Child features initialize via orthogonal projection relative to parents and maintain low covariance through regularization. This architecture eliminates manual hyperparameter tuning while enforcing both hierarchical activation patterns and feature independence across scales, potentially offering unprecedented clarity in learned feature hierarchies.",
        "novel": true
    },
    {
        "Name": "symmetric_tiered_sae",
        "Title": "Symmetric Tiered Sparse Autoencoders Through Inverse Penalty-Activation Scaling",
        "Experiment": "Set group activation thresholds \u03c4_g = 1/(2^{g-1}) exactly inverse to \u03bb_g scaling. Compare core metrics with previous versions. Track threshold compliance and feature specificity.",
        "Technical_Details": "Groups have \u03bb_g = \u03bb_base*2^{g-1} and \u03c4_g = 1/(2^{g-1}). When group's activation density p_g > \u03c4_g, apply \u03bb_g += \u03bb_base. Reset p_g every 100 steps. Loss: L = ||x-x\u0302||2 + \u03a3_g \u03bb_g||f_g||1. Symmetric design ensures high-index groups (rare features) must activate \u22641/2^{g-1} of tokens without penalty boosts.",
        "Implementation_Plan": "1. Compute \u03c4_g = 1/(2^{g-1}) during group init 2. Modify threshold checks to use \u03c4_g 3. Keep penalty boost logic 4. Add visualization of \u03c4_g vs p_g",
        "Interestingness_Evaluation": "Perfect symmetry between sparsity pressure and activation limits creates elegant theoretical framework.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Identical implementation complexity to threshold SAE - just changed threshold formula.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First SAE variant with symmetric penalty-activation scaling.",
        "Novelty": 9,
        "Expected_Research_Impact": "Maximizes hierarchical separation potential through theoretical symmetry, directly targeting absorption and core metrics.",
        "Research_Impact": 9,
        "Overall_Score": 9.3,
        "Abstract": "We present Symmetric Tiered Sparse Autoencoders (ST-SAE) featuring exact inverse scaling between sparsity penalties and activation density limits. Each group's base regularization strength \u03bb_g = \u03bb_base*2^{g-1} pairs with an activation threshold \u03c4_g = 1/(2^{g-1}), creating a theoretical symmetry where high-index groups face exponentially increasing sparsity pressure but exponentially decreasing activation allowances. This design enforces strict hierarchical feature specialization through geometric constraints, maintaining implementation simplicity while offering a principled approach to concept disentanglement. The symmetric scaling mechanism provides intuitive control over feature hierarchies without introducing new architectural complexity.",
        "novel": true
    },
    {
        "Name": "unified_ortho_sae",
        "Title": "Unified Orthogonality Sparse Autoencoders with Adaptive Temporal Regularization",
        "Experiment": "Final refinement: 1) Unified orthogonality targets combining current/persistent features via weighted EMA 2) Auto-adjusted \u03b3 decay based on feature survival rate 3) Single ortho loss computation. Validate via core and absorption metrics.",
        "Technical_Details": "Core components: 1) Unified feature set: top-m = argtopk(0.7*f_ema + 0.3*f_batch) 2) Adaptive \u03b3(t) = \u03b3_max*(1 - survived_features/total_features) 3) Ortho loss on unified set via \u03bb_ortho*mean(relu(cos_sim)). Survival rate tracked per-feature as EMA(1_{f>0}). Combines temporal adaptation with automatic complexity control.",
        "Implementation_Plan": "1. Replace dual tracking with unified EMA: f_combined = \u03b2*f_ema + (1-\u03b2)*f_batch\n2. Modify \u03b3 scheduler using survival ratio (10 LoC)\n3. Single ortho loss on combined top-64 features\n4. Config: \u03b2=0.9, \u03b3_max=0.3, \u03bb_ortho=0.002\nTotal changes: 40 LoC using existing EMA infrastructure",
        "Interestingness_Evaluation": "Unifies temporal dynamics into single efficient mechanism with auto-adjusted regularization.",
        "Interestingness": 10,
        "Feasibility_Evaluation": "Simplifies previous hybrid approach - single feature set. Survival rate leverages existing activation data. Ortho computation reduced from O(2m) to O(1).",
        "Feasibility": 10,
        "Novelty_Evaluation": "First SAE using feature survival rate for adaptive regularization decay.",
        "Novelty": 9,
        "Expected_Research_Impact": "Maximizes core metric gains through survival-based adaptation while minimizing absorption via unified orthogonality.",
        "Research_Impact": 10,
        "Overall_Score": 9.8,
        "Abstract": "We present a unified approach for temporal adaptation in sparse autoencoders, integrating feature survival rates into regularization decay schedules. The model dynamically adjusts sparsity pressure based on long-term feature utility while enforcing orthogonality constraints on a merged set of features selected through weighted combination of current activations and historical prominence. This unified temporal strategy eliminates separate tracking mechanisms, using a single exponential moving average to balance immediate and persistent feature importance. Adaptive regularization decay automatically scales with feature survival rates, promoting stable learning dynamics without manual schedule tuning.",
        "novel": true
    },
    {
        "Name": "integrated_semantic_hierarchy_sae",
        "Title": "Integrated Semantic Hierarchy Learning through Coincident Decoder Normalization",
        "Experiment": "Perform parent updates synchronously with SAE's existing decoder weight normalization steps. Compare hierarchy quality and training efficiency vs async update baseline. Measure absorption metrics and computational overhead.",
        "Technical_Details": "Leverage existing W_dec normalization calls: 1) After each W_dec normalization (via set_decoder_norm_to_unit_norm), compute S_ij = W_dec @ W_dec.T. 2) Update parent relationships using current S_ij. 3) Apply masked penalty \u2211_j [has_parent(j) * (S_parent[j]/\u03c4) * ReLU(f_j - f_parent(j))]. Eliminates separate parent update interval parameter.",
        "Implementation_Plan": "1. Modify set_decoder_norm_to_unit_norm() to trigger parent updates post-normalization. 2. Remove explicit update interval counter. 3. Add boolean mask for features with parents. Requires ~10 line change by hooking into existing normalization method.",
        "Interestingness_Evaluation": "Elegant integration of hierarchy learning into SAE's inherent weight maintenance routines.",
        "Interestingness": 10,
        "Feasibility_Evaluation": "Implementation now requires only modifying existing normalization method - zero new scheduling logic. Computational overhead identical to baseline SAE normalization plus similarity matrix (O(d2) but rare).",
        "Feasibility": 10,
        "Novelty_Evaluation": "First to exploit SAE's intrinsic normalization phase for hierarchy learning, creating 'free' feature analysis.",
        "Novelty": 10,
        "Expected_Research_Impact": "Tighter integration improves training stability and absorption metrics through aligned parameter updates.",
        "Research_Impact": 10,
        "Overall_Score": 9.6,
        "Abstract": "We present an integrated approach for semantic hierarchy learning in sparse autoencoders that synchronizes feature relationship analysis with the model's inherent decoder normalization steps. By performing parent-child similarity computations immediately after each decoder weight normalization, our method eliminates the need for separate hierarchy update scheduling while leveraging the SAE's existing geometric maintenance of feature directions. This creates a computationally 'free' hierarchy enforcement mechanism that naturally aligns with the autoencoder's training dynamics through minor modifications to the weight normalization procedure. The approach maintains all benefits of decoder-based semantic constraints while achieving maximal implementation efficiency.",
        "novel": true
    }
]