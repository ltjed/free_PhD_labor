{
  "training results for layer 12": {
    "config": {
      "trainer_class": "TrainerTopK",
      "dict_class": "AutoEncoderTopK",
      "lr": 0.0002,
      "steps": 488,
      "seed": 42,
      "activation_dim": 2304,
      "dict_size": 16384,
      "k": 40,
      "device": "cuda",
      "layer": 12,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "AutoEncoderTopK",
      "submodule_name": "resid_post_layer_12",
      "ortho_max": 0.1,
      "base_threshold": 0.8,
      "threshold_decay": 0.9
    },
    "final_info": {
      "training_steps": 488,
      "final_loss": 5630.48779296875,
      "layer": 12,
      "dict_size": 16384,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_12_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_12_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.9095496894409938,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 0.91015625
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.9078947368421053,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 3.8125,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": 0.515625,
        "mse": 3.078125,
        "cossim": 0.84375
      },
      "shrinkage": {
        "l2_norm_in": 149.0,
        "l2_norm_out": 126.5,
        "l2_ratio": 0.84375,
        "relative_reconstruction_bias": 0.99609375
      },
      "sparsity": {
        "l0": 40.0,
        "l1": 352.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  }
}