{
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_12_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_12_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.9941770186335404,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 0.05859375
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.9950657894736842,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 2.984375,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": 0.921875,
        "mse": 0.490234375,
        "cossim": 0.9765625
      },
      "shrinkage": {
        "l2_norm_in": 149.0,
        "l2_norm_out": 148.0,
        "l2_ratio": 0.9921875,
        "relative_reconstruction_bias": 1.0078125
      },
      "sparsity": {
        "l0": 4459.3154296875,
        "l1": 3520.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "f33dce55-95ed-401a-a75c-ecbc47757192",
    "datetime_epoch_millis": 1738808561639,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.95075625,
        "llm_top_1_test_accuracy": 0.65444375,
        "llm_top_2_test_accuracy": 0.71966875,
        "llm_top_5_test_accuracy": 0.7860875,
        "llm_top_10_test_accuracy": 0.83056875,
        "llm_top_20_test_accuracy": 0.8764437500000001,
        "llm_top_50_test_accuracy": 0.9228625,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9601000387221574,
        "sae_top_1_test_accuracy": 0.6985500000000001,
        "sae_top_2_test_accuracy": 0.7702499999999999,
        "sae_top_5_test_accuracy": 0.82179375,
        "sae_top_10_test_accuracy": 0.850475,
        "sae_top_20_test_accuracy": 0.87933125,
        "sae_top_50_test_accuracy": 0.9179687499999999,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.96,
        "llm_top_1_test_accuracy": 0.6436,
        "llm_top_2_test_accuracy": 0.6911999999999999,
        "llm_top_5_test_accuracy": 0.792,
        "llm_top_10_test_accuracy": 0.8333999999999999,
        "llm_top_20_test_accuracy": 0.897,
        "llm_top_50_test_accuracy": 0.9378,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9678000450134278,
        "sae_top_1_test_accuracy": 0.8,
        "sae_top_2_test_accuracy": 0.834,
        "sae_top_5_test_accuracy": 0.8704000000000001,
        "sae_top_10_test_accuracy": 0.8882,
        "sae_top_20_test_accuracy": 0.9077999999999999,
        "sae_top_50_test_accuracy": 0.9338000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9492,
        "llm_top_1_test_accuracy": 0.6794,
        "llm_top_2_test_accuracy": 0.717,
        "llm_top_5_test_accuracy": 0.767,
        "llm_top_10_test_accuracy": 0.8,
        "llm_top_20_test_accuracy": 0.8614,
        "llm_top_50_test_accuracy": 0.9086000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9532000422477722,
        "sae_top_1_test_accuracy": 0.7616,
        "sae_top_2_test_accuracy": 0.7828,
        "sae_top_5_test_accuracy": 0.8211999999999999,
        "sae_top_10_test_accuracy": 0.8478,
        "sae_top_20_test_accuracy": 0.8678000000000001,
        "sae_top_50_test_accuracy": 0.8998000000000002,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.9181999999999999,
        "llm_top_1_test_accuracy": 0.6862,
        "llm_top_2_test_accuracy": 0.732,
        "llm_top_5_test_accuracy": 0.7722,
        "llm_top_10_test_accuracy": 0.8008,
        "llm_top_20_test_accuracy": 0.8532,
        "llm_top_50_test_accuracy": 0.8922000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9324000477790833,
        "sae_top_1_test_accuracy": 0.7362,
        "sae_top_2_test_accuracy": 0.772,
        "sae_top_5_test_accuracy": 0.8204,
        "sae_top_10_test_accuracy": 0.8386000000000001,
        "sae_top_20_test_accuracy": 0.8572,
        "sae_top_50_test_accuracy": 0.8914,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.8948,
        "llm_top_1_test_accuracy": 0.6039999999999999,
        "llm_top_2_test_accuracy": 0.6508,
        "llm_top_5_test_accuracy": 0.6878,
        "llm_top_10_test_accuracy": 0.745,
        "llm_top_20_test_accuracy": 0.8051999999999999,
        "llm_top_50_test_accuracy": 0.8628,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9216000437736511,
        "sae_top_1_test_accuracy": 0.636,
        "sae_top_2_test_accuracy": 0.7087999999999999,
        "sae_top_5_test_accuracy": 0.7744,
        "sae_top_10_test_accuracy": 0.7916,
        "sae_top_20_test_accuracy": 0.8188000000000001,
        "sae_top_50_test_accuracy": 0.849,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.982,
        "llm_top_1_test_accuracy": 0.673,
        "llm_top_2_test_accuracy": 0.724,
        "llm_top_5_test_accuracy": 0.766,
        "llm_top_10_test_accuracy": 0.826,
        "llm_top_20_test_accuracy": 0.848,
        "llm_top_50_test_accuracy": 0.932,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9805000424385071,
        "sae_top_1_test_accuracy": 0.733,
        "sae_top_2_test_accuracy": 0.832,
        "sae_top_5_test_accuracy": 0.872,
        "sae_top_10_test_accuracy": 0.89,
        "sae_top_20_test_accuracy": 0.91,
        "sae_top_50_test_accuracy": 0.962,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9642,
        "llm_top_1_test_accuracy": 0.6634,
        "llm_top_2_test_accuracy": 0.7002,
        "llm_top_5_test_accuracy": 0.7648,
        "llm_top_10_test_accuracy": 0.8024000000000001,
        "llm_top_20_test_accuracy": 0.8608,
        "llm_top_50_test_accuracy": 0.9258,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9702000379562378,
        "sae_top_1_test_accuracy": 0.6218,
        "sae_top_2_test_accuracy": 0.6679999999999999,
        "sae_top_5_test_accuracy": 0.7108,
        "sae_top_10_test_accuracy": 0.7694,
        "sae_top_20_test_accuracy": 0.8402,
        "sae_top_50_test_accuracy": 0.9029999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.93825,
        "llm_top_1_test_accuracy": 0.63375,
        "llm_top_2_test_accuracy": 0.7607499999999999,
        "llm_top_5_test_accuracy": 0.8265,
        "llm_top_10_test_accuracy": 0.8727499999999999,
        "llm_top_20_test_accuracy": 0.8977499999999999,
        "llm_top_50_test_accuracy": 0.9265,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9555000215768814,
        "sae_top_1_test_accuracy": 0.713,
        "sae_top_2_test_accuracy": 0.802,
        "sae_top_5_test_accuracy": 0.84575,
        "sae_top_10_test_accuracy": 0.877,
        "sae_top_20_test_accuracy": 0.8932500000000001,
        "sae_top_50_test_accuracy": 0.9197500000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9994,
        "llm_top_1_test_accuracy": 0.6522,
        "llm_top_2_test_accuracy": 0.7814,
        "llm_top_5_test_accuracy": 0.9123999999999999,
        "llm_top_10_test_accuracy": 0.9642,
        "llm_top_20_test_accuracy": 0.9882,
        "llm_top_50_test_accuracy": 0.9972000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9996000289916992,
        "sae_top_1_test_accuracy": 0.5868,
        "sae_top_2_test_accuracy": 0.7624,
        "sae_top_5_test_accuracy": 0.8594000000000002,
        "sae_top_10_test_accuracy": 0.9012,
        "sae_top_20_test_accuracy": 0.9395999999999999,
        "sae_top_50_test_accuracy": 0.9850000000000001,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 16384,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "JumpReLU",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "jumprelu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null,
      "jump_coeff": 0.1
    },
    "eval_result_unstructured": null
  },
  "unlearning evaluation results": {
    "eval_type_id": "unlearning",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "wmdp-bio",
        "high_school_us_history",
        "college_computer_science",
        "high_school_geography",
        "human_aging"
      ],
      "intervention_method": "clamp_feature_activation",
      "retain_thresholds": [
        0.001,
        0.01
      ],
      "n_features_list": [
        10,
        20
      ],
      "multipliers": [
        25,
        50,
        100,
        200
      ],
      "dataset_size": 1024,
      "seq_len": 1024,
      "n_batch_loss_added": 50,
      "target_metric": "correct",
      "save_metrics": true,
      "model_name": "google/gemma-2-2b-it",
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16"
    },
    "eval_id": "819da717-067b-4828-8614-7df0d818be18",
    "datetime_epoch_millis": 1738808576160,
    "eval_result_metrics": {
      "unlearning": {
        "unlearning_score": 0.0
      }
    },
    "eval_result_details": [],
    "sae_bench_commit_hash": "c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 16384,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "JumpReLU",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "jumprelu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null,
      "jump_coeff": 0.1
    },
    "eval_result_unstructured": null
  }
}