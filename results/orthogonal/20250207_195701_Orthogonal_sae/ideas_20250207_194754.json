[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "exp_temporal_exclusive_sae",
        "Title": "Exponential Temporal Exclusivity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class to incorporate an exponential temporal exclusivity regularization term. This term computes cosine similarity between latent activation vectors within a sliding window (e.g., size 3) of adjacent tokens, applying weights computed as exp(-decay_rate * distance) for token pairs based on their distance. The overall loss will include reconstruction, auxiliary, intra-token exclusivity, and this exponential temporal exclusivity loss. Experiments will compare benchmark performance on sparse_probing and core with and without this modified loss.",
        "Technical_Details": "For each training batch containing sequences of tokens, obtain the top-k latent activation matrix f with shape [batch_size, num_features]. First, compute an intra-token exclusivity loss by calculating pairwise cosine similarity among non-zero features within each token and averaging the off-diagonal similarities. Then, for the temporal term, for each sequence, consider token pairs within a fixed sliding window (e.g., window size = 3) and compute the cosine similarity between their latent activation vectors. For each pair, use a weight defined as exp(-decay_rate * distance), where 'decay_rate' is a hyperparameter. Average these weighted similarities to obtain the exponential temporal exclusivity loss. The final loss function becomes: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_intra * intra_exclusive_loss + lambda_temporal * exp_temporal_loss, where lambda_intra and lambda_temporal are hyperparameters that control the strength of the intra-token and exponential temporal regularization terms, respectively.",
        "Implementation_Plan": "1. In TrainerTopK.loss, after computing the latent activation f, add code to compute the intra-token cosine similarity loss as before. 2. For the exponential temporal loss, iterate over tokens in each sequence within the training batch, and for each token, compute the cosine similarity with tokens in a sliding window ahead (e.g., next 2 tokens), applying a weight of exp(-decay_rate * distance) for each pair. 3. Average these weighted similarities across sequences to form the exp_temporal_loss. 4. Introduce a hyperparameter 'decay_rate' along with lambda_intra and lambda_temporal for balancing the new loss components. 5. Update the final loss calculation and log both the intra and exponential temporal loss values. 6. Train the modified SAE model and evaluate its latent interpretability using benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Integrating an exponential decay for temporal exclusivity adds nuance and precision in emphasizing temporally proximate activations, making it conceptually appealing.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The exponential decay function is computationally trivial to implement and integrates seamlessly into the existing loss computation, ensuring high feasibility.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Substituting a linear with an exponential decay for temporal regularization is a subtle yet innovative tweak that leverages natural sequential properties in language without increased complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "By smoothing the temporal exclusivity penalty with exponential decay, the model is expected to achieve clearer, more interpretable latent features, enhancing downstream interpretability benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We introduce an enhanced variant of sparse autoencoders that employs exponential temporal exclusivity regularization to improve the interpretability of latent representations. Our approach extends previous methods by enforcing decorrelation of latent features not only within tokens but also across adjacent tokens in a sequence using an exponential decay weighting function. The proposed loss function integrates reconstruction, auxiliary, intra-token, and exponential temporal exclusivity terms. We detail the implementation of this loss modification within the standard SAE framework and discuss its potential for enabling more distinct and monosemantic latent features in language models."
    },
    {
        "Name": "temporal_uniform_activation_loss_ramp",
        "Title": "Temporal Uniform Activation Loss with Ramp-Up for Stable Latent Interpretability",
        "Experiment": "Enhance the TrainerTopK loss function by incorporating a temporal uniform activation regularization term with a scheduled ramp-up. Maintain a running exponential moving average of activation frequencies of SAE latents across batches and penalize deviations from the expected uniform target after a warm-up period. The ramp-up scalar multiplier gradually increases the weight of the temporal loss from 0 to 1 over a predefined number of steps. Evaluate the impact on sparse_probing and core benchmarks.",
        "Technical_Details": "Within each training step, after obtaining the topk indices, update the running average vector for latent activation frequencies using exponential decay. Compute the target frequency as (k * batch_size) / dict_size and then calculate the L2 distance between the running average and this target. Introduce a ramp-up schedule for the temporal loss, defined by a warm-up parameter (ramp_steps), so that the final penalty is scaled by lambda_temporal * min(1, current_step / ramp_steps). This approach allows early training to focus primarily on reconstruction loss and gradually enforces the uniformity constraint as training proceeds.",
        "Implementation_Plan": "1. In the TrainerTopK class, add a persistent attribute for the running average of latent activations and initialize it to a zero vector of length dict_size. 2. Implement an exponential moving average update in the loss() function after computing top_indices. 3. Calculate the target frequency (target = (k * batch_size) / dict_size) and compute the L2 penalty between the running average and target vector. 4. Introduce a new hyperparameter, ramp_steps, and modify the temporal loss term to be scaled by min(1, current_step / ramp_steps) * lambda_temporal. 5. Add the scaled penalty to the overall loss. 6. Log the ramped temporal uniform activation loss for monitoring. 7. Evaluate the modified SAE on sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Incorporating a ramp-up schedule for the temporal loss adds a nuanced and practical enhancement that aligns well with training dynamics, making the idea even more compelling.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "The ramp-up mechanism is implemented with a straightforward scaling function in the loss computation, which adds minimal overhead and fits well within the existing training pipeline.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While temporal regularization is known, combining it with a scheduled ramp-up to adaptively introduce the constraint during training presents a novel twist within the context of sparse autoencoders.",
        "Novelty": 8,
        "Expected_Research_Impact": "This refinement is expected to further stabilize training and produce more interpretable, balanced latent representations, potentially yielding improvements on sparse_probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.75,
        "Abstract": "We propose an enhancement to the temporal uniform activation loss for sparse autoencoders by introducing a scheduled ramp-up mechanism. This approach maintains an exponential moving average of latent activation frequencies and enforces a uniform distribution through an L2 penalty that is gradually applied as the model trains. By delaying the full impact of the uniformity constraint during the early training stages, the method allows for robust reconstruction learning before imposing strict latent regularization. The technique is integrated into the existing training framework with minimal modifications, and we hypothesize that it will contribute to more stable and interpretable latent representations."
    },
    {
        "Name": "norm_exp_dynamic_thresh_exclusive_sae",
        "Title": "Normalized Exponential Dynamic Exclusive Sparsity Regularization with Temporal Awareness and Activation Thresholding for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding an exclusive loss term that computes cosine similarity between strong latent activations for temporally adjacent tokens, weighted by an exponential decay of the token distance and filtered using an activation threshold. Normalize the computed loss by the number of valid token pairs. The experiment involves comparing interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced SAE and the baseline.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and corresponding token positions, consider token pairs (i, j) within a specified window (e.g., |t_i - t_j| <= window_size). For each pair where both activations exceed act_threshold, compute the cosine similarity weighted by an exponential decay factor, exp(-|t_i - t_j|), and by the product of their activation magnitudes. Aggregate these weighted similarities to yield exclusive_loss_dynamic, and then normalize by dividing by the number of valid token pairs to ensure consistency. Introduce hyperparameters lambda_dynamic and act_threshold, and update the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_normalized.",
        "Implementation_Plan": "1. In TrainerTopK.loss, track token positions along with activations. 2. For each batch, iterate over token pairs within a predetermined window (e.g., window_size = 2), and apply a mask to consider only pairs where both activations exceed act_threshold. 3. For each valid pair, compute the cosine similarity, apply the exponential decay weight exp(-|t_i - t_j|), and multiply by the product of activation magnitudes. 4. Sum these values to get exclusive_loss_dynamic, and count the number of valid pairs. 5. Normalize the loss by dividing by the valid pair count (adding a small epsilon for numerical stability if needed). 6. Introduce lambda_dynamic and act_threshold as hyperparameters, and add lambda_dynamic * exclusive_loss_dynamic_normalized to the overall loss. 7. Log the new loss term and run evaluations using benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Normalizing the loss by the number of valid token pairs deepens the temporal regularization mechanism by ensuring stability and consistency in gradient contributions, enhancing the conceptual clarity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The additional normalization step is a minor modification that fits within the existing loss computation, and can easily be implemented and tested with minimal changes.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Introducing per-sample normalization to the exclusive loss term is a subtle yet original refinement that improves robustness without altering the core methodology.",
        "Novelty": 8,
        "Expected_Research_Impact": "This refinement is expected to produce more stable and distinct latent features, thus improving interpretability metrics on benchmarks like sparse_probing and core without impacting reconstruction quality.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We introduce an enhanced variant of sparse autoencoders that integrates a normalized exponential dynamic exclusive sparsity regularization term with temporal awareness and activation thresholding. By computing a cosine similarity penalty between strong latent activations of temporally adjacent tokens, weighting these penalties with an exponential decay function, and normalizing by the number of valid token pairs, our method focuses the loss on significant redundancies while maintaining stability across batches. This temporal-aware regularization is seamlessly integrated into the existing loss function, promoting more distinct and monosemantic latent representations without altering the core autoencoder architecture."
    },
    {
        "Name": "temporal_exclusive_anneal_sae",
        "Title": "Thresholded Sliding-Window Temporal Exclusive Sparsity Regularization with Annealing for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK.loss function by applying a sliding-window mechanism over latent activations to form overlapping temporal segments, perform mean pooling on these segments, and compute pairwise cosine similarity with a thresholding mechanism. Integrate an annealing schedule for the hyperparameter lambda_exclusive so that its contribution to the overall loss increases gradually during training. Evaluate the approach on benchmarks such as sparse_probing and core.",
        "Technical_Details": "For each training batch, use a sliding-window pooling operation on the latent activation matrix f (shape [batch_size, num_features]) to obtain overlapping temporal segments. Compute the mean activation for each window to get temporal representations. Calculate the cosine similarity matrix among these representations for each sample, ignoring self-similarities. Apply a threshold tau such that only similarity values above tau contribute to the temporal_exclusive_loss, computed as the average of these selected values. Additionally, implement an annealing schedule for lambda_exclusive, for example, lambda_exclusive = base_lambda * min(1, current_step / anneal_steps), to gradually increase the exclusive loss weight during training. The overall loss is defined as: loss_total = reconstruction_loss + lambda_aux * auxk_loss + lambda_exclusive * temporal_exclusive_loss, where tau and base_lambda are tunable hyperparameters.",
        "Implementation_Plan": "1. In TrainerTopK.loss, after obtaining latent activations f, apply sliding-window mean pooling to produce overlapping temporal segments. 2. Compute the cosine similarity matrix for these pooled segments for each sample, excluding the diagonal. 3. Introduce a threshold hyperparameter tau; set similarity values below tau to zero and average the remaining values to calculate temporal_exclusive_loss. 4. Define an annealing schedule for lambda_exclusive that scales its value from 0 to base_lambda over a fixed number of steps (anneal_steps). 5. Incorporate lambda_exclusive * temporal_exclusive_loss into the overall loss function. 6. Log the annealed lambda_exclusive and temporal_exclusive_loss for monitoring, then train the model and evaluate on established benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing an annealing schedule offers a refined control mechanism for the penalty term, enhancing the temporal exclusivity regularization without adding unnecessary complexity.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "The annealing schedule is a standard technique and can be implemented with minimal extra code, ensuring that the solution remains feasible and efficient with standard tensor operations.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The integration of thresholding with sliding-window pooling and an annealing schedule provides a novel yet concise refinement to sparsity regularization in autoencoders.",
        "Novelty": 8.5,
        "Expected_Research_Impact": "This approach is expected to enhance latent interpretability by progressively enforcing exclusivity across temporal segments, potentially yielding improvements on the sparse_probing and core benchmarks.",
        "Research_Impact": 8.5,
        "Overall_Score": 8.8,
        "Abstract": "We propose a variant of sparse autoencoders that incorporates thresholded sliding-window temporal exclusive sparsity regularization with an annealing schedule to improve mechanistic interpretability. Latent activations are segmented into overlapping temporal windows using a sliding-window pooling operation, and cosine similarity is computed among the mean representations of these windows. A penalty is selectively applied to similarity values exceeding a predefined threshold, with the weight of this penalty gradually increasing during training via an annealing schedule. This additional loss term is designed to encourage each latent feature to capture a distinct, non-redundant aspect of the data while maintaining the original architecture and training protocol. Experimental evaluation on established interpretability benchmarks is planned to assess the approach."
    },
    {
        "Name": "focused_temp_ortho_sae",
        "Title": "Focused Temporal Orthogonalized Sparse Autoencoder",
        "Experiment": "Modify the TrainerTopK loss function to compute an instantaneous cosine orthogonalization penalty and a targeted temporal consistency penalty exclusively on the top-k activated dictionary atoms. Integrate an adaptive decay schedule for the temporal penalty based on training progress, and update an exponential moving average (EMA) of W_enc to compute the temporal penalty. Evaluate the updated SAE using the existing evaluation pipeline such as core and sparse_probing.",
        "Technical_Details": "For each training step, select the top-k activated indices and retrieve their corresponding normalized dictionary vectors from W_enc. Compute an instantaneous cosine similarity penalty among these vectors, scaled by lambda_ortho. Maintain an EMA of W_enc (updated with the formula: EMA = alpha * current_W_enc + (1 - alpha) * previous_EMA, with a stop-gradient applied on EMA updates) and compute the temporal penalty exclusively for the top-k activated vectors by calculating the cosine similarity between each vector and its EMA counterpart. Use an adaptive decay schedule for lambda_temp (e.g., lambda_temp decaying linearly based on current_step/total_steps) to reduce its influence as training converges. Ensure numerical stability in cosine similarity computations by adding a small epsilon. The total loss is the sum of the reconstruction loss, the lambda_ortho-scaled instantaneous penalty, and the lambda_temp-scaled temporal penalty.",
        "Implementation_Plan": "1. In the TrainerTopK __init__ method, initialize an EMA copy of W_enc and introduce lambda_temp along with an adaptive decay scheduler. 2. In the loss() method, after obtaining top-k activated indices, compute the instantaneous cosine penalty from the corresponding normalized dictionary vectors. 3. Update the EMA for W_enc using the formula: EMA = alpha * current_W_enc + (1 - alpha) * previous_EMA, applying a stop-gradient to the EMA term. 4. Compute the temporal penalty only for the top-k activated vectors by computing 1 - cosine_similarity(current_vector, EMA_vector), averaging these values, and scaling by the decayed lambda_temp. 5. Combine the reconstruction loss, instantaneous penalty, and temporal penalty to form the total loss and log these metrics. 6. Evaluate the updated SAE using existing benchmarks defined in experiment.py.",
        "Interestingness_Evaluation": "This final refinement enhances robustness by ensuring the EMA remains a fixed target and applying numerical stability measures, thereby making the approach both nuanced and targeted.",
        "Interestingness": 8.7,
        "Feasibility_Evaluation": "The modifications remain within the scope of loss computation and EMA updates, and the integration of a stop-gradient is well supported in modern frameworks, ensuring the implementation is efficient and feasible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Focusing temporal penalties on dynamically selected top-k activations with adaptive decay and robust EMA updates presents a novel and precise refinement without adding undue complexity.",
        "Novelty": 8.7,
        "Expected_Research_Impact": "Focusing regularization on critical latent features with robust temporal consistency is expected to yield semantically distinct and stable latent representations, potentially benefiting interpretability benchmarks.",
        "Research_Impact": 8.6,
        "Overall_Score": 8.8,
        "Abstract": "We present a focused temporal orthogonalized sparse autoencoder that enhances latent interpretability through targeted regularization. Our method computes an instantaneous cosine alignment penalty on the top activated dictionary atoms and enforces temporal consistency by comparing these atoms to their exponential moving average. An adaptive decay schedule modulates the temporal penalty over training iterations, ensuring that regularization is strongest when most beneficial. This approach refines the latent space without altering the underlying architecture, preserving simplicity and computational efficiency."
    },
    {
        "Name": "smoothed_dynamic_temporal_threshold_sae",
        "Title": "Smoothed Dynamic Temporal Exclusive Sparsity with Adaptive Thresholding for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by introducing a temporal exclusive loss term with smoothed, dynamic thresholding. The latent activation batch is segmented into temporal windows, and for each segment, the pairwise cosine similarity matrix among non-zero latent activations is computed. For each segment, calculate a dynamic threshold as scaling_factor * median_cosine_similarity, and apply a smoothing mechanism (e.g., moving average or exponential smoothing) across adjacent segments to stabilize these thresholds. Only pairwise similarities exceeding the smoothed dynamic threshold contribute (excess similarity) to the exclusive loss, which is then averaged over segments and scaled by a hyperparameter lambda_temporal. Compare the baseline SAE with the smoothed dynamic threshold variant on interpretability benchmarks such as sparse_probing and core.",
        "Technical_Details": "For each batch, split the latent activation matrix f (shape [batch_size, num_features]) into fixed-length temporal segments. Within each segment, compute the cosine similarity matrix for active latent features (excluding self-similarity) and determine the median cosine similarity. Define the dynamic threshold for the segment as dynamic_threshold = scaling_factor * median_cosine_similarity. Then, smooth the dynamic thresholds across segments using a chosen method (e.g., a simple moving average over a window of adjacent segments or exponential smoothing with a smoothing factor beta). For each segment, subtract the smoothed dynamic threshold from each pairwise cosine similarity (zeroing negatives) and average these excess values to obtain the segment\u2019s exclusive loss. The overall temporal exclusive loss is the mean of the segment losses, and the final training objective becomes: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_temporal * temporal_exclusive_loss.",
        "Implementation_Plan": "1. In TrainerTopK.loss, after obtaining the latent activation f, segment f into temporal windows using a fixed token window. 2. For each segment, compute the cosine similarity matrix among active features (ignoring self-similarity) and calculate the median cosine similarity. 3. Compute the dynamic threshold per segment: dynamic_threshold = scaling_factor * median_cosine_similarity. 4. Apply a smoothing strategy (e.g., moving average or exponential smoothing) over the sequence of dynamic_threshold values to obtain a smoothed threshold for each segment. 5. For each segment, subtract the smoothed threshold from each pairwise cosine similarity value, zero out negatives, and average the positive differences to get the exclusive loss for that segment. 6. Average the exclusive losses across segments to compute temporal_exclusive_loss, then add lambda_temporal * temporal_exclusive_loss to the total loss. 7. Log the smoothed dynamic thresholds and temporal_exclusive_loss for monitoring, then train and evaluate the models using benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Incorporating a smoothing mechanism for the dynamic threshold increases conceptual clarity and robustness, adding a novel nuance to the temporal exclusivity approach.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The extra smoothing computation involves simple averaging or exponential decay operations over small vectors, making it both easy to implement and computationally negligible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The integration of smoothed, dynamic thresholding in temporal exclusive sparsity is a subtle yet innovative extension that refines the previous method without adding undue complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "Smoothing dynamic thresholds is expected to yield more stable and distinct latent features over time, enhancing interpretability metrics on benchmarks like sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refined variant of sparse autoencoders that incorporates smoothed dynamic temporal exclusive sparsity regularization with adaptive thresholding. In this approach, latent activations are divided into temporal segments, and a dynamic threshold is computed for each segment based on a scaled median of pairwise cosine similarities of active features. A smoothing mechanism is applied across segments to stabilize these thresholds, and only similarities exceeding the smoothed threshold contribute to an additional loss term. This loss is integrated into the training objective to promote uniquely responsive latent features while maintaining reconstruction quality."
    },
    {
        "Name": "exp_dynamic_exclusive_sae",
        "Title": "Exponential Decay-based Dynamic Exclusive Sparsity Regularization for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by incorporating a continuous exponential decay weighting into the dynamic exclusive loss term. This term computes the cosine similarity between pairs of high-activation latent vectors and weights each pair by an exponential decay factor based on their temporal distance, rather than using a fixed temporal window. The experiment will involve benchmarking the model's interpretability and sparsity metrics on datasets like sparse_probing and core, comparing the new loss to the baseline SAE.",
        "Technical_Details": "For each training batch, obtain the top-k latent activation matrix f (shape [batch_size, num_features]) along with token positions. First, apply a filtering step using an activation_threshold to retain only activations above a specified value. For all pairs (i, j) of such activations, compute the cosine similarity of their associated latent vectors. Weight each computed similarity by the product of the corresponding activation magnitudes and by an exponential decay factor exp(-|t_i - t_j| / tau), where tau is a hyperparameter controlling the decay rate with temporal distance. Aggregate these weighted similarities to form the exclusive_loss_dynamic term. Introduce a hyperparameter lambda_exponential so that the total loss is given by: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exponential * exclusive_loss_dynamic.",
        "Implementation_Plan": "1. In TrainerTopK.loss, record token positions along with the latent activations f. 2. Apply a filtering step on f using activation_threshold to discard low-magnitude activations. 3. For all pairs of remaining activations, compute their cosine similarity, activation product, and temporal distance. 4. Calculate the weight for each pair as the product of the activation product and exp(-absolute(position_difference) / tau). 5. Average these weighted cosine similarities to compute exclusive_loss_dynamic. 6. Incorporate the term lambda_exponential * exclusive_loss_dynamic into the overall loss function and log it for monitoring. 7. Run training and evaluate the method on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Integrating an exponential decay for temporal weighting introduces a refined, continuous approach to measuring latent redundancy, making the idea both clear and innovative.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The changes are limited to modifications in the loss function without altering existing network architecture, ensuring straightforward implementation and testing within the current framework.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of exponential decay to weight temporal proximity in exclusive sparsity loss represents a novel and effective refinement to address polysemanticity.",
        "Novelty": 8,
        "Expected_Research_Impact": "This approach is expected to further promote monosemantic latent representations by continuously attenuating the impact of distant activations, enhancing interpretability metrics on key benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an enhancement to sparse autoencoders through the incorporation of an exponential decay-based dynamic exclusive sparsity regularization term. Our method computes the cosine similarity between latent features corresponding to high-activation tokens, weighting each pair by an exponential decay factor based on their temporal distance. By smoothly attenuating the influence of token pairs that are temporally distant and focusing on high-confidence activations through thresholding, our approach encourages monosemantic latent representations while integrating seamlessly with standard reconstruction and auxiliary losses. This paper details the technical implementation of the proposed loss function modification and outlines the experimental setup for evaluating its impact."
    },
    {
        "Name": "exp_decay_temporal_sae_v3",
        "Title": "Adaptive Exponential Decay Temporal Exclusive Sparsity for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Enhance the TrainerTopK loss function by incorporating an adaptive exponential decay weighting into the exclusive sparsity term. For token pairs within a temporal window, compute cosine similarities of latent activations, multiply them by the product of activation magnitudes, and weight them with an exponential decay factor using an adaptive tau based on the batch's median temporal difference. Normalize the decay weights to account for variable sequence lengths and integrate the resulting term into the overall loss. Conduct experiments comparing this refined loss with the baseline SAE on benchmarks such as sparse_probing and core.",
        "Technical_Details": "For each training batch, obtain the latent activation matrix f (of shape [batch_size, num_features]) along with associated token positions. For every pair of tokens (i, j) with |t_i - t_j| <= window_size, compute the cosine similarity between their latent vectors. Weight this similarity by the product of their activation magnitudes and by an exponential decay factor exp(-|t_i - t_j| / tau_eff), where tau_eff is computed adaptively as tau_base multiplied by the median of the absolute temporal differences in the batch. Normalize the summed weighted similarities by the total number of valid token pairs to yield a normalized exclusive loss term, exclusive_loss_exp_norm. Integrate this term into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exclusive * exclusive_loss_exp_norm, with lambda_exclusive controlling the contribution of this penalty.",
        "Implementation_Plan": "1. Update the TrainerTopK.loss function in experiment.py to record token positions along with latent activations. 2. For each batch, calculate the absolute differences of token positions for all valid pairs within the temporal window and compute their median to derive tau_eff = tau_base * median_difference. 3. For every token pair (i, j) within the window, compute the cosine similarity between their latent activations, multiply by the product of activation magnitudes and by exp(-abs(t_i - t_j) / tau_eff). 4. Sum and normalize these values over all valid pairs to obtain exclusive_loss_exp_norm. 5. Introduce hyperparameter lambda_exclusive and add lambda_exclusive * exclusive_loss_exp_norm to the overall loss. 6. Log the new loss component, perform training experiments, and evaluate the model on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "The adaptive scaling of the decay parameter provides a meaningful refinement that tailors the exclusivity penalty to the temporal characteristics of each batch, maintaining innovation within a simple modification.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The adaptation only requires a simple median computation per batch and minor modifications to the loss code, ensuring it can be implemented with minimal added complexity.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adapting the decay parameter based on batch statistics introduces a new layer of responsiveness to the loss function, which is a novel yet straightforward extension within the existing framework.",
        "Novelty": 8,
        "Expected_Research_Impact": "By dynamically adapting the temporal decay to the data, the refined loss term is expected to better enforce monosemantic latent representations and improve interpretability on benchmarks like sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an enhancement to sparse autoencoders by introducing an adaptive exponential decay temporal exclusive sparsity loss. In our approach, for tokens within a predefined temporal window, cosine similarities between latent activations are weighted by an exponential decay factor, where the decay parameter is adaptively computed based on the median temporal differences of the batch. This adaptive weighting is normalized over the number of token pairs and integrated with standard reconstruction and auxiliary losses. The method aims to encourage distinct, monosemantic latent representations by penalizing redundant activations in temporally proximate tokens. We detail the technical integration of this loss and outline the experimental evaluation framework."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Consistent Latent Usage",
        "Experiment": "Modify the SAE training loss in TrainerTopK to include an auxiliary temporal penalty term that maintains an exponential moving average (EMA) of latent activations. Update the EMA after each batch based on the top-k indices from the encoder, compute the L2 loss between the EMA and the target frequency (calculated as (k * batch_size) / dict_size), and include this loss in the overall training objective weighted by a new hyperparameter, lambda_temporal. Evaluate the modification on the core and sparse_probing benchmarks while monitoring activation histograms over time.",
        "Technical_Details": "Within the TrainerTopK loss method, maintain an EMA vector for latent activation frequencies across batches. After each forward pass, update the EMA using a decay factor (e.g., beta) and compute the per-latent discrepancy from the target frequency. The target frequency is defined as (k * batch_size) / dict_size. The temporal uniform activation loss term is then the L2 norm of the difference between the EMA vector and the target frequency vector, scaled by a hyperparameter lambda_temporal. This term is added to the conventional reconstruction and auxiliary losses to encourage evenly distributed latent usage over time.",
        "Implementation_Plan": "1. Add a persistent EMA vector for latent frequencies to the TrainerTopK class, initialized to zeros or the target frequency. 2. In the loss() method, after retrieving top_indices from the encoder, update the EMA for each latent with a decay factor beta. 3. Calculate the L2 loss between the current EMA vector and the target frequency vector. 4. Introduce a new hyperparameter, lambda_temporal, to weight this temporal uniform activation loss. 5. Sum the temporal loss with the original reconstruction and auxiliary losses. 6. Evaluate the improved model on the established benchmarks and track changes in latent activation histograms over training time.",
        "Interestingness_Evaluation": "Incorporating temporal smoothing into the activation loss provides a novel approach to ensure stable and uniform latent usage, addressing polysemanticity in a creative yet straightforward manner.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation requires only minor modifications to the existing loss computation, is computationally light, and integrates seamlessly into the existing training pipeline, making it highly feasible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While uniform activation losses have been used in other contexts, the introduction of temporal smoothing via an EMA into sparse autoencoder training for interpretability is a fresh and innovative twist.",
        "Novelty": 8.5,
        "Expected_Research_Impact": "This modification is expected to foster more consistent and interpretable latent features over time, thereby improving performance on the core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 9,
        "Abstract": "We propose an enhancement to sparse autoencoder training by introducing a temporal uniform activation loss. This modification maintains an exponential moving average of latent activation frequencies and penalizes deviations from a target activation frequency over time. By integrating this temporal consistency constraint into the training objective, the model is encouraged to distribute latent usage uniformly, reducing feature absorption and polysemanticity. Our approach retains the original architecture and focuses solely on a refined loss formulation."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Robust Latent Interpretability",
        "Experiment": "Modify the loss function in the TrainerTopK class by adding a temporal uniform activation regularization term. This term will compute a moving average of activation frequencies for each latent feature over recent batches and penalize deviations from a target uniform distribution. The modified loss will be a sum of the reconstruction loss, auxiliary dead-feature loss, and the new temporal uniform activation loss. Results will be obtained by comparing the updated SAE's performance on the sparse_probing and core benchmarks.",
        "Technical_Details": "During training, maintain a rolling activation frequency vector for all latent features updated each batch using an exponential moving average with a smoothing factor alpha (e.g., 0.1). For each batch, compute the per-feature activation counts from the topk indices, update the moving average with these counts, and calculate the target frequency as (k * batch_size) / dict_size. The L2 penalty is then computed between the moving average frequency vector and the target frequency vector, scaled by a hyperparameter 'lambda_uniform'. This penalty is added to the overall loss, thereby enforcing balanced and temporally stable latent activations to mitigate redundancy and polysemanticity.",
        "Implementation_Plan": "1. In the TrainerTopK class, define a new buffer to store the moving average of activation frequencies for each latent feature (dimension = dict_size). 2. In the loss() function, after obtaining top_indices from the encoder, compute the activation counts for the current batch. 3. Update the moving average frequency vector using an exponential smoothing function (e.g., new_avg = alpha * current_counts + (1 - alpha) * old_avg). 4. Compute the target frequency for each latent as (k * batch_size) / dict_size. 5. Calculate the L2 penalty between the moving average vector and the target frequency vector, scale it by 'lambda_uniform', and add it to the existing loss. 6. Log the temporal activation loss for monitoring. 7. Evaluate the modified SAE on sparse_probing and core benchmarks to assess improvements in latent interpretability.",
        "Interestingness_Evaluation": "The temporal aspect adds a novel twist to uniform activation regularization, ensuring that latent usage is balanced over time, which is both intriguing and directly applicable to interpretability.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation is straightforward, requiring only minor modifications to the loss function and state tracking, making it practical to implement without significant overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Integrating temporal smoothing into activation regularization for SAE is a fresh modification that builds on the original idea in a non-trivial but comprehensible way.",
        "Novelty": 8,
        "Expected_Research_Impact": "By enforcing temporally stable and balanced latent activations, the model is expected to yield more interpretable and monosemantic features, leading to improvements in sparse probing and core evaluation benchmarks.",
        "Research_Impact": 8.5,
        "Overall_Score": 8.55,
        "Abstract": "We propose a temporal uniform activation loss for sparse autoencoders that extends the idea of uniform latent utilization by incorporating a moving average of activation frequencies over training batches. This method penalizes deviations from a target uniform distribution in a temporally consistent manner, encouraging balanced and stable latent feature usage. The approach is integrated as a simple auxiliary loss term added to the standard reconstruction and auxiliary losses, requiring minimal modification to the existing training pipeline. By promoting temporally stable and focused latent representations, this method aims to alleviate polysemanticity and improve mechanistic interpretability."
    },
    {
        "Name": "thresh_dynamic_exclusive_sae",
        "Title": "Thresholded Dynamic Exclusive Sparsity Regularization with Temporal Awareness for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a thresholded dynamic exclusive loss term. The term computes cosine similarities between pairs of strong latent activations for tokens within a fixed temporal window, weights the penalties by their activation magnitudes, and only penalizes pairs whose similarity exceeds a predefined threshold. Compare interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced model and the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and associated token position indices, restrict pairwise cosine similarity computation to token pairs within a temporal window defined by a hyperparameter window_size. For each valid pair (i, j) with token positions t_i and t_j such that |t_i - t_j| <= window_size, compute the cosine similarity of their active latent vectors. Weight each similarity by the product of the activation magnitudes and apply a threshold (cosine_threshold) so that only similarities above this threshold contribute to the exclusive loss. Aggregate the thresholded, weighted similarities to form exclusive_loss_dynamic_thresh and incorporate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_thresh.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, track token positions along with activations. 2. Identify token pairs within a fixed temporal window (e.g., tokens within 2 positions of each other). 3. Compute cosine similarity for these pairs and weight them by the product of their activation magnitudes. 4. Apply a threshold, cosine_threshold, to ignore similarities below this value. 5. Average the remaining weighted similarities to yield exclusive_loss_dynamic_thresh. 6. Introduce a hyperparameter lambda_dynamic to scale this loss term and add it to the overall loss. 7. Log the new loss term and run experiments to evaluate its impact on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing a threshold mechanism to focus on penalizing only high redundancy in latent activations adds a novel refinement without significant complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve straightforward computations within the loss function and add only minimal hyperparameters; implementation and experimentation should be feasible within the given constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a thresholded penalty in a temporal context provides a unique angle to existing regularization techniques in sparse autoencoders, enhancing the model's interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only high similarity between temporally adjacent tokens, the method should yield more distinct and interpretable latent features, potentially improving performance on core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a thresholded dynamic exclusive sparsity regularization, augmented with temporal awareness. Our approach selectively penalizes high cosine similarity between latent activations from temporally proximate tokens, weighting the penalty by the product of activation magnitudes and applying a threshold to disregard minor similarities. This additional loss term is integrated with the standard reconstruction and auxiliary losses, promoting monosemantic latent features without introducing significant complexity. We detail the technical formulation and implementation steps for this method, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "adaptive_temporal_activation_loss",
        "Title": "Adaptive Temporal Uniform Activation Loss for Stable Latent Consistency",
        "Experiment": "Modify the existing loss function in TrainerTopK to incorporate an exponential moving average (EMA) of latent activation frequencies along with an adaptive schedule for the temporal penalty weighting. The weighting parameter 'lambda_temporal' will be scaled from a low initial value to a predefined maximum over a warmup period. This adaptive adjustment allows the model to initially prioritize feature learning before gradually enforcing balanced activation, with evaluation on the sparse_probing and core benchmarks.",
        "Technical_Details": "During training, maintain an EMA for each latent's activation frequency using the update rule: EMA = beta * EMA + (1 - beta) * current_batch_frequency, using beta as the smoothing factor. Define the target frequency as (k * batch_size) / dict_size, where k is the number of active latents per input and dict_size is the total number of latent features. Compute the L2 norm between the EMA and the target frequency. Introduce an adaptive weighting for the penalty term where lambda_temporal increases linearly from 0 to max_lambda_temporal over a warmup period (warmup_steps). The overall loss is the sum of the reconstruction loss, the original auxiliary losses, and the weighted temporal uniform activation penalty.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize an EMA tensor for latent activation frequencies with size equal to dict_size, set to zeros. 2. In the loss() function, update the EMA using the current batch's activation histogram derived from top_indices. 3. Define a target frequency vector as (k * batch_size) / dict_size for each latent. 4. Compute the L2 norm between the EMA and the target vector to determine the temporal penalty. 5. Implement an adaptive scheduler for lambda_temporal that linearly scales from 0 to max_lambda_temporal over warmup_steps. 6. Multiply the temporal penalty by the current lambda_temporal and add it to the overall loss. 7. Log both the temporal loss term and the current lambda_temporal value for monitoring. 8. Evaluate the enhanced SAE on the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "This idea is interesting due to its strategic modulation of regularization strength, effectively balancing latent feature learning and enforcement of consistent activation.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation requires only minor modifications to loss computation and EMA tracking, making it feasible to implement within the current framework with minimal overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While adaptive regularization schedules are known, their application in controlling the temporal dynamics of latent activations in SAEs presents a novel and focused extension of the original method.",
        "Novelty": 8,
        "Expected_Research_Impact": "Integrating an adaptive schedule for temporal regularization is expected to encourage stable and interpretable latent representations, potentially yielding positive outcomes on downstream tasks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an adaptive temporal uniform activation loss for sparse autoencoders, incorporating an exponential moving average (EMA) of latent activations with a dynamically scaled penalty term. The penalty weighting is increased linearly over a warmup period to allow initial feature learning, before enforcing balanced latent activation. This method aims to promote stable and consistent latent representations while mitigating polysemanticity, with minimal modifications to the training process."
    },
    {
        "Name": "temporal_exclusive_sae",
        "Title": "Temporal Exclusive Sparsity Regularization via Consecutive Token Comparisons for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a temporal exclusive loss term that computes the cosine similarity between the latent activations of each token and its immediate neighbor in the sequence. This loss term will be weighted by the product of their activation magnitudes. Experiments will compare models trained with this simplified temporal exclusivity loss against baseline SAEs using benchmarks such as sparse_probing and core.",
        "Technical_Details": "For each training batch, obtain the top-k latent activation matrix f (shape [batch_size, num_features]) along with token positions. Instead of computing full pairwise cosine similarities, compute the cosine similarity between each token's active latent vector and that of its immediate neighbor (i.e., for token i and token i+1). Each similarity is weighted by the product of the corresponding activation magnitudes. Average these weighted similarities to form the temporal exclusive loss term, exclusive_loss_temporal. Introduce a hyperparameter lambda_temporal to scale this loss, and integrate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_temporal * exclusive_loss_temporal.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, after obtaining latent activations f, determine token sequence order. 2. Compute the cosine similarity between each token's activation and its immediate neighbor's activation. 3. Weight each similarity by the product of their activation magnitudes, and average over the batch to obtain exclusive_loss_temporal. 4. Introduce the hyperparameter lambda_temporal to control this term's impact, and add it to the overall loss. 5. Log the new loss term for monitoring. 6. Conduct experiments using benchmarks such as sparse_probing and core to evaluate interpretability and sparsity.",
        "Interestingness_Evaluation": "Streamlining the temporal exclusive loss to focus on consecutive tokens offers a conceptually clear and efficient modification that directly targets latent redundancy in sequential contexts.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The simplified consecutive token comparison reduces computational and implementation complexity, ensuring that the experiment can be completed by a junior PhD student within one month on standard GPU hardware.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Focusing on immediate temporal neighbors for exclusivity is a novel and straightforward extension of exclusivity regularization in sparse autoencoders, aligning well with the sequential nature of language.",
        "Novelty": 8,
        "Expected_Research_Impact": "The approach is expected to enhance latent interpretability by minimizing redundant activations in adjacent tokens, thereby benefiting benchmarks like sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoder training by incorporating a temporal exclusive sparsity regularization term based on consecutive token comparisons. This approach computes the cosine similarity between latent activations of adjacent tokens, weighting each similarity by the product of their activation magnitudes. The resulting loss term is integrated with the standard reconstruction and auxiliary losses to encourage distinct, monosemantic latent representations in sequential data. We detail the technical implementation and outline the experimental procedures for evaluating the method on interpretability benchmarks."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Consistent Latent Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK loss function to include a temporal component in the uniform activation penalty. Maintain an exponential moving average (EMA) of latent activations across batches, compute the activation histogram from this EMA, and compare it to an ideal uniform distribution. Add the L2 norm of the deviation as a loss term scaled by a hyperparameter beta. Evaluate on the sparse_probing and core benchmarks to assess enhanced interpretability of latent representations.",
        "Technical_Details": "Within TrainerTopK, introduce a persistent EMA vector for latent activations, updated at each training step using the formula: EMA_new = alpha * histogram_current + (1 - alpha) * EMA_previous, where alpha is a smoothing rate. In each batch, calculate the batch activation histogram from top-k latent indices, update the EMA, and normalize it to obtain the empirical activation frequency. Set the uniform target frequency vector with each element equal to (k / dict_size). Compute the L2 penalty between the EMA-derived frequency and the target vector. This temporal uniform activation loss is integrated into the overall loss function as: total_loss = reconstruction_loss + auxk_loss + beta * temporal_loss.",
        "Implementation_Plan": "1. In TrainerTopK, define a new persistent buffer (a tensor) for the EMA of latent activation counts, initialized to zeros with shape [dict_size]. 2. Within the loss function, after obtaining top_indices, calculate the current batch histogram, update the EMA buffer using the defined smoothing rate alpha, and compute the normalized frequency vector. 3. Define the uniform target vector as a constant tensor of size [dict_size] with value (k / dict_size) for each element. 4. Compute the L2 norm between the EMA frequency vector and the target, multiply by beta, and add it to the total loss. 5. Log the temporal loss separately for monitoring. 6. Run hyperparameter sweeps for alpha and beta to study their influence on latent balance and evaluate using the core and sparse_probing benchmarks.",
        "Interestingness_Evaluation": "This idea elegantly extends the original uniform activation loss by introducing temporal smoothing, providing a more stable and interpretable latent space over the course of training.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation requires a minimal change by adding an EMA buffer and slight modification of the loss function, making it feasible to implement and run within the given time constraints on a single NVIDIA H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Integrating a temporal component into uniform activation loss is a novel extension in the context of SAE interpretability, offering a deeper and temporally consistent solution without additional architectural complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "The temporal stabilization is expected to consistently balance latent activations, thus enhancing the interpretability as measured by the sparse_probing and core benchmarks, thereby providing clearer insights into feature separation and reducing polysemanticity.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We introduce a temporal uniform activation loss to enhance the interpretability of latent representations in sparse autoencoders. In our approach, an exponential moving average of latent feature activations is maintained during training, and deviations from a uniform activation target are penalized. This addition to the loss function promotes balanced usage of latent features over time, mitigating issues of polysemanticity and feature absorption without adding significant architectural or computational complexity. The proposed method is simple to implement and is evaluated using downstream mechanistic interpretability benchmarks."
    },
    {
        "Name": "local_temporal_exclusive_sae",
        "Title": "Local Temporal Exclusive Sparsity Regularization for Sparse Autoencoders",
        "Experiment": "Enhance the TrainerTopK loss function by adding a local temporal exclusive loss term, which computes the cosine similarity between each token's strong latent activations and those of its immediate neighbor. This term penalizes redundancy in adjacent tokens, weighted by the product of their activation magnitudes. The experiment will compare the interpretability and sparsity metrics from benchmarks such as sparse_probing and core for models with and without this local temporal loss.",
        "Technical_Details": "For each training batch with the latent activation matrix f (of shape [batch_size, num_features]) and their associated token positions, compute the cosine similarity between the latent activations of each token and its immediate successor. Weight the similarity by the product of the activation magnitudes of the two tokens. Average the weighted similarities to obtain the local temporal exclusive loss term, exclusive_loss_local. Introduce a hyperparameter lambda_local to balance this term, so the overall loss becomes: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_local * exclusive_loss_local. This systematically discourages the co-activation of similar latent features in adjacent tokens, promoting locally distinct, monosemantic representations.",
        "Implementation_Plan": "1. Update the TrainerTopK.loss function to, after calculating the latent activation f, iterate over each token in the batch (except the last) and compute the cosine similarity between f[i] and f[i+1]. 2. Weight each similarity by the product of the activation magnitudes of the current and next tokens. 3. Average these values to yield exclusive_loss_local. 4. Introduce a hyperparameter lambda_local to scale this loss, and add this term to the overall loss. 5. Log the new exclusive_loss_local during training and evaluate its impact using existing evaluation benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Focusing on local temporal relationships simplifies the approach while preserving the core interpretability gains, making the idea compelling.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modification requires only a slight change in the loss computation, making it well within the capabilities of a junior PhD student to implement and test on standard hardware within a short timeframe.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a local, token-to-token exclusive loss in sparse autoencoders for enhancing interpretability introduces a novel and focused extension to the baseline method.",
        "Novelty": 8,
        "Expected_Research_Impact": "By specifically penalizing adjacent latent similarities, the method is expected to foster distinct and interpretable features, positively affecting benchmarks like sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refined variant of sparse autoencoders that introduces a local temporal exclusive sparsity regularization, aimed at enhancing mechanistic interpretability. Our approach computes a weighted cosine similarity between the latent activations of consecutive tokens, penalizing redundant activations in an immediate temporal context. This local exclusive loss is integrated into the training objective alongside the standard reconstruction and auxiliary losses, encouraging distinct and monosemantic latent representations. We detail the algorithmic modifications and implementation steps required to incorporate this loss term into the existing framework, setting the stage for further evaluation."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Robust Latent Interpretability",
        "Experiment": "Modify the loss function in the TrainerTopK class by adding a temporal uniform activation regularization term. This term will compute a moving average of activation frequencies for each latent feature over recent batches and penalize deviations from a target uniform distribution. The modified loss will be a sum of the reconstruction loss, auxiliary dead-feature loss, and the new temporal uniform activation loss. Results will be obtained by comparing the updated SAE's performance on the sparse_probing and core benchmarks.",
        "Technical_Details": "During training, maintain a rolling activation frequency vector for all latent features updated each batch using an exponential moving average with a smoothing factor alpha (e.g., 0.1). For each batch, compute the per-feature activation counts from the topk indices, update the moving average with these counts, and calculate the target frequency as (k * batch_size) / dict_size. The L2 penalty is then computed between the moving average frequency vector and the target frequency vector, scaled by a hyperparameter 'lambda_uniform'. This penalty is added to the overall loss, thereby enforcing balanced and temporally stable latent activations to mitigate redundancy and polysemanticity.",
        "Implementation_Plan": "1. In the TrainerTopK class, define a new buffer to store the moving average of activation frequencies for each latent feature (dimension = dict_size). 2. In the loss() function, after obtaining top_indices from the encoder, compute the activation counts for the current batch. 3. Update the moving average frequency vector using an exponential smoothing function (e.g., new_avg = alpha * current_counts + (1 - alpha) * old_avg). 4. Compute the target frequency for each latent as (k * batch_size) / dict_size. 5. Calculate the L2 penalty between the moving average vector and the target frequency vector, scale it by 'lambda_uniform', and add it to the existing loss. 6. Log the temporal activation loss for monitoring. 7. Evaluate the modified SAE on sparse_probing and core benchmarks to assess improvements in latent interpretability.",
        "Interestingness_Evaluation": "The temporal aspect adds a novel twist to uniform activation regularization, ensuring that latent usage is balanced over time, which is both intriguing and directly applicable to interpretability.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation is straightforward, requiring only minor modifications to the loss function and state tracking, making it practical to implement without significant overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Integrating temporal smoothing into activation regularization for SAE is a fresh modification that builds on the original idea in a non-trivial but comprehensible way.",
        "Novelty": 8,
        "Expected_Research_Impact": "By enforcing temporally stable and balanced latent activations, the model is expected to yield more interpretable and monosemantic features, leading to improvements in sparse probing and core evaluation benchmarks.",
        "Research_Impact": 8.5,
        "Overall_Score": 8.55,
        "Abstract": "We propose a temporal uniform activation loss for sparse autoencoders that extends the idea of uniform latent utilization by incorporating a moving average of activation frequencies over training batches. This method penalizes deviations from a target uniform distribution in a temporally consistent manner, encouraging balanced and stable latent feature usage. The approach is integrated as a simple auxiliary loss term added to the standard reconstruction and auxiliary losses, requiring minimal modification to the existing training pipeline. By promoting temporally stable and focused latent representations, this method aims to alleviate polysemanticity and improve mechanistic interpretability."
    },
    {
        "Name": "dynamic_gated_temporal_sae",
        "Title": "Dynamic Gated Temporal Consistency and Decorrelation in Sparse Autoencoders for Enhanced Interpretability",
        "Experiment": "Modify the SAE training loss to include an intrabatch decorrelation loss and an enhanced temporal consistency loss with a dynamic gating mechanism. Compute an exponential moving average (EMA) of latent activations and calculate the L2 loss between current activations and the EMA. For each latent, determine a dynamic gating factor by normalizing its activation frequency against the batch average (e.g., gating_factor = activation_frequency / (mean_activation_frequency + epsilon)). Incorporate this gating factor into the temporal consistency loss, and evaluate the model on absorption, core, sparse_probing, and unlearning benchmarks.",
        "Technical_Details": "Within the TrainerTopK.loss() function, compute the usual reconstruction loss and intrabatch decorrelation loss. For the temporal consistency loss, maintain an EMA of latent activations with decay (e.g., 0.99) and compute the L2 norm between current activations and their EMA. For each latent feature, calculate its activation frequency over the mini-batch and derive the dynamic gating factor as the ratio of its frequency to the batch's mean activation frequency (with a small epsilon for stability). The final loss is L_total = L_reconstruction + lambda_auxk * L_auxk + lambda_decor * L_decor + lambda_temporal * (dynamic_gating_factor * L_temporal).",
        "Implementation_Plan": "1. In the TrainerTopK class, add functionality to compute activation frequencies for each latent over the batch. 2. Compute the batch mean activation frequency and derive the dynamic gating factor for each latent. 3. Update the temporal consistency loss calculation in the loss() function to multiply the L2 norm with the dynamic gating factor. 4. Introduce hyperparameters lambda_temporal and epsilon for stability. 5. Validate the modified loss function using the existing benchmarks to ensure seamless integration with the current system.",
        "Interestingness_Evaluation": "The introduction of a dynamic gating mechanism adds a nuanced, adaptive component to temporal regularization, enhancing the focus on critical features.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required changes involve simple tensor operations and normalization procedures, which are computationally inexpensive and straightforward to implement.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adapting the gating mechanism dynamically based on batch statistics is a novel enhancement in the context of temporal regularization for sparse autoencoders.",
        "Novelty": 8,
        "Expected_Research_Impact": "This dynamic approach is expected to yield more precise and robust latent regularization, promoting clearer and more stable feature representations, which can benefit performance on interpretability benchmarks.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We introduce a dynamic gated temporal regularization scheme for sparse autoencoders aimed at improving mechanistic interpretability in language models. Our approach augments the standard reconstruction and decorrelation losses with a temporal consistency loss that utilizes an exponential moving average of latent activations. A dynamic gating factor, computed by normalizing the activation frequency of each latent by the batch mean, is used to scale the temporal penalty. This method is designed to selectively stabilize frequently activated features while allowing less frequent ones to adapt, thereby promoting distinct and stable latent representations."
    },
    {
        "Name": "adaptive_temporal_uniform_activation_sae",
        "Title": "Adaptive Temporal Uniform Activation Sparse Autoencoder: Gradually Enforcing Long-Term Balanced Latent Usage",
        "Experiment": "Modify the TrainerTopK loss function to include an adaptive regularization term that compares the EMA of latent activation frequencies to a uniform distribution. Introduce a scheduling mechanism that gradually increases the weight of this term (uniform_lambda) after a specified warm-up period. Results will be evaluated on benchmarks such as sparse_probing and core to assess improvements in latent interpretability.",
        "Technical_Details": "Within the TrainerTopK class, maintain an EMA vector 'ema_activation_freq' that is updated on each batch using the rule: ema = beta * ema + (1 - beta) * current_batch_freq. Introduce a new training step counter and a warm-up threshold 'warmup_steps'. Compute an adaptive weight for the regularization term that starts at 0 and increases (linearly or using a simple schedule) to a predefined maximum 'uniform_lambda_final' after the warm-up period. Normalize the EMA vector to form a probability distribution, calculate the divergence (using L2 norm or KL divergence) with a uniform distribution, and multiply by the adaptive lambda. Add this term to the overall loss. This design ensures that the enforcement of uniform latent activation is applied gradually, reducing early training instability and promoting long-term balanced activations.",
        "Implementation_Plan": "1. In TrainerTopK, add a new instance variable 'training_step' and initialize it to zero along with 'ema_activation_freq' as a zero tensor. 2. After computing the current batch activation frequencies from the top-k activations, update 'ema_activation_freq' with the chosen decay factor beta. 3. Create a function to compute the adaptive lambda based on the training step and 'warmup_steps', scaling from 0 to 'uniform_lambda_final'. 4. Normalize the updated 'ema_activation_freq' vector to obtain a probability distribution. 5. Compute the divergence between this distribution and the uniform distribution. 6. Multiply the divergence by the adaptive lambda and add this term to the existing loss. 7. Increment 'training_step' after each batch. 8. Run experiments to compare the modified loss against the baseline on sparse_probing and core metrics.",
        "Interestingness_Evaluation": "Incorporating adaptive scheduling adds a nuanced improvement that addresses early training instability while reinforcing long-term balanced latent activation.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The scheduling mechanism involves minimal modifications with only a few additional lines of code, making it straightforward to implement and computationally inexpensive.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adaptive scheduling for regularization in temporal uniform activation provides a novel yet simple twist to the baseline idea, enhancing the control over latent activation dynamics.",
        "Novelty": 9,
        "Expected_Research_Impact": "This approach is expected to further stabilize and balance latent activations over time, leading to clearer and more interpretable representations that benefit sparse_probing and core benchmarks.",
        "Research_Impact": 10,
        "Overall_Score": 9.3,
        "Abstract": "We present an enhancement to sparse autoencoders that employs adaptive temporal regularization to enforce balanced activation across latent features. Our approach maintains an exponential moving average of latent activation frequencies and introduces a scheduled regularization term that gradually increases in weight after an initial warm-up period. This technique encourages sustained, uniform latent usage over training without incurring early instability. We detail the technical implementation of this adaptive mechanism and outline our experimental strategy to evaluate its impact on mechanistic interpretability benchmarks."
    },
    {
        "Name": "temporal_exclusive_sae",
        "Title": "Temporal Exclusive Sparsity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class to incorporate fixed-window temporal grouping of latent activations. Introduce a hyperparameter 'window_size' to partition the token sequence into contiguous segments. Compute mean pooled latent representations for each window and then derive a cosine similarity-based penalty among these representations. Evaluate the modified model on benchmarks such as sparse_probing and core to assess improvements in latent interpretability.",
        "Technical_Details": "For each training batch, latent activations f (of shape [batch_size, num_features]) are segmented along the token dimension using a fixed 'window_size'. For each segment, apply mean pooling to obtain a temporally aggregated latent vector. For each sample in the batch, compute a pairwise cosine similarity matrix across the pooled vectors of different windows, excluding self-similarity. Average the off-diagonal entries to yield the temporal exclusive loss term. Scale this term by a hyperparameter lambda_exclusive and add it to the existing reconstruction and auxiliary loss: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exclusive * temporal_exclusive_loss.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, after obtaining latent activations f, reshape or group them based on a new hyperparameter 'window_size' to form temporal segments. 2. Apply mean pooling over each window to yield a pooled representation for that segment. 3. Compute the cosine similarity matrix among these pooled representations for each sample, excluding self-similarity, and then average these similarities to compute the temporal exclusive loss term. 4. Introduce hyperparameters lambda_exclusive and window_size, and add lambda_exclusive * temporal_exclusive_loss to the original loss. 5. Log the new loss component for analysis. 6. Train and evaluate the modified model against benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Integrating a clearly defined temporal grouping mechanism increases the control over regularization and adds significant interpretability insights, making the idea highly interesting.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "This approach only requires minor adjustments to the loss function and straightforward pooling operations; the implementation remains simple and feasible within the resource and time constraints.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Systematically leveraging temporal grouping with a tunable window_size in the exclusivity regularization is a novel but incremental extension of the original concept.",
        "Novelty": 8,
        "Expected_Research_Impact": "By enforcing latent feature exclusivity over well-defined temporal windows, the proposed modification aims to enhance the monosemantic quality of latent features, which is expected to improve interpretability benchmarks.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose a refined temporal exclusive sparsity regularization technique for sparse autoencoders that leverages fixed-window temporal grouping of latent activations. By introducing a tunable 'window_size' hyperparameter, the latent activations are partitioned into contiguous segments, from which mean pooled representations are derived. A cosine similarity-based penalty is then applied to discourage redundant activation patterns across these segments. This method requires minimal modifications to the existing architecture and training pipeline, and is designed to promote distinct and interpretable latent features without compromising reconstruction quality."
    },
    {
        "Name": "adaptive_reconema_temporal_uniform_activation_loss",
        "Title": "Adaptive Temporal Uniform Activation Loss with Reconstruction EMA for Stable Latent Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the SAE training loss by integrating a temporal uniform activation regularizer with an adaptive conditional mechanism. For each batch, compute latent activation frequencies and update an exponential moving average (EMA) of these frequencies. Define the target frequency as (k/dict_size) and compute the squared difference between the EMA and target. Simultaneously, maintain an EMA of the reconstruction error. The regularization strength (lambda) decays exponentially over training steps using lambda_current = lambda_initial * exp(-step/decay_rate), but if the current reconstruction error exceeds the reconstruction EMA multiplied by a threshold factor (e.g., 1.05), then lambda_current is set to lambda_initial. This adaptive regularization term is added to the original reconstruction loss and evaluated on absorption, core, sparse_probing, and unlearning benchmarks.",
        "Technical_Details": "Let f_batch_i be the activation frequency for latent i in the current batch and update f_EMA_i via f_EMA_i = alpha * f_batch_i + (1 - alpha) * f_EMA_i_prev, where alpha is the smoothing factor. Set f_target = k/dict_size. Compute L_temporal = lambda_current * \u03a3_i (f_EMA_i - f_target)\u00b2. For the reconstruction loss, maintain an exponential moving average L_recon_EMA updated as L_recon_EMA = beta * L_recon_current + (1 - beta) * L_recon_EMA_prev, where beta is a smoothing factor. Define lambda_current = {lambda_initial * exp(-step/decay_rate) if L_recon_current <= factor * L_recon_EMA, or lambda_initial otherwise}, where factor > 1 (e.g., 1.05). This term is then added to the standard reconstruction loss in the TrainerTopK.loss function without modifying the encoder or decoder architecture.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize an EMA vector for latent activations and an EMA scalar for the reconstruction loss (L_recon_EMA). 2. In the TrainerTopK.loss function, compute the current batch activation frequencies and update f_EMA for each latent using the smoothing factor alpha. 3. Compute the current batch reconstruction loss L_recon_current and update L_recon_EMA using beta. 4. Calculate lambda_current as lambda_initial * exp(-step/decay_rate) if L_recon_current <= factor * L_recon_EMA; otherwise, set lambda_current = lambda_initial. 5. Compute the temporal uniform activation loss L_temporal and add it to the reconstruction loss. 6. Perform hyperparameter sweeps for lambda_initial, decay_rate, alpha, beta, and factor to balance consistency in latent activations with reconstruction quality. 7. Evaluate the modified SAE using the existing evaluation scripts on the designated benchmarks.",
        "Interestingness_Evaluation": "This idea is compelling as it couples latent uniformity with adaptive reconstruction error monitoring, achieving a dynamic balance that refines training stability.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "The approach is highly feasible since it involves only simple EMA computations and conditional checks within the loss function, with minimal additional overhead.",
        "Feasibility": 8.5,
        "Novelty_Evaluation": "The integration of a reconstruction EMA for adaptive thresholding introduces a noteworthy yet straightforward twist to the temporal regularization, adding moderate novelty.",
        "Novelty": 8.5,
        "Expected_Research_Impact": "Linking regularization strength to an adaptive reconstruction error measure is expected to produce more stable, interpretable latent representations without compromising reconstruction performance, benefiting core and sparse_probing benchmarks.",
        "Research_Impact": 8.5,
        "Overall_Score": 8.9,
        "Abstract": "We propose an adaptive temporal uniform activation loss that integrates a reconstruction EMA-based threshold to enhance latent interpretability in sparse autoencoders. This approach maintains an exponential moving average of latent activation frequencies and penalizes deviations from a target activation rate, with a regularization strength that decays over training steps. The decay is conditionally adjusted based on an EMA of the reconstruction error, ensuring that strong regularization is applied when the reconstruction error significantly exceeds its recent average. This method integrates into the existing training loss without modifying the autoencoder architecture and is evaluated using multiple mechanistic interpretability benchmarks."
    },
    {
        "Name": "temporal_exclusive_threshold_sae",
        "Title": "Thresholded Temporal Exclusive Sparsity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class to add a 'thresholded temporal exclusive loss' term. For each sample sequence, compute the pairwise cosine similarity of latent activations across adjacent time steps, and only penalize pairs whose similarity exceeds a set threshold. Evaluate the impact of this temporal exclusivity on interpretability benchmarks such as sparse_probing and core.",
        "Technical_Details": "Given a batch of sequential activations f with shape [batch_size, sequence_length, num_features], compute cosine similarities between latent feature vectors at each time step t and t+1 for t = 0 to sequence_length-2. Introduce a similarity threshold tau; only similarities that exceed tau contribute to the temporal exclusive loss. Formally, for each adjacent pair, define loss_t = max(0, cosine_similarity - tau) and average these values over the sequence and batch. Introduce a hyperparameter lambda_temporal to weight this term. The overall loss becomes: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_temporal * temporal_exclusive_loss, promoting distinct and stable latent features without penalizing natural semantic consistency.",
        "Implementation_Plan": "1. In TrainerTopK.loss, reshape or confirm the latent activations are structured with a temporal dimension. 2. For each sequence in the batch, compute cosine similarities between latent activations at time t and t+1. 3. Apply the threshold tau to each computed similarity and compute the average of the excess values to obtain temporal_exclusive_loss. 4. Introduce the hyperparameter lambda_temporal and add lambda_temporal * temporal_exclusive_loss to the overall loss. 5. Log the temporal_exclusive_loss for monitoring purposes. 6. Run training experiments and evaluate on benchmarks such as sparse_probing and core to assess interpretability improvements.",
        "Interestingness_Evaluation": "Incorporating a threshold into temporal exclusive loss fine-tunes the incentive for diverse latent features, making the idea distinctly interesting.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required modifications are minimal and confined to loss computation using existing temporal data, ensuring high feasibility.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The thresholding mechanism represents a novel refinement that tailors temporal regularization to natural latent dynamics.",
        "Novelty": 8,
        "Expected_Research_Impact": "Enforcing temporal exclusivity with thresholding is expected to yield more stable, monosemantic latent representations, thereby improving interpretability benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We propose a thresholded temporal exclusive sparsity regularization term for training sparse autoencoders, which leverages the sequential nature of language model activations. By penalizing only those adjacent activations with cosine similarities exceeding a predefined threshold, the method encourages latent features to capture distinct and stable concepts over time. Integrated seamlessly into the training loss, this approach aims to produce latent representations that are both monosemantic and interpretable. We detail the technical implementation of the thresholded temporal exclusive loss and outline an experimental framework for assessing its impact on interpretability benchmarks."
    },
    {
        "Name": "adaptive_threshold_temporal_exclusive_sae",
        "Title": "Adaptive Threshold Temporal Exclusive Regularization for Enhanced Latent Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class to include an adaptive temporal exclusivity term with a threshold. Update the moving average vector for each latent feature using an adaptive momentum that depends on the activation frequency in the current batch, but only update if the activation rate exceeds a predefined threshold. Compute the cosine similarity between current non-zero latent activations and their respective normalized moving averages, and evaluate the effect on benchmarks such as sparse_probing and core.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f, compute the activation rate for each latent dimension. For dimensions where activation_rate[d] is above a specified threshold (e.g., 0.05), compute an adaptive momentum beta_eff[d] = beta + (1 - beta) * (1 - activation_rate[d]). Update the moving average f_avg[d] = beta_eff[d] * f_avg[d] + (1 - beta_eff[d]) * mean(f[d] for samples where f[d] > 0). Normalize f_avg[d] to unit norm for stability. For dimensions below the threshold, skip the update to avoid incorporating noise. Next, compute the cosine similarity between each non-zero f[d] and its corresponding f_avg[d], average these similarities over all active dimensions to obtain the temporal_exclusive_loss, and add lambda_temporal * temporal_exclusive_loss to the total loss: loss_total = reconstruction_loss + auxk_alpha * auxk_loss + lambda_temporal * temporal_exclusive_loss.",
        "Implementation_Plan": "1. In TrainerTopK, initialize a moving average vector f_avg for latent features to zeros. 2. In the loss function, for each batch compute the activation_rate for each latent dimension. 3. For each latent dimension d with activation_rate[d] > threshold (e.g., 0.05), calculate adaptive beta_eff[d] = beta + (1 - beta) * (1 - activation_rate[d]) and update f_avg[d] from non-zero activations; otherwise, skip the update. 4. Normalize f_avg[d] for updated dimensions to unit norm. 5. Compute cosine similarities between the current non-zero activations and their normalized f_avg. 6. Average these similarities over all active dimensions to obtain temporal_exclusive_loss, multiply by lambda_temporal, and add to the overall loss. 7. Log temporal_exclusive_loss for analysis, then train and evaluate the model on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Incorporating an activation threshold to adaptively update the moving average adds a practical twist to our temporal regularization, making the idea technically richer and more nuanced.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "The additional threshold check is straightforward to implement within the current training loop, requiring minimal code changes and computational overhead, ensuring high feasibility.",
        "Feasibility": 8.5,
        "Novelty_Evaluation": "Introducing a threshold into the adaptive momentum update of latent activations is a novel enhancement that refines the temporal regularization approach without complicating the core architecture.",
        "Novelty": 8.5,
        "Expected_Research_Impact": "This approach is expected to yield more stable and representative moving averages for latent features, thereby promoting stronger temporal divergence and specialization, which is anticipated to have a positive impact on interpretability benchmarks.",
        "Research_Impact": 8.5,
        "Overall_Score": 8.8,
        "Abstract": "We present an adaptive threshold temporal exclusive regularization method for sparse autoencoders that enhances the interpretability of latent representations. Our approach maintains a moving average of latent activations for each feature using an adaptive momentum update based on batch activation frequencies, with updates performed only when the activation rate exceeds a predefined threshold. By penalizing the cosine similarity between current non-zero activations and their normalized historical averages, the method promotes sustained divergence and specialization of latent features over training iterations. Implementation details and integration with existing training frameworks are provided."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Consistent Latent Interpretability",
        "Experiment": "Modify the loss function in the TrainerTopK class by introducing a temporal uniform activation regularization term. This term computes an exponentially weighted moving average (EWMA) of normalized activation frequencies across batches and penalizes deviations from a predefined uniform distribution. The new loss term will be integrated with the existing reconstruction and auxiliary losses. Performance will be assessed on sparse_probing and core benchmarks.",
        "Technical_Details": "In the TrainerTopK class, initialize an EWMA vector (self.ewma_activation) of length dict_size with zeros. In the loss() function, after computing topk indices, calculate the per-feature activation counts normalized by (batch_size * k). Update the EWMA with the formula: ewma = beta * ewma + (1 - beta) * (current_activation / (batch_size * k)), where beta is a tunable decay factor (e.g., 0.9). For stability, begin updating the EWMA only after a warm-up period (a fixed number of batches). Define the target uniform frequency as 1/dict_size for every latent, and compute an L2 penalty between the EWMA vector and this target. Scale the penalty with a hyperparameter \"lambda_temporal\" and add it to the overall loss. Log this term separately to monitor its evolution over time.",
        "Implementation_Plan": "1. Add a new instance variable (self.ewma_activation) in the TrainerTopK constructor, initialized as a zero vector of length dict_size. 2. Introduce tunable hyperparameters: beta (the decay factor), lambda_temporal (scaling the temporal penalty), and a warm-up batch count before EWMA updates begin. 3. In the loss() function, after obtaining the topk indices, compute the normalized activation counts per latent. 4. If the current batch count exceeds the warm-up threshold, update the EWMA vector using the specified formula. 5. Calculate the L2 penalty between the EWMA and a target uniform frequency vector (1/dict_size for each latent), and scale it using lambda_temporal. 6. Add this temporal uniform activation loss to the existing loss computation. 7. Log the new loss term for monitoring during training. 8. Evaluate the model with this modified loss on the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Integrating a temporal component into activation regularization adds a novel dimension that stabilizes latent feature usage over time.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "These modifications require only minor adjustments to the existing loss computation and integration of a stateful buffer, making the approach highly feasible within current training frameworks.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The idea refines standard activation regularization by incorporating temporal dynamics and initialization safeguards, which is a fresh approach in the context of mechanistic interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "Consistent enforcement of uniform activation over time is expected to yield more interpretable and stable latent representations, supporting improved performance on mechanistic interpretability benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We propose a temporal uniform activation regularization for sparse autoencoders to address latent polysemanticity by enforcing a consistent, balanced usage of latent features over time. Our method maintains an exponentially weighted moving average of normalized activation frequencies and penalizes deviations from a target uniform distribution via an L2 loss. This approach, integrated into the standard reconstruction objective, aims to produce latent representations with improved interpretability while preserving overall model performance. The methodology is evaluated in the context of mechanistic interpretability tasks."
    },
    {
        "Name": "temporal_exclusive_threshold_sae",
        "Title": "Temporal Exclusive Sparsity Regularization with Activation Thresholding for Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK.loss function to incorporate a dynamic exclusive loss term that computes the cosine similarity between strong latent activations from tokens within a defined temporal window, with both activations required to exceed a specified threshold. Compare interpretability and sparsity metrics from benchmarks such as sparse_probing and core between models that use this enhanced loss and baseline SAEs.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (shape [batch_size, num_features]) along with token positions, first filter out activations below a threshold 'tau'. For token pairs (i, j) where both activations exceed 'tau' and |position_i - position_j| <= window_size, compute the cosine similarity of their active latent vectors weighted by the product of their activation magnitudes. Aggregate these weighted similarities to produce the dynamic exclusive loss term, exclusive_loss_dynamic. Introduce hyperparameters lambda_dynamic to weight this term and tau for the activation threshold, and update the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic.",
        "Implementation_Plan": "1. In TrainerTopK.loss, obtain token positions along with latent activations f. 2. Filter f to ignore activation values below a defined threshold tau. 3. For each remaining token pair within a temporal window (|t_i - t_j| <= window_size), compute the cosine similarity and weight it by the product of their activation magnitudes. 4. Aggregate these values to form exclusive_loss_dynamic and multiply by lambda_dynamic. 5. Add this term to the existing loss computation. 6. Log this new term for tracking and run experiments on sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Integrating an activation threshold to focus the temporal exclusive penalty refines the concept, making the loss more targeted and the idea distinctly innovative.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required modifications are minimal and confined to loss computation and filtering, ensuring they can be executed within one month by a junior PhD student on common GPU hardware.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adding a threshold to the temporal regularization is a novel extension that more precisely addresses redundant feature activations in a sequential context.",
        "Novelty": 8,
        "Expected_Research_Impact": "This refined approach is expected to yield clearer and more disentangled latent representations by precisely penalizing only significant redundant activations in temporal proximity, positively influencing core and sparse_probing measures.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refined variant of sparse autoencoders that introduces temporal exclusive sparsity regularization enhanced with activation thresholding. Our approach penalizes the cosine similarity between latent activations that are both above a predefined threshold and occur within a fixed temporal window, thereby specifically targeting redundant, high-magnitude activations. This selective penalty is integrated into the overall loss function alongside reconstruction and auxiliary losses. We detail the technical modifications and implementation steps required to incorporate this loss term, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "temporal_exclusive_sae_v2",
        "Title": "Enhanced Temporal Exclusive Sparsity Regularization with Normalization for Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK.loss function to include both an instantaneous exclusivity loss and a temporal consistency loss with added latent activation normalization and thresholding. Evaluate the updated SAE on benchmarks such as sparse_probing and core, comparing it to the baseline SAE.",
        "Technical_Details": "For each training batch, first compute the latent activation matrix f and normalize each feature vector to unit norm to ensure robust cosine similarity calculations. Then, compute the instantaneous exclusive_loss by averaging the pairwise cosine similarity across non-zero, normalized activations, excluding self-similarities. Next, maintain a moving average for each latent feature's normalized activation profile using an exponential decay update with a specified momentum parameter, but update only if the activation magnitude exceeds a defined threshold. Compute temporal_loss as the cosine similarity between the current batch's average normalized activation for each latent (only for those with sufficient activation) and its historical moving average. The total loss is given by: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exclusive * (exclusive_loss + temporal_loss). This formulation encourages both immediate and temporal exclusivity and consistency in the latent representations.",
        "Implementation_Plan": "1. In the TrainerTopK class, add a buffer for storing the moving average of each latent feature's normalized activation, and initialize it during the first batch. 2. In the loss function, after computing f, normalize each latent activation vector to unit norm. 3. Calculate the exclusive_loss by computing pairwise cosine similarities (excluding self-similarities) among non-zero normalized activations, and average these similarities. 4. Compute the average latent activation per feature for the current batch, and update the moving average using an exponential decay with a momentum parameter, but only for activations above a predefined threshold. 5. Compute the temporal_loss as the cosine similarity between the current average and the stored moving average for each latent feature. 6. Add lambda_exclusive*(exclusive_loss+temporal_loss) to the loss, and log both loss components. 7. Run training experiments and evaluate using sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Integrating activation normalization and thresholding within the temporal exclusive regularization framework adds a practical refinement that enhances robustness, making the idea both interesting and impactful.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The modifications involve standard tensor operations and conditional updates that are straightforward to implement and compute efficient on current hardware setups, ensuring high feasibility.",
        "Feasibility": 9,
        "Novelty_Evaluation": "This approach, although a refinement of established regularization techniques, provides a novel twist on temporal consistency by incorporating normalization and selective updating, in line with the prototype concept.",
        "Novelty": 8,
        "Expected_Research_Impact": "By further refining temporal consistency and exclusivity regularization, we expect clearer and more stable latent features, which should positively influence interpretability benchmarks without compromising reconstruction accuracy.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We propose an enhanced variant of sparse autoencoders that integrates temporal exclusive sparsity regularization with activation normalization and thresholding. Our method first normalizes latent activations to ensure robust cosine similarity computations, then computes an instantaneous exclusivity loss by penalizing pairwise similarities among non-zero activations. Simultaneously, a moving average of normalized activations is maintained using an exponential decay update, applied only to sufficiently active features, to compute a temporal consistency loss. The integrated loss function promotes both immediate and temporal exclusivity in the latent representations while preserving reconstruction fidelity. This paper details the implementation and evaluation of the proposed approach."
    },
    {
        "Name": "temp_weighted_exclusive_sae",
        "Title": "Temporally Weighted Exclusive Sparsity Regularization for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by introducing a temporally weighted exclusive loss term. This term computes the cosine similarity between pairs of strong latent activations for tokens within a defined temporal window, weighting the similarity by both the product of activation magnitudes and an exponential decay function based on the temporal distance between tokens. Evaluate the impact on interpretability benchmarks such as sparse_probing and core, comparing against the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f and corresponding token position indices, restrict pairwise cosine similarity computations to tokens within a predetermined temporal window. For each valid pair (i, j), compute the cosine similarity of their active latent vectors, weight this similarity by the product of their activation magnitudes, and further weight it by an exponential decay function exp(-|pos_i - pos_j|) to emphasize closer token pairs. Exclude self-similarity, then average the weighted similarities to form the exclusive_loss term. Integrate this into the overall loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exclusive * exclusive_loss, with lambda_exclusive as a tunable hyperparameter.",
        "Implementation_Plan": "1. In TrainerTopK.loss, after computing the latent activation matrix f, extract token position indices from the batch. 2. Define a temporal window size and compute pairwise cosine similarities for tokens within this window. 3. For each valid token pair, calculate the weight as the product of activation magnitudes multiplied by exp(-|pos_i - pos_j|). 4. Exclude diagonal entries, average the weighted cosine similarities to compute exclusive_loss, and introduce lambda_exclusive to modulate its contribution. 5. Log the exclusive_loss for monitoring during training. 6. Run training and evaluation on benchmarks (sparse_probing and core) and compare results with the baseline SAE.",
        "Interestingness_Evaluation": "By integrating a decay-based temporal weighting into the exclusive loss, the idea smartly leverages sequence structure to further refine latent exclusivity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The added complexity of computing exponential decay on temporal differences is minimal and can be efficiently implemented in the existing loss function, ensuring feasibility on standard hardware with modest computational overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The approach uniquely combines dynamic activation weighting with an explicit exponential decay based on temporal distance, marking a novel yet straightforward refinement within the framework of dynamic exclusive regularization.",
        "Novelty": 8,
        "Expected_Research_Impact": "Focusing the regularization more accurately on temporally adjacent tokens is expected to yield clearer, more distinct latent representations, likely benefiting the sparse_probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We propose a refined variant of sparse autoencoders that improves mechanistic interpretability through temporally weighted exclusive sparsity regularization. The method penalizes redundant latent activations by computing a weighted pairwise cosine similarity for tokens within a predefined temporal window, where the weighting incorporates both the products of activation strengths and an exponential decay function based on token distance. This targeted regularization encourages each latent feature to capture a distinct, monosemantic concept, while maintaining reconstruction quality. The approach requires minimal modifications to the existing loss function, effectively leveraging the inherent sequential structure of language."
    },
    {
        "Name": "temporal_contrastive_regularization",
        "Title": "Temporal Contrastive Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Extend the loss function in TrainerTopK by adding two additional terms: one based on pairwise cosine similarity among the top-k activated latent features within each token, and another that enforces temporal consistency by comparing similarly activated latent features in adjacent tokens in the input sequence. Conduct experiments by evaluating the modified loss function against the baseline SAE on sparse probing and core benchmarks.",
        "Technical_Details": "After obtaining the top-k activated latent features in the encode() function, compute two types of cosine similarity penalties. First, compute the pairwise cosine similarity between distinct top-k latent vectors within each sample, excluding the diagonal, and average these to form an 'intra-token' separation loss. Second, for adjacent tokens in the input sequence, identify overlapping activated features (using their indices) and compute the cosine similarity between these corresponding latent vectors, averaging these values to form a 'temporal consistency' loss. Each term is weighted by hyperparameters 'contrastive_weight' and 'temporal_weight' respectively. The overall loss becomes the sum of reconstruction loss, intra-token contrastive loss, and temporal consistency loss. This regularization scheme promotes both feature distinctiveness and temporal stability, reducing feature absorption and polysemanticity without adding architectural complexity.",
        "Implementation_Plan": "1. In TrainerTopK.loss(), after computing top-k activations and indices for each sample, compute the normalized latent vectors. 2. Calculate the intra-token cosine similarity matrix for each sample, exclude diagonal values, and average the off-diagonals. 3. For a given batch arranged in sequence order, identify matching latent indices in adjacent tokens and compute their cosine similarity, then average these values to obtain the temporal consistency term. 4. Introduce hyperparameters 'contrastive_weight' and 'temporal_weight' to balance the new loss terms. 5. Add both weighted loss terms to the original reconstruction loss. 6. Run an ablation study varying these weights to determine the optimal trade-off. 7. Evaluate the modified SAE on sparse probing and core benchmarks.",
        "Interestingness_Evaluation": "This idea is highly interesting as it innovatively integrates temporal dynamics into feature separation to address polysemanticity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve additional cosine similarity calculations and averaging, operations that are computationally lightweight and can be efficiently implemented in the existing framework within a month by a junior PhD student.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Incorporating temporal consistency into contrastive loss for SAEs is a novel approach to enhance latent interpretability without altering the fundamental architecture.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed temporal contrastive regularization is expected to produce more distinct and stable latent features, improving interpretability on the sparse probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an enhancement to sparse autoencoders that integrates temporal contrastive regularization to improve latent interpretability. By extending the loss function to include both an intra-token contrastive term and a temporal consistency term, our method encourages activated latent features to be both distinct within each token and consistent across adjacent tokens. This regularization approach is directly incorporated into the existing SAE training framework without introducing additional architectural complexity. We detail the implementation of both loss terms and outline an experimental plan to evaluate the impact on benchmarks such as sparse probing and core. The proposed method offers a promising direction for reducing feature absorption and polysemanticity in neural representations."
    },
    {
        "Name": "thresh_dynamic_exclusive_sae",
        "Title": "Thresholded Dynamic Exclusive Sparsity Regularization with Temporal Awareness for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a thresholded dynamic exclusive loss term. The term computes cosine similarities between pairs of strong latent activations for tokens within a fixed temporal window, weights the penalties by their activation magnitudes, and only penalizes pairs whose similarity exceeds a predefined threshold. Compare interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced model and the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and associated token position indices, restrict pairwise cosine similarity computation to token pairs within a temporal window defined by a hyperparameter window_size. For each valid pair (i, j) with token positions t_i and t_j such that |t_i - t_j| <= window_size, compute the cosine similarity of their active latent vectors. Weight each similarity by the product of the activation magnitudes and apply a threshold (cosine_threshold) so that only similarities above this threshold contribute to the exclusive loss. Aggregate the thresholded, weighted similarities to form exclusive_loss_dynamic_thresh and incorporate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_thresh.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, track token positions along with activations. 2. Identify token pairs within a fixed temporal window (e.g., tokens within 2 positions of each other). 3. Compute cosine similarity for these pairs and weight them by the product of their activation magnitudes. 4. Apply a threshold, cosine_threshold, to ignore similarities below this value. 5. Average the remaining weighted similarities to yield exclusive_loss_dynamic_thresh. 6. Introduce a hyperparameter lambda_dynamic to scale this loss term and add it to the overall loss. 7. Log the new loss term and run experiments to evaluate its impact on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing a threshold mechanism to focus on penalizing only high redundancy in latent activations adds a novel refinement without significant complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve straightforward computations within the loss function and add only minimal hyperparameters; implementation and experimentation should be feasible within the given constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a thresholded penalty in a temporal context provides a unique angle to existing regularization techniques in sparse autoencoders, enhancing the model's interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only high similarity between temporally adjacent tokens, the method should yield more distinct and interpretable latent features, potentially improving performance on core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a thresholded dynamic exclusive sparsity regularization, augmented with temporal awareness. Our approach selectively penalizes high cosine similarity between latent activations from temporally proximate tokens, weighting the penalty by the product of activation magnitudes and applying a threshold to disregard minor similarities. This additional loss term is integrated with the standard reconstruction and auxiliary losses, promoting monosemantic latent features without introducing significant complexity. We detail the technical formulation and implementation steps for this method, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "thresh_dynamic_exclusive_sae",
        "Title": "Thresholded Dynamic Exclusive Sparsity Regularization with Temporal Awareness for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a thresholded dynamic exclusive loss term. The term computes cosine similarities between pairs of strong latent activations for tokens within a fixed temporal window, weights the penalties by their activation magnitudes, and only penalizes pairs whose similarity exceeds a predefined threshold. Compare interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced model and the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and associated token position indices, restrict pairwise cosine similarity computation to token pairs within a temporal window defined by a hyperparameter window_size. For each valid pair (i, j) with token positions t_i and t_j such that |t_i - t_j| <= window_size, compute the cosine similarity of their active latent vectors. Weight each similarity by the product of the activation magnitudes and apply a threshold (cosine_threshold) so that only similarities above this threshold contribute to the exclusive loss. Aggregate the thresholded, weighted similarities to form exclusive_loss_dynamic_thresh and incorporate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_thresh.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, track token positions along with activations. 2. Identify token pairs within a fixed temporal window (e.g., tokens within 2 positions of each other). 3. Compute cosine similarity for these pairs and weight them by the product of their activation magnitudes. 4. Apply a threshold, cosine_threshold, to ignore similarities below this value. 5. Average the remaining weighted similarities to yield exclusive_loss_dynamic_thresh. 6. Introduce a hyperparameter lambda_dynamic to scale this loss term and add it to the overall loss. 7. Log the new loss term and run experiments to evaluate its impact on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing a threshold mechanism to focus on penalizing only high redundancy in latent activations adds a novel refinement without significant complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve straightforward computations within the loss function and add only minimal hyperparameters; implementation and experimentation should be feasible within the given constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a thresholded penalty in a temporal context provides a unique angle to existing regularization techniques in sparse autoencoders, enhancing the model's interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only high similarity between temporally adjacent tokens, the method should yield more distinct and interpretable latent features, potentially improving performance on core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a thresholded dynamic exclusive sparsity regularization, augmented with temporal awareness. Our approach selectively penalizes high cosine similarity between latent activations from temporally proximate tokens, weighting the penalty by the product of activation magnitudes and applying a threshold to disregard minor similarities. This additional loss term is integrated with the standard reconstruction and auxiliary losses, promoting monosemantic latent features without introducing significant complexity. We detail the technical formulation and implementation steps for this method, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "temporal_exclusive_sae",
        "Title": "Temporal Exclusive Sparsity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class by incorporating a temporal exclusive loss that operates on pooled latent activations across consecutive token segments. Compare the interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the baseline SAE and the temporal exclusive variant.",
        "Technical_Details": "For each training batch, the latent activation matrix f (shape: [batch_size, num_features]) is partitioned into temporal segments by grouping tokens (or pseudo-sequences). Mean pooling is applied to each segment to derive temporal latent representations. A pairwise cosine similarity matrix is computed over these pooled representations, excluding self-similarities. The temporal exclusive loss is defined as the average of these off-diagonal cosine similarity values and is scaled by a hyperparameter lambda_exclusive. The overall loss is then computed as: loss_total = reconstruction loss + lambda_aux * aux_loss + lambda_exclusive * temporal_exclusive_loss, which encourages latent features to be distinct across time.",
        "Implementation_Plan": "1. Modify TrainerTopK.loss to reshape or group the latent activations into temporal segments based on token sequences. 2. Apply mean pooling within each segment to obtain temporal latent representations. 3. Compute the cosine similarity matrix among these pooled segments, remove self-similarity terms, and calculate the temporal exclusive loss. 4. Introduce a hyperparameter lambda_exclusive to weight this loss component and add it to the overall loss. 5. Log the temporal exclusive loss for monitoring purposes. 6. Run the standard training loop and evaluate using sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Integrating a temporal component into the exclusive sparsity framework is a clear and innovative extension that provides refined control over latent feature uniqueness, making it highly interesting.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required modifications are minor\u2014simply pooling and cosine similarity computations\u2014and can be implemented within the existing training infrastructure, ensuring high feasibility.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Adding temporal exclusivity to the sparsity regularization setup introduces a novel twist in enforcing monosemantic latent representations in SAEs, offering a fresh perspective in the field.",
        "Novelty": 8,
        "Expected_Research_Impact": "This approach is expected to yield more contextually distinct latent features that enhance interpretability, potentially leading to significant improvements on benchmarks like sparse_probing and core.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose a temporal exclusive sparsity regularization technique for sparse autoencoders that enhances mechanistic interpretability by incorporating temporal pooling of latent activations. Our approach partitions latent activations into temporal segments and penalizes redundant activations across these segments using a cosine similarity-based loss term. The resulting loss function encourages each latent feature to capture a distinct and temporally relevant concept, without modifying the underlying architecture. We describe the technical details of this modification and outline the experimental setup designed to evaluate its impact on interpretability benchmarks."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Enhancing Uniform Activation Loss with Temporal Smoothing for Consistent Latent Utilization",
        "Experiment": "Modify the loss function in the TrainerTopK class to include a temporal smoothing term. Maintain an exponentially decaying moving average of per-batch latent activation frequencies, and include two penalty terms: one that penalizes the L2 deviation of the current batch's activation frequencies from a target uniform frequency, and another that penalizes abrupt deviations from the moving average. The combined loss is then evaluated on the sparse_probing and core benchmarks.",
        "Technical_Details": "For each training batch, compute the frequency vector for SAE latent activations from the topk indices. Define the target frequency as (k * batch_size) / dict_size for each latent. Update a moving average vector using momentum, e.g., moving_avg = beta * moving_avg + (1 - beta) * current_frequency, where beta is a decay parameter. Then, compute an L2 penalty (lambda_uniform) between the current batch frequency and the target frequency, and an additional L2 penalty (lambda_temp) between the current batch frequency and the moving average. The total loss is the sum of the reconstruction loss, the auxiliary loss for dead features, and these two new regularization terms.",
        "Implementation_Plan": "1. In TrainerTopK, add a new state variable (e.g., self.moving_avg_freq) initialized as a zero-vector of size dict_size. 2. Within the loss() function after obtaining topk indices, count the activation frequency per latent for the batch. 3. Update the moving average frequency using a momentum parameter beta (e.g., 0.9). 4. Compute the target frequency as (k * batch_size) / dict_size. 5. Calculate two L2 loss terms: one between the current frequency and the target frequency scaled by lambda_uniform, and one between the current frequency and the moving average scaled by lambda_temp. 6. Add these terms to the overall loss and log them. 7. Evaluate the modified SAE on the sparse_probing and core benchmarks to examine interpretability improvements.",
        "Interestingness_Evaluation": "The idea is engaging as it instills temporal consistency into latent modulation, promising a more stable and interpretable feature space.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The added moving average calculation and associated loss terms require minimal state maintenance and only slight modifications to the existing TrainerTopK, making it very feasible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Incorporating temporal smoothing into uniform activation regularization offers a novel twist that enhances latent consistency without increasing complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "The temporal smoothing is expected to further stabilize latent usage over training, improving interpretability on sparse probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We present an enhancement to sparse autoencoders by integrating a temporal smoothing component into the uniform activation regularization. This approach maintains a moving average of latent activation frequencies and penalizes both deviations from a target uniform distribution and abrupt fluctuations relative to this moving average. The proposed method requires only minimal modifications to the existing training framework, encouraging both balanced and temporally consistent latent utilization, which is anticipated to yield more interpretable representations."
    },
    {
        "Name": "temporal_overlapping_exclusive_sae",
        "Title": "Overlapping Temporal Exclusive Sparsity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by applying overlapping sliding windows on latent activations to perform temporal pooling and then compute pairwise cosine similarities across these overlapping segments to create an overlapping temporal exclusivity penalty. Evaluate this model on benchmarks such as sparse_probing and core to assess the interpretability of the latent features.",
        "Technical_Details": "For each training batch, apply a sliding window (with specified window_size and stride) to the latent activation matrix f. For each overlapping segment, perform mean pooling to obtain a per-segment representation. Then, compute a pairwise cosine similarity matrix among these pooled representations, ignoring self-similarity. Define the overlapping temporal exclusive loss as the mean of these off-diagonal similarities scaled by the hyperparameter lambda_exclusive. Adjust the total training loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_exclusive * overlapping_temporal_exclusive_loss. This ensures smoother enforcement of temporal exclusivity among latent features across adjacent time steps.",
        "Implementation_Plan": "1. In TrainerTopK.loss, after computing the latent activations f, implement a sliding window function to extract overlapping temporal segments (using parameters window_size and stride). 2. Perform mean pooling over each segment to generate a pooled representation. 3. Compute the pairwise cosine similarity matrix among these representations while excluding self-similarity, and then average the off-diagonal terms to obtain the overlapping_temporal_exclusive_loss. 4. Introduce and tune the hyperparameter lambda_exclusive to control the penalty's weight. 5. Integrate this term into the total loss computation, log it during training, and subsequently evaluate the modified model on sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "The use of overlapping sliding windows for temporal exclusivity adds a nuanced enhancement to feature decorrelation, making the approach both conceptually interesting and practically appealing.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "This modification involves straightforward pooling and cosine similarity computations with overlapping windows, which can be implemented efficiently with minimal changes to the existing codebase.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Employing overlapping temporal pooling within the exclusivity loss is a novel twist that deepens rather than widens the previous approach, offering a fine-grained regularization of latent features.",
        "Novelty": 8,
        "Expected_Research_Impact": "By promoting smoother temporal exclusivity, the latent activations are expected to be more interpretable and distinct, which is likely to positively impact measurements on benchmarks like sparse_probing and core.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose an overlapping temporal exclusive sparsity regularization method for sparse autoencoders that incorporates a sliding window mechanism to pool latent activations over overlapping temporal segments. This approach computes pairwise cosine similarities among the pooled representations and penalizes redundant activations to encourage distinct, temporally consistent latent features. We detail the loss function modifications and outline the experimental framework designed to assess the interpretability of the latent space using standard benchmarks."
    },
    {
        "Name": "thresh_dynamic_exclusive_sae",
        "Title": "Thresholded Dynamic Exclusive Sparsity Regularization with Temporal Awareness for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a thresholded dynamic exclusive loss term. The term computes cosine similarities between pairs of strong latent activations for tokens within a fixed temporal window, weights the penalties by their activation magnitudes, and only penalizes pairs whose similarity exceeds a predefined threshold. Compare interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced model and the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and associated token position indices, restrict pairwise cosine similarity computation to token pairs within a temporal window defined by a hyperparameter window_size. For each valid pair (i, j) with token positions t_i and t_j such that |t_i - t_j| <= window_size, compute the cosine similarity of their active latent vectors. Weight each similarity by the product of the activation magnitudes and apply a threshold (cosine_threshold) so that only similarities above this threshold contribute to the exclusive loss. Aggregate the thresholded, weighted similarities to form exclusive_loss_dynamic_thresh and incorporate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_thresh.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, track token positions along with activations. 2. Identify token pairs within a fixed temporal window (e.g., tokens within 2 positions of each other). 3. Compute cosine similarity for these pairs and weight them by the product of their activation magnitudes. 4. Apply a threshold, cosine_threshold, to ignore similarities below this value. 5. Average the remaining weighted similarities to yield exclusive_loss_dynamic_thresh. 6. Introduce a hyperparameter lambda_dynamic to scale this loss term and add it to the overall loss. 7. Log the new loss term and run experiments to evaluate its impact on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing a threshold mechanism to focus on penalizing only high redundancy in latent activations adds a novel refinement without significant complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve straightforward computations within the loss function and add only minimal hyperparameters; implementation and experimentation should be feasible within the given constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a thresholded penalty in a temporal context provides a unique angle to existing regularization techniques in sparse autoencoders, enhancing the model's interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only high similarity between temporally adjacent tokens, the method should yield more distinct and interpretable latent features, potentially improving performance on core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a thresholded dynamic exclusive sparsity regularization, augmented with temporal awareness. Our approach selectively penalizes high cosine similarity between latent activations from temporally proximate tokens, weighting the penalty by the product of activation magnitudes and applying a threshold to disregard minor similarities. This additional loss term is integrated with the standard reconstruction and auxiliary losses, promoting monosemantic latent features without introducing significant complexity. We detail the technical formulation and implementation steps for this method, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "temporal_uniform_activation",
        "Title": "Temporal Uniform Activation Loss for Enhanced Latent Interpretability",
        "Experiment": "Modify the loss function in TrainerTopK to include a temporal uniform activation loss. Maintain a running average of latent activations across training steps and compute an L2 penalty that measures the deviation of this running average from a uniform distribution. Integrate this loss term into the overall training objective and evaluate its effect on the core and sparse_probing benchmarks.",
        "Technical_Details": "During each training step, after obtaining the top-k activated latents, update a running average activation frequency vector for the dictionary. Normalize this vector so that it represents a probability distribution. Define the target distribution as uniform (each latent should be activated with probability 1/dictionary_size). Compute the L2 loss between the running average and the uniform distribution, scale it with a hyperparameter lambda_temporal, and add it to the overall loss. This term encourages a balanced and steady activation distribution over time without modifying the network architecture.",
        "Implementation_Plan": "1. In the TrainerTopK class in experiment.py, initialize a running average vector for latent activations with zeros. 2. At every training step, update this vector using the top-k indices from the encoder output (e.g., using an exponential moving average). 3. Normalize the running average to form a probability distribution. 4. Compute the L2 penalty between the running average and a uniform distribution vector (each entry equal to 1/dictionary_size). 5. Include the scaled penalty term in the overall loss by adding it to the reconstruction loss and auxiliary losses. 6. Tune the hyperparameter lambda_temporal and evaluate the outcomes on the core and sparse_probing benchmarks.",
        "Interestingness_Evaluation": "This idea is interesting because temporal smoothing provides a more stable and long-term incentive for balanced feature activation, potentially reducing polysemanticity effectively.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation is straightforward as it only requires maintaining a running average and updating the loss computation, ensuring minimal modification to the existing code.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Incorporating temporal statistics into uniform activation loss for sparse autoencoders is a novel extension that refines existing regularization methods.",
        "Novelty": 8,
        "Expected_Research_Impact": "By enforcing a steady, balanced activation pattern over time, this method is expected to enhance the interpretability of latent features as measured by the core and sparse_probing benchmarks.",
        "Research_Impact": 8,
        "Overall_Score": 8.8,
        "Abstract": "We propose a modification to sparse autoencoder training by incorporating a temporal uniform activation loss term into the loss function. This term maintains a running average of latent activations over training steps and penalizes deviations from a target uniform distribution. The proposed regularization encourages balanced activation across all latent features, thereby aiming to reduce polysemanticity and improve mechanistic interpretability. We present a detailed discussion of the implementation and plan to validate the method on established benchmarks for sparse autoencoders."
    },
    {
        "Name": "adaptive_temporal_uniform_activation_loss",
        "Title": "Adaptive Temporal Uniform Activation Loss for Stabilized Latent Feature Usage",
        "Experiment": "Enhance the loss function in the TrainerTopK class by introducing an adaptive temporal uniform activation regularization term. This involves computing the per-batch latent activation frequencies, maintaining a moving average of these frequencies, and applying two L2 penalties: one for the deviation from a uniform target and another for deviation from the moving average. Crucially, the temporal penalty weight (lambda_temporal) decays over time following a predefined schedule. The modified SAE will be evaluated on the sparse_probing and core benchmarks.",
        "Technical_Details": "Within each training batch, compute the activation count for each latent from the topk indices. Set the target frequency as (k * batch_size) / dict_size. Maintain a moving average of activation frequencies using exponential smoothing. Introduce two L2 penalties: the uniform loss (difference between current counts and target) scaled by lambda_uniform, and the temporal loss (difference between current counts and the moving average) scaled by lambda_temporal. Implement an adaptive decay for lambda_temporal, such that its value decreases linearly (or exponentially) from an initial high value to a lower bound as training progresses. Add the sum of these penalties to the overall loss alongside reconstruction and auxiliary losses.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize a state variable 'moving_avg_freq' with the target frequency and a variable 'current_lambda_temporal' set to an initial value. 2. In the loss() function, compute the per-feature activation counts from the topk indices. 3. Calculate the uniform activation loss as the L2 norm between these counts and the target frequency. 4. Calculate the temporal loss as the L2 norm between these counts and the moving_avg_freq. 5. Update the moving_avg_freq using exponential smoothing. 6. Update 'current_lambda_temporal' based on a decay schedule (e.g., linear decay over training steps). 7. Add the scaled sum of the uniform loss (multiplied by lambda_uniform) and the temporal loss (multiplied by current_lambda_temporal) to the total loss. 8. Log both loss components and the current lambda value for monitoring. 9. Evaluate the updated SAE on the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "The adaptive scheduling of the temporal penalty adds a dynamic aspect to the regularization that is both innovative and conceptually appealing in the context of mechanistic interpretability.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "The implementation only requires modifications to the loss function and the addition of a decay schedule for a hyperparameter, keeping the overall complexity minimal and the experiment highly feasible.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adaptive scheduling in activation regularization introduces a novel refinement that balances early training stability with later flexibility, a new twist in this context.",
        "Novelty": 8,
        "Expected_Research_Impact": "This modification is expected to further stabilize latent usage over time, contributing to more interpretable latent representations and improved performance on monitoring metrics.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an adaptive temporal uniform activation regularization for sparse autoencoders to promote balanced and consistent latent feature usage. Our method computes per-batch activation frequencies and maintains a moving average to provide a temporal reference, applying L2 penalties for deviations from both a uniform target and the moving average. Crucially, the temporal penalty weight is adaptively decayed over time to allow early strong regularization while gradually permitting adaptive fine-tuning. This method is integrated as a minimal modification to the existing loss function, aiming to yield latent representations that are more stable and interpretable."
    },
    {
        "Name": "dynamic_uniform_activation_loss",
        "Title": "Dynamic Uniform Activation Loss for Enhanced Latent Interpretability",
        "Experiment": "Modify the loss function in the TrainerTopK class by adding a dynamic uniform activation regularization term that scales with training progress. This term computes the activation frequency of each latent from the topk indices during training, evaluates the L2 error between the observed and the target uniform frequency, and multiplies it by a time-dependent lambda_uniform that increases over training steps. Results will be obtained by comparing the updated SAE on the sparse_probing and core benchmarks.",
        "Technical_Details": "During each training batch, compute the per-feature activation counts based on the topk indices. Define the target activation frequency as (k * batch_size) / dict_size. Calculate the L2 distance between the observed frequency vector and this target. Introduce a hyperparameter lambda_uniform that is dynamically scheduled (for instance, lambda_uniform = base_value * (current_step / total_steps)) so that the penalty increases over time. This dynamic regularization encourages balanced latent usage without hindering early-stage learning, thus mitigating polysemanticity and enhancing interpretability.",
        "Implementation_Plan": "1. In the TrainerTopK loss() function, after obtaining top_indices from self.ae.encode(x, return_topk=True), compute the activation counts for each latent over the batch. 2. Calculate the target frequency for each latent as (k * batch_size) / dict_size. 3. Compute the L2 penalty between the observed frequency vector and the target frequency vector. 4. Define a dynamic lambda_uniform as a function of the current training step (e.g., base_value * current_step / total_steps). 5. Add the weighted penalty to the overall loss. 6. Log the dynamic lambda_uniform value and the uniform activation loss term during training to monitor its effect. 7. Evaluate the modified SAE on the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "The idea is compelling as it introduces a simple dynamic regularizer that directly targets latent imbalance and polysemanticity with minimal architectural change.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The dynamic scheduling of the regularization term requires only minor modifications to the loss computation and can be effectively integrated into the existing training workflow without significant computational overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While activation regularization is known, applying a dynamic uniformity constraint tailored to the training progress for SAE mechanistic interpretability represents a novel twist in this context.",
        "Novelty": 8,
        "Expected_Research_Impact": "By adapting the regularization strength over time, the model is expected to develop better-separated and monosemantic latent features, potentially yielding improved performance on the sparse_probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We propose a dynamic uniform activation regularization for sparse autoencoders that addresses polysemanticity by promoting a balanced utilization of latent features. This technique dynamically scales the regularization penalty during training, allowing the model to first learn robust features and subsequently enforce uniform activation frequencies. The regularization term, integrated as an auxiliary loss based on the deviation of latent activation frequencies from a predefined target, offers a straightforward enhancement to the standard training objective. We detail the implementation plan and hypothesize that this modification will yield more interpretable and monosemantic latent representations, thereby improving mechanistic interpretability."
    },
    {
        "Name": "scheduled_temporal_uniform_activation_loss",
        "Title": "Scheduled Temporal Uniform Activation Loss for Enhanced Latent Balance",
        "Experiment": "Modify the loss function in the TrainerTopK class by adding a temporal uniform activation regularization term with a scheduling mechanism. This term computes an exponential moving average (EMA) of activation frequencies for each latent over training batches, and both the EMA decay rate (alpha) and the regularization strength (lambda_temporal) are scheduled to gradually increase over training. Results will be obtained by evaluating the modified SAE on the sparse_probing and core benchmarks.",
        "Technical_Details": "During each training step, compute per-feature activation frequencies from the topk indices. Update an EMA of these frequencies using a time-varying decay rate alpha(t) that increases over time. Define the target frequency for each latent as (k * batch_size) / dict_size. Calculate the L2 norm squared between the EMA vector and the target uniform vector, and scale this penalty with a scheduled hyperparameter lambda_temporal(t) that increases as training progresses. This scheduled regularization allows the SAE to gradually shift its focus from reconstruction to achieving balanced latent activation, mitigating polysemantic feature formation.",
        "Implementation_Plan": "1. In the TrainerTopK class constructor, initialize a variable self.activation_ema as a zero vector of length dict_size. 2. Define a scheduling function for alpha(t) and lambda_temporal(t) based on the current training step, e.g., a linear or exponential schedule that starts from a low value and increases toward a maximum value. 3. In the loss() function, update the EMA of activation counts from the topk indices using: self.activation_ema = alpha(t) * current_counts + (1 - alpha(t)) * self.activation_ema. 4. Compute the target frequency as (k * batch_size) / dict_size. 5. Calculate the temporal regularization penalty as lambda_temporal(t) * ||self.activation_ema - target||^2 and add it to the overall loss. 6. Log this additional regularization term for monitoring, and tune the scheduling functions for alpha(t) and lambda_temporal(t) using validation performance. 7. Evaluate the updated SAE on the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "Incorporating a scheduling mechanism adds a dynamic aspect to the regularization, making the idea both conceptually and practically appealing without excessive complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The added scheduling functions are simple to implement and computationally inexpensive, requiring only minor modifications to the existing loss function, ensuring high feasibility.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Scheduling the temporal regularization parameters in the context of latent activation balancing is a nuanced enhancement that offers a new perspective on controlling feature utilization over training time.",
        "Novelty": 8,
        "Expected_Research_Impact": "The scheduled temporal regularization is expected to further improve the interpretability metrics on sparse_probing and core benchmarks by promoting stable and balanced latent activation throughout training.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We introduce a scheduled temporal uniform activation loss for sparse autoencoders, which incorporates an exponential moving average of latent activation frequencies along with a scheduling mechanism for both the decay rate and regularization strength. This approach penalizes deviations from a target uniform frequency distribution over time, promoting stable and balanced latent usage during training. Our method maintains the architectural simplicity of sparse autoencoders while enhancing mechanistic interpretability through improved latent feature regulation."
    },
    {
        "Name": "local_temporal_exclusive_sae",
        "Title": "Local Temporal Exclusive Sparsity Regularization for Sparse Autoencoders",
        "Experiment": "Enhance the TrainerTopK loss function by adding a local temporal exclusive loss term, which computes the cosine similarity between each token's strong latent activations and those of its immediate neighbor. This term penalizes redundancy in adjacent tokens, weighted by the product of their activation magnitudes. The experiment will compare the interpretability and sparsity metrics from benchmarks such as sparse_probing and core for models with and without this local temporal loss.",
        "Technical_Details": "For each training batch with the latent activation matrix f (of shape [batch_size, num_features]) and their associated token positions, compute the cosine similarity between the latent activations of each token and its immediate successor. Weight the similarity by the product of the activation magnitudes of the two tokens. Average the weighted similarities to obtain the local temporal exclusive loss term, exclusive_loss_local. Introduce a hyperparameter lambda_local to balance this term, so the overall loss becomes: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_local * exclusive_loss_local. This systematically discourages the co-activation of similar latent features in adjacent tokens, promoting locally distinct, monosemantic representations.",
        "Implementation_Plan": "1. Update the TrainerTopK.loss function to, after calculating the latent activation f, iterate over each token in the batch (except the last) and compute the cosine similarity between f[i] and f[i+1]. 2. Weight each similarity by the product of the activation magnitudes of the current and next tokens. 3. Average these values to yield exclusive_loss_local. 4. Introduce a hyperparameter lambda_local to scale this loss, and add this term to the overall loss. 5. Log the new exclusive_loss_local during training and evaluate its impact using existing evaluation benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Focusing on local temporal relationships simplifies the approach while preserving the core interpretability gains, making the idea compelling.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modification requires only a slight change in the loss computation, making it well within the capabilities of a junior PhD student to implement and test on standard hardware within a short timeframe.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a local, token-to-token exclusive loss in sparse autoencoders for enhancing interpretability introduces a novel and focused extension to the baseline method.",
        "Novelty": 8,
        "Expected_Research_Impact": "By specifically penalizing adjacent latent similarities, the method is expected to foster distinct and interpretable features, positively affecting benchmarks like sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refined variant of sparse autoencoders that introduces a local temporal exclusive sparsity regularization, aimed at enhancing mechanistic interpretability. Our approach computes a weighted cosine similarity between the latent activations of consecutive tokens, penalizing redundant activations in an immediate temporal context. This local exclusive loss is integrated into the training objective alongside the standard reconstruction and auxiliary losses, encouraging distinct and monosemantic latent representations. We detail the algorithmic modifications and implementation steps required to incorporate this loss term into the existing framework, setting the stage for further evaluation."
    },
    {
        "Name": "temporal_frequency_regularized_sae",
        "Title": "Temporal Frequency Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK loss function to incorporate two additional regularization terms: a uniformity penalty for latent activation frequencies and a temporal consistency penalty that minimizes the change in the moving average of activations over consecutive training steps; evaluate this modified SAE on core and sparse_probing benchmarks.",
        "Technical_Details": "During training, maintain an exponential moving average (EMA) vector of activation frequencies for each latent feature. Compute an L1 norm penalty that measures the deviation of this EMA from an ideal uniform distribution. In addition, compute a temporal difference penalty by taking the L1 norm of the difference between the current EMA and the previous EMA. Weight these two terms with hyperparameters (freq_reg_lambda for uniformity and temp_reg_lambda for temporal consistency) and add them to the overall reconstruction loss. This approach penalizes both imbalances in feature usage and abrupt temporal shifts, thereby promoting stable and interpretable latent representations.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize a new buffer (activation_frequency) to track the EMA of activations for each latent feature and a variable to store the previous EMA. 2. Update the EMA each training step using an exponential decay formula. 3. In the loss function, compute the L1 distance between the current EMA and a uniform vector (uniformity penalty) and also compute the L1 distance between the current EMA and the previous EMA (temporal consistency penalty). 4. Multiply these penalties by their respective hyperparameters (freq_reg_lambda and temp_reg_lambda) and add them to the existing reconstruction loss. 5. Experiment with different hyperparameter values to balance the reconstruction and regularization terms, and evaluate the effect on interpretability benchmarks.",
        "Interestingness_Evaluation": "By combining uniformity and temporal consistency, the idea provides a fresh perspective on regularizing latent activations in SAEs with minimal complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required modifications involve simple additions to the loss function and EMA tracking logic, ensuring that the implementation is manageable and can be executed within the given resource constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Introducing a temporal consistency penalty in the context of SAE regularization is a modest yet novel extension that deepens the approach without adding undue complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "This refined method is expected to yield more stable and interpretable latent features, positively impacting the performance on core and sparse_probing benchmarks by reducing polysemanticity over time.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refined variant of sparse autoencoders that incorporates temporal frequency regularization to enhance the interpretability of latent representations. Our approach adds two lightweight regularization terms during training: one that penalizes deviations from a uniform activation frequency distribution, and a second that enforces temporal consistency by minimizing changes in the moving average of activations across training steps. This dual regularization strategy aims to prevent feature absorption and stabilize the learned features over time, without introducing additional network complexity. Evaluations are proposed across standard interpretability and probing benchmarks."
    },
    {
        "Name": "temporal_activation_decorrelation",
        "Title": "Temporal Activation Decorrelation Regularization for Interpretable SAE Latents",
        "Experiment": "Modify the loss function in TrainerTopK to include an additional temporal decorrelation penalty. This is computed by maintaining an exponential moving average (EMA) of pairwise cosine similarities for the activated latent features over training steps and then applying a penalty when the EMA exceeds a predefined threshold. Results will be compared using the core and sparse_probing benchmarks against the baseline SAE.",
        "Technical_Details": "This enhancement computes a pairwise cosine similarity matrix for the top-k activated latent features for each training batch, then updates an EMA of these similarities across batches. For each feature pair with an EMA value exceeding a threshold tau (e.g., 0.1), a penalty proportional to the excess (EMA - tau) is imposed. The penalty is scaled by a hyperparameter lambda and added to the reconstruction loss. This temporal regularization encourages the sustained independence of latent features, mitigating persistent polysemanticity without modifying the network architecture.",
        "Implementation_Plan": "Step 1: In TrainerTopK.loss(), compute the cosine similarity matrix for non-zero activations from f. Step 2: Update an EMA (stored as a persistent state) for the cosine similarities across batches. Step 3: For feature pairs where the EMA exceeds a threshold tau, calculate a penalty term proportional to (EMA - tau) and average over the batch. Step 4: Multiply the penalty by a hyperparameter lambda and add it to the overall loss. Step 5: Run initial tests to ensure no significant increase in computation, then evaluate on the core and sparse_probing benchmarks.",
        "Interestingness_Evaluation": "This idea is compelling as it introduces a dynamic, temporal regularization that targets persistent feature correlations, potentially leading to more robust and interpretable representations.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation involves only modest changes to the existing loss computation with the addition of an EMA for pairwise similarities, a computation that is efficient and easily integrated.",
        "Feasibility": 8,
        "Novelty_Evaluation": "While decorrelation penalties are known, incorporating a temporal aspect to focus on persistent correlations is a novel twist that aligns well with the interpretability goals of SAEs.",
        "Novelty": 8,
        "Expected_Research_Impact": "By targeting and penalizing sustained, redundant activations, this method is expected to yield improvements in interpretability as measured by the core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We propose an extension to sparse autoencoder training that introduces a temporal activation decorrelation regularizer. By maintaining an exponential moving average of pairwise cosine similarities among active latent features, the method penalizes persistent correlations that may indicate polysemanticity. This regularizer is integrated into the loss function with minimal computational overhead, encouraging the development of distinct, monosemantic latent representations. We detail the implementation of the temporal update and discuss its potential to enhance mechanistic interpretability in deep networks."
    },
    {
        "Name": "adaptive_temporal_uniform_activation_loss",
        "Title": "Adaptive Temporal Uniform Activation Loss for Enhanced Latent Specialization",
        "Experiment": "Modify the TrainerTopK loss function by incorporating an adaptive decay for the temporal uniform activation penalty. Maintain an exponentially decaying moving average of latent feature activations and compute the penalty based on the difference between the current batch's activation frequencies and this moving average. Introduce a lambda_uniform factor that decays over training iterations, emphasizing uniformity early and relaxing later. Evaluate the impact using standard core and sparse probing benchmarks.",
        "Technical_Details": "After obtaining the topk activations, calculate the activation frequency for each latent element in the current batch. Update a running average using an exponential decay with a momentum parameter alpha. The target activation frequency is set as (k / dict_size). Compute the uniformity loss as the mean squared error between the current batch frequencies and the current moving average. Multiply this loss by a lambda_uniform factor that decays over training iterations (e.g., lambda_uniform = initial_lambda * (1 - decay_rate)^(current_step)). Add this adaptive penalty to the reconstruction loss to encourage uniform usage of latent features over time while allowing more flexibility as training progresses.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize a persistent tensor for the moving average of activation frequencies per latent and set initial values to zero. 2. In the loss() function, after obtaining topk indices, count activations per latent and update the moving average using exponential decay (avg = alpha * current_batch_freq + (1 - alpha) * previous_avg). 3. Define the target frequency as (k / dict_size). 4. Compute the mean squared error between the updated moving average and the target frequency. 5. Compute an adaptive penalty term by multiplying the error with a decaying lambda_uniform factor (e.g., initial_lambda * (1 - decay_rate)^(current_step)). 6. Add the adaptive penalty term to the original reconstruction loss. 7. Run experiments with different initial lambda_uniform and decay_rate values and evaluate using core and sparse probing benchmarks.",
        "Interestingness_Evaluation": "This idea is interesting as it introduces an adaptive mechanism to balance early regularization with later reconstruction optimization, using a minimal, non-complex change.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve simple additions to the existing loss function and tracking of extra variables, ensuring an easy implementation and negligible computational overhead.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing an adaptive decay for the uniformity loss in sparse autoencoders is a novel refinement that naturally extends standard regularization techniques in a targeted way.",
        "Novelty": 8,
        "Expected_Research_Impact": "By adapting the intensity of the uniform activation loss over time, this method is expected to better regulate latent usage and reduce polysemanticity, positively impacting sparse probing and core interpretability benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an adaptive temporal uniform activation loss for sparse autoencoders that incorporates an exponentially decaying moving average of latent feature activations along with a decaying penalty term. This adaptive mechanism imposes a strong uniform regularization in the early stages of training, gradually relaxing as training proceeds, thereby promoting distinct, uniformly activated latent representations. Our approach integrates seamlessly within the existing SAE framework and offers a simple yet effective strategy to mitigate polysemanticity."
    },
    {
        "Name": "thresh_dynamic_exclusive_sae",
        "Title": "Thresholded Dynamic Exclusive Sparsity Regularization with Temporal Awareness for Sparse Autoencoders",
        "Experiment": "Modify the loss function in TrainerTopK by adding a thresholded dynamic exclusive loss term. The term computes cosine similarities between pairs of strong latent activations for tokens within a fixed temporal window, weights the penalties by their activation magnitudes, and only penalizes pairs whose similarity exceeds a predefined threshold. Compare interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the enhanced model and the baseline SAE.",
        "Technical_Details": "For each training batch, after obtaining the top-k latent activation matrix f (of shape [batch_size, num_features]) and associated token position indices, restrict pairwise cosine similarity computation to token pairs within a temporal window defined by a hyperparameter window_size. For each valid pair (i, j) with token positions t_i and t_j such that |t_i - t_j| <= window_size, compute the cosine similarity of their active latent vectors. Weight each similarity by the product of the activation magnitudes and apply a threshold (cosine_threshold) so that only similarities above this threshold contribute to the exclusive loss. Aggregate the thresholded, weighted similarities to form exclusive_loss_dynamic_thresh and incorporate it into the total loss as: loss_total = l2_loss + auxk_alpha * auxk_loss + lambda_dynamic * exclusive_loss_dynamic_thresh.",
        "Implementation_Plan": "1. In the TrainerTopK.loss function, track token positions along with activations. 2. Identify token pairs within a fixed temporal window (e.g., tokens within 2 positions of each other). 3. Compute cosine similarity for these pairs and weight them by the product of their activation magnitudes. 4. Apply a threshold, cosine_threshold, to ignore similarities below this value. 5. Average the remaining weighted similarities to yield exclusive_loss_dynamic_thresh. 6. Introduce a hyperparameter lambda_dynamic to scale this loss term and add it to the overall loss. 7. Log the new loss term and run experiments to evaluate its impact on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "Introducing a threshold mechanism to focus on penalizing only high redundancy in latent activations adds a novel refinement without significant complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve straightforward computations within the loss function and add only minimal hyperparameters; implementation and experimentation should be feasible within the given constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Employing a thresholded penalty in a temporal context provides a unique angle to existing regularization techniques in sparse autoencoders, enhancing the model's interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only high similarity between temporally adjacent tokens, the method should yield more distinct and interpretable latent features, potentially improving performance on core and sparse_probing benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a thresholded dynamic exclusive sparsity regularization, augmented with temporal awareness. Our approach selectively penalizes high cosine similarity between latent activations from temporally proximate tokens, weighting the penalty by the product of activation magnitudes and applying a threshold to disregard minor similarities. This additional loss term is integrated with the standard reconstruction and auxiliary losses, promoting monosemantic latent features without introducing significant complexity. We detail the technical formulation and implementation steps for this method, setting the stage for further evaluation on interpretability benchmarks."
    },
    {
        "Name": "temporal_exclusive_sae",
        "Title": "Temporal Exclusive Sparsity Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the loss function in the TrainerTopK class by incorporating a temporal exclusive loss that operates on pooled latent activations across consecutive token segments. Compare the interpretability and sparsity metrics on benchmarks such as sparse_probing and core between the baseline SAE and the temporal exclusive variant.",
        "Technical_Details": "For each training batch, the latent activation matrix f (shape: [batch_size, num_features]) is partitioned into temporal segments by grouping tokens (or pseudo-sequences). Mean pooling is applied to each segment to derive temporal latent representations. A pairwise cosine similarity matrix is computed over these pooled representations, excluding self-similarities. The temporal exclusive loss is defined as the average of these off-diagonal cosine similarity values and is scaled by a hyperparameter lambda_exclusive. The overall loss is then computed as: loss_total = reconstruction loss + lambda_aux * aux_loss + lambda_exclusive * temporal_exclusive_loss, which encourages latent features to be distinct across time.",
        "Implementation_Plan": "1. Modify TrainerTopK.loss to reshape or group the latent activations into temporal segments based on token sequences. 2. Apply mean pooling within each segment to obtain temporal latent representations. 3. Compute the cosine similarity matrix among these pooled segments, excluding self-similarity terms. 4. Aggregate the similarities to calculate the temporal exclusive loss. 5. Introduce a hyperparameter lambda_exclusive and add the loss term to the existing loss. 6. Log and monitor the new loss component for debugging. 7. Train the modified SAE and evaluate using sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "The introduction of temporal grouping into the exclusive sparsity regularization offers a novel and insightful approach to reducing feature absorption, making it highly interesting.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The required modifications are minor and involve standard pooling and cosine similarity operations, ensuring the approach is straightforward to implement within the expected time frame.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Adding a temporal dimension to the exclusivity penalty is a fresh extension to standard sparse autoencoder regularizations, providing a novel perspective in the domain.",
        "Novelty": 8,
        "Expected_Research_Impact": "This method is expected to produce more distinct and temporally relevant latent features, increasing interpretability and potentially enhancing performance on key benchmarks.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose a temporal exclusive sparsity regularization technique for sparse autoencoders, which extends the conventional exclusive sparsity approach by incorporating temporal aggregation of latent activations. In our method, latent activations are partitioned into segments representing consecutive tokens, and mean pooling is applied to obtain per-segment representations. A cosine similarity penalty is then imposed on these representations to encourage mutually distinct activation patterns across time. This modification enhances the interpretability of latent features by reducing redundancy in temporal contexts, while preserving reconstruction quality. The paper details the loss function reformulation and outlines an experimental protocol to evaluate the method on established benchmarks."
    },
    {
        "Name": "windowed_temporal_exclusive_sae",
        "Title": "Windowed Temporal Exclusive Regularization for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the training loss in TrainerTopK to incorporate an exclusivity penalty and a windowed temporal consistency penalty. The exclusivity penalty remains as the average pairwise cosine similarity among non-zero latent activations per token. For temporal consistency, compute a sliding-window variance loss by dividing the sequential token activations into small overlapping windows (e.g., window size of 3) and calculate the average L2 variance of latent activations within each window. Evaluate this modified loss on benchmarks such as sparse_probing and core by comparing the interpretability of latent features with the prototype SAE.",
        "Technical_Details": "Within the TrainerTopK.loss() function, after obtaining the encoded activations 'f', first calculate the exclusivity loss as the mean pairwise cosine similarity among non-zero latent activations, scaled by a hyperparameter 'lambda_exclusive'. Next, partition the sequence of tokens into overlapping sliding windows (for instance, with a window size of 3 tokens) and compute the variance in latent activations within each window. The temporal loss is defined as the mean L2 norm of the deviations from the window mean, scaled by 'lambda_temporal'. The overall loss function becomes: L_total = L2_loss + auxk_alpha * auxk_loss + lambda_exclusive * (exclusivity_loss) + lambda_temporal * (temporal_loss). This combination encourages each latent to capture a unique, stable feature across a short temporal context, mitigating polysemanticity while retaining reconstruction quality.",
        "Implementation_Plan": "1) In the TrainerTopK.loss() function, after encoding activations, add the computation for exclusivity loss as in the previous version; scale it with 'lambda_exclusive'. 2) Implement a sliding window over the batch sequence (e.g., window size 3, with overlaps) and compute the L2 variance for latent activations within each window; scale this with the hyperparameter 'lambda_temporal'. 3) Add both the exclusivity loss and the temporal loss to the primary loss. 4) Expose 'lambda_exclusive' and 'lambda_temporal' as configurable hyperparameters in the training configuration. 5) Run training using the modified loss function and evaluate performance on benchmarks such as sparse_probing and core.",
        "Interestingness_Evaluation": "This idea is very interesting as it leverages a nuanced sliding window approach to enhance temporal consistency while preserving latent exclusivity, directly addressing the interpretability challenge in a novel manner.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve clear, minimal code changes to the loss computation, and the additional sliding window computation is lightweight and scalable on a single GPU, ensuring high feasibility.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Integrating a windowed temporal consistency penalty with exclusivity regularization in sparse autoencoders is a novel refinement that directly aims to reduce polysemanticity with minimal additional complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "The refined approach is expected to produce latent features that are both exclusive and temporally smooth, thereby enhancing interpretability on benchmarks such as sparse_probing and core.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates a dual regularization scheme combining exclusivity and windowed temporal consistency penalties. The exclusivity penalty minimizes the pairwise cosine similarity of non-zero latent activations for each token, while the windowed temporal penalty enforces smooth latent transitions over small overlapping token windows by penalizing high variance in activations. These loss modifications are integrated into the standard training objective without altering the autoencoder architecture. The resulting approach encourages each latent to capture a distinct and temporally stable feature, thereby mitigating polysemanticity and improving interpretability."
    },
    {
        "Name": "temporal_uniform_activation_loss_adaptive",
        "Title": "Adaptive Temporal Uniform Activation Loss for Interpretable Sparse Autoencoders",
        "Experiment": "Modify the SAE training loop in experiment.py by maintaining an exponential moving average (EMA) of the per-latent activation frequencies across batches, and introduce an adaptive weighting factor for the uniform activation loss that increases gradually during training. Evaluate improvements by analyzing changes in feature density statistics and performance on sparse probing tasks.",
        "Technical_Details": "During each training update, compute a binary activation mask from the latent activations and update an EMA vector for each latent's activation frequency using a decay rate (e.g., 0.9). Define a target frequency (for example, desired_k / dict_size) and calculate an L2 loss between the EMA vector and the target vector. To allow the SAE to initially learn robust reconstructions, the loss weight 'lambda_uniform' is increased gradually (for example, using a linear or exponential schedule across training steps). This temporally smoothed, adaptively weighted loss is added to the overall reconstruction loss, encouraging stable, uniform latent usage and mitigating feature absorption.",
        "Implementation_Plan": "1. In the TrainerTopK class, introduce a persistent EMA vector for activation frequencies, initialized as a zero vector with the length equal to the number of latents. 2. In the loss() function, compute a binary activation mask for each latent per batch and update the EMA vector using an exponential decay formula. 3. Define a target activation frequency and calculate the L2 loss between the EMA vector and the target vector. 4. Implement an adaptive schedule for the hyperparameter 'lambda_uniform' that increases its value gradually over training steps. 5. Add the scaled uniform activation loss to the overall loss before the backward pass. 6. Validate the effect of this modification with ablation studies focusing on feature density and sparse probing benchmarks.",
        "Interestingness_Evaluation": "This idea is interesting due to its adaptive mechanism which balances reconstruction and interpretability objectives over time, maintaining clear simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation involves minor changes to the training loop using standard techniques, making it feasible with minimal coding effort and computational overhead.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Incorporating an adaptive weighting factor into the temporal uniform activation loss introduces a novel and practical refinement to promote latent uniformity without significant complexity.",
        "Novelty": 8,
        "Expected_Research_Impact": "This adaptive temporal approach is expected to robustly improve the balance of latent activations, thereby enhancing interpretability metrics on benchmarks like sparse_probing and core.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose an adaptive temporal uniform activation loss to enhance the interpretability of sparse autoencoders. By maintaining an exponential moving average of per-latent activation frequencies and gradually increasing the weighting of a uniform activation regularization term during training, our method promotes a balanced and stable usage of latent features. This modification integrates seamlessly into the existing SAE training framework with minimal computational overhead, aiming to mitigate feature absorption and foster monosemantic representations."
    },
    {
        "Name": "dynamic_selective_temporal_decor_sae",
        "Title": "Dynamic Selective Temporal and Decorrelated Sparse Autoencoders for Enhanced Latent Disentanglement",
        "Experiment": "Modify the TrainerTopK loss function to include both a decorrelation penalty computed on the pairwise cosine similarities of the top-k activations and a temporal consistency penalty that compares current top-k activations with an EMA of previous activations. Incorporate a dynamic EMA momentum schedule that decays over training steps to update the EMA only for activations exceeding the defined 'ema_activation_threshold'. Evaluate the modifications on the sparse_probing and core benchmarks.",
        "Technical_Details": "During each forward pass, compute the decorrelation loss by forming a pairwise cosine similarity matrix of the top-k activation vectors for each sample, masking out the diagonal, and summing the squared off-diagonal elements weighted by 'lambda_decor'. Maintain an EMA for the latent activations and compute the temporal consistency loss as the L2 distance between current top-k activations and their corresponding EMA values (only for activations above 'ema_activation_threshold'), scaled by 'lambda_temp'. Update the EMA using an exponential decay. Additionally, instead of a fixed momentum, use a dynamic EMA momentum schedule that decays over training steps, allowing the EMA to be more stable early on and more adaptive later. The final loss is the sum of the reconstruction, auxiliary, decorrelation, and temporal consistency losses.",
        "Implementation_Plan": "1. Add a new member variable 'ema_latents' in TrainerTopK, initialize it appropriately, and introduce hyperparameters 'ema_activation_threshold', 'lambda_decor', 'lambda_temp', 'initial_ema_momentum', and 'ema_decay_rate'. 2. In the loss() function, after computing top-k activations, calculate the pairwise cosine similarity matrix, mask the diagonal, and compute the decorrelation loss using 'lambda_decor'. 3. Compute the temporal consistency loss by selecting activations above 'ema_activation_threshold', calculate the L2 difference with corresponding 'ema_latents', and weight this penalty by 'lambda_temp'. 4. Update 'ema_latents' using the formula: ema = current_momentum * ema_latents + (1 - current_momentum) * current_activations, where current_momentum decays over training steps using a schedule (e.g., current_momentum = initial_ema_momentum * exp(-step/ema_decay_rate)). 5. Combine all loss terms (reconstruction, auxiliary, decorrelation, and temporal consistency) for the final loss. 6. Run controlled experiments on the sparse_probing and core benchmarks, tuning the new hyperparameters for optimal performance.",
        "Interestingness_Evaluation": "Using a dynamic momentum schedule within the selective EMA update further refines temporal consistency in a simple yet effective manner, adding a nuanced layer to latent stabilization.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The dynamic momentum schedule is a lightweight addition that leverages existing EMA mechanics without significant computational overhead, ensuring ease of implementation.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Adapting the EMA momentum dynamically in the context of SAE temporal consistency offers a moderate degree of novelty, enhancing the overall approach without overcomplicating the design.",
        "Novelty": 8,
        "Expected_Research_Impact": "The refined approach is expected to yield more stable and distinct latent representations, which should positively affect interpretability benchmarks such as sparse_probing and core.",
        "Research_Impact": 8,
        "Overall_Score": 8.7,
        "Abstract": "We propose a refinement to sparse autoencoders that integrates both a decorrelation penalty and a temporal consistency penalty with a selective EMA update mechanism. The model computes a decorrelation loss by penalizing high pairwise cosine similarity among top-k activations and enforces temporal stability by comparing current activations against an EMA of previous activations, updated only for salient activations. A dynamic EMA momentum schedule is incorporated to allow for high stability during early training and increased responsiveness later on. This approach promotes distinct and stable latent features without incurring significant additional computational burden."
    },
    {
        "Name": "temporal_normalized_dynamic_decorrelation_sae",
        "Title": "Normalized Temporal Smoothing for Dynamic Decorrelation in Sparse Autoencoders",
        "Experiment": "Enhance the TrainerTopK loss function by incorporating a normalized temporal dynamic decorrelation loss. After computing the dynamic decorrelation penalty and updating its EMA, introduce a normalization factor based on the reconstruction loss magnitude to balance both terms. The final loss is the sum of the reconstruction loss and the weighted, normalized EMA decorrelation loss. The performance will be compared on core and sparse_probing benchmarks with and without the normalization.",
        "Technical_Details": "Within the TrainerTopK loss() function, calculate the pairwise cosine similarities among top-k activations using a dynamic threshold tau(t). Compute the current decorrelation penalty and update its exponential moving average (EMA) using a smoothing factor alpha. Introduce a normalization factor computed as the ratio between the current reconstruction loss and its running average (or a fixed constant for stability) to scale the EMA decorrelation loss. The final loss becomes: loss = reconstruction_loss + lambda_decorr * (ema_decorrelation / normalization_factor). This method maintains balance between the reconstruction and decorrelation components, ensuring stable and distinct latent feature evolution.",
        "Implementation_Plan": "1. In the TrainerTopK loss() function, after computing the top-k activations and pairwise cosine similarities, update the dynamic decorrelation loss as before using a time-dependent threshold tau(t). 2. Update ema_decorrelation using the formula: ema_decorrelation = alpha * current_decorrelation + (1 - alpha) * ema_decorrelation. 3. Compute a normalization_factor based on the current reconstruction loss (e.g., using a running average or a small constant to avoid division by zero). 4. Combine the reconstruction loss and the weighted, normalized ema_decorrelation loss as: final_loss = reconstruction_loss + lambda_decorr * (ema_decorrelation / normalization_factor). 5. Integrate these changes into the existing training script and run comparative experiments on core and sparse_probing benchmarks.",
        "Interestingness_Evaluation": "The addition of normalization refines the balance between loss components, ensuring smooth and interpretable latent feature learning, which is both clever and practically significant.",
        "Interestingness": 8.5,
        "Feasibility_Evaluation": "This refinement only requires minor modifications to the loss function and minimal extra computation, making it easily implementable within the current framework.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Normalizing the temporal dynamic decorrelation loss to balance its contribution is a novel twist that deepens the simplicity of decorrelation without diverging from the core methodology.",
        "Novelty": 8.5,
        "Expected_Research_Impact": "By ensuring a balanced and stable training loss, the latent representations are expected to become more distinct and interpretable, benefiting mechanistic interpretability benchmarks like core and sparse_probing.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose an enhancement to sparse autoencoder training by introducing a normalized temporal dynamic decorrelation loss. Our method computes a dynamic decorrelation penalty on top-k activations with a time-dependent threshold and maintains its exponential moving average (EMA) to enforce temporal consistency. A normalization factor based on the reconstruction loss is then used to balance the contribution of the decorrelation term, ensuring stability throughout training. This approach aims to promote distinct and stable latent representations while retaining computational efficiency and simplicity. We detail the technical integration of the normalization mechanism and outline an experimental plan for its evaluation."
    },
    {
        "Name": "temporal_uniform_activation_loss",
        "Title": "Temporal Uniform Activation Loss for Enhanced Latent Interpretability",
        "Experiment": "Modify the loss function in the TrainerTopK class by incorporating a temporal uniform activation regularization term. This term maintains an exponential moving average (EMA) of latent activation frequencies and penalizes deviations from a predetermined uniform target distribution. The EMA is updated using a momentum parameter beta, and the penalty, scaled by a hyperparameter lambda_temporal, is added to the overall loss. The experiment involves comparing the modified SAE on the sparse_probing and core benchmarks with the baseline SAE.",
        "Technical_Details": "During each training batch, compute activation counts from the top-k indices obtained from the encoder. Update an EMA vector for latent activations using the formula: EMA = beta * EMA + (1 - beta) * (current_batch_counts), where beta is a momentum factor (e.g., 0.9). The target frequency for each latent is set as (k * batch_size) / dict_size. Compute the L2 norm between the EMA vector and the uniform target vector, and scale this difference by a hyperparameter lambda_temporal. Add this penalty term to the reconstruction and auxiliary losses. This mechanism encourages balanced and temporally consistent latent usage.",
        "Implementation_Plan": "1. In the TrainerTopK class, initialize an EMA vector for latent activation frequencies once on model setup. 2. Within the loss() function, after obtaining the top-k indices, compute per-feature activation counts for the current batch. 3. Update the EMA vector using: EMA = beta * EMA + (1 - beta) * (current batch counts). 4. Define the target frequency as (k * batch_size) / dict_size for each latent. 5. Compute the L2 penalty between the EMA vector and the target frequency vector, and multiply by lambda_temporal. 6. Add this penalty term to the overall loss. 7. Log and monitor the temporal activation loss over training, and evaluate the modified SAE using the sparse_probing and core benchmarks.",
        "Interestingness_Evaluation": "The idea elegantly incorporates temporal smoothing into activation regularization, ensuring stable latent usage with minimal added complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "This approach requires only minor modifications to the existing loss code for state tracking and averaging, making it highly feasible for implementation within a short timeframe.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While building on existing activation regularization concepts, integrating EMA-based temporal dynamics for interpretability is a novel yet straightforward refinement in this context.",
        "Novelty": 8,
        "Expected_Research_Impact": "Encouraging balanced and temporally stable activations is expected to improve interpretability, particularly on benchmarks like sparse_probing and core, by mitigating polysemanticity.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We propose a temporal uniform activation regularization for sparse autoencoders that leverages an exponential moving average (EMA) to track latent activation frequencies over time. This new loss term penalizes deviations from an ideal uniform distribution of activations, encouraging consistent and interpretable latent representations throughout training. Our method integrates seamlessly into the existing training framework with minimal architectural modifications."
    },
    {
        "Name": "thresholded_squared_windowed_temporal_decorrelation",
        "Title": "Thresholded Squared Windowed Temporal Decorrelation for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "Modify the TrainerTopK loss function to include a thresholded squared windowed temporal decorrelation penalty. Compute cosine similarities between each token's top-k latent activations and those of its immediate neighbors, square these similarities, then apply a threshold (sim_thresh) such that only similarity values above this threshold contribute to the penalty. Average these thresholded squared similarities, weight by the hyperparameter lambda_decorr, and add to the overall loss. Evaluate the modified SAE on core and sparse probing benchmarks.",
        "Technical_Details": "After obtaining the top-k latent activations in sequence order, for each token compute the cosine similarity with its immediate previous and next tokens (handling edge cases appropriately). Square these similarity values to accentuate redundancies. Introduce a threshold sim_thresh so that only squared similarities above sim_thresh contribute to the penalty; values below are zeroed out. For each token, average the thresholded squared similarities of available neighbors. Aggregate these penalties over the sequence and batch, scale by lambda_decorr, and add to the reconstruction and auxk losses. This approach uses standard tensor operations like torch.nn.functional.cosine_similarity and element-wise operations without altering the SAE architecture.",
        "Implementation_Plan": "1. Add new hyperparameters lambda_decorr and sim_thresh to TrainerTopK. 2. In the loss method, after extracting the top-k activations, reshape the tensor to maintain sequence order. 3. For each token, compute cosine similarities with immediate neighbors; square these similarities. 4. Apply a threshold by zeroing out squared values below sim_thresh. 5. Average the resulting values per token, then aggregate across the batch and sequence. 6. Multiply by lambda_decorr and add to the total loss. 7. Conduct controlled experiments to tune lambda_decorr and sim_thresh, and evaluate the model on core and sparse probing benchmarks.",
        "Interestingness_Evaluation": "Introducing a threshold makes the temporal decorrelation penalty more selective and targeted, which is a subtle yet impactful refinement of the original idea.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The addition of a threshold involves minimal computational overhead and straightforward tensor operations, ensuring that the approach remains highly feasible within the existing training framework.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While the basic method is based on established temporal decorrelation approaches, the selective penalty using a threshold for squared similarities is a novel and precise refinement that builds directly on the original concept.",
        "Novelty": 8,
        "Expected_Research_Impact": "By selectively penalizing only highly redundant latent activations, this approach is expected to further enhance the distinctiveness of latent representations, thereby improving interpretability on key benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose a thresholded squared windowed temporal decorrelation penalty for training sparse autoencoders aimed at enhancing mechanistic interpretability. Our method computes the cosine similarity between top-k activated latent features of each token and its immediate neighbors, squares these values to emphasize redundancies, and applies a threshold to selectively penalize high similarity. The resulting penalty, integrated into the overall loss via a weighting hyperparameter, encourages temporally diverse and distinct latent representations. This approach integrates seamlessly within the existing SAE framework without additional architectural modifications."
    },
    {
        "Name": "temporal_decor_sae",
        "Title": "Temporal Decorrelated Sparse Autoencoders: Enhancing Latent Stability and Disentanglement",
        "Experiment": "Extend the existing decorrelation penalty by adding a temporal consistency term. After computing the top-k activations in the encode() function, compute two additional loss terms: a pairwise cosine similarity penalty among the top-k activations (excluding the diagonal) and a temporal smoothness penalty computed as the mean squared difference between consecutive latent activations within each input sequence. Modify the TrainerTopK loss function to include both penalties, scaled by hyperparameters lambda_corr and lambda_temporal respectively, and evaluate the modified SAE using standard sparse_probing and core benchmarks.",
        "Technical_Details": "Let f be the latent activations obtained from the SAE encoder. First, normalize f and compute the cosine similarity matrix for the top-k activations, then mask out the diagonal and sum the squared off-diagonal values to obtain the decorrelation penalty, scaled by lambda_corr. Second, assume that the input x represents tokens in sequence order and reshape f accordingly. Compute the temporal penalty as the mean squared difference between consecutive latent vectors in each sequence and scale it by lambda_temporal. The final loss is the sum of the original reconstruction loss, the decorrelation penalty, and the temporal smoothness penalty. This dual penalty approach enforces both orthogonality among features and temporal stability.",
        "Implementation_Plan": "1. In the TrainerTopK class, modify the loss() method to extract latent activations f for each token. 2. Implement the decorrelation penalty as before by computing pairwise cosine similarities, masking the diagonal, and summing the squared values, then scaling by lambda_corr. 3. Reshape f to reflect the sequential nature of the input (assuming tokens are in order) and compute the mean squared difference between adjacent latent representations; scale this term by lambda_temporal. 4. Add both penalty terms to the original reconstruction loss. 5. Introduce and tune the hyperparameters lambda_corr and lambda_temporal through experiments to balance the penalties. 6. Evaluate the modified model on sparse_probing and core benchmarks using the existing evaluation pipeline.",
        "Interestingness_Evaluation": "Combining decorrelation with a temporal consistency penalty provides a novel and insightful approach to reducing polysemanticity by enforcing both spatial and temporal structure in the latent space.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The modifications involve only additional loss computations and minimal changes in the TrainerTopK class, which can be implemented and tested within a short timeframe using the current codebase and resources.",
        "Feasibility": 8,
        "Novelty_Evaluation": "While decorrelation penalties are known, their combination with a temporal smoothness term for SAE latent stabilization is a novel yet straightforward extension that remains close to established techniques.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed approach is anticipated to yield more interpretable, stable latent features, thereby potentially improving performance on sparse_probing and core benchmarks without adding significant complexity.",
        "Research_Impact": 8,
        "Overall_Score": 8.9,
        "Abstract": "We propose a refinement to sparse autoencoders that incorporates both a decorrelation penalty and a temporal consistency term to promote monosemantic and stable latent representations. The method computes pairwise cosine similarity losses among top activated latent features to reduce feature overlap, and a temporal smoothness penalty by measuring mean squared differences between consecutive latent activations in sequence. These minimal modifications are integrated within the existing SAE training framework to encourage latent features that are both distinct and temporally coherent. This enhanced formulation aims to improve mechanistic interpretability without altering the core architecture."
    }
]