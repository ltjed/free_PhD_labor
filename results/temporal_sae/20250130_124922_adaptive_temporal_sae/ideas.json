[
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Sparse Autoencoders: Efficient Feature Stabilization with EMA Tracking",
        "Experiment": "1. Implement EMA feature tracking\n2. Add adaptive temporal consistency loss\n3. Train on standard datasets\n4. Evaluate feature stability and frequency patterns\n5. Compare feature interpretability with baseline",
        "Technical_Details": "Architecture modifications:\n1. EMA tracking:\n   - Update feature statistics: s_t = \u03b3 * s_{t-1} + (1-\u03b3) * f_t\n   where \u03b3=0.99 is decay rate\n   - Compute activation frequency: freq = mean(f_t > 0)\n\n2. Loss function:\n   L = ||x - Dec(f_t)||^2 + \u03b1 * ||f_t||_1 + \u03b2 * w_t * ||f_t - s_t||^2\n   where \u03b1=0.04 is sparsity penalty\n   \u03b2=0.01 is base temporal weight\n   w_t = clip(freq, 0.1, 1.0) is adaptive weight\n\n3. Implementation details:\n   - Single EMA state per feature\n   - Update frequency every 100 steps\n   - Batch size 2048\n   - Learning rate 3e-4",
        "Implementation_Plan": "1. Add EMA tracking to SAE\n2. Implement adaptive temporal loss\n3. Add frequency-based weighting\n4. Create evaluation metrics\n5. Add visualization tools",
        "Interestingness_Evaluation": "The combination of EMA tracking and adaptive temporal consistency provides an elegant and efficient solution to feature stabilization.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation extremely simple with EMA approach; no memory overhead; runtime well within 30-min limit on H100; complete implementation feasible within 1 week for junior PhD student.",
        "Feasibility": 10,
        "Novelty_Evaluation": "The adaptive temporal consistency based on feature frequency represents a novel approach to balancing stability and flexibility in SAEs.",
        "Novelty": 8,
        "Expected_Research_Impact": "The adaptive approach should significantly improve both sparse_probing and core benchmarks by providing better feature stability while preserving rare but important features.",
        "Research_Impact": 9.5,
        "Overall_Score": 9.2,
        "Abstract": "We propose Temporal Sparse Autoencoders (T-SAE), a novel approach to improving feature interpretability in neural networks through efficient temporal consistency constraints. Our method employs exponential moving average tracking of feature activations combined with an adaptive temporal consistency loss that scales based on feature activation frequency. This approach allows the model to maintain stability for common features while preserving the flexibility needed for rare but important patterns. The method is computationally efficient, requiring minimal additional memory and computation compared to standard sparse autoencoders. We evaluate ATSAEs on standard interpretability benchmarks and analyze how the adaptive temporal consistency affects feature learning patterns.",
        "novel": true
    },
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Consistency Regularization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement efficient 2-timestep buffer\n2. Add adaptive temporal consistency loss\n3. Compare against baseline SAE on interpretability benchmarks\n4. Analyze feature stability patterns\n5. Measure impact on feature absorption",
        "Technical_Details": "1. Temporal consistency loss:\n   L_temp = ||f_t - f_{t-1}||_1\n\n2. Dynamic activation threshold:\n   \u03c4_t = \u03b2 * \u03c4_{t-1} + (1-\u03b2) * ||f_t||_1\n   where \u03b2=0.99 is EMA decay\n\n3. Adaptive weighting:\n   \u03bb_2(f) = \u03bb_base * min(1, ||f||_1/\u03c4_t)\n   \u03bb_base = 0.02, increased during warmup\n\n4. Total loss:\n   L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(f) * L_temp\n\n5. Implementation optimizations:\n   - Double buffer for temporal states\n   - Sparse computation of temporal loss\n   - Skip temporal loss at sequence boundaries",
        "Implementation_Plan": "1. Add previous_features buffer to CustomSAE\n2. Implement EMA threshold tracking\n3. Add adaptive temporal loss computation\n4. Modify training loop for state management\n5. Add feature stability analysis tools\n6. Optimize memory usage with sparse operations",
        "Interestingness_Evaluation": "The combination of adaptive temporal regularization with dynamic thresholding provides a practical and principled approach to preventing feature absorption while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation is highly practical with minimal memory overhead from 2-timestep buffer; dynamic threshold adds negligible computation; can be completed within 2 weeks and runs within 30-min limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive temporal consistency approach with dynamic thresholding represents a novel and theoretically grounded solution to the feature absorption problem.",
        "Novelty": 8,
        "Expected_Research_Impact": "The simplified yet robust approach directly targets feature absorption while maintaining sparse, interpretable representations, promising significant improvements on both sparse_probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We present an adaptive temporal consistency regularization approach for improving sparse autoencoder (SAE) interpretability. Current SAEs can suffer from feature absorption, where features inconsistently represent different concepts across time. We introduce a temporal consistency loss with dynamic threshold adaptation based on exponential moving averages of feature activations. This approach maintains the simplicity of standard SAEs while directly targeting the feature absorption problem. Our method uses efficient double buffering and sparse operations to minimize computational overhead. We evaluate the approach on standard mechanistic interpretability benchmarks and analyze its impact on feature stability and absorption patterns.",
        "novel": true
    },
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Regularization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Add previous activation state tracking\n2. Implement adaptive temporal difference penalty\n3. Train on standard datasets with modified loss\n4. Analyze feature stability patterns\n5. Evaluate using standard benchmarks",
        "Technical_Details": "Loss function modification:\n1. Adaptive temporal penalty:\n   L = L_recon + \u03bb1 * L1(f_t) + \u03bb2 * \u03b1(L_recon) * ||f_t - f_{t-1}||_1\n   where:\n   - f_t is current feature activation\n   - f_{t-1} is previous activation\n   - L1 is standard sparsity penalty\n   - \u03b1(L_recon) = sigmoid(\u03b2 * (L_max - L_recon))\n   - L_max is moving maximum of reconstruction loss\n   - \u03b2 controls adaptation rate\n\n2. Implementation details:\n   - Single state variable for previous activation\n   - Moving maximum with momentum 0.99\n   - Direct gradient computation\n   - Zero initialization for f_{t-1} at sequence starts\n\nHyperparameters:\n- Sparsity weight \u03bb1 = 0.04\n- Temporal weight \u03bb2 = 0.02\n- Adaptation rate \u03b2 = 5.0\n- Learning rate = 3e-4\n- Batch size = 2048",
        "Implementation_Plan": "1. Add previous activation state to CustomSAE\n2. Implement adaptive temporal loss\n3. Add moving maximum tracking\n4. Modify training loop for state updates\n5. Add temporal stability metrics\n6. Create visualization tools",
        "Interestingness_Evaluation": "The adaptive temporal regularization provides an elegant self-tuning mechanism that automatically balances reconstruction quality with temporal consistency.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only single state variable and simple loss scaling; minimal overhead from moving maximum computation; runtime virtually identical to baseline; complete implementation feasible within 4 days; adaptive approach adds robustness without complexity.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of temporal regularization with adaptive scaling based on reconstruction quality for sparse autoencoders is novel and well-motivated.",
        "Novelty": 9,
        "Expected_Research_Impact": "The adaptive temporal regularization should excel on both sparse_probing and core metrics by ensuring features maintain consistent meanings while not compromising reconstruction quality.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "We present an adaptive temporal regularization approach for improving sparse autoencoder interpretability. By penalizing sudden changes in feature activations with a penalty that scales based on reconstruction quality, we encourage features to maintain consistent behavior while ensuring good reconstructions. This simple yet effective modification requires minimal changes to existing architectures while directly addressing the feature absorption problem. The approach automatically adjusts its temporal consistency enforcement based on current reconstruction performance, allowing for natural feature development during early training while preventing absorption once good reconstructions are achieved. We evaluate our method on standard mechanistic interpretability benchmarks and analyze the temporal stability characteristics of learned features.",
        "novel": true
    }
]