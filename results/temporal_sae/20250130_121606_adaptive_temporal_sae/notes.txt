# Title: Adaptive Temporal Consistency Regularization for Interpretable Sparse Autoencoders
# Experiment description: 1. Implement efficient 2-timestep buffer
2. Add adaptive temporal consistency loss
3. Compare against baseline SAE on interpretability benchmarks
4. Analyze feature stability patterns
5. Measure impact on feature absorption
## Run 0: Baseline
Results: Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0001, 'steps': 4882, 'seed': 42, 'activation_dim': 2304, 'dict_size': 65536, 'k': 320, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 2314.96337890625, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '8a52411b-0254-4a79-9c7d-5ac4b0605b84', 'datetime_epoch_millis': 1738123643311, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.991702251552795, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.08349609375}, 'model_performance_preservation': {'ce_loss_score': 0.9917763157894737, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.015625, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.7890625, 'mse': 1.328125, 'cossim': 0.9375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 140.0, 'l2_ratio': 0.93359375, 'relative_reconstruction_bias': 1.0}, 'sparsity': {'l0': 320.0, 'l1': 856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '0e9b952b-871d-4ce7-9ed9-96e818c1216c', 'datetime_epoch_millis': 1738123994045, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.13237432110574746, 'scr_metric_threshold_2': 0.06470180818617062, 'scr_dir2_threshold_2': 0.06404012878979931, 'scr_dir1_threshold_5': 0.19715911444320258, 'scr_metric_threshold_5': 0.10614614555416035, 'scr_dir2_threshold_5': 0.10750522265458036, 'scr_dir1_threshold_10': 0.20602627219129846, 'scr_metric_threshold_10': 0.13110591778208402, 'scr_dir2_threshold_10': 0.13181100792268205, 'scr_dir1_threshold_20': 0.20329280529998314, 'scr_metric_threshold_20': 0.15287015862526643, 'scr_dir2_threshold_20': 0.15768317185099726, 'scr_dir1_threshold_50': 0.2120491811053764, 'scr_metric_threshold_50': 0.1802298448942073, 'scr_dir2_threshold_50': 0.18891571918146965, 'scr_dir1_threshold_100': 0.1863658869644765, 'scr_metric_threshold_100': 0.21474250080724952, 'scr_dir2_threshold_100': 0.22145872177849663, 'scr_dir1_threshold_500': 0.13944711141292782, 'scr_metric_threshold_500': 0.22965224222697211, 'scr_dir2_threshold_500': 0.2437974632444002}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.359374374267165, 'scr_metric_threshold_2': 0.0024571168813214595, 'scr_dir2_threshold_2': 0.0024571168813214595, 'scr_dir1_threshold_5': 0.45312510186348476, 'scr_metric_threshold_5': 0.01474211549298745, 'scr_dir2_threshold_5': 0.01474211549298745, 'scr_dir1_threshold_10': 0.42187454889028175, 'scr_metric_threshold_10': 0.019656056358159712, 'scr_dir2_threshold_10': 0.019656056358159712, 'scr_dir1_threshold_20': 0.46874991268844163, 'scr_metric_threshold_20': 0.03439802540241183, 'scr_dir2_threshold_20': 0.03439802540241183, 'scr_dir1_threshold_50': 0.46874991268844163, 'scr_metric_threshold_50': 0.05405408176057155, 'scr_dir2_threshold_50': 0.05405408176057155, 'scr_dir1_threshold_100': 0.45312510186348476, 'scr_metric_threshold_100': 0.0638821099396514, 'scr_dir2_threshold_100': 0.0638821099396514, 'scr_dir1_threshold_500': 0.2968751309673376, 'scr_metric_threshold_500': 0.09582316490947711, 'scr_dir2_threshold_500': 0.09582316490947711}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 0.01941722852250481, 'scr_metric_threshold_2': 0.048710490144909316, 'scr_dir2_threshold_2': 0.048710490144909316, 'scr_dir1_threshold_5': 0.19417459996886668, 'scr_metric_threshold_5': 0.08309451623569147, 'scr_dir2_threshold_5': 0.08309451623569147, 'scr_dir1_threshold_10': 0.2038832142301191, 'scr_metric_threshold_10': 0.10601720029621291, 'scr_dir2_threshold_10': 0.10601720029621291, 'scr_dir1_threshold_20': 0.19417459996886668, 'scr_metric_threshold_20': 0.14040105560003607, 'scr_dir2_threshold_20': 0.14040105560003607, 'scr_dir1_threshold_50': 0.1844659857076143, 'scr_metric_threshold_50': 0.18338113091025351, 'scr_dir2_threshold_50': 0.18338113091025351, 'scr_dir1_threshold_100': 0.1844659857076143, 'scr_metric_threshold_100': 0.21489969340325124, 'scr_dir2_threshold_100': 0.21489969340325124, 'scr_dir1_threshold_500': -0.03883503573096427, 'scr_metric_threshold_500': 0.09455585826595218, 'scr_dir2_threshold_500': 0.09455585826595218}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.42857142857142855, 'scr_metric_threshold_2': 0.015189978091145603, 'scr_dir2_threshold_2': 0.015189978091145603, 'scr_dir1_threshold_5': 0.5396829601737049, 'scr_metric_threshold_5': 0.02784819256855401, 'scr_dir2_threshold_5': 0.02784819256855401, 'scr_dir1_threshold_10': 0.5555557658011382, 'scr_metric_threshold_10': 0.03291141800038962, 'scr_dir2_threshold_10': 0.03291141800038962, 'scr_dir1_threshold_20': 0.5555557658011382, 'scr_metric_threshold_20': 0.05316462152337083, 'scr_dir2_threshold_20': 0.05316462152337083, 'scr_dir1_threshold_50': 0.5396829601737049, 'scr_metric_threshold_50': 0.07848105047818764, 'scr_dir2_threshold_50': 0.07848105047818764, 'scr_dir1_threshold_100': 0.4603179859314168, 'scr_metric_threshold_100': 0.10379747943300446, 'scr_dir2_threshold_100': 0.10379747943300446, 'scr_dir1_threshold_500': 0.17460370021713112, 'scr_metric_threshold_500': 0.017721590807063405, 'scr_dir2_threshold_500': 0.017721590807063405}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': 0.1171878346939954, 'scr_metric_threshold_2': 0.035608379457657886, 'scr_dir2_threshold_2': 0.035608379457657886, 'scr_dir1_threshold_5': 0.17968748544808716, 'scr_metric_threshold_5': 0.09198818094766784, 'scr_dir2_threshold_5': 0.09198818094766784, 'scr_dir1_threshold_10': 0.2265628055901697, 'scr_metric_threshold_10': 0.12166174206152826, 'scr_dir2_threshold_10': 0.12166174206152826, 'scr_dir1_threshold_20': 0.14062502910382568, 'scr_metric_threshold_20': 0.14540066169996685, 'scr_dir2_threshold_20': 0.14540066169996685, 'scr_dir1_threshold_50': 0.10156257275956422, 'scr_metric_threshold_50': 0.16913958133840545, 'scr_dir2_threshold_50': 0.16913958133840545, 'scr_dir1_threshold_100': -0.046874854480871565, 'scr_metric_threshold_100': 0.20771528153377425, 'scr_dir2_threshold_100': 0.20771528153377425, 'scr_dir1_threshold_500': 0.03906245634426147, 'scr_metric_threshold_500': 0.15430280078147524, 'scr_dir2_threshold_500': 0.15430280078147524}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.00546440968122594, 'scr_metric_threshold_2': 0.2549018966193889, 'scr_dir2_threshold_2': 0.2549018966193889, 'scr_dir1_threshold_5': 0.016393554752069144, 'scr_metric_threshold_5': 0.3843138373204936, 'scr_dir2_threshold_5': 0.3843138373204936, 'scr_dir1_threshold_10': 0.03825119347697291, 'scr_metric_threshold_10': 0.466666635500846, 'scr_dir2_threshold_10': 0.466666635500846, 'scr_dir1_threshold_20': -0.027322374114521025, 'scr_metric_threshold_20': 0.5215685788689659, 'scr_dir2_threshold_20': 0.5215685788689659, 'scr_dir1_threshold_50': 0.01092881936245188, 'scr_metric_threshold_50': 0.5607842193113866, 'scr_dir2_threshold_50': 0.5607842193113866, 'scr_dir1_threshold_100': -0.1092894964580841, 'scr_metric_threshold_100': 0.5921568251627849, 'scr_dir2_threshold_100': 0.5921568251627849, 'scr_dir1_threshold_500': -0.1311474608913792, 'scr_metric_threshold_500': 0.6549020368655817, 'scr_dir2_threshold_500': 0.6549020368655817}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.0410254200064373, 'scr_metric_threshold_2': 0.06854845881151235, 'scr_dir2_threshold_2': 0.06854845881151235, 'scr_dir1_threshold_5': 0.06666642213478169, 'scr_metric_threshold_5': 0.08870972781349516, 'scr_dir2_threshold_5': 0.08870972781349516, 'scr_dir1_threshold_10': 0.05641014354938642, 'scr_metric_threshold_10': 0.12096785435318151, 'scr_dir2_threshold_10': 0.12096785435318151, 'scr_dir1_threshold_20': 0.11282028709877284, 'scr_metric_threshold_20': 0.1290322658174608, 'scr_dir2_threshold_20': 0.1290322658174608, 'scr_dir1_threshold_50': 0.158974152062764, 'scr_metric_threshold_50': 0.16935480382142643, 'scr_dir2_threshold_50': 0.16935480382142643, 'scr_dir1_threshold_100': 0.17948701489841076, 'scr_metric_threshold_100': 0.18951607282340924, 'scr_dir2_threshold_100': 0.18951607282340924, 'scr_dir1_threshold_500': 0.21538460127700665, 'scr_metric_threshold_500': 0.30241927537102686, 'scr_dir2_threshold_500': 0.30241927537102686}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.049327255975766836, 'scr_metric_threshold_2': 0.05357152835597378, 'scr_dir2_threshold_2': 0.05357152835597378, 'scr_dir1_threshold_5': 0.07174876026342582, 'scr_metric_threshold_5': 0.10267856311319266, 'scr_dir2_threshold_5': 0.10267856311319266, 'scr_dir1_threshold_10': 0.09417026455108479, 'scr_metric_threshold_10': 0.12946419424511926, 'scr_dir2_threshold_10': 0.12946419424511926, 'scr_dir1_threshold_20': 0.13452902572595388, 'scr_metric_threshold_20': 0.1517858639625321, 'scr_dir2_threshold_20': 0.1517858639625321, 'scr_dir1_threshold_50': 0.20627805327479426, 'scr_metric_threshold_50': 0.20089289871975097, 'scr_dir2_threshold_50': 0.20089289871975097, 'scr_dir1_threshold_100': 0.31390132731264025, 'scr_metric_threshold_100': 0.29017851322092003, 'scr_dir2_threshold_100': 0.29017851322092003, 'scr_dir1_threshold_500': 0.4394618582376964, 'scr_metric_threshold_500': 0.3973215699328676, 'scr_dir2_threshold_500': 0.3973215699328676}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.03862661712745569, 'scr_metric_threshold_2': 0.03862661712745569, 'scr_dir2_threshold_2': 0.033333181956485214, 'scr_dir1_threshold_5': 0.05579403094120067, 'scr_metric_threshold_5': 0.05579403094120067, 'scr_dir2_threshold_5': 0.06666664774456064, 'scr_dir1_threshold_10': 0.05150224144123495, 'scr_metric_threshold_10': 0.05150224144123495, 'scr_dir2_threshold_10': 0.05714296256601923, 'scr_dir1_threshold_20': 0.047210196127387125, 'scr_metric_threshold_20': 0.047210196127387125, 'scr_dir2_threshold_20': 0.08571430193323373, 'scr_dir1_threshold_50': 0.025750992813676425, 'scr_metric_threshold_50': 0.025750992813676425, 'scr_dir2_threshold_50': 0.09523798711177515, 'scr_dir1_threshold_100': 0.05579403094120067, 'scr_metric_threshold_100': 0.05579403094120067, 'scr_dir2_threshold_100': 0.10952379871117751, 'scr_dir1_threshold_500': 0.12017164088233277, 'scr_metric_threshold_500': 0.12017164088233277, 'scr_dir2_threshold_500': 0.2333334090217574}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'cec3b1d1-bb8b-41f8-90b8-b713eb734514', 'datetime_epoch_millis': 1738124057848, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9597812928259372, 'sae_top_1_test_accuracy': 0.7900625, 'sae_top_2_test_accuracy': 0.8382124999999999, 'sae_top_5_test_accuracy': 0.8636625, 'sae_top_10_test_accuracy': 0.89578125, 'sae_top_20_test_accuracy': 0.9201812500000001, 'sae_top_50_test_accuracy': 0.9356312500000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9636000394821167, 'sae_top_1_test_accuracy': 0.7896, 'sae_top_2_test_accuracy': 0.852, 'sae_top_5_test_accuracy': 0.8827999999999999, 'sae_top_10_test_accuracy': 0.925, 'sae_top_20_test_accuracy': 0.9328, 'sae_top_50_test_accuracy': 0.9536, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.841, 'sae_top_2_test_accuracy': 0.844, 'sae_top_5_test_accuracy': 0.8646, 'sae_top_10_test_accuracy': 0.8907999999999999, 'sae_top_20_test_accuracy': 0.9208000000000001, 'sae_top_50_test_accuracy': 0.9236000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9338000297546387, 'sae_top_1_test_accuracy': 0.7988, 'sae_top_2_test_accuracy': 0.8311999999999999, 'sae_top_5_test_accuracy': 0.8629999999999999, 'sae_top_10_test_accuracy': 0.8832000000000001, 'sae_top_20_test_accuracy': 0.8992000000000001, 'sae_top_50_test_accuracy': 0.9092, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000476837158, 'sae_top_1_test_accuracy': 0.7072, 'sae_top_2_test_accuracy': 0.7442, 'sae_top_5_test_accuracy': 0.7985999999999999, 'sae_top_10_test_accuracy': 0.8291999999999999, 'sae_top_20_test_accuracy': 0.8544, 'sae_top_50_test_accuracy': 0.8806, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000641345978, 'sae_top_1_test_accuracy': 0.797, 'sae_top_2_test_accuracy': 0.944, 'sae_top_5_test_accuracy': 0.95, 'sae_top_10_test_accuracy': 0.947, 'sae_top_20_test_accuracy': 0.952, 'sae_top_50_test_accuracy': 0.96, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9698000311851501, 'sae_top_1_test_accuracy': 0.6492, 'sae_top_2_test_accuracy': 0.6674, 'sae_top_5_test_accuracy': 0.7058, 'sae_top_10_test_accuracy': 0.8048, 'sae_top_20_test_accuracy': 0.8785999999999999, 'sae_top_50_test_accuracy': 0.931, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9567500352859497, 'sae_top_1_test_accuracy': 0.8265, 'sae_top_2_test_accuracy': 0.8704999999999999, 'sae_top_5_test_accuracy': 0.8835, 'sae_top_10_test_accuracy': 0.91125, 'sae_top_20_test_accuracy': 0.92825, 'sae_top_50_test_accuracy': 0.93025, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9982000470161438, 'sae_top_1_test_accuracy': 0.9112, 'sae_top_2_test_accuracy': 0.9523999999999999, 'sae_top_5_test_accuracy': 0.961, 'sae_top_10_test_accuracy': 0.975, 'sae_top_20_test_accuracy': 0.9954000000000001, 'sae_top_50_test_accuracy': 0.9968, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'abf22d2a-8885-4d9d-a3cf-367a20df38a5', 'datetime_epoch_millis': 1738124069976, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from jumprelu SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'JumpReLUTrainer', 'activation_dim': 2304, 'dict_size': 65536, 'lr': 0.0003, 'sparsity_penalty': 1.0, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'JumpReLUTrainer', 'jump_coeff': 0.1, 'target_l0': 20.0, 'bandwidth': 0.001}, 'final_info': {'training_steps': 4882, 'final_loss': 26473.599609375, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '5aea69e4-4345-41a4-8b18-70b17142f013', 'datetime_epoch_millis': 1738125160695, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9957540760869565, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.042724609375}, 'model_performance_preservation': {'ce_loss_score': 0.9967105263157895, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.96875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.84375, 'mse': 0.98046875, 'cossim': 0.953125}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 143.0, 'l2_ratio': 0.953125, 'relative_reconstruction_bias': 1.0}, 'sparsity': {'l0': 319.994384765625, 'l1': 924.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '5b676c82-4ea1-4c24-a45b-81285a8d0e77', 'datetime_epoch_millis': 1738125503941, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.02163248562962354, 'scr_metric_threshold_2': 0.013775576175600678, 'scr_dir2_threshold_2': 0.011747162721208801, 'scr_dir1_threshold_5': 0.00797513200245279, 'scr_metric_threshold_5': 0.012243467722626217, 'scr_dir2_threshold_5': 0.01581488514352803, 'scr_dir1_threshold_10': 0.005620844078476961, 'scr_metric_threshold_10': 0.021725976871906364, 'scr_dir2_threshold_10': 0.0254149430440832, 'scr_dir1_threshold_20': -0.0042442461395118146, 'scr_metric_threshold_20': 0.02798841060137652, 'scr_dir2_threshold_20': 0.03114087110932238, 'scr_dir1_threshold_50': -0.037368683077093755, 'scr_metric_threshold_50': 0.03933976502243071, 'scr_dir2_threshold_50': 0.04725410359859606, 'scr_dir1_threshold_100': -0.041929115762227756, 'scr_metric_threshold_100': 0.04968711624808444, 'scr_dir2_threshold_100': 0.058850707586679374, 'scr_dir1_threshold_500': -0.061147481563771526, 'scr_metric_threshold_500': 0.1546941453598473, 'scr_dir2_threshold_500': 0.15686816891559102}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.10937460709798731, 'scr_metric_threshold_2': -0.0024569704325861324, 'scr_dir2_threshold_2': -0.0024569704325861324, 'scr_dir1_threshold_5': 0.10937460709798731, 'scr_metric_threshold_5': 0.0024571168813214595, 'scr_dir2_threshold_5': 0.0024571168813214595, 'scr_dir1_threshold_10': 0.10937460709798731, 'scr_metric_threshold_10': 0.007371057746493725, 'scr_dir2_threshold_10': 0.007371057746493725, 'scr_dir1_threshold_20': 0.12499941792294417, 'scr_metric_threshold_20': 0.017199085925573582, 'scr_dir2_threshold_20': 0.017199085925573582, 'scr_dir1_threshold_50': 0.1562499708961472, 'scr_metric_threshold_50': 0.02948408453723957, 'scr_dir2_threshold_50': 0.02948408453723957, 'scr_dir1_threshold_100': 0.17187478172110407, 'scr_metric_threshold_100': 0.039312112716319424, 'scr_dir2_threshold_100': 0.039312112716319424, 'scr_dir1_threshold_500': 0.21875014551926394, 'scr_metric_threshold_500': 0.10565119308855696, 'scr_dir2_threshold_500': 0.10565119308855696}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 0.009708614261252404, 'scr_metric_threshold_2': 0.005730585621650868, 'scr_dir2_threshold_2': 0.005730585621650868, 'scr_dir1_threshold_5': -0.05825284293942373, 'scr_metric_threshold_5': -0.005730756408609849, 'scr_dir2_threshold_5': -0.005730756408609849, 'scr_dir1_threshold_10': -0.06796145720067613, 'scr_metric_threshold_10': 0.011461342030260717, 'scr_dir2_threshold_10': 0.011461342030260717, 'scr_dir1_threshold_20': -0.06796145720067613, 'scr_metric_threshold_20': 0.017191927651911583, 'scr_dir2_threshold_20': 0.017191927651911583, 'scr_dir1_threshold_50': -0.23300963569983096, 'scr_metric_threshold_50': 0.02005722046273702, 'scr_dir2_threshold_50': 0.02005722046273702, 'scr_dir1_threshold_100': -0.3495147428927238, 'scr_metric_threshold_100': 0.06876788139460532, 'scr_dir2_threshold_100': 0.06876788139460532, 'scr_dir1_threshold_500': -0.7572817500389166, 'scr_metric_threshold_500': 0.19484230215355525, 'scr_dir2_threshold_500': 0.19484230215355525}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.0634921686148548, 'scr_metric_threshold_2': -0.0025316127159178037, 'scr_dir2_threshold_2': -0.0025316127159178037, 'scr_dir1_threshold_5': 0.015873751732555005, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0, 'scr_dir1_threshold_10': 0.0, 'scr_metric_threshold_10': -0.0025316127159178037, 'scr_dir2_threshold_10': -0.0025316127159178037, 'scr_dir1_threshold_20': -0.04761841688229979, 'scr_metric_threshold_20': -0.0025316127159178037, 'scr_dir2_threshold_20': -0.0025316127159178037, 'scr_dir1_threshold_50': -0.1111105854971546, 'scr_metric_threshold_50': 0.010126601761490606, 'scr_dir2_threshold_50': 0.010126601761490606, 'scr_dir1_threshold_100': -0.1111105854971546, 'scr_metric_threshold_100': 0.02025320352298121, 'scr_dir2_threshold_100': 0.02025320352298121, 'scr_dir1_threshold_500': -0.1111105854971546, 'scr_metric_threshold_500': 0.07594943776226984, 'scr_dir2_threshold_500': 0.07594943776226984}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.015624796273220196, 'scr_metric_threshold_2': 0.005934818343797456, 'scr_dir2_threshold_2': 0.005934818343797456, 'scr_dir1_threshold_5': -0.03125005820765137, 'scr_metric_threshold_5': 0.014836957425305832, 'scr_dir2_threshold_5': 0.014836957425305832, 'scr_dir1_threshold_10': -0.046874854480871565, 'scr_metric_threshold_10': 0.011869459819219295, 'scr_dir2_threshold_10': 0.011869459819219295, 'scr_dir1_threshold_20': -0.12499976716939451, 'scr_metric_threshold_20': 0.020771598900727672, 'scr_dir2_threshold_20': 0.020771598900727672, 'scr_dir1_threshold_50': -0.22656233992895872, 'scr_metric_threshold_50': 0.011869459819219295, 'scr_dir2_threshold_50': 0.011869459819219295, 'scr_dir1_threshold_100': -0.2890624563442615, 'scr_metric_threshold_100': -0.023738742770062975, 'scr_dir2_threshold_100': -0.023738742770062975, 'scr_dir1_threshold_500': -0.5937497089617432, 'scr_metric_threshold_500': 0.07715140039073762, 'scr_dir2_threshold_500': 0.07715140039073762}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': -0.027322374114521025, 'scr_metric_threshold_2': 0.03529412314690952, 'scr_dir2_threshold_2': 0.03529412314690952, 'scr_dir1_threshold_5': -0.04371592886659017, 'scr_metric_threshold_5': 0.03137260585139841, 'scr_dir2_threshold_5': 0.03137260585139841, 'scr_dir1_threshold_10': -0.03825151918536423, 'scr_metric_threshold_10': 0.06274521170279682, 'scr_dir2_threshold_10': 0.06274521170279682, 'scr_dir1_threshold_20': -0.03825151918536423, 'scr_metric_threshold_20': 0.06666672899830793, 'scr_dir2_threshold_20': 0.06666672899830793, 'scr_dir1_threshold_50': -0.07103830298111119, 'scr_metric_threshold_50': 0.05490194336811984, 'scr_dir2_threshold_50': 0.05490194336811984, 'scr_dir1_threshold_100': -0.00546440968122594, 'scr_metric_threshold_100': 0.06274521170279682, 'scr_dir2_threshold_100': 0.06274521170279682, 'scr_dir1_threshold_500': 0.13661187057260513, 'scr_metric_threshold_500': 0.23921582743734443, 'scr_dir2_threshold_500': 0.23921582743734443}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': -0.010256584250251485, 'scr_metric_threshold_2': 0.020161269001982816, 'scr_dir2_threshold_2': 0.020161269001982816, 'scr_dir1_threshold_5': 0.03589728071373967, 'scr_metric_threshold_5': 0.028225920807546715, 'scr_dir2_threshold_5': 0.028225920807546715, 'scr_dir1_threshold_10': 0.03076914142104203, 'scr_metric_threshold_10': 0.056451601273808806, 'scr_dir2_threshold_10': 0.056451601273808806, 'scr_dir1_threshold_20': 0.03076914142104203, 'scr_metric_threshold_20': 0.060483807005948444, 'scr_dir2_threshold_20': 0.060483807005948444, 'scr_dir1_threshold_50': 0.06153828284208406, 'scr_metric_threshold_50': 0.11290320254761761, 'scr_dir2_threshold_50': 0.11290320254761761, 'scr_dir1_threshold_100': 0.08717928497042846, 'scr_metric_threshold_100': 0.1370969176230247, 'scr_dir2_threshold_100': 0.1370969176230247, 'scr_dir1_threshold_500': 0.18974359914866223, 'scr_metric_threshold_500': 0.18951607282340924, 'scr_dir2_threshold_500': 0.18951607282340924}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.0179372568872101, 'scr_metric_threshold_2': 0.02232140362529227, 'scr_dir2_threshold_2': 0.02232140362529227, 'scr_dir1_threshold_5': 0.03587424648900561, 'scr_metric_threshold_5': 0.02678589722404717, 'scr_dir2_threshold_5': 0.02678589722404717, 'scr_dir1_threshold_10': 0.049327255975766836, 'scr_metric_threshold_10': 0.017857176118657926, 'scr_dir2_threshold_10': 0.017857176118657926, 'scr_dir1_threshold_20': 0.0762330076638747, 'scr_metric_threshold_20': 0.03125012473068151, 'scr_dir2_threshold_20': 0.03125012473068151, 'scr_dir1_threshold_50': 0.11210752143829489, 'scr_metric_threshold_50': 0.062499983369242465, 'scr_dir2_threshold_50': 0.062499983369242465, 'scr_dir1_threshold_100': 0.14349778781226621, 'scr_metric_threshold_100': 0.07589293198126605, 'scr_dir2_threshold_100': 0.07589293198126605, 'scr_dir1_threshold_500': 0.26905831873732233, 'scr_metric_threshold_500': 0.19642867121311663, 'scr_dir2_threshold_500': 0.19642867121311663}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.025750992813676425, 'scr_metric_threshold_2': 0.025750992813676425, 'scr_dir2_threshold_2': 0.009523685178541423, 'scr_dir1_threshold_5': 0.0, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0285713393672145, 'scr_dir1_threshold_10': 0.008583578999931441, 'scr_metric_threshold_10': 0.008583578999931441, 'scr_dir2_threshold_10': 0.03809530837734615, 'scr_dir1_threshold_20': 0.012875624313779262, 'scr_metric_threshold_20': 0.012875624313779262, 'scr_dir2_threshold_20': 0.03809530837734615, 'scr_dir1_threshold_50': 0.012875624313779262, 'scr_metric_threshold_50': 0.012875624313779262, 'scr_dir2_threshold_50': 0.07619033292310208, 'scr_dir1_threshold_100': 0.01716741381374498, 'scr_metric_threshold_100': 0.01716741381374498, 'scr_dir2_threshold_100': 0.09047614452250444, 'scr_dir1_threshold_500': 0.15879825800978847, 'scr_metric_threshold_500': 0.15879825800978847, 'scr_dir2_threshold_500': 0.17619044645573817}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '350f2df7-bc1a-4b57-a180-ccdeedf190e0', 'datetime_epoch_millis': 1738125574126, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9616312891244888, 'sae_top_1_test_accuracy': 0.69450625, 'sae_top_2_test_accuracy': 0.7569937499999999, 'sae_top_5_test_accuracy': 0.8077375, 'sae_top_10_test_accuracy': 0.8382624999999999, 'sae_top_20_test_accuracy': 0.8718437500000001, 'sae_top_50_test_accuracy': 0.9135812499999999, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9712000370025635, 'sae_top_1_test_accuracy': 0.7774000000000001, 'sae_top_2_test_accuracy': 0.8262, 'sae_top_5_test_accuracy': 0.8396000000000001, 'sae_top_10_test_accuracy': 0.8615999999999999, 'sae_top_20_test_accuracy': 0.8918000000000001, 'sae_top_50_test_accuracy': 0.9244, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9562000513076783, 'sae_top_1_test_accuracy': 0.7114, 'sae_top_2_test_accuracy': 0.7346000000000001, 'sae_top_5_test_accuracy': 0.775, 'sae_top_10_test_accuracy': 0.8155999999999999, 'sae_top_20_test_accuracy': 0.8508000000000001, 'sae_top_50_test_accuracy': 0.9154, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9362000346183776, 'sae_top_1_test_accuracy': 0.6599999999999999, 'sae_top_2_test_accuracy': 0.7213999999999999, 'sae_top_5_test_accuracy': 0.7972, 'sae_top_10_test_accuracy': 0.8362, 'sae_top_20_test_accuracy': 0.858, 'sae_top_50_test_accuracy': 0.8814, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9216000437736511, 'sae_top_1_test_accuracy': 0.6537999999999999, 'sae_top_2_test_accuracy': 0.686, 'sae_top_5_test_accuracy': 0.758, 'sae_top_10_test_accuracy': 0.7934000000000001, 'sae_top_20_test_accuracy': 0.8168, 'sae_top_50_test_accuracy': 0.8504000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.979500025510788, 'sae_top_1_test_accuracy': 0.689, 'sae_top_2_test_accuracy': 0.772, 'sae_top_5_test_accuracy': 0.83, 'sae_top_10_test_accuracy': 0.855, 'sae_top_20_test_accuracy': 0.891, 'sae_top_50_test_accuracy': 0.948, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9702000379562378, 'sae_top_1_test_accuracy': 0.5944, 'sae_top_2_test_accuracy': 0.6974, 'sae_top_5_test_accuracy': 0.7425999999999999, 'sae_top_10_test_accuracy': 0.7882, 'sae_top_20_test_accuracy': 0.8656, 'sae_top_50_test_accuracy': 0.9100000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9587500393390656, 'sae_top_1_test_accuracy': 0.76425, 'sae_top_2_test_accuracy': 0.80575, 'sae_top_5_test_accuracy': 0.8684999999999999, 'sae_top_10_test_accuracy': 0.8795, 'sae_top_20_test_accuracy': 0.90075, 'sae_top_50_test_accuracy': 0.91825, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9994000434875489, 'sae_top_1_test_accuracy': 0.7058, 'sae_top_2_test_accuracy': 0.8126, 'sae_top_5_test_accuracy': 0.851, 'sae_top_10_test_accuracy': 0.8766, 'sae_top_20_test_accuracy': 0.9, 'sae_top_50_test_accuracy': 0.9608000000000001, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'a04e01fa-e075-47c1-b969-a23d14e827cc', 'datetime_epoch_millis': 1738125586025, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 65536, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 554.7803955078125, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'db99d2ac-f339-444f-abc3-ab3dbff3ac4f', 'datetime_epoch_millis': 1738122093122, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995905716226708, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004119873046875}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.98828125, 'mse': 0.08154296875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.9921875}, 'sparsity': {'l0': 8311.291015625, 'l1': 10624.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '2cd18ab3-69b8-49fb-b329-542d864ccd8b', 'datetime_epoch_millis': 1738122399973, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.027608449030321582, 'scr_metric_threshold_2': 0.005305287226446962, 'scr_dir2_threshold_2': 0.0022115513779289633, 'scr_dir1_threshold_5': 0.030822806181597877, 'scr_metric_threshold_5': 0.02093731432318256, 'scr_dir2_threshold_5': 0.015513756858591759, 'scr_dir1_threshold_10': 0.018658375052698073, 'scr_metric_threshold_10': 0.027692526555416326, 'scr_dir2_threshold_10': 0.035908344251160164, 'scr_dir1_threshold_20': -0.09747346364731523, 'scr_metric_threshold_20': 0.036469946578223826, 'scr_dir2_threshold_20': 0.04206977745351752, 'scr_dir1_threshold_50': -0.22401566666710962, 'scr_metric_threshold_50': 0.09246180292099579, 'scr_dir2_threshold_50': 0.09956122701857269, 'scr_dir1_threshold_100': -0.3645932090682159, 'scr_metric_threshold_100': 0.1386625503200265, 'scr_dir2_threshold_100': 0.13324664981303627, 'scr_dir1_threshold_500': -1.1858990549174069, 'scr_metric_threshold_500': 0.24651666349430923, 'scr_dir2_threshold_500': 0.22477133045516998}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.03124962164991371, 'scr_metric_threshold_2': -0.004913940865172265, 'scr_dir2_threshold_2': -0.004913940865172265, 'scr_dir1_threshold_5': 0.04687443247487057, 'scr_metric_threshold_5': 0.009828028179079856, 'scr_dir2_threshold_5': 0.009828028179079856, 'scr_dir1_threshold_10': 0.04687443247487057, 'scr_metric_threshold_10': 0.017199085925573582, 'scr_dir2_threshold_10': 0.017199085925573582, 'scr_dir1_threshold_20': 0.015624810824956856, 'scr_metric_threshold_20': 0.036855142283733294, 'scr_dir2_threshold_20': 0.036855142283733294, 'scr_dir1_threshold_50': 0.015624810824956856, 'scr_metric_threshold_50': 0.1670761861468869, 'scr_dir2_threshold_50': 0.1670761861468869, 'scr_dir1_threshold_100': 0.06249924329982742, 'scr_metric_threshold_100': 0.1793611847585529, 'scr_dir2_threshold_100': 0.1793611847585529, 'scr_dir1_threshold_500': -0.34375049476549746, 'scr_metric_threshold_500': 0.23341526651912445, 'scr_dir2_threshold_500': 0.23341526651912445}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': -0.08737868572318094, 'scr_metric_threshold_2': 0.034383855303823166, 'scr_dir2_threshold_2': 0.034383855303823166, 'scr_dir1_threshold_5': -0.2718446714307952, 'scr_metric_threshold_5': 0.03151856249299773, 'scr_dir2_threshold_5': 0.03151856249299773, 'scr_dir1_threshold_10': -0.24271882864703803, 'scr_metric_threshold_10': 0.06876788139460532, 'scr_dir2_threshold_10': 0.06876788139460532, 'scr_dir1_threshold_20': -0.7475731357776643, 'scr_metric_threshold_20': 0.04011461171243302, 'scr_dir2_threshold_20': 0.04011461171243302, 'scr_dir1_threshold_50': -1.3009710929005072, 'scr_metric_threshold_50': 0.12034383513729906, 'scr_dir2_threshold_50': 0.12034383513729906, 'scr_dir1_threshold_100': -2.242718828647038, 'scr_metric_threshold_100': 0.18911171653190437, 'scr_dir2_threshold_100': 0.18911171653190437, 'scr_dir1_threshold_500': -3.2330102143857857, 'scr_metric_threshold_500': 0.21489969340325124, 'scr_dir2_threshold_500': 0.21489969340325124}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.04761936298742153, 'scr_metric_threshold_2': 0.005063376329654997, 'scr_dir2_threshold_2': 0.005063376329654997, 'scr_dir1_threshold_5': 0.04761936298742153, 'scr_metric_threshold_5': -0.0025316127159178037, 'scr_dir2_threshold_5': -0.0025316127159178037, 'scr_dir1_threshold_10': 0.07936497424228806, 'scr_metric_threshold_10': 0.025316579852636207, 'scr_dir2_threshold_10': 0.025316579852636207, 'scr_dir1_threshold_20': 0.07936497424228806, 'scr_metric_threshold_20': 0.03544318161412681, 'scr_dir2_threshold_20': 0.03544318161412681, 'scr_dir1_threshold_50': -0.015872805627433265, 'scr_metric_threshold_50': 0.09873425400116885, 'scr_dir2_threshold_50': 0.09873425400116885, 'scr_dir1_threshold_100': -0.3333327025965855, 'scr_metric_threshold_100': 0.13924051014931188, 'scr_dir2_threshold_100': 0.13924051014931188, 'scr_dir1_threshold_500': -4.761903185062892, 'scr_metric_threshold_500': 0.3772152742997926, 'scr_dir2_threshold_500': 0.3772152742997926}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.007812398136610098, 'scr_metric_threshold_2': 0.002967497606086536, 'scr_dir2_threshold_2': 0.002967497606086536, 'scr_dir1_threshold_5': 0.007812863797821078, 'scr_metric_threshold_5': 0.03264105871994697, 'scr_dir2_threshold_5': 0.03264105871994697, 'scr_dir1_threshold_10': -0.08593731082513303, 'scr_metric_threshold_10': 0.026706240376149513, 'scr_dir2_threshold_10': 0.026706240376149513, 'scr_dir1_threshold_20': -0.22656233992895872, 'scr_metric_threshold_20': 0.035608379457657886, 'scr_dir2_threshold_20': 0.035608379457657886, 'scr_dir1_threshold_50': -0.7734371944098303, 'scr_metric_threshold_50': 0.09792299929146529, 'scr_dir2_threshold_50': 0.09792299929146529, 'scr_dir1_threshold_100': -1.1171873690327845, 'scr_metric_threshold_100': 0.16617226060069454, 'scr_dir2_threshold_100': 0.16617226060069454, 'scr_dir1_threshold_500': -2.257811932475399, 'scr_metric_threshold_500': 0.2818991843184253, 'scr_dir2_threshold_500': 0.2818991843184253}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.1092894964580841, 'scr_metric_threshold_2': -0.03529412314690952, 'scr_dir2_threshold_2': -0.03529412314690952, 'scr_dir1_threshold_5': 0.12568305121015325, 'scr_metric_threshold_5': 0.011764785630188087, 'scr_dir2_threshold_5': 0.011764785630188087, 'scr_dir1_threshold_10': 0.05464474822904205, 'scr_metric_threshold_10': 0.04705890877709761, 'scr_dir2_threshold_10': 0.04705890877709761, 'scr_dir1_threshold_20': -0.00546440968122594, 'scr_metric_threshold_20': 0.06666672899830793, 'scr_dir2_threshold_20': 0.06666672899830793, 'scr_dir1_threshold_50': 0.027322374114521025, 'scr_metric_threshold_50': 0.08235303192400713, 'scr_dir2_threshold_50': 0.08235303192400713, 'scr_dir1_threshold_100': 0.20765017355371632, 'scr_metric_threshold_100': 0.09019606651502937, 'scr_dir2_threshold_100': 0.09019606651502937, 'scr_dir1_threshold_500': 0.4863386500885438, 'scr_metric_threshold_500': -0.14509800988314922, 'scr_dir2_threshold_500': -0.14509800988314922}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.05128200425668879, 'scr_metric_threshold_2': 0.04435498407738989, 'scr_dir2_threshold_2': 0.04435498407738989, 'scr_dir1_threshold_5': 0.07692300638503319, 'scr_metric_threshold_5': 0.032258126539686356, 'scr_dir2_threshold_5': 0.032258126539686356, 'scr_dir1_threshold_10': 0.07692300638503319, 'scr_metric_threshold_10': -0.0040322057321396385, 'scr_dir2_threshold_10': -0.0040322057321396385, 'scr_dir1_threshold_20': -0.17435918127056935, 'scr_metric_threshold_20': 0.04032253800396563, 'scr_dir2_threshold_20': 0.04032253800396563, 'scr_dir1_threshold_50': 0.07179486709233554, 'scr_metric_threshold_50': 0.12500006008532116, 'scr_dir2_threshold_50': 0.12500006008532116, 'scr_dir1_threshold_100': 0.10256400851337757, 'scr_metric_threshold_100': 0.11290320254761761, 'scr_dir2_threshold_100': 0.11290320254761761, 'scr_dir1_threshold_500': -0.07179517275719176, 'scr_metric_threshold_500': 0.4516130505317551, 'scr_dir2_threshold_500': 0.4516130505317551}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.08520176975018703, 'scr_metric_threshold_2': 0.0044642275066343415, 'scr_dir2_threshold_2': 0.0044642275066343415, 'scr_dir1_threshold_5': 0.17488778690082293, 'scr_metric_threshold_5': 0.013392948612023585, 'scr_dir2_threshold_5': 0.013392948612023585, 'scr_dir1_threshold_10': 0.22869955756245325, 'scr_metric_threshold_10': 0.04910730084933944, 'scr_dir2_threshold_10': 0.04910730084933944, 'scr_dir1_threshold_20': 0.30493256522632795, 'scr_metric_threshold_20': 0.062499983369242465, 'scr_dir2_threshold_20': 0.062499983369242465, 'scr_dir1_threshold_50': 0.27354256613777117, 'scr_metric_threshold_50': 0.13839291535050852, 'scr_dir2_threshold_50': 0.13839291535050852, 'scr_dir1_threshold_100': 0.40807159186372505, 'scr_metric_threshold_100': 0.23660725095706683, 'scr_dir2_threshold_100': 0.23660725095706683, 'scr_dir1_threshold_500': 0.5874438934504115, 'scr_metric_threshold_500': 0.4508928321967208, 'scr_dir2_threshold_500': 0.4508928321967208}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': -0.008583578999931441, 'scr_metric_threshold_2': -0.008583578999931441, 'scr_dir2_threshold_2': -0.03333346578807544, 'scr_dir1_threshold_5': 0.03862661712745569, 'scr_metric_threshold_5': 0.03862661712745569, 'scr_dir2_threshold_5': -0.004761842589270712, 'scr_dir1_threshold_10': -0.008583578999931441, 'scr_metric_threshold_10': -0.008583578999931441, 'scr_dir2_threshold_10': 0.05714296256601923, 'scr_dir1_threshold_20': -0.025750992813676425, 'scr_metric_threshold_20': -0.025750992813676425, 'scr_dir2_threshold_20': 0.019047654188673074, 'scr_dir1_threshold_50': -0.09012885856869063, 'scr_metric_threshold_50': -0.09012885856869063, 'scr_dir2_threshold_50': -0.03333346578807544, 'scr_dir1_threshold_100': -0.004291789499965721, 'scr_metric_threshold_100': -0.004291789499965721, 'scr_dir2_threshold_100': -0.04761899355588758, 'scr_dir1_threshold_500': 0.10729601656855352, 'scr_metric_threshold_500': 0.10729601656855352, 'scr_dir2_threshold_500': -0.06666664774456064}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'e6ee1070-9fb3-4e3f-b0fd-b799643b4aed', 'datetime_epoch_millis': 1738122464283, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.95824378952384, 'sae_top_1_test_accuracy': 0.697525, 'sae_top_2_test_accuracy': 0.73530625, 'sae_top_5_test_accuracy': 0.78453125, 'sae_top_10_test_accuracy': 0.824875, 'sae_top_20_test_accuracy': 0.8698687500000001, 'sae_top_50_test_accuracy': 0.9177312499999999, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9658000469207764, 'sae_top_1_test_accuracy': 0.7064, 'sae_top_2_test_accuracy': 0.7888, 'sae_top_5_test_accuracy': 0.8160000000000001, 'sae_top_10_test_accuracy': 0.841, 'sae_top_20_test_accuracy': 0.8868, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9510000467300415, 'sae_top_1_test_accuracy': 0.6986, 'sae_top_2_test_accuracy': 0.7236000000000001, 'sae_top_5_test_accuracy': 0.7632, 'sae_top_10_test_accuracy': 0.8211999999999999, 'sae_top_20_test_accuracy': 0.8642, 'sae_top_50_test_accuracy': 0.8995999999999998, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.93340003490448, 'sae_top_1_test_accuracy': 0.6783999999999999, 'sae_top_2_test_accuracy': 0.6918, 'sae_top_5_test_accuracy': 0.7516, 'sae_top_10_test_accuracy': 0.8006, 'sae_top_20_test_accuracy': 0.8476000000000001, 'sae_top_50_test_accuracy': 0.8834, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9178000450134277, 'sae_top_1_test_accuracy': 0.6314, 'sae_top_2_test_accuracy': 0.666, 'sae_top_5_test_accuracy': 0.7090000000000001, 'sae_top_10_test_accuracy': 0.7612, 'sae_top_20_test_accuracy': 0.7952000000000001, 'sae_top_50_test_accuracy': 0.8513999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9800000488758087, 'sae_top_1_test_accuracy': 0.887, 'sae_top_2_test_accuracy': 0.892, 'sae_top_5_test_accuracy': 0.903, 'sae_top_10_test_accuracy': 0.898, 'sae_top_20_test_accuracy': 0.91, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9674000382423401, 'sae_top_1_test_accuracy': 0.6392, 'sae_top_2_test_accuracy': 0.6900000000000001, 'sae_top_5_test_accuracy': 0.7327999999999999, 'sae_top_10_test_accuracy': 0.7622, 'sae_top_20_test_accuracy': 0.8298, 'sae_top_50_test_accuracy': 0.9067999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9517500400543213, 'sae_top_1_test_accuracy': 0.714, 'sae_top_2_test_accuracy': 0.76525, 'sae_top_5_test_accuracy': 0.8572500000000001, 'sae_top_10_test_accuracy': 0.886, 'sae_top_20_test_accuracy': 0.9087500000000001, 'sae_top_50_test_accuracy': 0.92625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9988000154495239, 'sae_top_1_test_accuracy': 0.6252, 'sae_top_2_test_accuracy': 0.665, 'sae_top_5_test_accuracy': 0.7434000000000001, 'sae_top_10_test_accuracy': 0.8288, 'sae_top_20_test_accuracy': 0.9166000000000001, 'sae_top_50_test_accuracy': 0.9894000000000001, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'bd06b69e-471f-4cf2-a376-eba3a6c0c86f', 'datetime_epoch_millis': 1738122476463, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Description: Baseline results.
