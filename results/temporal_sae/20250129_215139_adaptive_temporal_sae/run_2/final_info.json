{
  "training results for layer 12": {
    "config": {
      "trainer_class": "JumpReLUTrainer",
      "activation_dim": 2304,
      "dict_size": 65536,
      "lr": 0.0003,
      "sparsity_penalty": 1.0,
      "warmup_steps": 1000,
      "resample_steps": null,
      "device": "cuda",
      "layer": 12,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "JumpReLUTrainer",
      "jump_coeff": 0.1,
      "target_l0": 20.0,
      "bandwidth": 0.001
    },
    "final_info": {
      "training_steps": 4882,
      "final_loss": 23767.09375,
      "layer": 12,
      "dict_size": 65536,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "d0fdb752-503b-4bd7-a2f4-ba4325fb1d55",
    "datetime_epoch_millis": 1738216024785,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.0,
        "mean_num_split_features": 1.0
      }
    },
    "eval_result_details": [
      {
        "first_letter": "d",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1648,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 718,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1137,
        "num_split_features": 1
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1869,
        "num_split_features": 1
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 178,
        "num_split_features": 1
      },
      {
        "first_letter": "x",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 81,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 149,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 222,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 65536,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "JumpReLU",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "jumprelu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null,
      "jump_coeff": 0.1
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_12_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_12_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.9797166149068323,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 0.2041015625
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.9802631578947368,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 3.125,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": 0.7734375,
        "mse": 1.4453125,
        "cossim": 0.93359375
      },
      "shrinkage": {
        "l2_norm_in": 149.0,
        "l2_norm_out": 146.0,
        "l2_ratio": 0.98046875,
        "relative_reconstruction_bias": 1.046875
      },
      "sparsity": {
        "l0": 3216.169677734375,
        "l1": 2256.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr and tpp evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "f2a48e07-f593-462a-8b24-7b40e9ba0020",
    "datetime_epoch_millis": 1738216369479,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": 0.06215963425313179,
        "scr_metric_threshold_2": 0.016688798642914265,
        "scr_dir2_threshold_2": 0.01466038518852239,
        "scr_dir1_threshold_5": 0.06951341818751589,
        "scr_metric_threshold_5": 0.01792213007063347,
        "scr_dir2_threshold_5": 0.019649064405446864,
        "scr_dir1_threshold_10": 0.0873434754400127,
        "scr_metric_threshold_10": 0.02562280014875172,
        "scr_dir2_threshold_10": 0.027584764530431127,
        "scr_dir1_threshold_20": 0.0764890079319361,
        "scr_metric_threshold_20": 0.03316693593608922,
        "scr_dir2_threshold_20": 0.0340559529427772,
        "scr_dir1_threshold_50": 0.0321414098124637,
        "scr_metric_threshold_50": 0.04428255229433174,
        "scr_dir2_threshold_50": 0.0497572094374851,
        "scr_dir1_threshold_100": 0.02818014722692308,
        "scr_metric_threshold_100": 0.05284203008756051,
        "scr_dir2_threshold_100": 0.065224495527865,
        "scr_dir1_threshold_500": 0.036198689823621275,
        "scr_metric_threshold_500": 0.11780465866970014,
        "scr_dir2_threshold_500": 0.13004663996487667
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": 0.17187478172110407,
        "scr_metric_threshold_2": -0.0024569704325861324,
        "scr_dir2_threshold_2": -0.0024569704325861324,
        "scr_dir1_threshold_5": 0.20312440337101778,
        "scr_metric_threshold_5": 0.0024571168813214595,
        "scr_dir2_threshold_5": 0.0024571168813214595,
        "scr_dir1_threshold_10": 0.21875014551926394,
        "scr_metric_threshold_10": 0.0024571168813214595,
        "scr_dir2_threshold_10": 0.0024571168813214595,
        "scr_dir1_threshold_20": 0.20312440337101778,
        "scr_metric_threshold_20": 0.012284998611665989,
        "scr_dir2_threshold_20": 0.012284998611665989,
        "scr_dir1_threshold_50": 0.21875014551926394,
        "scr_metric_threshold_50": 0.036855142283733294,
        "scr_dir2_threshold_50": 0.036855142283733294,
        "scr_dir1_threshold_100": 0.23437495634422081,
        "scr_metric_threshold_100": 0.02948408453723957,
        "scr_dir2_threshold_100": 0.02948408453723957,
        "scr_dir1_threshold_500": 0.2656245779941345,
        "scr_metric_threshold_500": 0.05405408176057155,
        "scr_dir2_threshold_500": 0.05405408176057155
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": 0.09708729998443334,
        "scr_metric_threshold_2": 0.005730585621650868,
        "scr_dir2_threshold_2": 0.005730585621650868,
        "scr_dir1_threshold_5": 0.11650452850693815,
        "scr_metric_threshold_5": 0.002865292810825434,
        "scr_dir2_threshold_5": 0.002865292810825434,
        "scr_dir1_threshold_10": 0.1359223357153976,
        "scr_metric_threshold_10": -0.011461342030260717,
        "scr_dir2_threshold_10": -0.011461342030260717,
        "scr_dir1_threshold_20": 0.11650452850693815,
        "scr_metric_threshold_20": -0.005730756408609849,
        "scr_dir2_threshold_20": -0.005730756408609849,
        "scr_dir1_threshold_50": -0.07767007146192854,
        "scr_metric_threshold_50": 0.011461342030260717,
        "scr_dir2_threshold_50": 0.011461342030260717,
        "scr_dir1_threshold_100": -0.1359223357153976,
        "scr_metric_threshold_100": 0.0286532696821723,
        "scr_dir2_threshold_100": 0.0286532696821723,
        "scr_dir1_threshold_500": -0.43689342861590474,
        "scr_metric_threshold_500": 0.05444124655351917,
        "scr_dir2_threshold_500": 0.05444124655351917
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.14285714285714285,
        "scr_metric_threshold_2": 0.002531763613737194,
        "scr_dir2_threshold_2": 0.002531763613737194,
        "scr_dir1_threshold_5": 0.17460370021713112,
        "scr_metric_threshold_5": 0.002531763613737194,
        "scr_dir2_threshold_5": 0.002531763613737194,
        "scr_dir1_threshold_10": 0.19047650584456438,
        "scr_metric_threshold_10": 0.002531763613737194,
        "scr_dir2_threshold_10": 0.002531763613737194,
        "scr_dir1_threshold_20": 0.1269843372297096,
        "scr_metric_threshold_20": 0.005063376329654997,
        "scr_dir2_threshold_20": 0.005063376329654997,
        "scr_dir1_threshold_50": 0.07936497424228806,
        "scr_metric_threshold_50": 0.025316579852636207,
        "scr_dir2_threshold_50": 0.025316579852636207,
        "scr_dir1_threshold_100": 0.1587299484845761,
        "scr_metric_threshold_100": 0.03291141800038962,
        "scr_dir2_threshold_100": 0.03291141800038962,
        "scr_dir1_threshold_500": 0.26984148008685244,
        "scr_metric_threshold_500": 0.022784816238899015,
        "scr_dir2_threshold_500": 0.022784816238899015
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": 0.05468771827869265,
        "scr_metric_threshold_2": 0.011869459819219295,
        "scr_dir2_threshold_2": 0.011869459819219295,
        "scr_dir1_threshold_5": 0.03906245634426147,
        "scr_metric_threshold_5": 0.017804278163016753,
        "scr_dir2_threshold_5": 0.017804278163016753,
        "scr_dir1_threshold_10": 0.09375017462295412,
        "scr_metric_threshold_10": 0.02373891963843859,
        "scr_dir2_threshold_10": 0.02373891963843859,
        "scr_dir1_threshold_20": 0.09375017462295412,
        "scr_metric_threshold_20": 0.03857570019536881,
        "scr_dir2_threshold_20": 0.03857570019536881,
        "scr_dir1_threshold_50": -0.046874854480871565,
        "scr_metric_threshold_50": 0.029673737982236048,
        "scr_dir2_threshold_50": 0.029673737982236048,
        "scr_dir1_threshold_100": -0.1484374272404358,
        "scr_metric_threshold_100": 0.008902139081508376,
        "scr_dir2_threshold_100": 0.008902139081508376,
        "scr_dir1_threshold_500": -0.3984374272404358,
        "scr_metric_threshold_500": 0.02373891963843859,
        "scr_dir2_threshold_500": 0.02373891963843859
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": -0.03278678379574697,
        "scr_metric_threshold_2": 0.04313739148158649,
        "scr_dir2_threshold_2": 0.04313739148158649,
        "scr_dir1_threshold_5": -0.03825151918536423,
        "scr_metric_threshold_5": 0.05490194336811984,
        "scr_dir2_threshold_5": 0.05490194336811984,
        "scr_dir1_threshold_10": -0.03825151918536423,
        "scr_metric_threshold_10": 0.08627454921951824,
        "scr_dir2_threshold_10": 0.08627454921951824,
        "scr_dir1_threshold_20": -0.06010915791026799,
        "scr_metric_threshold_20": 0.06666672899830793,
        "scr_dir2_threshold_20": 0.06666672899830793,
        "scr_dir1_threshold_50": -0.09289626741440628,
        "scr_metric_threshold_50": 0.10196085214521745,
        "scr_dir2_threshold_50": 0.10196085214521745,
        "scr_dir1_threshold_100": -0.09836067709563222,
        "scr_metric_threshold_100": 0.1529412782178262,
        "scr_dir2_threshold_100": 0.1529412782178262,
        "scr_dir1_threshold_500": -0.04371592886659017,
        "scr_metric_threshold_500": 0.2705881995450881,
        "scr_dir2_threshold_500": 0.2705881995450881
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.015384417878092908,
        "scr_metric_threshold_2": 0.020161269001982816,
        "scr_dir2_threshold_2": 0.020161269001982816,
        "scr_dir1_threshold_5": 0.020512557170790546,
        "scr_metric_threshold_5": 0.04032253800396563,
        "scr_dir2_threshold_5": 0.04032253800396563,
        "scr_dir1_threshold_10": 0.03589728071373967,
        "scr_metric_threshold_10": 0.04838718980952953,
        "scr_dir2_threshold_10": 0.04838718980952953,
        "scr_dir1_threshold_20": 0.020512557170790546,
        "scr_metric_threshold_20": 0.06451625307937271,
        "scr_dir2_threshold_20": 0.06451625307937271,
        "scr_dir1_threshold_50": 0.05128200425668879,
        "scr_metric_threshold_50": 0.06451625307937271,
        "scr_dir2_threshold_50": 0.06451625307937271,
        "scr_dir1_threshold_100": 0.10256400851337757,
        "scr_metric_threshold_100": 0.08467752208135552,
        "scr_dir2_threshold_100": 0.08467752208135552,
        "scr_dir1_threshold_500": 0.18974359914866223,
        "scr_metric_threshold_500": 0.21774199363095595,
        "scr_dir2_threshold_500": 0.21774199363095595
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.02242150428765898,
        "scr_metric_threshold_2": 0.02678589722404717,
        "scr_dir2_threshold_2": 0.02678589722404717,
        "scr_dir1_threshold_5": 0.04484300857531796,
        "scr_metric_threshold_5": 0.02678589722404717,
        "scr_dir2_threshold_5": 0.02678589722404717,
        "scr_dir1_threshold_10": 0.049327255975766836,
        "scr_metric_threshold_10": 0.0401785797439502,
        "scr_dir2_threshold_10": 0.0401785797439502,
        "scr_dir1_threshold_20": 0.08968601715063591,
        "scr_metric_threshold_20": 0.062499983369242465,
        "scr_dir2_threshold_20": 0.062499983369242465,
        "scr_dir1_threshold_50": 0.11659176883874377,
        "scr_metric_threshold_50": 0.07589293198126605,
        "scr_dir2_threshold_50": 0.07589293198126605,
        "scr_dir1_threshold_100": 0.12107628352460723,
        "scr_metric_threshold_100": 0.09375010809992398,
        "scr_dir2_threshold_100": 0.09375010809992398,
        "scr_dir1_threshold_500": 0.33183858419985035,
        "scr_metric_threshold_500": 0.18749995010772738,
        "scr_dir2_threshold_500": 0.18749995010772738
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": 0.025750992813676425,
        "scr_metric_threshold_2": 0.025750992813676425,
        "scr_dir2_threshold_2": 0.009523685178541423,
        "scr_dir1_threshold_5": -0.004291789499965721,
        "scr_metric_threshold_5": -0.004291789499965721,
        "scr_dir2_threshold_5": 0.009523685178541423,
        "scr_dir1_threshold_10": 0.012875624313779262,
        "scr_metric_threshold_10": 0.012875624313779262,
        "scr_dir2_threshold_10": 0.0285713393672145,
        "scr_dir1_threshold_20": 0.021459203313710703,
        "scr_metric_threshold_20": 0.021459203313710703,
        "scr_dir2_threshold_20": 0.0285713393672145,
        "scr_dir1_threshold_50": 0.008583578999931441,
        "scr_metric_threshold_50": 0.008583578999931441,
        "scr_dir2_threshold_50": 0.05238083614515829,
        "scr_dir1_threshold_100": -0.008583578999931441,
        "scr_metric_threshold_100": -0.008583578999931441,
        "scr_dir2_threshold_100": 0.09047614452250444,
        "scr_dir1_threshold_500": 0.11158806188240133,
        "scr_metric_threshold_500": 0.11158806188240133,
        "scr_dir2_threshold_500": 0.20952391224381361
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 65536,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "JumpReLU",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "jumprelu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null,
      "jump_coeff": 0.1
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "2df633af-446f-4197-b6f3-268f61d670ba",
    "datetime_epoch_millis": 1738223859132,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.9508437500000002,
        "llm_top_1_test_accuracy": 0.6564375,
        "llm_top_2_test_accuracy": 0.7215312500000001,
        "llm_top_5_test_accuracy": 0.7830750000000001,
        "llm_top_10_test_accuracy": 0.8302625,
        "llm_top_20_test_accuracy": 0.87730625,
        "llm_top_50_test_accuracy": 0.92238125,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9614250432699918,
        "sae_top_1_test_accuracy": 0.7018812499999999,
        "sae_top_2_test_accuracy": 0.746075,
        "sae_top_5_test_accuracy": 0.8025312500000001,
        "sae_top_10_test_accuracy": 0.841025,
        "sae_top_20_test_accuracy": 0.87071875,
        "sae_top_50_test_accuracy": 0.913225,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.9596,
        "llm_top_1_test_accuracy": 0.6432,
        "llm_top_2_test_accuracy": 0.6916,
        "llm_top_5_test_accuracy": 0.7912,
        "llm_top_10_test_accuracy": 0.8336,
        "llm_top_20_test_accuracy": 0.8962,
        "llm_top_50_test_accuracy": 0.9384,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9696000456809998,
        "sae_top_1_test_accuracy": 0.7615999999999999,
        "sae_top_2_test_accuracy": 0.7916,
        "sae_top_5_test_accuracy": 0.8431999999999998,
        "sae_top_10_test_accuracy": 0.867,
        "sae_top_20_test_accuracy": 0.8952,
        "sae_top_50_test_accuracy": 0.9228,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9442,
        "llm_top_1_test_accuracy": 0.67,
        "llm_top_2_test_accuracy": 0.7163999999999999,
        "llm_top_5_test_accuracy": 0.7699999999999999,
        "llm_top_10_test_accuracy": 0.8004,
        "llm_top_20_test_accuracy": 0.8645999999999999,
        "llm_top_50_test_accuracy": 0.908,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9568000435829163,
        "sae_top_1_test_accuracy": 0.6940000000000001,
        "sae_top_2_test_accuracy": 0.7488,
        "sae_top_5_test_accuracy": 0.8296000000000001,
        "sae_top_10_test_accuracy": 0.8455999999999999,
        "sae_top_20_test_accuracy": 0.8635999999999999,
        "sae_top_50_test_accuracy": 0.8996000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.9120000000000001,
        "llm_top_1_test_accuracy": 0.6921999999999999,
        "llm_top_2_test_accuracy": 0.7408,
        "llm_top_5_test_accuracy": 0.7689999999999999,
        "llm_top_10_test_accuracy": 0.8001999999999999,
        "llm_top_20_test_accuracy": 0.8480000000000001,
        "llm_top_50_test_accuracy": 0.8867999999999998,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9360000371932984,
        "sae_top_1_test_accuracy": 0.6464,
        "sae_top_2_test_accuracy": 0.7459999999999999,
        "sae_top_5_test_accuracy": 0.8008,
        "sae_top_10_test_accuracy": 0.841,
        "sae_top_20_test_accuracy": 0.8702,
        "sae_top_50_test_accuracy": 0.8883999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.8995999999999998,
        "llm_top_1_test_accuracy": 0.6024,
        "llm_top_2_test_accuracy": 0.6516,
        "llm_top_5_test_accuracy": 0.6706000000000001,
        "llm_top_10_test_accuracy": 0.7550000000000001,
        "llm_top_20_test_accuracy": 0.8026,
        "llm_top_50_test_accuracy": 0.8638,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9224000453948975,
        "sae_top_1_test_accuracy": 0.65,
        "sae_top_2_test_accuracy": 0.6751999999999999,
        "sae_top_5_test_accuracy": 0.724,
        "sae_top_10_test_accuracy": 0.7818000000000002,
        "sae_top_20_test_accuracy": 0.8102,
        "sae_top_50_test_accuracy": 0.8545999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9815,
        "llm_top_1_test_accuracy": 0.671,
        "llm_top_2_test_accuracy": 0.724,
        "llm_top_5_test_accuracy": 0.766,
        "llm_top_10_test_accuracy": 0.826,
        "llm_top_20_test_accuracy": 0.847,
        "llm_top_50_test_accuracy": 0.933,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9790000319480896,
        "sae_top_1_test_accuracy": 0.805,
        "sae_top_2_test_accuracy": 0.804,
        "sae_top_5_test_accuracy": 0.828,
        "sae_top_10_test_accuracy": 0.854,
        "sae_top_20_test_accuracy": 0.884,
        "sae_top_50_test_accuracy": 0.936,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9678000000000001,
        "llm_top_1_test_accuracy": 0.6662,
        "llm_top_2_test_accuracy": 0.686,
        "llm_top_5_test_accuracy": 0.7731999999999999,
        "llm_top_10_test_accuracy": 0.7944,
        "llm_top_20_test_accuracy": 0.8698,
        "llm_top_50_test_accuracy": 0.9251999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9706000447273254,
        "sae_top_1_test_accuracy": 0.6066,
        "sae_top_2_test_accuracy": 0.6662000000000001,
        "sae_top_5_test_accuracy": 0.7247999999999999,
        "sae_top_10_test_accuracy": 0.7869999999999999,
        "sae_top_20_test_accuracy": 0.8328,
        "sae_top_50_test_accuracy": 0.9182,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.94225,
        "llm_top_1_test_accuracy": 0.6655,
        "llm_top_2_test_accuracy": 0.77725,
        "llm_top_5_test_accuracy": 0.8250000000000001,
        "llm_top_10_test_accuracy": 0.8725,
        "llm_top_20_test_accuracy": 0.90025,
        "llm_top_50_test_accuracy": 0.92825,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.958000048995018,
        "sae_top_1_test_accuracy": 0.74825,
        "sae_top_2_test_accuracy": 0.801,
        "sae_top_5_test_accuracy": 0.86425,
        "sae_top_10_test_accuracy": 0.8859999999999999,
        "sae_top_20_test_accuracy": 0.90075,
        "sae_top_50_test_accuracy": 0.92,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9998000000000001,
        "llm_top_1_test_accuracy": 0.641,
        "llm_top_2_test_accuracy": 0.7846,
        "llm_top_5_test_accuracy": 0.8996000000000001,
        "llm_top_10_test_accuracy": 0.96,
        "llm_top_20_test_accuracy": 0.99,
        "llm_top_50_test_accuracy": 0.9955999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9990000486373901,
        "sae_top_1_test_accuracy": 0.7032,
        "sae_top_2_test_accuracy": 0.7358,
        "sae_top_5_test_accuracy": 0.8055999999999999,
        "sae_top_10_test_accuracy": 0.8657999999999999,
        "sae_top_20_test_accuracy": 0.909,
        "sae_top_50_test_accuracy": 0.9662,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 65536,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "JumpReLU",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "jumprelu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null,
      "jump_coeff": 0.1
    },
    "eval_result_unstructured": null
  }
}