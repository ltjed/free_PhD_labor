\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bussmann et~al.(2024)Bussmann, Leask, and
  Nanda]{bussmannBatchTopKSparseAutoencoders2024}
Bart Bussmann, Patrick Leask, and Neel Nanda.
\newblock {{BatchTopK Sparse Autoencoders}}, December 2024.

\bibitem[Chanin et~al.(2024)Chanin, {Wilken-Smith}, Dulka, Bhatnagar, and
  Bloom]{chaninAbsorptionStudyingFeature2024}
David Chanin, James {Wilken-Smith}, Tom{\'a}{\v s} Dulka, Hardik Bhatnagar, and
  Joseph Bloom.
\newblock A is for {{Absorption}}: {{Studying Feature Splitting}} and
  {{Absorption}} in {{Sparse Autoencoders}}, September 2024.

\bibitem[Gao et~al.()Gao, Goh, and Sutskever]{gaoScalingEvaluatingSparse}
Leo Gao, Gabriel Goh, and Ilya Sutskever.
\newblock Scaling and evaluating sparse autoencoders.

\bibitem[Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever,
  Leike, and Wu]{Gao2024ScalingAE}
Leo Gao, Tom~Dupr'e la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec
  Radford, I.~Sutskever, J.~Leike, and Jeffrey Wu.
\newblock Scaling and evaluating sparse autoencoders.
\newblock \emph{ArXiv}, abs/2406.04093, 2024.

\bibitem[Ghilardi et~al.(2024)Ghilardi, Belotti, and
  Molinari]{ghilardiEfficientTrainingSparse2024a}
Davide Ghilardi, Federico Belotti, and Marco Molinari.
\newblock Efficient {{Training}} of {{Sparse Autoencoders}} for {{Large
  Language Models}} via {{Layer Groups}}, October 2024.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and
  Bertsimas]{gurneeFindingNeuronsHaystack2023}
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and
  Dimitris Bertsimas.
\newblock Finding {{Neurons}} in a {{Haystack}}: {{Case Studies}} with {{Sparse
  Probing}}, June 2023.

\bibitem[Karvonen et~al.(2024)Karvonen, Rager, Marks, and
  Nanda]{karvonenEvaluatingSparseAutoencoders2024}
Adam Karvonen, Can Rager, Samuel Marks, and Neel Nanda.
\newblock Evaluating {{Sparse Autoencoders}} on {{Targeted Concept Erasure
  Tasks}}, November 2024.

\bibitem[Marks et~al.(2024)Marks, Rager, Michaud, Belinkov, Bau, and
  Mueller]{marksSparseFeatureCircuits2024}
Samuel Marks, Can Rager, Eric~J. Michaud, Yonatan Belinkov, David Bau, and
  Aaron Mueller.
\newblock Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing
  Interpretable Causal Graphs}} in {{Language Models}}, March 2024.
\newblock Comment: Code and data at
  https://github.com/saprmarks/feature-circuits. Demonstration at
  https://feature-circuits.xyz.

\bibitem[Mudide et~al.(2024)Mudide, Engels, Michaud, Tegmark, and
  de~Witt]{mudideEfficientDictionaryLearning2024a}
Anish Mudide, Joshua Engels, Eric~J. Michaud, Max Tegmark, and
  Christian~Schroeder de~Witt.
\newblock Efficient {{Dictionary Learning}} with {{Switch Sparse
  Autoencoders}}, October 2024.
\newblock Comment: Code available at https://github.com/amudide/switch\_sae.

\bibitem[Paulo et~al.(2024)Paulo, Mallen, Juang, and
  Belrose]{pauloAutomaticallyInterpretingMillions2024}
Gon{\c c}alo Paulo, Alex Mallen, Caden Juang, and Nora Belrose.
\newblock Automatically {{Interpreting Millions}} of {{Features}} in {{Large
  Language Models}}, December 2024.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{a}})Rajamanoharan, Conmy, Smith,
  Lieberum, Varma, Kram{\'a}r, Shah, and
  Nanda]{rajamanoharanImprovingDictionaryLearning2024}
Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant
  Varma, J{\'a}nos Kram{\'a}r, Rohin Shah, and Neel Nanda.
\newblock Improving {{Dictionary Learning}} with {{Gated Sparse Autoencoders}},
  April 2024{\natexlab{a}}.
\newblock Comment: 15 main text pages, 22 appendix pages.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{b}})Rajamanoharan, Lieberum,
  Sonnerat, Conmy, Varma, Kram{\'a}r, and
  Nanda]{rajamanoharanJumpingAheadImproving2024}
Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant
  Varma, J{\'a}nos Kram{\'a}r, and Neel Nanda.
\newblock Jumping {{Ahead}}: {{Improving Reconstruction Fidelity}} with
  {{JumpReLU Sparse Autoencoders}}, August 2024{\natexlab{b}}.
\newblock Comment: v2: new appendix H comparing kernel functions \& bug-fixes
  to pseudo-code in Appendix J v3: further bug-fix to pseudo-code in Appendix
  J.

\end{thebibliography}
