[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "Orthogonal_sae",
        "Title": "Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders",
        "Experiment": "1. Implement adaptive orthogonality loss with controlled feature sharing\n2. Add batch-wise feature grouping with periodic updates\n3. Train on Pythia-70M using WMDP-bio and WikiText datasets\n4. Compare unlearning performance against baseline and fixed orthogonal SAE\n5. Analyze condition numbers of feature subspaces\n6. Evaluate impact of different \u03b1 values for controlled sharing",
        "Technical_Details": "The method uses an adaptive objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_ortho where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_ortho = ||W_1^T W_2 - \u03b1I||_F allows controlled feature sharing through parameter \u03b1. Feature groups are updated every n=100 steps using efficient batch statistics. Implementation includes early stopping based on condition number thresholds. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality.",
        "Research_Impact": "A key challenge in selective unlearning is finding the optimal balance between knowledge separation and model performance. Current approaches either sacrifice reconstruction quality for separation or vice versa. This research addresses the challenge through adaptive training and controlled feature sharing, achieving better separation while maintaining performance. The theoretical framework provides precise control over the separation-performance trade-off.",
        "Implementation_Plan": "1. Add AdaptiveOrthogonalityLoss with scheduling\n2. Implement efficient batch-wise feature grouping\n3. Modify CustomSAE to include group-specific biases\n4. Add condition number calculation utilities\n5. Update CustomTrainer with adaptive loss weight\n6. Add evaluation metrics for controlled sharing",
        "Interestingness_Evaluation": "The combination of theoretical insights about feature sharing with practical adaptive training creates a more nuanced and effective approach to knowledge separation.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient with batch-wise updates and simple matrix operations; adaptive weighting adds minimal overhead; controlled sharing through \u03b1 provides easy tuning; all computations well within 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive approach with controlled feature sharing provides a novel theoretical framework for knowledge separation that bridges the gap between strict orthogonality and practical performance.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": false
    },
    {
        "Name": "Orthogonal_sae",
        "Title": "Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders",
        "Experiment": "1. Implement adaptive orthogonality loss with controlled feature sharing\n2. Add batch-wise feature grouping with periodic updates\n3. Train on Pythia-70M using WMDP-bio and WikiText datasets\n4. Compare unlearning performance against baseline and fixed orthogonal SAE\n5. Analyze condition numbers of feature subspaces\n6. Evaluate impact of different \u03b1 values for controlled sharing",
        "Technical_Details": "The method uses an adaptive objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_ortho where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_ortho = ||W_1^T W_2 - \u03b1I||_F allows controlled feature sharing through parameter \u03b1. Feature groups are updated every n=100 steps using efficient batch statistics. Implementation includes early stopping based on condition number thresholds. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality.",
        "Research_Impact": "A key challenge in selective unlearning is finding the optimal balance between knowledge separation and model performance. Current approaches either sacrifice reconstruction quality for separation or vice versa. This research addresses the challenge through adaptive training and controlled feature sharing, achieving better separation while maintaining performance. The theoretical framework provides precise control over the separation-performance trade-off.",
        "Implementation_Plan": "1. Add AdaptiveOrthogonalityLoss with scheduling\n2. Implement efficient batch-wise feature grouping\n3. Modify CustomSAE to include group-specific biases\n4. Add condition number calculation utilities\n5. Update CustomTrainer with adaptive loss weight\n6. Add evaluation metrics for controlled sharing",
        "Interestingness_Evaluation": "The combination of theoretical insights about feature sharing with practical adaptive training creates a more nuanced and effective approach to knowledge separation.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient with batch-wise updates and simple matrix operations; adaptive weighting adds minimal overhead; controlled sharing through \u03b1 provides easy tuning; all computations well within 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive approach with controlled feature sharing provides a novel theoretical framework for knowledge separation that bridges the gap between strict orthogonality and practical performance.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": false
    },
    {
        "Name": "temporal_quantile_sae",
        "Title": "Efficient Temporal Modeling with Quantile Thresholding for Sparse Autoencoders",
        "Experiment": "1. Implement bit-packed circular buffer for activations\n2. Add quantile-based temporal correlation tracking\n3. Train with simplified temporal loss\n4. Compare feature absorption metrics\n5. Analyze temporal patterns",
        "Technical_Details": "Temporal tracking:\n1. Store binary activations in bit-packed circular buffer B[t]\n2. Compute temporal correlations efficiently:\n   C[i,j] = popcnt(B[t-k:t,i] & B[t-k:t,j]) / k\n   where k is window size\n3. Use fixed quantile q for thresholding:\n   \u03c4 = quantile(C, q)\n4. Loss function:\n   L = ||x - D(f)||^2 + \u03bb_1||f||_1 + \u03bb_2 * sum(C[C > \u03c4])\n\nHyperparameters:\n- Window size k: 32\n- Quantile q: 0.95\n- Temporal weight \u03bb_2: 0.01\n- L1 penalty \u03bb_1: 0.04\n- Learning rate: 3e-4\n- Batch size: 2048",
        "Implementation_Plan": "1. Create BitPackedBuffer class for efficient storage\n2. Implement quantile-based correlation tracking\n3. Modify CustomSAE with temporal components\n4. Update training loop\n5. Add analysis tools",
        "Interestingness_Evaluation": "The combination of efficient binary operations and quantile-based thresholding provides a simple yet effective approach to preventing feature absorption.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation greatly simplified through bit packing and fixed thresholding; binary operations ensure efficiency; complete implementation feasible within 2 weeks; runtime under 15 mins on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of bit-packed temporal tracking with quantile thresholding for preventing feature absorption represents a novel and practical contribution.",
        "Novelty": 8,
        "Expected_Research_Impact": "The efficient and stable temporal tracking directly addresses feature absorption while maintaining computational efficiency, promising improvements in both sparse probing and core metrics.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We propose an efficient temporal modeling approach for preventing feature absorption in sparse autoencoders. Our method uses a bit-packed circular buffer to track feature activations and employs quantile-based thresholding to identify and penalize consistent feature co-activations. This provides a simple and robust way to prevent feature absorption without requiring complex architectures or expensive computation. The approach leverages efficient binary operations and maintains a fixed-size temporal window, making it both memory and computationally efficient. We evaluate our method through standard mechanistic interpretability benchmarks, focusing on feature absorption and interpretability metrics.",
        "novel": true
    },
    {
        "Name": "temporal_absorption_sae",
        "Title": "Preventing Feature Absorption in Sparse Autoencoders through Temporal Regularization",
        "Experiment": "1. Implement temporal feature absorption detection\n2. Add targeted temporal consistency loss\n3. Compare feature stability and absorption metrics\n4. Analyze impact on interpretability\n5. Evaluate on standard benchmarks",
        "Technical_Details": "Architecture additions:\n1. Absorption detection:\n   - Track feature co-activation statistics\n   - Identify potential absorptions when feature i activates strongly (>0.8) while feature j is suppressed (<0.2) compared to historical average\n\n2. Temporal consistency loss:\n   L_temp = \u03a3_{i,j\u2208P} ||f_i(x_t) - f_i(x_{t-1})||\u2081\n   where P is set of detected absorption pairs\n\n3. Combined loss function:\n   L = L_recon + \u03bb\u2081L_sparse + \u03bb\u2082L_temp\n   \u03bb\u2081=0.04 (standard sparsity)\n   \u03bb\u2082=0.02 (temporal consistency)\n\n4. Implementation details:\n   - Maintain running statistics of feature activations\n   - Update absorption pairs every 100 steps\n   - Zero temporal loss when no absorptions detected\n   - Efficient batch processing using mask for relevant features",
        "Implementation_Plan": "1. Add absorption detection to CustomSAE\n2. Implement targeted temporal loss\n3. Add running statistics tracking\n4. Modify training loop for absorption updates\n5. Create evaluation metrics for absorption\n6. Add visualization tools for absorption patterns",
        "Interestingness_Evaluation": "The targeted approach to preventing feature absorption through temporal consistency provides a focused and interpretable solution to a key challenge in SAE training.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only simple statistics tracking and modified loss computation; no complex matrices or state tracking; training time only slightly increased; complete implementation feasible within 1 week; runtime well within 30-min limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The specific focus on preventing feature absorption through temporal consistency and the efficient detection method represent novel contributions to SAE interpretability.",
        "Novelty": 9,
        "Expected_Research_Impact": "The targeted prevention of feature absorption should significantly improve sparse probing performance by maintaining distinct features, while the temporal consistency helps maintain strong core metrics.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders (SAEs) by specifically targeting and preventing feature absorption through temporal regularization. Our method introduces an efficient mechanism to detect potential feature absorptions by tracking activation patterns across timesteps, combined with a targeted temporal consistency loss that prevents harmful feature merging. The key insight is that feature absorption can be detected and prevented by maintaining temporal consistency specifically for features showing absorption patterns. This targeted approach aims to produce more interpretable representations by preserving distinct features that might otherwise be absorbed, while maintaining the computational efficiency of standard SAEs. We evaluate our method on standard interpretability benchmarks and analyze the prevention of feature absorption patterns.",
        "novel": true
    },
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Consistency Regularization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement efficient 2-timestep buffer\n2. Add adaptive temporal consistency loss\n3. Compare against baseline SAE on interpretability benchmarks\n4. Analyze feature stability patterns\n5. Measure impact on feature absorption",
        "Technical_Details": "1. Temporal consistency loss:\n   L_temp = ||f_t - f_{t-1}||_1\n\n2. Dynamic activation threshold:\n   \u03c4_t = \u03b2 * \u03c4_{t-1} + (1-\u03b2) * ||f_t||_1\n   where \u03b2=0.99 is EMA decay\n\n3. Adaptive weighting:\n   \u03bb_2(f) = \u03bb_base * min(1, ||f||_1/\u03c4_t)\n   \u03bb_base = 0.02, increased during warmup\n\n4. Total loss:\n   L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(f) * L_temp\n\n5. Implementation optimizations:\n   - Double buffer for temporal states\n   - Sparse computation of temporal loss\n   - Skip temporal loss at sequence boundaries",
        "Implementation_Plan": "1. Add previous_features buffer to CustomSAE\n2. Implement EMA threshold tracking\n3. Add adaptive temporal loss computation\n4. Modify training loop for state management\n5. Add feature stability analysis tools\n6. Optimize memory usage with sparse operations",
        "Interestingness_Evaluation": "The combination of adaptive temporal regularization with dynamic thresholding provides a practical and principled approach to preventing feature absorption while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation is highly practical with minimal memory overhead from 2-timestep buffer; dynamic threshold adds negligible computation; can be completed within 2 weeks and runs within 30-min limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive temporal consistency approach with dynamic thresholding represents a novel and theoretically grounded solution to the feature absorption problem.",
        "Novelty": 8,
        "Expected_Research_Impact": "The simplified yet robust approach directly targets feature absorption while maintaining sparse, interpretable representations, promising significant improvements on both sparse_probing and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We present an adaptive temporal consistency regularization approach for improving sparse autoencoder (SAE) interpretability. Current SAEs can suffer from feature absorption, where features inconsistently represent different concepts across time. We introduce a temporal consistency loss with dynamic threshold adaptation based on exponential moving averages of feature activations. This approach maintains the simplicity of standard SAEs while directly targeting the feature absorption problem. Our method uses efficient double buffering and sparse operations to minimize computational overhead. We evaluate the approach on standard mechanistic interpretability benchmarks and analyze its impact on feature stability and absorption patterns.",
        "novel": true
    },
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Sparse Autoencoders: Efficient Feature Stabilization with EMA Tracking",
        "Experiment": "1. Implement EMA feature tracking\n2. Add adaptive temporal consistency loss\n3. Train on standard datasets\n4. Evaluate feature stability and frequency patterns\n5. Compare feature interpretability with baseline",
        "Technical_Details": "Architecture modifications:\n1. EMA tracking:\n   - Update feature statistics: s_t = \u03b3 * s_{t-1} + (1-\u03b3) * f_t\n   where \u03b3=0.99 is decay rate\n   - Compute activation frequency: freq = mean(f_t > 0)\n\n2. Loss function:\n   L = ||x - Dec(f_t)||^2 + \u03b1 * ||f_t||_1 + \u03b2 * w_t * ||f_t - s_t||^2\n   where \u03b1=0.04 is sparsity penalty\n   \u03b2=0.01 is base temporal weight\n   w_t = clip(freq, 0.1, 1.0) is adaptive weight\n\n3. Implementation details:\n   - Single EMA state per feature\n   - Update frequency every 100 steps\n   - Batch size 2048\n   - Learning rate 3e-4",
        "Implementation_Plan": "1. Add EMA tracking to SAE\n2. Implement adaptive temporal loss\n3. Add frequency-based weighting\n4. Create evaluation metrics\n5. Add visualization tools",
        "Interestingness_Evaluation": "The combination of EMA tracking and adaptive temporal consistency provides an elegant and efficient solution to feature stabilization.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation extremely simple with EMA approach; no memory overhead; runtime well within 30-min limit on H100; complete implementation feasible within 1 week for junior PhD student.",
        "Feasibility": 10,
        "Novelty_Evaluation": "The adaptive temporal consistency based on feature frequency represents a novel approach to balancing stability and flexibility in SAEs.",
        "Novelty": 8,
        "Expected_Research_Impact": "The adaptive approach should significantly improve both sparse_probing and core benchmarks by providing better feature stability while preserving rare but important features.",
        "Research_Impact": 9.5,
        "Overall_Score": 9.2,
        "Abstract": "We propose Adaptive Temporal Sparse Autoencoders (ATSAEs), a novel approach to improving feature interpretability in neural networks through efficient temporal consistency constraints. Our method employs exponential moving average tracking of feature activations combined with an adaptive temporal consistency loss that scales based on feature activation frequency. This approach allows the model to maintain stability for common features while preserving the flexibility needed for rare but important patterns. The method is computationally efficient, requiring minimal additional memory and computation compared to standard sparse autoencoders. We evaluate ATSAEs on standard interpretability benchmarks and analyze how the adaptive temporal consistency affects feature learning patterns.",
        "novel": true
    },
    {
        "Name": "sequential_temporal_sae",
        "Title": "Sequential Temporal Sparse Autoencoders for Feature Disentanglement",
        "Experiment": "1. Implement efficient sequential feature tracking\n2. Add temporal consistency loss with data-derived expectations\n3. Handle sequence boundaries and padding\n4. Train on standard datasets with temporal modeling\n5. Evaluate using feature absorption benchmark\n6. Analyze temporal feature patterns",
        "Technical_Details": "Core modifications:\n1. Sequential tracking:\n   - Maintain single-step history: f_prev\n   - Mask padding tokens in sequences\n   - Reset history at sequence boundaries\n   - Compute sparse transition matrix: T[i,j] = mean(f_prev[i] * f[j])\n\n2. Data-derived temporal consistency:\n   - Running average of transitions: S[i,j] = 0.9 * S[i,j] + 0.1 * T[i,j]\n   - Stability metric: V[i,j] = variance of T[i,j] over batch\n   - Temporal penalty: sum(V[i,j] * T[i,j])\n   This penalizes unstable feature transitions\n\n3. Efficient implementation:\n   - Use sparse operations for transition matrix\n   - Compute variance incrementally\n   - Cache intermediate results\n\nHyperparameters:\n- Update rate: 0.1\n- Penalty weight: 0.01\n- Minimum activation threshold: 0.1\n- Batch size: 2048",
        "Implementation_Plan": "1. Create SequentialTracker class\n   - Add padding mask handling\n   - Implement boundary detection\n2. Modify forward pass\n   - Add feature history tracking\n   - Compute transition matrices\n3. Implement temporal loss\n   - Add variance computation\n   - Optimize sparse operations\n4. Add visualization tools\n5. Run evaluation pipeline",
        "Interestingness_Evaluation": "The data-derived approach to temporal consistency provides a principled way to identify and prevent feature absorption while adapting to the natural patterns in the data.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only standard matrix operations and simple state tracking; efficient sparse computations ensure runtime within 30-min limit; robust handling of edge cases makes it practical; complete implementation feasible within 2 weeks for junior PhD student.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using data-derived temporal consistency with variance-based penalties for feature stability represents a novel approach to preventing feature absorption.",
        "Novelty": 8,
        "Expected_Research_Impact": "The method directly addresses feature absorption through temporal consistency while remaining computationally efficient, promising significant improvements on both feature absorption and core benchmarks.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders through sequential temporal modeling with data-derived consistency constraints. Our method tracks feature transitions across consecutive tokens while handling sequence boundaries and padding tokens robustly. We introduce an efficient temporal consistency loss based on transition stability, which adapts to natural patterns in the data rather than imposing fixed expectations. The approach extends standard sparse autoencoders with temporal awareness while maintaining computational efficiency through sparse operations and incremental computations. By encouraging stable feature transitions across tokens, our method provides a principled way to prevent feature absorption and improve feature interpretability.",
        "novel": true
    },
    {
        "Name": "temporal_difference_sae",
        "Title": "Temporal Difference Regularization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement single-step temporal difference tracking\n2. Add temporal stability loss\n3. Train on standard datasets with combined loss\n4. Compare feature stability between consecutive tokens\n5. Evaluate impact on feature absorption\n6. Analyze semantic consistency of features",
        "Technical_Details": "Architecture modifications:\n1. Single-step temporal difference:\n   diff(t) = f(x_t) - f(x_{t-1})\n   where f(x) is the feature activation vector\n\n2. Temporal stability loss:\n   L_temp = ||diff(t)||_2 + \u03b2 * ReLU(||diff(t)||_2 - \u03c4)\n   where \u03c4 is a small threshold and \u03b2 adds extra penalty for large changes\n\n3. Combined loss function:\n   L = MSE(x, decode(f(x))) + \u03bb_1 * L1(f(x)) + \u03bb_2 * L_temp\n\nOptimizations:\n- No buffer needed, just store previous activation\n- Single forward pass per sample\n- Minimal memory overhead\n\nHyperparameters:\n- Stability threshold \u03c4 = 0.1\n- Large change penalty \u03b2 = 2.0\n- Base sparsity penalty \u03bb_1 = 0.04\n- Temporal penalty \u03bb_2 = 0.02\n- Learning rate = 3e-4\n- Batch size = 2048",
        "Implementation_Plan": "1. Add previous activation tracking to CustomSAE\n2. Implement temporal difference loss\n3. Modify training loop for temporal difference\n4. Add stability metrics\n5. Create visualization tools for activation changes",
        "Interestingness_Evaluation": "The temporal difference approach provides an elegant and computationally efficient way to ensure feature stability while directly targeting the feature absorption problem.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires minimal changes to existing SAE code; single-step difference is extremely efficient; no complex computations or extra memory needed; complete implementation feasible within 1 week; runtime well within 30-min limit on H100.",
        "Feasibility": 10,
        "Novelty_Evaluation": "Using temporal differences with adaptive thresholding to improve SAE interpretability is a novel and well-motivated approach.",
        "Novelty": 8,
        "Expected_Research_Impact": "The direct targeting of sudden activation changes should significantly improve sparse_probing and core metrics by preventing feature absorption and maintaining stable feature semantics.",
        "Research_Impact": 9,
        "Overall_Score": 9.2,
        "Abstract": "We present Temporal Difference SAE, a novel approach to improving the interpretability of learned features in neural networks through temporal stability regularization. Our method introduces a temporal difference loss that penalizes sudden changes in feature activations between consecutive tokens, directly addressing the challenges of feature absorption and polysemanticity. The approach uses an adaptive thresholding mechanism to allow natural variations while strongly discouraging abrupt semantic shifts. By focusing on single-step differences, the method maintains minimal computational overhead while providing a principled way to encourage more interpretable and stable feature representations.",
        "novel": true
    },
    {
        "Name": "temporal_nested_sae",
        "Title": "Temporally-Nested Sparse Autoencoders for Multi-Scale Feature Separation",
        "Experiment": "1. Modify SAE to use linguistically-motivated nested dictionaries\n2. Implement efficient sliding-window reconstruction\n3. Train on standard text datasets with varied context lengths\n4. Evaluate feature stability across temporal scales\n5. Compare against baseline SAE on standard benchmarks",
        "Technical_Details": "Architecture modifications:\n1. Four nested decoders aligned with linguistic units:\n   - d_sae/8 for sentence level (\u224825 tokens)\n   - d_sae/4 for clause level (\u224815 tokens)\n   - d_sae/2 for phrase level (\u22485 tokens)\n   - d_sae for token level\n2. Efficient multi-scale reconstruction loss:\n   L = \u03a3_i \u03bb_i ||x_i - D_i(E(x)[:k_i])||^2 + \u03b1||E(x)||_1\n   where x_i uses sliding window averaging\n   \u03bb_i proportional to linguistic unit size [0.4, 0.3, 0.2, 0.1]\n3. Single forward pass with cached window averages\n4. Shared encoder with gradient normalization\n\nHyperparameters:\n- Base dictionary size d_sae=65536\n- L1 penalty \u03b1=0.04\n- Learning rate 3e-4\n- Batch size 2048",
        "Implementation_Plan": "1. Add sliding window buffer to CustomSAE\n2. Implement nested decoders with linguistic alignment\n3. Optimize loss computation with caching\n4. Add efficient activation averaging\n5. Update training loop\n6. Add temporal stability metrics",
        "Interestingness_Evaluation": "The alignment of dictionary sizes with linguistic structures provides a principled and interpretable approach to temporal feature separation.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only standard matrix operations with sliding windows; memory usage optimized through caching; can be completed within 2 weeks; runtime under 30-min on H100 due to efficient design.",
        "Feasibility": 9,
        "Novelty_Evaluation": "First approach to align nested dictionary sizes with linguistic structures for temporal feature separation in interpretability work.",
        "Novelty": 8,
        "Expected_Research_Impact": "The linguistically-motivated temporal hierarchy directly addresses polysemanticity by separating features based on natural language structures.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We propose Temporally-Nested Sparse Autoencoders (TNSAEs), a novel approach to improving feature interpretability in language models through linguistically-motivated feature separation. TNSAEs extend traditional sparse autoencoders by introducing multiple nested decoder dictionaries aligned with natural language structures, from individual tokens to full sentences. This architecture enables systematic separation of features based on their linguistic scope, with smaller dictionaries capturing sentence-level concepts and larger ones handling local token-level patterns. Unlike previous approaches that require complex temporal modeling, TNSAEs achieve temporal awareness through a simple nested structure with linguistically-aligned dictionaries. The approach maintains computational efficiency through sliding window averaging and cached computations while providing a principled framework for addressing polysemanticity.",
        "novel": true
    },
    {
        "Name": "selective_temporal_sae",
        "Title": "Selective Temporal Consistency for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement selective consistency loss\n2. Train with varying activation thresholds\n3. Compare feature stability metrics:\n   - Selective consistency scores\n   - Active feature stability\n   - Context-dependent patterns\n4. Evaluate using standard benchmarks\n5. Analyze selective stability patterns",
        "Technical_Details": "Architecture modification:\n1. Maintain activation buffer B_t = [z_{t-w}, ..., z_t] for window w\n2. Create activation mask M_t = (max(|B_t|) > \u03c4)\n3. Compute masked consistency loss:\n   L_temp = \u03a3_{i,j} M_t * ||z_i - z_j||\u2082 * cos_sim(x_i, x_j) / w\n4. Total loss: L = L_recon + \u03bb_1 L_sparse + \u03bb_2 L_temp\n\nOptimizations:\n- Efficient mask computation using max pooling\n- Circular buffer for activation history\n- Batch-wise similarity computation\n\nHyperparameters:\n- Window size w = 4\n- Activation threshold \u03c4 = 0.1\n- Consistency weight \u03bb_2 = 0.15\n- Base learning rate 3e-4\n- Batch size 2048",
        "Implementation_Plan": "1. Create ActivationBuffer class\n2. Add selective_consistency_loss function\n3. Modify training loop for masked consistency\n4. Add selective visualization tools\n5. Implement masked stability metrics",
        "Interestingness_Evaluation": "Selective temporal consistency provides a more focused and principled approach to feature stability by only enforcing consistency when features are actively relevant.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only simple buffer and mask operations; all computations remain efficient; complete implementation feasible within 1 week; runtime well within 30-min limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of activation-based selection with temporal consistency for SAE interpretability is novel.",
        "Novelty": 8,
        "Expected_Research_Impact": "The selective consistency approach should significantly improve sparse probing through more focused feature stability, and enhance core metrics by ensuring meaningful temporal consistency.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "We present Selective Temporal Consistency Sparse Autoencoders (STCSAEs), which improve interpretability by enforcing temporal consistency only for actively relevant features. Using an activation-based masking mechanism, STCSAEs focus the temporal consistency constraint on features that show significant activation within a sliding context window. This selective approach better matches how language models process sequential information while remaining computationally efficient. The method requires only simple modifications to standard sparse autoencoders through masked consistency losses. We evaluate STCSAEs on standard interpretability benchmarks and analyze how selective temporal consistency affects feature stability and interpretability.",
        "novel": true
    },
    {
        "Name": "dual_temporal_sae",
        "Title": "Dual-Scale Temporal Sparse Autoencoders for Interpretable Feature Learning",
        "Experiment": "1. Implement two-scale temporal dictionary\n2. Add efficient context buffer management\n3. Train on standard datasets with dual temporal windows\n4. Evaluate immediate vs contextual feature stability\n5. Compare against baseline SAE",
        "Technical_Details": "Architecture modifications:\n1. Two temporal scales:\n   - Immediate (T1=1): token-level features\n   - Contextual (T2=8): windowed features\n2. Nested dictionaries:\n   - D1 (size n): immediate features\n   - D2 (size 2n): includes D1 plus contextual features\n3. Dual-scale loss:\n   L = \u03bb1||x[t] - f1(x[t])||^2 + \u03bb2||x[t-7:t] - f2(x[t-7:t])||^2 + \u03b11||z1||_1 + \u03b12||z2||_1\n   where f1 uses D1 features, f2 uses D2 features\n4. Efficient implementation:\n   - Single circular buffer for context\n   - Shared feature computation between scales\n   - Progressive sparsity: \u03b12 > \u03b11\n\nHyperparameters:\n- Base dictionary size (n): 512\n- Context window: 8 tokens\n- Scale weights: \u03bb1=0.6, \u03bb2=0.4\n- L1 penalties: \u03b11=0.03, \u03b12=0.06",
        "Implementation_Plan": "1. Implement dual-scale SAE architecture\n2. Add circular context buffer\n3. Modify loss computation\n4. Implement shared feature computation\n5. Add stability metrics\n6. Update training loop",
        "Interestingness_Evaluation": "The dual-scale approach provides a clear and intuitive framework for learning both immediate and contextual features while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Simplified two-scale design reduces implementation complexity; efficient buffer management ensures performance; all components use standard operations; implementation feasible within 2 weeks; runtime well within 30-min limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The explicit separation of immediate and contextual features through dual-scale temporal modeling represents a novel approach to SAE design.",
        "Novelty": 8,
        "Expected_Research_Impact": "The clear separation between immediate and contextual features should significantly improve interpretability and reduce feature absorption.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We propose Dual-Scale Temporal Sparse Autoencoders (DT-SAE), a novel approach to learning interpretable features in neural networks through two-scale temporal dictionary learning. DT-SAE employs a nested dictionary structure with distinct immediate and contextual feature spaces, encouraging the emergence of both token-level and context-aware interpretable features. The architecture uses efficient circular buffers and shared computations to maintain computational feasibility while capturing temporal dependencies. This approach provides a clear separation between immediate and contextual features, potentially offering new insights into how language models process information across different temporal scales. We evaluate DT-SAE on standard interpretability benchmarks and analyze the relationship between immediate and contextual features.",
        "novel": true
    },
    {
        "Name": "adaptive_temporal_sae",
        "Title": "Adaptive Temporal Regularization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Add previous activation state tracking\n2. Implement adaptive temporal difference penalty\n3. Train on standard datasets with modified loss\n4. Analyze feature stability patterns\n5. Evaluate using standard benchmarks",
        "Technical_Details": "Loss function modification:\n1. Adaptive temporal penalty:\n   L = L_recon + \u03bb1 * L1(f_t) + \u03bb2 * \u03b1(L_recon) * ||f_t - f_{t-1}||_1\n   where:\n   - f_t is current feature activation\n   - f_{t-1} is previous activation\n   - L1 is standard sparsity penalty\n   - \u03b1(L_recon) = sigmoid(\u03b2 * (L_max - L_recon))\n   - L_max is moving maximum of reconstruction loss\n   - \u03b2 controls adaptation rate\n\n2. Implementation details:\n   - Single state variable for previous activation\n   - Moving maximum with momentum 0.99\n   - Direct gradient computation\n   - Zero initialization for f_{t-1} at sequence starts\n\nHyperparameters:\n- Sparsity weight \u03bb1 = 0.04\n- Temporal weight \u03bb2 = 0.02\n- Adaptation rate \u03b2 = 5.0\n- Learning rate = 3e-4\n- Batch size = 2048",
        "Implementation_Plan": "1. Add previous activation state to CustomSAE\n2. Implement adaptive temporal loss\n3. Add moving maximum tracking\n4. Modify training loop for state updates\n5. Add temporal stability metrics\n6. Create visualization tools",
        "Interestingness_Evaluation": "The adaptive temporal regularization provides an elegant self-tuning mechanism that automatically balances reconstruction quality with temporal consistency.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "Implementation requires only single state variable and simple loss scaling; minimal overhead from moving maximum computation; runtime virtually identical to baseline; complete implementation feasible within 4 days; adaptive approach adds robustness without complexity.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of temporal regularization with adaptive scaling based on reconstruction quality for sparse autoencoders is novel and well-motivated.",
        "Novelty": 9,
        "Expected_Research_Impact": "The adaptive temporal regularization should excel on both sparse_probing and core metrics by ensuring features maintain consistent meanings while not compromising reconstruction quality.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "We present an adaptive temporal regularization approach for improving sparse autoencoder interpretability. By penalizing sudden changes in feature activations with a penalty that scales based on reconstruction quality, we encourage features to maintain consistent behavior while ensuring good reconstructions. This simple yet effective modification requires minimal changes to existing architectures while directly addressing the feature absorption problem. The approach automatically adjusts its temporal consistency enforcement based on current reconstruction performance, allowing for natural feature development during early training while preventing absorption once good reconstructions are achieved. We evaluate our method on standard mechanistic interpretability benchmarks and analyze the temporal stability characteristics of learned features.",
        "novel": true
    }
]