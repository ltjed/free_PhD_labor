[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_sae",
        "Title": "Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning",
        "Experiment": "1. Implement periodic feature correlation computation\n2. Add agglomerative clustering for feature hierarchy construction\n3. Modify unlearning process to use hierarchical feature groups\n4. Train on Gemma-2B activations with standard SAE objectives\n5. Compare against baseline using expanded evaluation metrics\n6. Analyze hierarchy stability and feature group coherence\n7. Measure fine-grained unlearning precision",
        "Technical_Details": "The hierarchical SAE extends standard SAEs with:\n1. Periodic computation of feature correlation matrix C_ij = corr(f_i, f_j) every N steps\n2. Agglomerative clustering using average linkage and threshold t=0.5\n3. Feature groups G_k containing correlated features identified by clustering\n4. Modified unlearning: When feature f_i is targeted, all features in its group G_k are modified\n5. Unlearning precision metric: ratio of intended vs unintended knowledge removal\n6. Group coherence score: average pairwise correlation within groups",
        "Research_Impact": "A key challenge in knowledge unlearning is maintaining model performance on unrelated tasks while removing specific knowledge. Current methods using individual feature modification often have unintended side effects. This research addresses this by identifying and modifying groups of related features together, improving unlearning precision while preserving desired capabilities. The simplified hierarchical approach makes this practical for large-scale models.",
        "Implementation_Plan": "1. Add feature_correlations() method to CustomSAE\n2. Implement hierarchical_clustering() using scipy.cluster.hierarchy\n3. Modify update() to compute correlations every N steps\n4. Add get_feature_group() method for unlearning\n5. Update evaluation metrics in unlearning.py\n6. Add correlation and group statistics tracking\n7. Implement visualization for feature groups",
        "Interestingness_Evaluation": "The simplified hierarchical approach offers a practical solution to a fundamental challenge in knowledge unlearning while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is now much simpler with fixed periodic correlation computation and standard clustering algorithms, the main computation remains similar to standard SAEs with minimal overhead, and the evaluation can be completed within the time constraints.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While feature correlation analysis exists in other contexts, using hierarchical feature groups for targeted knowledge unlearning in SAEs is novel and practical.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": true
    },
    {
        "Name": "temporal_contrastive_sae",
        "Title": "Temporal Contrastive Learning for Precise Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Implement sliding window buffer (size W=4) for activation sequences\n2. Add lightweight projection head (2-layer MLP, 256 hidden dim)\n3. Modify SAE to process fixed-size windows with efficient strided computation\n4. Train on Gemma-2B using combined loss with \u03bb=0.1 weight on contrastive term\n5. Evaluate unlearning using WMDP-bio benchmark\n6. Compare temporal feature stability pre/post unlearning\n7. Measure impact on unrelated MMLU subjects",
        "Technical_Details": "The TC-SAE architecture:\n1. Fixed window size W=4 for temporal context\n2. Projection head h(x): Linear(d_sae, 256) \u2192 ReLU \u2192 Linear(256, 128)\n3. Window encoding: e(x_t) = mean([SAE(x_i) for i in (t-W,t)])\n4. Contrastive loss using positive pairs (e(x_t), e(x_t+1)) and negative queue size 4096\n5. Temperature parameter \u03c4=0.07 for InfoNCE loss\n6. Gradient stopping between SAE and projection head\n7. Temporal stability score: cosine similarity between adjacent window encodings",
        "Research_Impact": "A key challenge in knowledge unlearning is precisely targeting specific knowledge while minimizing impact on unrelated capabilities. Current SAE approaches that modify individual features often have unintended side effects due to feature reuse across different knowledge domains. This research addresses this by learning temporally consistent features that better align with semantic knowledge boundaries, enabling more precise unlearning while better preserving desired model capabilities. The simplified sliding window approach makes this practical for large-scale models.",
        "Implementation_Plan": "1. Add SlidingWindowBuffer class with efficient strided sampling\n2. Implement ProjectionHead as lightweight MLP\n3. Add temporal_contrastive_loss() using queue-based negatives\n4. Modify forward() to handle windows with mean pooling\n5. Add temporal_stability_score() metric\n6. Update unlearning.py evaluation code\n7. Add configuration options for window size and loss weights",
        "Interestingness_Evaluation": "The temporal contrastive approach with fixed windows offers a practical solution to improve unlearning precision while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The simplified sliding window design with fixed size, lightweight projection head, and efficient negative sampling makes implementation straightforward and keeps computational overhead minimal, easily fitting within the 30-minute constraint on an H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While contrastive learning exists in other contexts, applying it with temporal windows for knowledge unlearning in SAEs is novel and practical.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": true
    },
    {
        "Name": "graph_guided_sae",
        "Title": "Graph-Guided Sparse Autoencoders for Precise Knowledge Unlearning",
        "Experiment": "1. Implement sparse binary adjacency matrix with fixed connectivity k\n2. Add simple message passing layer for feature interaction\n3. Modify encoder to combine direct and graph-propagated features\n4. Train on Gemma-2B with joint reconstruction and co-activation losses\n5. Implement efficient feature module caching\n6. Compare unlearning precision against baseline SAE\n7. Analyze graph connectivity patterns\n8. Measure impact on unrelated MMLU subjects",
        "Technical_Details": "The simplified GG-SAE architecture:\n1. Binary adjacency matrix A \u2208 {0,1}^(d_sae \u00d7 d_sae) with exactly k neighbors per feature\n2. Simple message passing: h_i = \u03c3(\u03a3_j A_ij f_j / k)\n3. Modified encoder output: f = f_direct + \u03bb*h where f_direct is standard SAE output\n4. Loss terms:\n   - L_rec: Standard reconstruction loss\n   - L_coact: Binary cross-entropy between feature activations of connected nodes\n5. Feature modules cached using connected components with updates every 1000 steps\n6. Module assignment using fast connected components algorithm\n7. Unlearning applies uniform suppression to all features in identified modules",
        "Research_Impact": "A critical challenge in knowledge unlearning is precisely targeting specific knowledge while minimizing impact on unrelated capabilities. Current SAE approaches that modify individual features often have unintended side effects due to feature entanglement across different knowledge domains. This research addresses this by learning an explicit graph structure over features that aligns with natural knowledge boundaries, enabling more precise unlearning while better preserving desired model capabilities. The simplified binary graph and efficient message passing make this highly practical for large-scale models.",
        "Implementation_Plan": "1. Add GraphGuidedSAE class extending CustomSAE\n2. Implement sparse_binary_adjacency_init() with fixed k\n3. Add simple_message_passing() using sparse operations\n4. Modify encode() to combine direct and propagated features\n5. Add coactivation_loss() for graph learning\n6. Implement cache_feature_modules() using scipy.sparse.csgraph\n7. Update unlearning.py evaluation code\n8. Add configuration for k and \u03bb",
        "Interestingness_Evaluation": "The simplified graph-guided approach offers an elegant and practical solution to feature entanglement by making knowledge relationships explicit through binary connectivity.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is now highly feasible with simple binary connectivity, efficient sparse operations, and cached feature modules, ensuring training and evaluation complete within 30 minutes on an H100 GPU while maintaining the core benefits of graph-guided feature organization.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While graph structures are common, the simplified binary approach to organizing SAE features for knowledge unlearning is novel and highly practical.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": true
    },
    {
        "Name": "adaptive_suppression_sae",
        "Title": "Adaptive Feature Suppression for Precise Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Implement streaming activation statistics\n2. Add basis vectors for suppression parameters\n3. Modify feature selection to use piece-wise linear suppression\n4. Train baseline SAE on Gemma-2B activations\n5. Compare against binary suppression on unlearning benchmark\n6. Analyze learned basis vectors\n7. Evaluate impact on unrelated MMLU subjects\n8. Measure fine-grained unlearning precision",
        "Technical_Details": "The optimized adaptive suppression approach consists of:\n1. Streaming importance computation: EMA_retain and EMA_forget updated during training\n2. Importance score I_i = clip(EMA_retain_i/(EMA_forget_i + \u03b5), 0, 1)\n3. Learned basis vectors B \u2208 \u211d^(k\u00d72) where k=8 is basis size\n4. Feature parameters [\u03b1_i, \u03b2_i] = Bw_i where w_i \u2208 \u211d^k are feature-specific weights\n5. Piece-wise linear multiplier m_i = max(0, min(1, \u03b1_i*I_i - \u03b2_i))\n6. Efficient batch implementation using sparse operations\n7. Unified optimization with main SAE parameters",
        "Research_Impact": "A critical challenge in knowledge unlearning is achieving precise removal of targeted knowledge while minimizing impact on desired capabilities. Current binary suppression approaches often cause unnecessary degradation of model performance on unrelated tasks. This research addresses this limitation by introducing scalable adaptive suppression using learned basis vectors, enabling more fine-grained control over knowledge removal while better preserving model capabilities on unrelated tasks. The streaming computation and basis decomposition make this highly practical for large-scale models.",
        "Implementation_Plan": "1. Add StreamingStatistics class for EMA updates\n2. Implement BasisDecomposition for suppression parameters\n3. Add piece_wise_multipliers() using efficient operations\n4. Modify inference code to use continuous suppression\n5. Integrate parameter updates with main optimizer\n6. Update evaluation metrics in unlearning.py\n7. Add configuration for basis size and EMA rate\n8. Implement visualization for basis vectors",
        "Interestingness_Evaluation": "The streaming adaptive suppression approach with basis decomposition offers an elegant, scalable, and practical solution to the precision-vs-preservation trade-off in knowledge unlearning.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is extremely feasible with streaming statistics, efficient basis decomposition, and unified optimization, ensuring training and evaluation complete within 30 minutes on an H100 GPU while maintaining core benefits.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While feature attribution methods exist, using streaming statistics and basis decomposition for adaptive knowledge unlearning in SAEs is novel and highly practical.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": true
    },
    {
        "Name": "momentum_grouped_sae",
        "Title": "Memory-Efficient Momentum-Based Feature Grouping for Precise Knowledge Unlearning",
        "Experiment": "1. Implement circular buffer for recent co-activations\n2. Add normalized distance computation\n3. Implement binary search for threshold adaptation\n4. Train on Gemma-2B activations with standard SAE objectives\n5. Compare unlearning precision against baseline SAE\n6. Analyze memory usage and runtime efficiency\n7. Measure impact on unrelated MMLU subjects\n8. Evaluate group stability for active features",
        "Technical_Details": "The optimized momentum-based approach consists of:\n1. Circular buffer C of size k=10000 storing (i,j,v) tuples for co-activations\n2. Update rule: insert((i,j,v_t)) where v_t = \u03b2v_(t-1) + (1-\u03b2)f_i*f_j if v_t > \u03b5\n3. Normalized distance: d(i,j) = ||BN(v_i) - BN(v_j)||_2 where BN is batch normalization\n4. Binary search for threshold \u03c4 targeting sparsity s: adjust \u03c4 until |{v > \u03c4}|/|v| \u2248 s\n5. Efficient stability tracking: only update scores for features active in current batch\n6. O(1) memory usage through circular buffer\n7. O(n) runtime complexity per batch where n is batch size",
        "Research_Impact": "A critical challenge in knowledge unlearning is precisely targeting specific knowledge while minimizing impact on unrelated capabilities, especially for large models where memory and computational efficiency are crucial. Current approaches often scale poorly with model size and lack precise control over unlearning granularity. This research addresses these limitations by introducing a memory-efficient momentum-based mechanism that maintains constant memory usage regardless of model size while enabling fine-grained control over knowledge removal. The circular buffer approach and normalized distances make this highly practical for large-scale models while improving unlearning precision.",
        "Implementation_Plan": "1. Add CircularCoactivationBuffer class\n2. Implement efficient_update() using numpy circular buffer\n3. Add normalized_distances() with batch normalization\n4. Implement binary_search_threshold()\n5. Add active_feature_stability_tracking()\n6. Update unlearning code to use normalized distances\n7. Add memory monitoring\n8. Implement visualization for buffer statistics",
        "Interestingness_Evaluation": "The memory-efficient momentum approach with circular buffer and normalized distances offers an elegant, highly scalable solution for precise knowledge unlearning that works even for very large models.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is extremely feasible with constant memory usage, simple circular buffer operations, and efficient normalized distances, ensuring training and evaluation complete within 30 minutes on an H100 GPU while maintaining core benefits.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While circular buffers are common in systems, applying them with normalized momentum tracking for scalable knowledge unlearning in SAEs is highly novel and practical.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": true
    }
]