{
    "Summary": "The paper proposes FeatureForget, a dual-objective sparse autoencoder for selective knowledge unlearning in large language models (LLMs). The method introduces gradient-guided feature management, semantic clustering, and adaptive thresholding to achieve fine-grained control over knowledge representations. Experiments on the Gemma-2B model demonstrate successful unlearning while maintaining model performance.",
    "Strengths": [
        "Addresses an important problem in LLMs: selective knowledge unlearning.",
        "Proposes a novel combination of semantic clustering and adaptive thresholding.",
        "Experimental results show promising improvements in unlearning scores."
    ],
    "Weaknesses": [
        "The novelty of the approach is not clearly established compared to existing methods.",
        "The paper lacks detailed explanations of the technical components, particularly gradient-guided importance scoring and adaptive thresholding.",
        "Writing is dense and difficult to follow, with key concepts not well explained.",
        "Experimental evaluation lacks comprehensive ablation studies and comparisons with a broader range of baselines.",
        "The paper does not discuss potential negative societal impacts or ethical considerations."
    ],
    "Originality": 3,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can the authors provide a more detailed explanation of the gradient-guided importance scoring and adaptive thresholding mechanisms?",
        "How does the proposed method compare with a wider range of existing unlearning techniques?",
        "Can the authors include more comprehensive ablation studies to validate the contributions of each component?",
        "Why were 32 semantic clusters chosen for the feature organization?",
        "Can you provide more detailed explanations or visualizations for the gradient-guided importance scoring?",
        "How does the adaptive thresholding mechanism specifically improve unlearning performance?",
        "Can additional ablation studies be provided to further validate the contributions of each component?",
        "How does FeatureForget perform on other datasets beyond WMDP-bio?"
    ],
    "Limitations": [
        "The paper mentions sensitivity to hyperparameters and resource requirements but does not discuss potential negative societal impacts or other limitations in detail.",
        "The method's sensitivity to hyperparameters, particularly sparsity penalty and learning rate, is a concern that needs further exploration.",
        "The layer-dependent effectiveness suggests that deeper layers require longer training times, which could be a limitation in practical applications.",
        "Resource requirements scale with model size and feature dictionary, indicating potential challenges in deploying the method on very large models."
    ],
    "Ethical Concerns": true,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}