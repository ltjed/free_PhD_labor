\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gpt4}
\citation{radford2019language}
\citation{vaswani2017attention}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{vaswani2017attention}
\citation{radford2019language}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{sec:background}{{2}{2}{Background}{section.2}{}}
\newlabel{sec:background@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem Setting}{2}{subsection.2.1}\protected@file@percent }
\citation{Meng2022LocatingAE}
\citation{Cohen2023EvaluatingTR}
\citation{vaswani2017attention}
\citation{bahdanau2014neural}
\citation{ba2016layer}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{3}{section.3}\protected@file@percent }
\newlabel{sec:related}{{3}{3}{Related Work}{section.3}{}}
\newlabel{sec:related@cref}{{[section][3][]3}{[1][2][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Direct Weight Modification}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Constrained Fine-tuning}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature-based Approaches}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}{}{}{}}
\newlabel{eq:reconstruction}{{4}{3}{Method}{equation.4}{}}
\newlabel{eq:reconstruction@cref}{{[equation][4][]4}{[1][3][]3}{}{}{}}
\citation{paszke2019pytorch,loshchilov2017adamw,ba2016layer}
\newlabel{eq:importance}{{5}{4}{Method}{equation.5}{}}
\newlabel{eq:importance@cref}{{[equation][5][]5}{[1][3][]4}{}{}{}}
\newlabel{eq:threshold}{{6}{4}{Method}{equation.6}{}}
\newlabel{eq:threshold@cref}{{[equation][6][]6}{[1][4][]4}{}{}{}}
\newlabel{eq:loss}{{7}{4}{Method}{equation.7}{}}
\newlabel{eq:loss@cref}{{[equation][7][]7}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{5}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][4][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Training Dynamics}{5}{subsection.6.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:unlearning}{{1a}{5}{Unlearning Performance}{figure.caption.1}{}}
\newlabel{fig:unlearning@cref}{{[subfigure][1][1]1a}{[1][5][]5}{}{}{}}
\newlabel{sub@fig:unlearning}{{a}{5}{Unlearning Performance}{figure.caption.1}{}}
\newlabel{sub@fig:unlearning@cref}{{[subfigure][1][1]1a}{[1][5][]5}{}{}{}}
\newlabel{fig:losses}{{1b}{5}{Training Loss Curves}{figure.caption.1}{}}
\newlabel{fig:losses@cref}{{[subfigure][2][1]1b}{[1][5][]5}{}{}{}}
\newlabel{sub@fig:losses}{{b}{5}{Training Loss Curves}{figure.caption.1}{}}
\newlabel{sub@fig:losses@cref}{{[subfigure][2][1]1b}{[1][5][]5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Unlearning scores across model layers, showing consistent performance improvement over baseline SAE. (b) Training loss curves over time, demonstrating stable convergence across different architectural variants.}}{5}{figure.caption.1}\protected@file@percent }
\newlabel{fig:training_dynamics}{{1}{5}{(a) Unlearning scores across model layers, showing consistent performance improvement over baseline SAE. (b) Training loss curves over time, demonstrating stable convergence across different architectural variants}{figure.caption.1}{}}
\newlabel{fig:training_dynamics@cref}{{[figure][1][]1}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ablation Studies}{5}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Performance Analysis}{5}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Limitations}{5}{subsection.6.4}\protected@file@percent }
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{ba2016layer}{{1}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{bahdanau2014neural}{{2}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{Cohen2023EvaluatingTR}{{3}{2023}{{Cohen et~al.}}{{Cohen, Biran, Yoran, Globerson, and Geva}}}
\bibcite{goodfellow2016deep}{{4}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{loshchilov2017adamw}{{5}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{Meng2022LocatingAE}{{6}{2022}{{Meng et~al.}}{{Meng, Bau, Andonian, and Belinkov}}}
\bibcite{gpt4}{{7}{2024}{{OpenAI}}{{}}}
\bibcite{paszke2019pytorch}{{8}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{6}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{6}{Conclusions and Future Work}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][6][]6}{}{}{}}
\bibcite{radford2019language}{{9}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{vaswani2017attention}{{10}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\ttl@finishall
\gdef \@abspage@last{7}
