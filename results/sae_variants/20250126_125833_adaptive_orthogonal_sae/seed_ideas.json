[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
<<<<<<< HEAD:results/sae_variants/20250126_125833_adaptive_orthogonal_sae/seed_ideas.json
        "Novelty": 7
=======
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "coactivation_sae",
        "Title": "Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders",
        "Experiment": "1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns",
        "Technical_Details": "The method scales sparsity penalties based on co-activation patterns: L = L_recon + \u03a3_i (\u03bb_1 * (1 + \u03b3*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. \u03b3 controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j\u2260i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.",
        "Implementation_Plan": "1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics",
        "Interestingness_Evaluation": "The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.",
        "Feasibility": 8,
        "Novelty_Evaluation": "While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns.",
        "novel": true
    },
    {
        "Name": "active_feature_absorption_sae",
        "Title": "Active Feature-Guided Prevention of Absorption in Sparse Autoencoders",
        "Experiment": "1. Implement active feature detection per batch\n2. Add normalized weight dot product computation\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns",
        "Technical_Details": "The method prevents feature absorption by focusing on active features: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute normalized dot products between decoder weight vectors. When dot(w_i, w_j) > \u03c4 (typically 0.8), we compute the running average activation ratio R = avg(f_i & f_j) / avg(f_j) with decay 0.99. If R > \u03c1 (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - \u03c1) * dot(w_i, w_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.",
        "Implementation_Plan": "1. Add active feature identification\n2. Implement normalized dot product computation\n3. Add running average activation tracking\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns",
        "Interestingness_Evaluation": "The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation is extremely simple - just basic dot products and running averages; minimal overhead; single clear threshold parameter; fits easily within training loop.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.",
        "Novelty": 6,
        "Expected_Research_Impact": "The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.",
        "Research_Impact": 7,
        "Overall_Score": 7.7,
        "Abstract": "Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations.",
        "novel": true
    },
    {
        "Name": "decoder_selective_orthogonal_sae",
        "Title": "Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
        "Experiment": "1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation",
        "Technical_Details": "The method uses a weight-based selective orthogonality loss: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding \u03c4=0.3. \u03bb_2(t) decreases linearly: \u03bb_2(t) = \u03bb_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > \u03c4 else 0. L_selective_ortho = ||W_dec^T W_dec \u2299 M||_F.",
        "Implementation_Plan": "1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity",
        "Interestingness_Evaluation": "The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.",
        "Novelty": 7,
        "Expected_Research_Impact": "The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.",
        "Research_Impact": 8,
        "Overall_Score": 8.1,
        "Abstract": "We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies.",
        "novel": true
    },
    {
        "Name": "model_aligned_sae",
        "Title": "Model-Aligned Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies",
        "Experiment": "1. Implement SAE with model-aligned dictionary sizes (full and d/3)\n2. Train on Gemma-2-2b using size-weighted reconstruction losses\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns",
        "Technical_Details": "The method uses two reconstruction objectives with model-aligned dictionary sizes [d, d/3] and size-weighted losses: L = (d*L_recon(x, f[:d]) + (d/3)*L_recon(x, f[:d/3])) / (d + d/3). For Gemma-2-2b, this means 2304 and 768 features. Feature hierarchy is measured through feature presence in both dictionaries versus full only. The architecture remains unchanged except for the weighted dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.",
        "Implementation_Plan": "1. Add MODEL_ALIGNED_SIZES = [d, d/3] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with size-weighted reconstruction losses\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries",
        "Interestingness_Evaluation": "The model-aligned approach provides a principled way to encourage natural feature hierarchies that match the underlying model architecture.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While building on previous hierarchical approaches, the model-aligned dictionary sizes and weighted losses provide a novel twist.",
        "Novelty": 7,
        "Expected_Research_Impact": "The alignment with model dimensions should produce more natural and interpretable feature hierarchies.",
        "Research_Impact": 8,
        "Overall_Score": 8.2,
        "Abstract": "We propose a principled approach to improving interpretability in sparse autoencoders through model-aligned dual-dictionary feature learning. Our method uses two reconstruction objectives with dictionary sizes aligned to the underlying model dimensions (full model dimension and one-third) and size-weighted losses. This alignment with model structure encourages the autoencoder to learn natural feature hierarchies that reflect the model's inherent organizational principles. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective with model-aligned dimensions and simple feature presence analysis.",
        "novel": true
    },
    {
        "Name": "frequency_ordered_sae",
        "Title": "Frequency-Based Feature Ordering in Sparse Autoencoders",
        "Experiment": "1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability",
        "Technical_Details": "The method uses activation frequency for ordering: L = L_recon + \u03bb_1 * \u03a3|f_i| + \u03bb_2 * \u03a3_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > \u03b5). This creates a natural ordering based on how often features are used, with \u03b5 being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.",
        "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking",
        "Interestingness_Evaluation": "The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only simple threshold counting; no additional parameters beyond threshold \u03b5; minimal overhead as thresholding operations are very efficient.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.",
        "Novelty": 7,
        "Expected_Research_Impact": "The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations.",
        "novel": true
    },
    {
        "Name": "grouped_sparse_sae",
        "Title": "Feature Group Sparsity for Preventing Absorption in Sparse Autoencoders",
        "Experiment": "1. Implement group-based sparsity penalties\n2. Train on standard datasets with discrete penalty levels\n3. Analyze feature usage patterns within groups\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different group sizes and penalty ratios",
        "Technical_Details": "The method uses a single dictionary with group-based L1 penalties. Features are divided into K=3 equal-sized groups with penalties \u03bb_k = \u03bb_base * \u03b3^k, where \u03bb_base=0.04 is the base penalty and \u03b3=2 is the scaling factor between groups. The loss is L = L_recon + \u03a3(\u03bb_g(i) * |h_i|) where g(i) maps feature i to its group index. This creates distinct feature groups with different sparsity levels, naturally preventing absorption between groups while maintaining simplicity.",
        "Implementation_Plan": "1. Add vector of group-based penalties to CustomSAE\n2. Modify loss function to use grouped penalties\n3. Add utilities for analyzing within-group patterns\n4. Implement visualization tools for group behavior",
        "Interestingness_Evaluation": "The group-based approach provides a clear and interpretable way to prevent feature absorption while maintaining simplicity.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation is extremely simple - just one vector of fixed group penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While grouped penalties exist in other contexts, their specific application to prevent feature absorption in SAEs is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The clear separation between feature groups should directly improve sparse probing by preventing absorption. The discrete penalty levels should help core metrics by creating stable feature groups.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We propose a group-based sparsity approach to preventing feature absorption in sparse autoencoders. Our method divides features into distinct groups with different sparsity penalties, creating natural barriers that discourage features from absorbing patterns across group boundaries. This approach requires minimal modifications to standard sparse autoencoders while providing a clear and interpretable way to organize features. The discrete penalty levels create well-defined feature groups that aim to capture patterns at different levels of specificity, helping to prevent the undesirable mixing of features that occurs in feature absorption.",
        "novel": true
    },
    {
        "Name": "threshold_orthogonal_sae",
        "Title": "Threshold-Based Orthogonality for Targeted Feature Separation in Sparse Autoencoders",
        "Experiment": "1. Add activation frequency tracking\n2. Implement threshold-based orthogonality loss\n3. Train on standard datasets with varying thresholds\n4. Compare feature separation quality\n5. Analyze high-frequency feature interactions",
        "Technical_Details": "The method applies orthogonality constraints selectively: L_ortho = \u03a3_{i,j} h_i * h_j * (w_i^T w_j)^2 where h_i is 1 if feature i's activation frequency exceeds threshold \u03c4 (typically 0.1) and 0 otherwise. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_ortho. Only high-frequency features participate in orthogonality constraints.",
        "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Implement threshold-based feature masking\n3. Modify orthogonality loss to use masks\n4. Add frequency analysis utilities\n5. Update training loop with mask updates",
        "Interestingness_Evaluation": "The approach provides a clear and focused way to address polysemanticity by targeting orthogonality constraints specifically at features most likely to need them.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation is extremely simple; frequency tracking and thresholding are basic operations; orthogonality loss computation reduced by masking; no complex algorithms needed; training time less than baseline due to selective application.",
        "Feasibility": 10,
        "Novelty_Evaluation": "Using a threshold-based approach to selectively apply orthogonality constraints is a novel simplification of adaptive feature separation.",
        "Novelty": 7,
        "Expected_Research_Impact": "The targeted application of constraints should effectively reduce polysemanticity in key features while maintaining model flexibility, improving both sparse_probing and core metrics.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We present a targeted approach to improving feature separation in sparse autoencoders (SAEs) through threshold-based orthogonality constraints. Current SAEs apply constraints uniformly across all features, potentially over-constraining rarely used features while under-constraining frequently used ones that are more likely to be polysemantic. Our method selectively applies orthogonality constraints only between frequently activating features, identified through a simple threshold on activation frequency. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate masked orthogonality constraints. This approach provides a simple yet principled way to focus feature separation efforts on the subset of features most likely to benefit from them.",
        "novel": true
    },
    {
        "Name": "step_progressive_sae",
        "Title": "Step-Based Progressive Feature Learning for Sparse Autoencoders",
        "Experiment": "1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold",
        "Technical_Details": "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > \u03c4 (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + \u03bb * L_sparse with masked features f' = M \u2299 f. Streak counting provides clear criteria for feature stability.",
        "Implementation_Plan": "1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression",
        "Interestingness_Evaluation": "The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.",
        "Novelty": 6,
        "Expected_Research_Impact": "The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.",
        "novel": true
    },
    {
        "Name": "matryoshka_sae",
        "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
        "Experiment": "1. Implement SAE with 2 nested dictionaries (d and \u03b1d size, \u03b1 \u2208 [0.3, 0.7])\n2. Train on standard datasets with different \u03b1 values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of \u03b1 on feature monosemanticity",
        "Technical_Details": "The architecture uses 2 nested dictionaries D_1 \u2282 D_2 where |D_1| = \u03b1d, |D_2| = d, with \u03b1 as a tunable hyperparameter. The encoder matrix W_enc has its first \u03b1d columns shared between both dictionaries, while the remaining (1-\u03b1)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + \u03bb||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and \u03bb = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.",
        "Implementation_Plan": "1. Add \u03b1 parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over \u03b1 values",
        "Interestingness_Evaluation": "The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; \u03b1 parameter provides easy tuning.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.",
        "Novelty": 6,
        "Expected_Research_Impact": "The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.",
        "Research_Impact": 7,
        "Overall_Score": 7.6,
        "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter \u03b1 that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures.",
        "novel": true
>>>>>>> e2daa75f2c1985eec403651ce0de7f7046ec40ab:templates/sae_variants/ideas.json
    }
]
