\begin{thebibliography}{10}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio(2007)]{Bengio2007LearningDA}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock \emph{Found. Trends Mach. Learn.}, 2:\penalty0 1--127, 2007.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and
  Manning]{Clark2019WhatDB}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning.
\newblock What does bert look at? an analysis of bertâ€™s attention.
\newblock pp.\  276--286, 2019.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[He et~al.(2024)He, Chen, Nie, Li, and Brennan]{He2024DecodingPR}
Linyang He, Peili Chen, Ercong Nie, Yuanning Li, and Jonathan~R. Brennan.
\newblock Decoding probing: Revealing internal linguistic structures in neural
  language models using minimal pairs.
\newblock \emph{ArXiv}, abs/2403.17299, 2024.

\bibitem[Kazemnejad et~al.(2023)Kazemnejad, Padhi, Ramamurthy, Das, and
  Reddy]{Kazemnejad2023TheIO}
Amirhossein Kazemnejad, Inkit Padhi, K.~Ramamurthy, Payel Das, and Siva Reddy.
\newblock The impact of positional encoding on length generalization in
  transformers.
\newblock \emph{ArXiv}, abs/2305.19466, 2023.

\bibitem[Marks et~al.(2024)Marks, Paren, Krueger, and
  Barez]{Marks2024EnhancingNN}
Luke Marks, Alisdair Paren, David Krueger, and Fazl Barez.
\newblock Enhancing neural network interpretability with feature-aligned sparse
  autoencoders.
\newblock \emph{ArXiv}, abs/2411.01220, 2024.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{Voita2019AnalyzingMS}
Elena Voita, David Talbot, F.~Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{ArXiv}, abs/1905.09418, 2019.

\end{thebibliography}
