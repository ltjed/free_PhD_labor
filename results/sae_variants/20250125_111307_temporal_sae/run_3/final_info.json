{
  "eval_type_id": "sparse_probing",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "LabHC/bias_in_bios_class_set2",
      "LabHC/bias_in_bios_class_set3",
      "canrager/amazon_reviews_mcauley_1and5",
      "canrager/amazon_reviews_mcauley_1and5_sentiment",
      "codeparrot/github-code",
      "fancyzhx/ag_news",
      "Helsinki-NLP/europarl"
    ],
    "probe_train_set_size": 4000,
    "probe_test_set_size": 1000,
    "context_length": 128,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "model_name": "google/gemma-2-2b",
    "k_values": [
      1,
      2,
      5,
      10,
      20,
      50
    ],
    "lower_vram_usage": false
  },
  "eval_id": "53ed9f87-e91e-4ebd-aafb-7bd49b2528e4",
  "datetime_epoch_millis": 1737823072537,
  "eval_result_metrics": {
    "llm": {
      "llm_test_accuracy": 0.9509062500000001,
      "llm_top_1_test_accuracy": 0.70174375,
      "llm_top_2_test_accuracy": 0.75609375,
      "llm_top_5_test_accuracy": 0.81760625,
      "llm_top_10_test_accuracy": 0.869725,
      "llm_top_20_test_accuracy": 0.9025375,
      "llm_top_50_test_accuracy": 0.93210625,
      "llm_top_100_test_accuracy": null
    },
    "sae": {
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_results",
      "llm_test_accuracy": 0.9606,
      "llm_top_1_test_accuracy": 0.6564,
      "llm_top_2_test_accuracy": 0.7246,
      "llm_top_5_test_accuracy": 0.8052000000000001,
      "llm_top_10_test_accuracy": 0.8672000000000001,
      "llm_top_20_test_accuracy": 0.9133999999999999,
      "llm_top_50_test_accuracy": 0.9513999999999999,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set2_results",
      "llm_test_accuracy": 0.9448000000000001,
      "llm_top_1_test_accuracy": 0.6738,
      "llm_top_2_test_accuracy": 0.7026,
      "llm_top_5_test_accuracy": 0.7729999999999999,
      "llm_top_10_test_accuracy": 0.8268000000000001,
      "llm_top_20_test_accuracy": 0.8892,
      "llm_top_50_test_accuracy": 0.9154,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set3_results",
      "llm_test_accuracy": 0.9154,
      "llm_top_1_test_accuracy": 0.6926,
      "llm_top_2_test_accuracy": 0.7132,
      "llm_top_5_test_accuracy": 0.78,
      "llm_top_10_test_accuracy": 0.8436,
      "llm_top_20_test_accuracy": 0.8688,
      "llm_top_50_test_accuracy": 0.9086000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
      "llm_test_accuracy": 0.9099999999999999,
      "llm_top_1_test_accuracy": 0.629,
      "llm_top_2_test_accuracy": 0.6984,
      "llm_top_5_test_accuracy": 0.7567999999999999,
      "llm_top_10_test_accuracy": 0.8154,
      "llm_top_20_test_accuracy": 0.8583999999999999,
      "llm_top_50_test_accuracy": 0.8800000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
      "llm_test_accuracy": 0.9695,
      "llm_top_1_test_accuracy": 0.697,
      "llm_top_2_test_accuracy": 0.743,
      "llm_top_5_test_accuracy": 0.79,
      "llm_top_10_test_accuracy": 0.86,
      "llm_top_20_test_accuracy": 0.874,
      "llm_top_50_test_accuracy": 0.942,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "codeparrot/github-code_results",
      "llm_test_accuracy": 0.9648,
      "llm_top_1_test_accuracy": 0.6279999999999999,
      "llm_top_2_test_accuracy": 0.6918,
      "llm_top_5_test_accuracy": 0.8064,
      "llm_top_10_test_accuracy": 0.875,
      "llm_top_20_test_accuracy": 0.9156000000000001,
      "llm_top_50_test_accuracy": 0.9378,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "fancyzhx/ag_news_results",
      "llm_test_accuracy": 0.94275,
      "llm_top_1_test_accuracy": 0.6977499999999999,
      "llm_top_2_test_accuracy": 0.7897500000000001,
      "llm_top_5_test_accuracy": 0.8382499999999999,
      "llm_top_10_test_accuracy": 0.871,
      "llm_top_20_test_accuracy": 0.9015,
      "llm_top_50_test_accuracy": 0.92225,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "Helsinki-NLP/europarl_results",
      "llm_test_accuracy": 0.9994,
      "llm_top_1_test_accuracy": 0.9394,
      "llm_top_2_test_accuracy": 0.9853999999999999,
      "llm_top_5_test_accuracy": 0.9911999999999999,
      "llm_top_10_test_accuracy": 0.9987999999999999,
      "llm_top_20_test_accuracy": 0.9994,
      "llm_top_50_test_accuracy": 0.9994,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  ],
  "sae_bench_commit_hash": "79294abe9b112dfcb31b6c1aac0265ad22ed9a72",
  "sae_lens_id": "custom_sae",
  "sae_lens_release_id": "google/gemma-2-2b_layer_19_sae",
  "sae_lens_version": "5.3.0",
  "sae_cfg_dict": {
    "model_name": "google/gemma-2-2b",
    "d_in": 2304,
    "d_sae": 2304,
    "hook_layer": 19,
    "hook_name": "blocks.19.hook_resid_post",
    "context_size": 128,
    "hook_head_index": null,
    "architecture": "PositionalSAE",
    "apply_b_dec_to_input": true,
    "finetuning_scaling_factor": null,
    "activation_fn_str": "relu",
    "prepend_bos": true,
    "normalize_activations": "none",
    "dtype": "bfloat16",
    "device": "",
    "dataset_path": "",
    "dataset_trust_remote_code": true,
    "seqpos_slice": [
      null
    ],
    "training_tokens": -100000,
    "sae_lens_training_version": null,
    "neuronpedia_id": null
  },
  "eval_result_unstructured": null
}