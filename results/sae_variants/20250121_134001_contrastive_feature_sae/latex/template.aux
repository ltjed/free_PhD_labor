\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gpt4}
\citation{goodfellow2016deep}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{goodfellow2016deep}
\citation{vaswani2017attention}
\citation{bahdanau2014neural}
\citation{kingma2014adam}
\citation{goodfellow2016deep}
\citation{vaswani2017attention}
\citation{radford2019language}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{2}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}{}{}{}}
\citation{kingma2014adam}
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Setting}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}{}{}{}}
\citation{paszke2019pytorch}
\citation{kingma2014adam}
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Implementation Details}{4}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training Process}{4}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Evaluation Metrics}{5}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{5}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Training Dynamics and Metrics}{5}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ablation Study}{5}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Feature Importance Analysis}{6}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Limitations}{6}{subsection.6.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:first-run}{{\caption@xref {fig:first-run}{ on input line 343}}{6}{Limitations}{figure.caption.1}{}}
\newlabel{fig:first-run@cref}{{[subsection][4][6]6.4}{[1][6][]6}{}{}{}}
\newlabel{sub@fig:first-run}{{}{6}{Limitations}{figure.caption.1}{}}
\newlabel{sub@fig:first-run@cref}{{[subsection][4][6]6.4}{[1][6][]6}{}{}{}}
\newlabel{fig:second-run}{{\caption@xref {fig:second-run}{ on input line 348}}{6}{Limitations}{figure.caption.1}{}}
\newlabel{fig:second-run@cref}{{[subsection][4][6]6.4}{[1][6][]6}{}{}{}}
\newlabel{sub@fig:second-run}{{}{6}{Limitations}{figure.caption.1}{}}
\newlabel{sub@fig:second-run@cref}{{[subsection][4][6]6.4}{[1][6][]6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance metrics across SAE variants. (a) Unlearning scores showing consistent 0.0 values across all architectural modifications, indicating fundamental limitations in knowledge modification. (b) Training loss trajectories demonstrating stable optimization behavior despite increasing model complexity. All variants maintained similar convergence characteristics while failing to achieve effective knowledge unlearning.}}{6}{figure.caption.1}\protected@file@percent }
\newlabel{fig:first_figure}{{1}{6}{Performance metrics across SAE variants. (a) Unlearning scores showing consistent 0.0 values across all architectural modifications, indicating fundamental limitations in knowledge modification. (b) Training loss trajectories demonstrating stable optimization behavior despite increasing model complexity. All variants maintained similar convergence characteristics while failing to achieve effective knowledge unlearning}{figure.caption.1}{}}
\newlabel{fig:first_figure@cref}{{[figure][1][]1}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{6}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{6}{Conclusions}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][6][]6}{}{}{}}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{ba2016layer}{{1}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{bahdanau2014neural}{{2}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{goodfellow2016deep}{{3}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{kingma2014adam}{{4}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{loshchilov2017adamw}{{5}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{gpt4}{{6}{2024}{{OpenAI}}{{}}}
\bibcite{paszke2019pytorch}{{7}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.}}}
\bibcite{radford2019language}{{8}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{vaswani2017attention}{{9}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\ttl@finishall
\gdef \@abspage@last{7}
