# Title: Contrastive Feature Learning for Improved Knowledge Unlearning in Sparse Autoencoders

# Generated Figures Analysis

## Figure: unlearning_scores.png
This figure presents a bar chart comparing the unlearning scores across different SAE variants. The x-axis shows the different approaches (Baseline SAE, Basic Contrastive, Enhanced Contrastive, Adaptive Selection, and Hierarchical + MI), while the y-axis represents the unlearning score. A higher score indicates better unlearning capability. The consistent 0.0 scores across all variants suggest that none of the implemented approaches successfully improved unlearning performance, indicating a fundamental challenge in the current methodology.

## Figure: training_losses.png
This plot tracks the training loss progression over time for each SAE variant. The x-axis represents training steps, while the y-axis shows the loss value. Each variant is represented by a different colored line with 0.7 alpha transparency for better visualization. The plot reveals:
- The convergence speed of different approaches
- The stability of training (shown by loss fluctuations)
- The final loss values achieved by each variant
- Any potential overfitting or underfitting patterns
The relative positioning of the loss curves provides insights into the effectiveness of each modification in terms of optimization behavior.

## Figure: feature_thresholds.png
This bar chart visualizes the feature importance thresholds used in each SAE variant. The x-axis shows the different approaches, while the y-axis represents the threshold values. This plot is particularly important because it shows:
- The varying levels of feature selection stringency across approaches
- The relationship between threshold values and unlearning performance
- The evolution of feature importance criteria through different iterations
The consistent threshold of 0.01 across variants suggests that this parameter might need more dynamic adjustment in future experiments.

## Run 0: Baseline
Description: Baseline results using standard SAE training.
Results: [baseline results preserved as above]

## Run 1: Core Contrastive Learning Infrastructure
Description: Implemented initial contrastive learning framework with the following components:
1. Feature correlation tracking: Added periodic computation (every 100 steps) of feature correlations using encoded activations
2. Adaptive threshold mechanism: Implemented dynamic threshold calculation based on correlation distribution statistics
3. Basic feature grouping: Added initial grouping mechanism using correlation-based clustering
4. Modified loss function: Introduced contrastive term to encourage feature differentiation

Implementation Details:
- Correlation computation using batch-wise feature activations
- Threshold calculation using percentile-based approach (initial threshold at 75th percentile)
- Simple agglomerative clustering for feature grouping
- Contrastive loss weight: 0.1 (initial conservative value)

Results: Initial unlearning evaluation shows unlearning_score of 0.0, indicating:
- Basic infrastructure is in place but needs optimization
- Feature groups may need better coherence
- Contrastive loss weight may need adjustment
- Threshold calculation might need refinement

Next Steps:
- Optimize correlation thresholds
- Adjust group formation criteria
- Fine-tune contrastive loss weight

## Run 2: Optimize Contrastive Learning Parameters
Description: Enhanced contrastive learning framework with refined feature correlation and grouping mechanisms
Key Modifications:
1. Increased contrastive weight from 0.1 to 0.2
2. Raised correlation threshold percentile from 75% to 85%
3. Added minimum feature group size requirement (5 features)
4. Implemented more sophisticated contrastive loss with diversity and separation terms
5. Added correlation history tracking for deeper analysis

Implementation Details:
- Dynamic feature group formation based on correlation thresholds
- Enhanced loss function encouraging feature differentiation
- More selective feature clustering approach
- Layer: 19 (residual post-attention)
- Model: google/gemma-2-2b
- Dictionary Size: 2304
- Learning Rate: 0.0003
- Sparsity Penalty: 0.04

Results:
- Unlearning score remained at 0.0
- No improvement in feature differentiation despite enhanced grouping
- Correlation tracking revealed persistent feature entanglement

Observations:
- Current approach insufficient for meaningful unlearning
- Need more aggressive feature selection strategy
- Group formation may be too conservative
- Correlation-based approach alone may be inadequate

Next Steps:
- Implement explicit unlearning objective in loss function
- Add gradient-based feature importance tracking
- Introduce adaptive feature pruning mechanism
- Use information-theoretic metrics for feature selection
- Consider multi-objective optimization approach

## Run 3: Adaptive Feature Selection with Explicit Unlearning
Description: Implemented aggressive feature selection and explicit unlearning mechanisms with the following components:

Key Modifications:
1. Gradient-based feature importance tracking
2. Adaptive feature pruning (10% pruning rate)
3. Explicit unlearning objective using KL divergence
4. More frequent feature updates (every 50 steps)
5. Combined activation and gradient importance scoring (40/60 split)

Implementation Details:
- Layer: 19 (residual post-attention)
- Model: google/gemma-2-2b
- Dictionary Size: 2304
- Learning Rate: 0.0003
- Sparsity Penalty: 0.04
- Feature importance threshold: 0.01
- Pruning rate: 0.1
- Unlearning weight: 0.2
- Gradient memory factor: 0.9

Results:
- Unlearning score remained at 0.0
- Feature importance tracking showed expected behavior
- Pruning mechanism activated successfully
- KL divergence term properly implemented

Analysis:
- Despite more sophisticated mechanisms, unlearning performance did not improve
- Gradient-based importance tracking may be too aggressive
- Feature pruning rate might need adjustment
- KL divergence term may need different weighting

Next Steps:
1. Implement hierarchical feature clustering
2. Add mutual information maximization
3. Introduce feature reinitialization strategy
4. Consider curriculum learning approach
5. Add feature stability tracking

## Run 4: Hierarchical Clustering with Mutual Information
Description: Implemented hierarchical feature clustering and mutual information maximization to improve feature differentiation and unlearning capabilities.

Key Modifications:
1. Added hierarchical clustering with 32 clusters
2. Implemented mutual information term in feature importance scoring
3. Reduced pruning rate from 10% to 5% for more stable feature retention
4. Increased unlearning weight from 0.2 to 0.3
5. Added cluster-based feature tracking
6. Adjusted importance weighting (35% activation, 50% gradient, 15% MI)

Implementation Details:
- Layer: 19 (residual post-attention)
- Model: google/gemma-2-2b
- Dictionary Size: 2304
- Learning Rate: 0.0003
- Sparsity Penalty: 0.04
- Feature importance threshold: 0.01
- Pruning rate: 0.05
- Unlearning weight: 0.3
- Gradient memory: 0.9
- Number of clusters: 32
- MI weight: 0.15

Results:
- Unlearning score remained at 0.0
- Hierarchical clustering successfully implemented
- Mutual information term properly integrated
- Reduced pruning rate maintained feature stability
- Cluster assignments showed expected behavior

Analysis:
- Despite more sophisticated feature organization, unlearning performance did not improve
- Hierarchical clustering may need different granularity
- MI term weight might need adjustment
- Current approach may be fundamentally limited

Next Steps:
1. Implement feature reinitialization strategy
2. Consider curriculum learning approach
3. Add feature stability tracking
4. Explore alternative unlearning objectives
5. Consider ensemble approaches
