{
  "eval_type_id": "sparse_probing",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "LabHC/bias_in_bios_class_set2",
      "LabHC/bias_in_bios_class_set3",
      "canrager/amazon_reviews_mcauley_1and5",
      "canrager/amazon_reviews_mcauley_1and5_sentiment",
      "codeparrot/github-code",
      "fancyzhx/ag_news",
      "Helsinki-NLP/europarl"
    ],
    "probe_train_set_size": 4000,
    "probe_test_set_size": 1000,
    "context_length": 128,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "model_name": "google/gemma-2-2b",
    "k_values": [
      1,
      2,
      5,
      10,
      20,
      50
    ],
    "lower_vram_usage": false
  },
  "eval_id": "15933ab7-0556-4791-bc39-97d39bfbec05",
  "datetime_epoch_millis": 1737843425164,
  "eval_result_metrics": {
    "llm": {
      "llm_test_accuracy": 0.9396187500000001,
      "llm_top_1_test_accuracy": 0.684275,
      "llm_top_2_test_accuracy": 0.7262625,
      "llm_top_5_test_accuracy": 0.77453125,
      "llm_top_10_test_accuracy": 0.8209125,
      "llm_top_20_test_accuracy": 0.8590249999999999,
      "llm_top_50_test_accuracy": 0.8998999999999999,
      "llm_top_100_test_accuracy": null
    },
    "sae": {
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_results",
      "llm_test_accuracy": 0.958,
      "llm_top_1_test_accuracy": 0.6641999999999999,
      "llm_top_2_test_accuracy": 0.6843999999999999,
      "llm_top_5_test_accuracy": 0.7469999999999999,
      "llm_top_10_test_accuracy": 0.8282,
      "llm_top_20_test_accuracy": 0.8606,
      "llm_top_50_test_accuracy": 0.9118,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set2_results",
      "llm_test_accuracy": 0.9385999999999999,
      "llm_top_1_test_accuracy": 0.6878,
      "llm_top_2_test_accuracy": 0.7232000000000001,
      "llm_top_5_test_accuracy": 0.7634000000000001,
      "llm_top_10_test_accuracy": 0.8058,
      "llm_top_20_test_accuracy": 0.8488,
      "llm_top_50_test_accuracy": 0.8892,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set3_results",
      "llm_test_accuracy": 0.9044000000000001,
      "llm_top_1_test_accuracy": 0.6802,
      "llm_top_2_test_accuracy": 0.7070000000000001,
      "llm_top_5_test_accuracy": 0.7434000000000001,
      "llm_top_10_test_accuracy": 0.7984,
      "llm_top_20_test_accuracy": 0.8174000000000001,
      "llm_top_50_test_accuracy": 0.8714000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
      "llm_test_accuracy": 0.8842000000000001,
      "llm_top_1_test_accuracy": 0.6072,
      "llm_top_2_test_accuracy": 0.6444,
      "llm_top_5_test_accuracy": 0.6806,
      "llm_top_10_test_accuracy": 0.707,
      "llm_top_20_test_accuracy": 0.772,
      "llm_top_50_test_accuracy": 0.8310000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
      "llm_test_accuracy": 0.926,
      "llm_top_1_test_accuracy": 0.628,
      "llm_top_2_test_accuracy": 0.686,
      "llm_top_5_test_accuracy": 0.738,
      "llm_top_10_test_accuracy": 0.767,
      "llm_top_20_test_accuracy": 0.8,
      "llm_top_50_test_accuracy": 0.8545,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "codeparrot/github-code_results",
      "llm_test_accuracy": 0.9686,
      "llm_top_1_test_accuracy": 0.6642,
      "llm_top_2_test_accuracy": 0.7012,
      "llm_top_5_test_accuracy": 0.7834,
      "llm_top_10_test_accuracy": 0.8342,
      "llm_top_20_test_accuracy": 0.8932,
      "llm_top_50_test_accuracy": 0.9304,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "fancyzhx/ag_news_results",
      "llm_test_accuracy": 0.9377500000000001,
      "llm_top_1_test_accuracy": 0.733,
      "llm_top_2_test_accuracy": 0.7685,
      "llm_top_5_test_accuracy": 0.79925,
      "llm_top_10_test_accuracy": 0.8454999999999999,
      "llm_top_20_test_accuracy": 0.887,
      "llm_top_50_test_accuracy": 0.9125000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "Helsinki-NLP/europarl_results",
      "llm_test_accuracy": 0.9994,
      "llm_top_1_test_accuracy": 0.8096,
      "llm_top_2_test_accuracy": 0.8954000000000001,
      "llm_top_5_test_accuracy": 0.9411999999999999,
      "llm_top_10_test_accuracy": 0.9812000000000001,
      "llm_top_20_test_accuracy": 0.9932000000000001,
      "llm_top_50_test_accuracy": 0.9984,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  ],
  "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
  "sae_lens_id": "custom_sae",
  "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
  "sae_lens_version": "5.3.0",
  "sae_cfg_dict": {
    "model_name": "google/gemma-2-2b",
    "d_in": 2304,
    "d_sae": 2304,
    "hook_layer": 5,
    "hook_name": "blocks.5.hook_resid_post",
    "context_size": null,
    "hook_head_index": null,
    "architecture": "Custom",
    "apply_b_dec_to_input": true,
    "finetuning_scaling_factor": null,
    "activation_fn_str": "relu",
    "prepend_bos": true,
    "normalize_activations": "l2",
    "dtype": "bfloat16",
    "device": "",
    "dataset_path": "",
    "dataset_trust_remote_code": true,
    "seqpos_slice": [
      null
    ],
    "training_tokens": -100000,
    "sae_lens_training_version": null,
    "neuronpedia_id": null
  },
  "eval_result_unstructured": null,
  "unique_id": "google/gemma-2-2b_layer_12_sae_custom_sae",
  "sae_set": "google/gemma-2-2b_layer_12_sae",
  "sae_id": "custom_sae",
  "eval_cfg": {
    "model_name": "google/gemma-2-2b",
    "llm_dtype": "bfloat16",
    "batch_size_prompts": 16,
    "n_eval_reconstruction_batches": 200,
    "n_eval_sparsity_variance_batches": 2000,
    "dataset": "Skylion007/openwebtext",
    "context_size": 128,
    "compute_kl": true,
    "compute_ce_loss": true,
    "compute_l2_norms": true,
    "compute_sparsity_metrics": true,
    "compute_variance_metrics": true,
    "compute_featurewise_density_statistics": false,
    "compute_featurewise_weight_based_metrics": false,
    "exclude_special_tokens_from_reconstruction": true,
    "verbose": false
  },
  "metrics": {
    "model_behavior_preservation": {
      "kl_div_score": NaN,
      "kl_div_with_ablation": 10.0625,
      "kl_div_with_sae": NaN
    },
    "model_performance_preservation": {
      "ce_loss_score": NaN,
      "ce_loss_with_ablation": 12.4375,
      "ce_loss_with_sae": NaN,
      "ce_loss_without_sae": 2.9375
    },
    "reconstruction_quality": {
      "explained_variance": NaN,
      "mse": NaN,
      "cossim": NaN
    },
    "shrinkage": {
      "l2_norm_in": 149.0,
      "l2_norm_out": NaN,
      "l2_ratio": NaN,
      "relative_reconstruction_bias": NaN
    },
    "sparsity": {
      "l0": 0.0,
      "l1": 0.0
    },
    "token_stats": {
      "total_tokens_eval_reconstruction": 409600,
      "total_tokens_eval_sparsity_variance": 4096000
    }
  }
}