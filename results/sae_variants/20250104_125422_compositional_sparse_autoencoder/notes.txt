# Title: Compositional Sparse Autoencoders: Learning Interpretable Feature Combinations in Language Models
# Experiment description: 1. Architecture modifications:
   - Two-stream architecture: feature extractor (d_f=256) and composition network (d_c=128)
   - Multi-head attention (8 heads) for feature composition
   - Residual connections with gating mechanism
   - Bottleneck layer enforcing sparsity (k=32)
2. Training procedure:
   - End-to-end training with graduated sparsity
   - Batch size 512, learning rate 3e-4
   - Composition complexity curriculum (2->4 feature combinations)
3. Evaluation framework:
   - Synthetic tests: arithmetic, boolean logic, simple grammar
   - Feature interaction analysis via attention patterns
   - Human evaluation (n=50) of feature interpretability
   - Comparison with PCA, NMF, and standard sparse AE
4. Analysis:
   - Ablation studies on architecture components
   - Composition pattern visualization
   - Feature reuse statistics
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
