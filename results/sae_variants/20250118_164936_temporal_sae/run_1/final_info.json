{
  "eval_type_id": "sparse_probing",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "LabHC/bias_in_bios_class_set2",
      "LabHC/bias_in_bios_class_set3",
      "canrager/amazon_reviews_mcauley_1and5",
      "canrager/amazon_reviews_mcauley_1and5_sentiment",
      "codeparrot/github-code",
      "fancyzhx/ag_news",
      "Helsinki-NLP/europarl"
    ],
    "probe_train_set_size": 4000,
    "probe_test_set_size": 1000,
    "context_length": 128,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "model_name": "google/gemma-2-2b",
    "k_values": [
      1,
      2,
      5,
      10,
      20,
      50
    ],
    "lower_vram_usage": false
  },
  "eval_id": "977e4b6e-043d-40bd-821a-670d595d8ac2",
  "datetime_epoch_millis": 1737238012310,
  "eval_result_metrics": {
    "llm": {
      "llm_test_accuracy": 0.9499875,
      "llm_top_1_test_accuracy": 0.6724687500000001,
      "llm_top_2_test_accuracy": 0.71830625,
      "llm_top_5_test_accuracy": 0.7809249999999999,
      "llm_top_10_test_accuracy": 0.828575,
      "llm_top_20_test_accuracy": 0.8789624999999999,
      "llm_top_50_test_accuracy": 0.9207875,
      "llm_top_100_test_accuracy": null
    },
    "sae": {
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_results",
      "llm_test_accuracy": 0.9596,
      "llm_top_1_test_accuracy": 0.6432,
      "llm_top_2_test_accuracy": 0.6916,
      "llm_top_5_test_accuracy": 0.7912,
      "llm_top_10_test_accuracy": 0.8336,
      "llm_top_20_test_accuracy": 0.8962,
      "llm_top_50_test_accuracy": 0.9384,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set2_results",
      "llm_test_accuracy": 0.9428000000000001,
      "llm_top_1_test_accuracy": 0.6764,
      "llm_top_2_test_accuracy": 0.7276,
      "llm_top_5_test_accuracy": 0.762,
      "llm_top_10_test_accuracy": 0.8013999999999999,
      "llm_top_20_test_accuracy": 0.866,
      "llm_top_50_test_accuracy": 0.9018,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set3_results",
      "llm_test_accuracy": 0.9128000000000001,
      "llm_top_1_test_accuracy": 0.6928,
      "llm_top_2_test_accuracy": 0.7374,
      "llm_top_5_test_accuracy": 0.7682,
      "llm_top_10_test_accuracy": 0.8023999999999999,
      "llm_top_20_test_accuracy": 0.8472000000000002,
      "llm_top_50_test_accuracy": 0.8906000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
      "llm_test_accuracy": 0.8989999999999998,
      "llm_top_1_test_accuracy": 0.6092,
      "llm_top_2_test_accuracy": 0.6437999999999999,
      "llm_top_5_test_accuracy": 0.6728000000000001,
      "llm_top_10_test_accuracy": 0.7424000000000001,
      "llm_top_20_test_accuracy": 0.8182,
      "llm_top_50_test_accuracy": 0.8583999999999999,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
      "llm_test_accuracy": 0.981,
      "llm_top_1_test_accuracy": 0.671,
      "llm_top_2_test_accuracy": 0.724,
      "llm_top_5_test_accuracy": 0.766,
      "llm_top_10_test_accuracy": 0.826,
      "llm_top_20_test_accuracy": 0.847,
      "llm_top_50_test_accuracy": 0.933,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "codeparrot/github-code_results",
      "llm_test_accuracy": 0.967,
      "llm_top_1_test_accuracy": 0.6616,
      "llm_top_2_test_accuracy": 0.6966000000000001,
      "llm_top_5_test_accuracy": 0.7576,
      "llm_top_10_test_accuracy": 0.7984,
      "llm_top_20_test_accuracy": 0.8707999999999998,
      "llm_top_50_test_accuracy": 0.9258000000000001,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "fancyzhx/ag_news_results",
      "llm_test_accuracy": 0.9385,
      "llm_top_1_test_accuracy": 0.69275,
      "llm_top_2_test_accuracy": 0.74225,
      "llm_top_5_test_accuracy": 0.8160000000000001,
      "llm_top_10_test_accuracy": 0.8640000000000001,
      "llm_top_20_test_accuracy": 0.8965,
      "llm_top_50_test_accuracy": 0.9205,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "Helsinki-NLP/europarl_results",
      "llm_test_accuracy": 0.9992000000000001,
      "llm_top_1_test_accuracy": 0.7328,
      "llm_top_2_test_accuracy": 0.7832,
      "llm_top_5_test_accuracy": 0.9136,
      "llm_top_10_test_accuracy": 0.9603999999999999,
      "llm_top_20_test_accuracy": 0.9898,
      "llm_top_50_test_accuracy": 0.9978,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  ],
  "sae_bench_commit_hash": "bcb003afd6045deaee4be8dd883ae42863da9163",
  "sae_lens_id": "custom_sae",
  "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
  "sae_lens_version": "5.3.0",
  "sae_cfg_dict": {
    "model_name": "google/gemma-2-2b",
    "d_in": 2304,
    "d_sae": 2304,
    "hook_layer": 12,
    "hook_name": "blocks.12.hook_resid_post",
    "context_size": null,
    "hook_head_index": null,
    "architecture": "Custom",
    "apply_b_dec_to_input": true,
    "finetuning_scaling_factor": null,
    "activation_fn_str": "relu",
    "prepend_bos": true,
    "normalize_activations": "none",
    "dtype": "bfloat16",
    "device": "",
    "dataset_path": "",
    "dataset_trust_remote_code": true,
    "seqpos_slice": [
      null
    ],
    "training_tokens": -100000,
    "sae_lens_training_version": null,
    "neuronpedia_id": null
  },
  "eval_result_unstructured": null
}