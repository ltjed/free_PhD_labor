# Title: Temporal Sparse Autoencoders: Learning Interpretable Feature Dynamics in Language Models
# Experiment description: 1. Architecture implementation:
   - LSTM-based encoder/decoder with sparse gates
   - Sliding window feature memory (window size w=16)
   - Feature lifecycle tracking module
   - Static-temporal bridge connections
2. Training procedure:
   - Initialize from pretrained static autoencoder
   - Three-phase training:
     a) Static feature distillation
     b) Temporal feature introduction
     c) End-to-end refinement
   - Progressive sparsity annealing
3. Evaluation methods:
   - Temporal stability score (TSS)
   - Feature lifetime analysis
   - Causal intervention tests
   - Temporal probing tasks:
     * Subject-verb agreement
     * Coreference resolution
     * Context-dependent disambiguation
4. Analysis:
   - Feature evolution visualization
   - Transition pattern mining
   - Ablation studies on window size
   - Comparison with baseline methods
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
