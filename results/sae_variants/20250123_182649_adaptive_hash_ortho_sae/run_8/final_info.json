{
  "training_steps": 0,
  "final_loss": null,
  "layer": 19,
  "dict_size": 2304,
  "learning_rate": 0.0003,
  "sparsity_penalty": 0.04,
  "unique_id": "google/gemma-2-2b_layer_5_sae_custom_sae",
  "sae_set": "google/gemma-2-2b_layer_5_sae",
  "sae_id": "custom_sae",
  "eval_cfg": {
    "model_name": "google/gemma-2-2b",
    "llm_dtype": "bfloat16",
    "batch_size_prompts": 16,
    "n_eval_reconstruction_batches": 200,
    "n_eval_sparsity_variance_batches": 2000,
    "dataset": "Skylion007/openwebtext",
    "context_size": 128,
    "compute_kl": true,
    "compute_ce_loss": true,
    "compute_l2_norms": true,
    "compute_sparsity_metrics": true,
    "compute_variance_metrics": true,
    "compute_featurewise_density_statistics": false,
    "compute_featurewise_weight_based_metrics": false,
    "exclude_special_tokens_from_reconstruction": true,
    "verbose": false
  },
  "metrics": {
    "model_behavior_preservation": {
      "kl_div_score": 0.08695652173913043,
      "kl_div_with_ablation": 10.0625,
      "kl_div_with_sae": 9.1875
    },
    "model_performance_preservation": {
      "ce_loss_score": 0.05921052631578947,
      "ce_loss_with_ablation": 12.4375,
      "ce_loss_with_sae": 11.875,
      "ce_loss_without_sae": 2.9375
    },
    "reconstruction_quality": {
      "explained_variance": -0.498046875,
      "mse": 4.0625,
      "cossim": 0.008056640625
    },
    "shrinkage": {
      "l2_norm_in": 90.0,
      "l2_norm_out": 0.671875,
      "l2_ratio": 0.007659912109375,
      "relative_reconstruction_bias": 1.015625
    },
    "sparsity": {
      "l0": 2304.0,
      "l1": 23.0
    },
    "token_stats": {
      "total_tokens_eval_reconstruction": 409600,
      "total_tokens_eval_sparsity_variance": 4096000
    }
  }
}