{
  "training results": {
    "training_log": {
      "step 0": {
        "loss": 280.3907165527344,
        "l1_loss": 2692.192626953125,
        "l2_loss": 172.6608123779297,
        "ortho_loss": 0.4221651554107666
      },
      "step 1": {
        "loss": 243.6199188232422,
        "l1_loss": 2632.935546875,
        "l2_loss": 138.26095581054688,
        "ortho_loss": 0.4154910743236542
      },
      "step 2": {
        "loss": 249.9102020263672,
        "l1_loss": 2702.074462890625,
        "l2_loss": 141.78500366210938,
        "ortho_loss": 0.4222780466079712
      },
      "step 3": {
        "loss": 249.84877014160156,
        "l1_loss": 2702.375244140625,
        "l2_loss": 141.71153259277344,
        "ortho_loss": 0.42221471667289734
      },
      "step 4": {
        "loss": 252.69287109375,
        "l1_loss": 2733.424560546875,
        "l2_loss": 143.3136749267578,
        "ortho_loss": 0.42208123207092285
      },
      "step 5": {
        "loss": 244.94029235839844,
        "l1_loss": 2648.0673828125,
        "l2_loss": 138.97555541992188,
        "ortho_loss": 0.42035529017448425
      },
      "step 6": {
        "loss": 247.5320281982422,
        "l1_loss": 2678.136962890625,
        "l2_loss": 140.36439514160156,
        "ortho_loss": 0.42159366607666016
      },
      "step 7": {
        "loss": 250.3660888671875,
        "l1_loss": 2709.99560546875,
        "l2_loss": 141.92401123046875,
        "ortho_loss": 0.4226093590259552
      },
      "step 8": {
        "loss": 247.73471069335938,
        "l1_loss": 2682.0224609375,
        "l2_loss": 140.41152954101562,
        "ortho_loss": 0.4228806793689728
      },
      "step 9": {
        "loss": 246.87814331054688,
        "l1_loss": 2673.4326171875,
        "l2_loss": 139.89877319335938,
        "ortho_loss": 0.4206482172012329
      },
      "step 10": {
        "loss": 245.63624572753906,
        "l1_loss": 2660.15087890625,
        "l2_loss": 139.1881866455078,
        "ortho_loss": 0.4201667010784149
      },
      "step 11": {
        "loss": 250.17922973632812,
        "l1_loss": 2711.39306640625,
        "l2_loss": 141.68130493164062,
        "ortho_loss": 0.4221228063106537
      },
      "step 12": {
        "loss": 247.02552795410156,
        "l1_loss": 2678.1611328125,
        "l2_loss": 139.85678100585938,
        "ortho_loss": 0.4230659604072571
      },
      "step 13": {
        "loss": 242.2106170654297,
        "l1_loss": 2626.711669921875,
        "l2_loss": 137.10018920898438,
        "ortho_loss": 0.4195951819419861
      },
      "step 14": {
        "loss": 249.42825317382812,
        "l1_loss": 2708.19580078125,
        "l2_loss": 141.0580596923828,
        "ortho_loss": 0.4235248565673828
      },
      "step 15": {
        "loss": 240.16995239257812,
        "l1_loss": 2606.272705078125,
        "l2_loss": 135.87765502929688,
        "ortho_loss": 0.41380584239959717
      },
      "step 16": {
        "loss": 247.0205841064453,
        "l1_loss": 2684.57568359375,
        "l2_loss": 139.59527587890625,
        "ortho_loss": 0.4227837324142456
      },
      "step 17": {
        "loss": 246.485107421875,
        "l1_loss": 2679.7392578125,
        "l2_loss": 139.25314331054688,
        "ortho_loss": 0.4239138960838318
      },
      "step 18": {
        "loss": 252.9932403564453,
        "l1_loss": 2753.6337890625,
        "l2_loss": 142.80564880371094,
        "ortho_loss": 0.42244407534599304
      },
      "step 19": {
        "loss": 237.98654174804688,
        "l1_loss": 2588.939453125,
        "l2_loss": 134.38735961914062,
        "ortho_loss": 0.41599950194358826
      },
      "step 20": {
        "loss": 243.49710083007812,
        "l1_loss": 2651.2138671875,
        "l2_loss": 137.4064178466797,
        "ortho_loss": 0.4212970733642578
      },
      "step 21": {
        "loss": 238.7278594970703,
        "l1_loss": 2601.4892578125,
        "l2_loss": 134.62606811523438,
        "ortho_loss": 0.4222373962402344
      },
      "step 22": {
        "loss": 244.7586669921875,
        "l1_loss": 2670.92236328125,
        "l2_loss": 137.87945556640625,
        "ortho_loss": 0.4232076108455658
      },
      "step 23": {
        "loss": 240.25845336914062,
        "l1_loss": 2623.8896484375,
        "l2_loss": 135.26055908203125,
        "ortho_loss": 0.4231085777282715
      },
      "step 24": {
        "loss": 238.20726013183594,
        "l1_loss": 2600.95654296875,
        "l2_loss": 134.12692260742188,
        "ortho_loss": 0.4207592010498047
      },
      "step 25": {
        "loss": 236.8837890625,
        "l1_loss": 2589.65869140625,
        "l2_loss": 133.25531005859375,
        "ortho_loss": 0.4214593172073364
      },
      "step 26": {
        "loss": 232.59762573242188,
        "l1_loss": 2542.3388671875,
        "l2_loss": 130.8629913330078,
        "ortho_loss": 0.4108352065086365
      },
      "step 27": {
        "loss": 237.78897094726562,
        "l1_loss": 2601.419921875,
        "l2_loss": 133.68984985351562,
        "ortho_loss": 0.4233032166957855
      },
      "step 28": {
        "loss": 237.5467071533203,
        "l1_loss": 2602.43994140625,
        "l2_loss": 133.4066162109375,
        "ortho_loss": 0.42495933175086975
      },
      "step 29": {
        "loss": 242.72506713867188,
        "l1_loss": 2662.43212890625,
        "l2_loss": 136.1852569580078,
        "ortho_loss": 0.42528030276298523
      },
      "step 30": {
        "loss": 245.05023193359375,
        "l1_loss": 2690.788818359375,
        "l2_loss": 137.3761444091797,
        "ortho_loss": 0.4253597855567932
      },
      "step 31": {
        "loss": 240.69540405273438,
        "l1_loss": 2642.8017578125,
        "l2_loss": 134.94058227539062,
        "ortho_loss": 0.4275137782096863
      },
      "step 32": {
        "loss": 230.78933715820312,
        "l1_loss": 2533.22412109375,
        "l2_loss": 129.4184112548828,
        "ortho_loss": 0.41963088512420654
      },
      "step 33": {
        "loss": 237.1259307861328,
        "l1_loss": 2607.98388671875,
        "l2_loss": 132.76385498046875,
        "ortho_loss": 0.42721423506736755
      },
      "step 34": {
        "loss": 244.72402954101562,
        "l1_loss": 2695.02978515625,
        "l2_loss": 136.8802490234375,
        "ortho_loss": 0.4260730445384979
      },
      "step 35": {
        "loss": 239.04908752441406,
        "l1_loss": 2633.6142578125,
        "l2_loss": 133.6617431640625,
        "ortho_loss": 0.42767244577407837
      },
      "step 36": {
        "loss": 234.52601623535156,
        "l1_loss": 2585.570556640625,
        "l2_loss": 131.06051635742188,
        "ortho_loss": 0.4267939627170563
      },
      "step 37": {
        "loss": 238.423828125,
        "l1_loss": 2630.92578125,
        "l2_loss": 133.14395141601562,
        "ortho_loss": 0.4284401834011078
      },
      "step 38": {
        "loss": 235.66921997070312,
        "l1_loss": 2602.45556640625,
        "l2_loss": 131.52813720703125,
        "ortho_loss": 0.428545206785202
      },
      "step 39": {
        "loss": 228.67074584960938,
        "l1_loss": 2522.71484375,
        "l2_loss": 127.7196044921875,
        "ortho_loss": 0.42534565925598145
      },
      "step 40": {
        "loss": 233.14251708984375,
        "l1_loss": 2578.728515625,
        "l2_loss": 129.950439453125,
        "ortho_loss": 0.42943525314331055
      },
      "step 41": {
        "loss": 238.63540649414062,
        "l1_loss": 2643.2177734375,
        "l2_loss": 132.86395263671875,
        "ortho_loss": 0.42751073837280273
      },
      "step 42": {
        "loss": 234.5117645263672,
        "l1_loss": 2595.01513671875,
        "l2_loss": 130.6681671142578,
        "ortho_loss": 0.4298849403858185
      },
      "step 43": {
        "loss": 232.88479614257812,
        "l1_loss": 2577.80712890625,
        "l2_loss": 129.7294464111328,
        "ortho_loss": 0.43067923188209534
      },
      "step 44": {
        "loss": 228.06239318847656,
        "l1_loss": 2527.269287109375,
        "l2_loss": 126.92855072021484,
        "ortho_loss": 0.4308200478553772
      },
      "step 45": {
        "loss": 228.6568603515625,
        "l1_loss": 2532.59130859375,
        "l2_loss": 127.31005859375,
        "ortho_loss": 0.4314955770969391
      },
      "step 46": {
        "loss": 223.45240783691406,
        "l1_loss": 2473.06494140625,
        "l2_loss": 124.48709869384766,
        "ortho_loss": 0.42716217041015625
      },
      "step 47": {
        "loss": 226.3173065185547,
        "l1_loss": 2510.07470703125,
        "l2_loss": 125.87115478515625,
        "ortho_loss": 0.43173667788505554
      },
      "step 48": {
        "loss": 232.82266235351562,
        "l1_loss": 2586.45947265625,
        "l2_loss": 129.32110595703125,
        "ortho_loss": 0.43188098073005676
      },
      "step 49": {
        "loss": 223.4110565185547,
        "l1_loss": 2478.955078125,
        "l2_loss": 124.209716796875,
        "ortho_loss": 0.4313502907752991
      },
      "step 50": {
        "loss": 221.4095916748047,
        "l1_loss": 2453.626708984375,
        "l2_loss": 123.22134399414062,
        "ortho_loss": 0.43186625838279724
      },
      "step 51": {
        "loss": 220.2021026611328,
        "l1_loss": 2441.0029296875,
        "l2_loss": 122.51902770996094,
        "ortho_loss": 0.429572731256485
      },
      "step 52": {
        "loss": 221.00685119628906,
        "l1_loss": 2450.572021484375,
        "l2_loss": 122.94052124023438,
        "ortho_loss": 0.4344063997268677
      },
      "step 53": {
        "loss": 222.65609741210938,
        "l1_loss": 2471.0166015625,
        "l2_loss": 123.77202606201172,
        "ortho_loss": 0.43418169021606445
      },
      "step 54": {
        "loss": 225.6358184814453,
        "l1_loss": 2506.21875,
        "l2_loss": 125.3435287475586,
        "ortho_loss": 0.435351699590683
      },
      "step 55": {
        "loss": 216.721923828125,
        "l1_loss": 2401.363525390625,
        "l2_loss": 120.62411499023438,
        "ortho_loss": 0.43275153636932373
      },
      "step 56": {
        "loss": 225.687744140625,
        "l1_loss": 2505.92431640625,
        "l2_loss": 125.40718078613281,
        "ortho_loss": 0.43598103523254395
      },
      "step 57": {
        "loss": 220.2184600830078,
        "l1_loss": 2443.62890625,
        "l2_loss": 122.42967224121094,
        "ortho_loss": 0.43628817796707153
      },
      "step 58": {
        "loss": 218.7888641357422,
        "l1_loss": 2424.345947265625,
        "l2_loss": 121.77131652832031,
        "ortho_loss": 0.4371398985385895
      },
      "step 59": {
        "loss": 211.33111572265625,
        "l1_loss": 2338.6591796875,
        "l2_loss": 117.74163818359375,
        "ortho_loss": 0.4311926066875458
      },
      "step 60": {
        "loss": 217.20950317382812,
        "l1_loss": 2408.24853515625,
        "l2_loss": 120.83570861816406,
        "ortho_loss": 0.438566654920578
      },
      "step 61": {
        "loss": 212.93753051757812,
        "l1_loss": 2355.712158203125,
        "l2_loss": 118.66553497314453,
        "ortho_loss": 0.43516746163368225
      },
      "step 62": {
        "loss": 211.67959594726562,
        "l1_loss": 2341.85302734375,
        "l2_loss": 117.9616928100586,
        "ortho_loss": 0.43799710273742676
      },
      "step 63": {
        "loss": 217.55709838867188,
        "l1_loss": 2410.140625,
        "l2_loss": 121.10752868652344,
        "ortho_loss": 0.4394141137599945
      },
      "step 64": {
        "loss": 210.29237365722656,
        "l1_loss": 2322.2109375,
        "l2_loss": 117.36024475097656,
        "ortho_loss": 0.4369109570980072
      },
      "step 65": {
        "loss": 208.74099731445312,
        "l1_loss": 2306.698974609375,
        "l2_loss": 116.42927551269531,
        "ortho_loss": 0.4375721514225006
      },
      "step 66": {
        "loss": 207.96051025390625,
        "l1_loss": 2294.7001953125,
        "l2_loss": 116.12871551513672,
        "ortho_loss": 0.43788549304008484
      },
      "step 67": {
        "loss": 213.31744384765625,
        "l1_loss": 2358.114990234375,
        "l2_loss": 118.9486312866211,
        "ortho_loss": 0.44213002920150757
      },
      "step 68": {
        "loss": 214.48052978515625,
        "l1_loss": 2373.481201171875,
        "l2_loss": 119.49710845947266,
        "ortho_loss": 0.44168365001678467
      },
      "step 69": {
        "loss": 211.53451538085938,
        "l1_loss": 2336.442626953125,
        "l2_loss": 118.032470703125,
        "ortho_loss": 0.44343239068984985
      },
      "step 70": {
        "loss": 208.36288452148438,
        "l1_loss": 2294.0009765625,
        "l2_loss": 116.55853271484375,
        "ortho_loss": 0.4431510269641876
      },
      "step 71": {
        "loss": 216.0298614501953,
        "l1_loss": 2387.291015625,
        "l2_loss": 120.49396514892578,
        "ortho_loss": 0.44258540868759155
      },
      "step 72": {
        "loss": 214.51504516601562,
        "l1_loss": 2364.64306640625,
        "l2_loss": 119.88500213623047,
        "ortho_loss": 0.44311827421188354
      },
      "step 73": {
        "loss": 202.94866943359375,
        "l1_loss": 2224.54833984375,
        "l2_loss": 113.92268371582031,
        "ortho_loss": 0.44048628211021423
      },
      "step 74": {
        "loss": 207.8404998779297,
        "l1_loss": 2283.55712890625,
        "l2_loss": 116.45368194580078,
        "ortho_loss": 0.4454282522201538
      },
      "step 75": {
        "loss": 203.6467742919922,
        "l1_loss": 2231.40283203125,
        "l2_loss": 114.34602355957031,
        "ortho_loss": 0.446276992559433
      },
      "step 76": {
        "loss": 206.2648162841797,
        "l1_loss": 2259.520263671875,
        "l2_loss": 115.83927917480469,
        "ortho_loss": 0.4472665786743164
      },
      "step 77": {
        "loss": 213.1735076904297,
        "l1_loss": 2346.58349609375,
        "l2_loss": 119.26580810546875,
        "ortho_loss": 0.44360506534576416
      },
      "step 78": {
        "loss": 200.3435516357422,
        "l1_loss": 2182.4296875,
        "l2_loss": 113.00180053710938,
        "ortho_loss": 0.445711225271225
      },
      "step 79": {
        "loss": 202.51380920410156,
        "l1_loss": 2207.84033203125,
        "l2_loss": 114.15543365478516,
        "ortho_loss": 0.4475923478603363
      },
      "step 80": {
        "loss": 198.74082946777344,
        "l1_loss": 2157.398193359375,
        "l2_loss": 112.40021514892578,
        "ortho_loss": 0.4468638002872467
      },
      "step 81": {
        "loss": 207.142822265625,
        "l1_loss": 2261.6474609375,
        "l2_loss": 116.63201141357422,
        "ortho_loss": 0.4492571949958801
      },
      "step 82": {
        "loss": 206.31239318847656,
        "l1_loss": 2250.591796875,
        "l2_loss": 116.24374389648438,
        "ortho_loss": 0.44978266954421997
      },
      "step 83": {
        "loss": 200.1201934814453,
        "l1_loss": 2170.81640625,
        "l2_loss": 113.242431640625,
        "ortho_loss": 0.4510636627674103
      },
      "step 84": {
        "loss": 199.4132843017578,
        "l1_loss": 2161.56005859375,
        "l2_loss": 112.90570068359375,
        "ortho_loss": 0.4518556594848633
      },
      "step 85": {
        "loss": 198.1529083251953,
        "l1_loss": 2138.157470703125,
        "l2_loss": 112.58147430419922,
        "ortho_loss": 0.4513212740421295
      },
      "step 86": {
        "loss": 206.8594207763672,
        "l1_loss": 2246.79833984375,
        "l2_loss": 116.94253540039062,
        "ortho_loss": 0.4495036005973816
      },
      "step 87": {
        "loss": 201.56228637695312,
        "l1_loss": 2180.5947265625,
        "l2_loss": 114.2931137084961,
        "ortho_loss": 0.45382216572761536
      },
      "step 88": {
        "loss": 193.24346923828125,
        "l1_loss": 2072.8583984375,
        "l2_loss": 110.28399658203125,
        "ortho_loss": 0.45130425691604614
      },
      "step 89": {
        "loss": 198.76036071777344,
        "l1_loss": 2143.705322265625,
        "l2_loss": 112.96676635742188,
        "ortho_loss": 0.45378825068473816
      },
      "step 90": {
        "loss": 196.6474609375,
        "l1_loss": 2109.17529296875,
        "l2_loss": 112.23486328125,
        "ortho_loss": 0.4559539258480072
      },
      "step 91": {
        "loss": 200.9404754638672,
        "l1_loss": 2162.9130859375,
        "l2_loss": 114.37852478027344,
        "ortho_loss": 0.45428380370140076
      },
      "step 92": {
        "loss": 188.01036071777344,
        "l1_loss": 1994.865478515625,
        "l2_loss": 108.17147827148438,
        "ortho_loss": 0.44265174865722656
      },
      "step 93": {
        "loss": 201.3666534423828,
        "l1_loss": 2163.162109375,
        "l2_loss": 114.79484558105469,
        "ortho_loss": 0.45337870717048645
      },
      "step 94": {
        "loss": 196.972412109375,
        "l1_loss": 2106.2841796875,
        "l2_loss": 112.67536926269531,
        "ortho_loss": 0.45687177777290344
      },
      "step 95": {
        "loss": 193.13563537597656,
        "l1_loss": 2054.727783203125,
        "l2_loss": 110.9004898071289,
        "ortho_loss": 0.4603647291660309
      },
      "step 96": {
        "loss": 196.23004150390625,
        "l1_loss": 2092.13427734375,
        "l2_loss": 112.49884796142578,
        "ortho_loss": 0.458266943693161
      },
      "step 97": {
        "loss": 190.82644653320312,
        "l1_loss": 2015.5198974609375,
        "l2_loss": 110.15945434570312,
        "ortho_loss": 0.4620974063873291
      },
      "step 98": {
        "loss": 195.06494140625,
        "l1_loss": 2069.845703125,
        "l2_loss": 112.22520446777344,
        "ortho_loss": 0.45908769965171814
      },
      "step 99": {
        "loss": 195.7116241455078,
        "l1_loss": 2081.075439453125,
        "l2_loss": 112.4228515625,
        "ortho_loss": 0.457598477602005
      },
      "step 100": {
        "loss": 188.42666625976562,
        "l1_loss": 1980.8330078125,
        "l2_loss": 109.14692687988281,
        "ortho_loss": 0.46416985988616943
      },
      "step 101": {
        "loss": 185.25735473632812,
        "l1_loss": 1938.6353759765625,
        "l2_loss": 107.66546630859375,
        "ortho_loss": 0.46482911705970764
      },
      "step 102": {
        "loss": 191.52639770507812,
        "l1_loss": 2017.140625,
        "l2_loss": 110.7945556640625,
        "ortho_loss": 0.4622259736061096
      },
      "step 103": {
        "loss": 193.432861328125,
        "l1_loss": 2037.5203857421875,
        "l2_loss": 111.88606262207031,
        "ortho_loss": 0.45993709564208984
      },
      "step 104": {
        "loss": 185.4490509033203,
        "l1_loss": 1929.304931640625,
        "l2_loss": 108.23009490966797,
        "ortho_loss": 0.46775567531585693
      },
      "step 105": {
        "loss": 192.6478729248047,
        "l1_loss": 2019.9097900390625,
        "l2_loss": 111.80533599853516,
        "ortho_loss": 0.4615952968597412
      },
      "step 106": {
        "loss": 191.41525268554688,
        "l1_loss": 2001.9124755859375,
        "l2_loss": 111.2926025390625,
        "ortho_loss": 0.4614054262638092
      },
      "step 107": {
        "loss": 182.83421325683594,
        "l1_loss": 1886.634033203125,
        "l2_loss": 107.32176971435547,
        "ortho_loss": 0.4707171618938446
      },
      "step 108": {
        "loss": 190.3443145751953,
        "l1_loss": 1983.438232421875,
        "l2_loss": 110.96053314208984,
        "ortho_loss": 0.46245065331459045
      },
      "step 109": {
        "loss": 181.08213806152344,
        "l1_loss": 1859.3514404296875,
        "l2_loss": 106.66081237792969,
        "ortho_loss": 0.4727095663547516
      },
      "step 110": {
        "loss": 193.86300659179688,
        "l1_loss": 2016.13037109375,
        "l2_loss": 113.17186737060547,
        "ortho_loss": 0.4593426287174225
      },
      "step 111": {
        "loss": 187.02310180664062,
        "l1_loss": 1930.8916015625,
        "l2_loss": 109.74075317382812,
        "ortho_loss": 0.46695956587791443
      },
      "step 112": {
        "loss": 185.13861083984375,
        "l1_loss": 1904.1142578125,
        "l2_loss": 108.92720031738281,
        "ortho_loss": 0.4684600830078125
      },
      "step 113": {
        "loss": 181.78366088867188,
        "l1_loss": 1851.32275390625,
        "l2_loss": 107.68331146240234,
        "ortho_loss": 0.4744504392147064
      },
      "step 114": {
        "loss": 177.6208953857422,
        "l1_loss": 1795.5615234375,
        "l2_loss": 105.75077819824219,
        "ortho_loss": 0.4765879809856415
      },
      "step 115": {
        "loss": 175.65676879882812,
        "l1_loss": 1763.841064453125,
        "l2_loss": 105.05558776855469,
        "ortho_loss": 0.4753892421722412
      },
      "step 116": {
        "loss": 180.9003448486328,
        "l1_loss": 1833.73046875,
        "l2_loss": 107.50373840332031,
        "ortho_loss": 0.4737730622291565
      },
      "step 117": {
        "loss": 183.50048828125,
        "l1_loss": 1862.542724609375,
        "l2_loss": 108.95169067382812,
        "ortho_loss": 0.47094184160232544
      },
      "step 118": {
        "loss": 177.0426025390625,
        "l1_loss": 1771.2978515625,
        "l2_loss": 106.14266967773438,
        "ortho_loss": 0.4802455008029938
      },
      "step 119": {
        "loss": 179.6228790283203,
        "l1_loss": 1809.9156494140625,
        "l2_loss": 107.17880249023438,
        "ortho_loss": 0.47449609637260437
      },
      "step 120": {
        "loss": 182.36268615722656,
        "l1_loss": 1832.8026123046875,
        "l2_loss": 109.00321960449219,
        "ortho_loss": 0.4736902713775635
      },
      "step 121": {
        "loss": 174.30029296875,
        "l1_loss": 1729.7158203125,
        "l2_loss": 105.06356811523438,
        "ortho_loss": 0.480881005525589
      },
      "step 122": {
        "loss": 173.80575561523438,
        "l1_loss": 1720.751708984375,
        "l2_loss": 104.92745971679688,
        "ortho_loss": 0.482250839471817
      },
      "step 123": {
        "loss": 175.06239318847656,
        "l1_loss": 1737.162109375,
        "l2_loss": 105.52792358398438,
        "ortho_loss": 0.4798926115036011
      },
      "step 124": {
        "loss": 180.13662719726562,
        "l1_loss": 1800.689697265625,
        "l2_loss": 108.06187438964844,
        "ortho_loss": 0.47165557742118835
      },
      "step 125": {
        "loss": 181.24835205078125,
        "l1_loss": 1812.016357421875,
        "l2_loss": 108.72067260742188,
        "ortho_loss": 0.4702911376953125
      },
      "step 126": {
        "loss": 171.81259155273438,
        "l1_loss": 1680.128173828125,
        "l2_loss": 104.55902099609375,
        "ortho_loss": 0.48444896936416626
      },
      "step 127": {
        "loss": 175.50067138671875,
        "l1_loss": 1728.030029296875,
        "l2_loss": 106.33169555664062,
        "ortho_loss": 0.47778549790382385
      },
      "step 128": {
        "loss": 172.7992401123047,
        "l1_loss": 1685.945068359375,
        "l2_loss": 105.31294250488281,
        "ortho_loss": 0.48501142859458923
      },
      "step 129": {
        "loss": 180.82015991210938,
        "l1_loss": 1792.0511474609375,
        "l2_loss": 109.09130096435547,
        "ortho_loss": 0.46819958090782166
      },
      "step 130": {
        "loss": 174.91482543945312,
        "l1_loss": 1709.705322265625,
        "l2_loss": 106.4788589477539,
        "ortho_loss": 0.4775950014591217
      },
      "step 131": {
        "loss": 177.32916259765625,
        "l1_loss": 1733.262451171875,
        "l2_loss": 107.95108032226562,
        "ortho_loss": 0.4758179187774658
      },
      "step 132": {
        "loss": 170.7712860107422,
        "l1_loss": 1643.08544921875,
        "l2_loss": 104.99920654296875,
        "ortho_loss": 0.4866045117378235
      },
      "step 133": {
        "loss": 167.48681640625,
        "l1_loss": 1598.799072265625,
        "l2_loss": 103.48570251464844,
        "ortho_loss": 0.49149832129478455
      },
      "step 134": {
        "loss": 162.4848175048828,
        "l1_loss": 1533.503173828125,
        "l2_loss": 101.09528350830078,
        "ortho_loss": 0.4940180778503418
      },
      "step 135": {
        "loss": 164.93641662597656,
        "l1_loss": 1560.591552734375,
        "l2_loss": 102.46327209472656,
        "ortho_loss": 0.4948282241821289
      },
      "step 136": {
        "loss": 168.66981506347656,
        "l1_loss": 1603.381103515625,
        "l2_loss": 104.48568725585938,
        "ortho_loss": 0.48871007561683655
      },
      "step 137": {
        "loss": 171.5791015625,
        "l1_loss": 1639.1695556640625,
        "l2_loss": 105.96420288085938,
        "ortho_loss": 0.481256365776062
      },
      "step 138": {
        "loss": 164.47068786621094,
        "l1_loss": 1542.132568359375,
        "l2_loss": 102.73600006103516,
        "ortho_loss": 0.4938614070415497
      },
      "step 139": {
        "loss": 167.7326202392578,
        "l1_loss": 1582.861083984375,
        "l2_loss": 104.36949157714844,
        "ortho_loss": 0.4869747757911682
      },
      "step 140": {
        "loss": 167.8235321044922,
        "l1_loss": 1577.324951171875,
        "l2_loss": 104.68171691894531,
        "ortho_loss": 0.48809534311294556
      },
      "step 141": {
        "loss": 168.3066864013672,
        "l1_loss": 1581.917724609375,
        "l2_loss": 104.98135375976562,
        "ortho_loss": 0.48630332946777344
      },
      "step 142": {
        "loss": 163.5264434814453,
        "l1_loss": 1513.62255859375,
        "l2_loss": 102.93203735351562,
        "ortho_loss": 0.4950243830680847
      },
      "step 143": {
        "loss": 165.34115600585938,
        "l1_loss": 1541.120849609375,
        "l2_loss": 103.64759063720703,
        "ortho_loss": 0.487399697303772
      },
      "step 144": {
        "loss": 166.80984497070312,
        "l1_loss": 1548.377685546875,
        "l2_loss": 104.82597351074219,
        "ortho_loss": 0.4876209795475006
      },
      "step 145": {
        "loss": 158.3641357421875,
        "l1_loss": 1436.931396484375,
        "l2_loss": 100.83651733398438,
        "ortho_loss": 0.503690779209137
      },
      "step 146": {
        "loss": 165.63388061523438,
        "l1_loss": 1529.2353515625,
        "l2_loss": 104.41557312011719,
        "ortho_loss": 0.4889376163482666
      },
      "step 147": {
        "loss": 159.3621368408203,
        "l1_loss": 1441.0888671875,
        "l2_loss": 101.66857147216797,
        "ortho_loss": 0.500085175037384
      },
      "step 148": {
        "loss": 158.9978790283203,
        "l1_loss": 1430.354248046875,
        "l2_loss": 101.73344421386719,
        "ortho_loss": 0.5026153326034546
      },
      "step 149": {
        "loss": 155.46981811523438,
        "l1_loss": 1385.027587890625,
        "l2_loss": 100.01789093017578,
        "ortho_loss": 0.5082018971443176
      },
      "step 150": {
        "loss": 161.47486877441406,
        "l1_loss": 1453.8104248046875,
        "l2_loss": 103.27291107177734,
        "ortho_loss": 0.4954952597618103
      },
      "step 151": {
        "loss": 162.2032012939453,
        "l1_loss": 1465.154541015625,
        "l2_loss": 103.54791259765625,
        "ortho_loss": 0.49106571078300476
      },
      "step 152": {
        "loss": 167.5631866455078,
        "l1_loss": 1536.91162109375,
        "l2_loss": 106.03866577148438,
        "ortho_loss": 0.48053795099258423
      },
      "step 153": {
        "loss": 152.55874633789062,
        "l1_loss": 1330.03759765625,
        "l2_loss": 99.30601501464844,
        "ortho_loss": 0.5122008323669434
      },
      "step 154": {
        "loss": 159.95423889160156,
        "l1_loss": 1429.3428955078125,
        "l2_loss": 102.73129272460938,
        "ortho_loss": 0.49247345328330994
      },
      "step 155": {
        "loss": 159.3050994873047,
        "l1_loss": 1414.010498046875,
        "l2_loss": 102.69520568847656,
        "ortho_loss": 0.4949166476726532
      },
      "step 156": {
        "loss": 160.46932983398438,
        "l1_loss": 1427.152099609375,
        "l2_loss": 103.33412170410156,
        "ortho_loss": 0.4912569522857666
      },
      "step 157": {
        "loss": 154.58343505859375,
        "l1_loss": 1342.961669921875,
        "l2_loss": 100.81464385986328,
        "ortho_loss": 0.5032727122306824
      },
      "step 158": {
        "loss": 165.1343994140625,
        "l1_loss": 1486.1842041015625,
        "l2_loss": 105.63917541503906,
        "ortho_loss": 0.4785445034503937
      },
      "step 159": {
        "loss": 157.90171813964844,
        "l1_loss": 1389.7222900390625,
        "l2_loss": 102.26365661621094,
        "ortho_loss": 0.49174895882606506
      },
      "step 160": {
        "loss": 158.1091766357422,
        "l1_loss": 1381.921142578125,
        "l2_loss": 102.783203125,
        "ortho_loss": 0.49137288331985474
      },
      "step 161": {
        "loss": 158.52406311035156,
        "l1_loss": 1382.13330078125,
        "l2_loss": 103.18955993652344,
        "ortho_loss": 0.4917408525943756
      },
      "step 162": {
        "loss": 153.12486267089844,
        "l1_loss": 1310.2376708984375,
        "l2_loss": 100.6650390625,
        "ortho_loss": 0.5032446980476379
      },
      "step 163": {
        "loss": 156.48831176757812,
        "l1_loss": 1344.6357421875,
        "l2_loss": 102.65341186523438,
        "ortho_loss": 0.4946146309375763
      },
      "step 164": {
        "loss": 148.7065887451172,
        "l1_loss": 1242.98681640625,
        "l2_loss": 98.93582153320312,
        "ortho_loss": 0.5129990577697754
      },
      "step 165": {
        "loss": 156.01416015625,
        "l1_loss": 1336.1300048828125,
        "l2_loss": 102.51963806152344,
        "ortho_loss": 0.49317625164985657
      },
      "step 166": {
        "loss": 150.94981384277344,
        "l1_loss": 1266.55126953125,
        "l2_loss": 100.23721313476562,
        "ortho_loss": 0.5055667161941528
      },
      "step 167": {
        "loss": 153.2933349609375,
        "l1_loss": 1294.667724609375,
        "l2_loss": 101.45706176757812,
        "ortho_loss": 0.49563392996788025
      },
      "step 168": {
        "loss": 152.03021240234375,
        "l1_loss": 1274.089599609375,
        "l2_loss": 101.01676940917969,
        "ortho_loss": 0.49865326285362244
      },
      "step 169": {
        "loss": 148.05795288085938,
        "l1_loss": 1215.50146484375,
        "l2_loss": 99.38671112060547,
        "ortho_loss": 0.5117304921150208
      },
      "step 170": {
        "loss": 147.32785034179688,
        "l1_loss": 1209.318115234375,
        "l2_loss": 98.90422058105469,
        "ortho_loss": 0.5089848637580872
      },
      "step 171": {
        "loss": 152.8289031982422,
        "l1_loss": 1280.63232421875,
        "l2_loss": 101.55441284179688,
        "ortho_loss": 0.4919936954975128
      },
      "step 172": {
        "loss": 152.02838134765625,
        "l1_loss": 1262.4053955078125,
        "l2_loss": 101.48267364501953,
        "ortho_loss": 0.49500730633735657
      },
      "step 173": {
        "loss": 151.12535095214844,
        "l1_loss": 1249.9759521484375,
        "l2_loss": 101.07679748535156,
        "ortho_loss": 0.4952033758163452
      },
      "step 174": {
        "loss": 149.2330780029297,
        "l1_loss": 1214.435791015625,
        "l2_loss": 100.60552978515625,
        "ortho_loss": 0.5011923313140869
      },
      "step 175": {
        "loss": 146.45083618164062,
        "l1_loss": 1174.991455078125,
        "l2_loss": 99.4002685546875,
        "ortho_loss": 0.5090986490249634
      },
      "step 176": {
        "loss": 154.339599609375,
        "l1_loss": 1281.305908203125,
        "l2_loss": 103.03845977783203,
        "ortho_loss": 0.4889882802963257
      },
      "step 177": {
        "loss": 143.73887634277344,
        "l1_loss": 1130.03955078125,
        "l2_loss": 98.48568725585938,
        "ortho_loss": 0.51597660779953
      },
      "step 178": {
        "loss": 148.13002014160156,
        "l1_loss": 1184.9832763671875,
        "l2_loss": 100.68035125732422,
        "ortho_loss": 0.5033736824989319
      },
      "step 179": {
        "loss": 143.53578186035156,
        "l1_loss": 1128.3857421875,
        "l2_loss": 98.3492431640625,
        "ortho_loss": 0.5110839605331421
      },
      "step 180": {
        "loss": 142.85740661621094,
        "l1_loss": 1115.443115234375,
        "l2_loss": 98.18827819824219,
        "ortho_loss": 0.5140944719314575
      },
      "step 181": {
        "loss": 145.31570434570312,
        "l1_loss": 1145.148681640625,
        "l2_loss": 99.45918273925781,
        "ortho_loss": 0.5057321190834045
      },
      "step 182": {
        "loss": 143.22607421875,
        "l1_loss": 1104.8587646484375,
        "l2_loss": 98.98052978515625,
        "ortho_loss": 0.5119495391845703
      },
      "step 183": {
        "loss": 139.3062744140625,
        "l1_loss": 1056.4776611328125,
        "l2_loss": 96.99507141113281,
        "ortho_loss": 0.5208714604377747
      },
      "step 184": {
        "loss": 144.22091674804688,
        "l1_loss": 1116.7998046875,
        "l2_loss": 99.49852752685547,
        "ortho_loss": 0.5040320158004761
      },
      "step 185": {
        "loss": 142.17506408691406,
        "l1_loss": 1088.2464599609375,
        "l2_loss": 98.59423828125,
        "ortho_loss": 0.5096777081489563
      },
      "step 186": {
        "loss": 143.26426696777344,
        "l1_loss": 1097.71142578125,
        "l2_loss": 99.30529022216797,
        "ortho_loss": 0.5051430463790894
      },
      "step 187": {
        "loss": 139.79673767089844,
        "l1_loss": 1044.0601806640625,
        "l2_loss": 97.98263549804688,
        "ortho_loss": 0.5169521570205688
      },
      "step 188": {
        "loss": 142.97357177734375,
        "l1_loss": 1090.178955078125,
        "l2_loss": 99.31617736816406,
        "ortho_loss": 0.5023590922355652
      },
      "step 189": {
        "loss": 136.22396850585938,
        "l1_loss": 995.1475830078125,
        "l2_loss": 96.36537170410156,
        "ortho_loss": 0.5270456075668335
      },
      "step 190": {
        "loss": 136.65213012695312,
        "l1_loss": 992.9852905273438,
        "l2_loss": 96.88029479980469,
        "ortho_loss": 0.524326503276825
      },
      "step 191": {
        "loss": 136.4429473876953,
        "l1_loss": 987.002197265625,
        "l2_loss": 96.91023254394531,
        "ortho_loss": 0.5262693762779236
      },
      "step 192": {
        "loss": 147.25291442871094,
        "l1_loss": 1139.760986328125,
        "l2_loss": 101.61361694335938,
        "ortho_loss": 0.4885757267475128
      },
      "step 193": {
        "loss": 138.0378875732422,
        "l1_loss": 1008.8095703125,
        "l2_loss": 97.63398742675781,
        "ortho_loss": 0.5151050686836243
      },
      "step 194": {
        "loss": 140.4985809326172,
        "l1_loss": 1036.747802734375,
        "l2_loss": 98.97821044921875,
        "ortho_loss": 0.5046557188034058
      },
      "step 195": {
        "loss": 139.4232177734375,
        "l1_loss": 1019.7591552734375,
        "l2_loss": 98.58213806152344,
        "ortho_loss": 0.5072072744369507
      },
      "step 196": {
        "loss": 137.5189971923828,
        "l1_loss": 994.17822265625,
        "l2_loss": 97.70091247558594,
        "ortho_loss": 0.5094870328903198
      },
      "step 197": {
        "loss": 141.19436645507812,
        "l1_loss": 1045.860107421875,
        "l2_loss": 99.3101806640625,
        "ortho_loss": 0.4978100061416626
      },
      "step 198": {
        "loss": 135.20303344726562,
        "l1_loss": 953.418212890625,
        "l2_loss": 97.01426696777344,
        "ortho_loss": 0.5202796459197998
      },
      "step 199": {
        "loss": 137.36099243164062,
        "l1_loss": 983.7884521484375,
        "l2_loss": 97.95863342285156,
        "ortho_loss": 0.5083234906196594
      },
      "step 200": {
        "loss": 136.07275390625,
        "l1_loss": 956.1475830078125,
        "l2_loss": 97.775390625,
        "ortho_loss": 0.5145791172981262
      },
      "step 201": {
        "loss": 143.7179412841797,
        "l1_loss": 1059.5330810546875,
        "l2_loss": 101.28722381591797,
        "ortho_loss": 0.4939020872116089
      },
      "step 202": {
        "loss": 135.40948486328125,
        "l1_loss": 948.2652587890625,
        "l2_loss": 97.42770385742188,
        "ortho_loss": 0.5117544531822205
      },
      "step 203": {
        "loss": 134.74380493164062,
        "l1_loss": 942.5007934570312,
        "l2_loss": 96.99267578125,
        "ortho_loss": 0.5110836625099182
      },
      "step 204": {
        "loss": 132.29078674316406,
        "l1_loss": 901.9342651367188,
        "l2_loss": 96.16120910644531,
        "ortho_loss": 0.5220521688461304
      },
      "step 205": {
        "loss": 140.9574737548828,
        "l1_loss": 1017.34033203125,
        "l2_loss": 100.2144546508789,
        "ortho_loss": 0.49402210116386414
      },
      "step 206": {
        "loss": 138.3689422607422,
        "l1_loss": 985.4215087890625,
        "l2_loss": 98.90225219726562,
        "ortho_loss": 0.4982876479625702
      },
      "step 207": {
        "loss": 133.4093475341797,
        "l1_loss": 911.2908325195312,
        "l2_loss": 96.9063720703125,
        "ortho_loss": 0.5135136842727661
      },
      "step 208": {
        "loss": 131.44635009765625,
        "l1_loss": 881.17333984375,
        "l2_loss": 96.1478271484375,
        "ortho_loss": 0.5158724188804626
      },
      "step 209": {
        "loss": 130.81983947753906,
        "l1_loss": 860.459228515625,
        "l2_loss": 96.34907531738281,
        "ortho_loss": 0.5239966511726379
      },
      "step 210": {
        "loss": 136.30422973632812,
        "l1_loss": 937.5283203125,
        "l2_loss": 98.75262451171875,
        "ortho_loss": 0.5047630667686462
      },
      "step 211": {
        "loss": 131.45440673828125,
        "l1_loss": 870.3226928710938,
        "l2_loss": 96.58976745605469,
        "ortho_loss": 0.5172101855278015
      },
      "step 212": {
        "loss": 129.23175048828125,
        "l1_loss": 835.9999389648438,
        "l2_loss": 95.73927307128906,
        "ortho_loss": 0.5247113704681396
      },
      "step 213": {
        "loss": 131.3632049560547,
        "l1_loss": 857.431884765625,
        "l2_loss": 97.01438903808594,
        "ortho_loss": 0.5154297947883606
      },
      "step 214": {
        "loss": 123.75505828857422,
        "l1_loss": 750.8328857421875,
        "l2_loss": 93.66644287109375,
        "ortho_loss": 0.5529481172561646
      },
      "step 215": {
        "loss": 128.50440979003906,
        "l1_loss": 823.3525390625,
        "l2_loss": 95.5181655883789,
        "ortho_loss": 0.5213747024536133
      },
      "step 216": {
        "loss": 125.76332092285156,
        "l1_loss": 777.5369262695312,
        "l2_loss": 94.60861206054688,
        "ortho_loss": 0.5323193073272705
      },
      "step 217": {
        "loss": 130.2323455810547,
        "l1_loss": 838.9005737304688,
        "l2_loss": 96.62461853027344,
        "ortho_loss": 0.5171040296554565
      },
      "step 218": {
        "loss": 131.12713623046875,
        "l1_loss": 837.5338134765625,
        "l2_loss": 97.57424926757812,
        "ortho_loss": 0.5154028534889221
      },
      "step 219": {
        "loss": 131.8191680908203,
        "l1_loss": 850.3388061523438,
        "l2_loss": 97.75470733642578,
        "ortho_loss": 0.5092554092407227
      },
      "step 220": {
        "loss": 124.77776336669922,
        "l1_loss": 753.3021240234375,
        "l2_loss": 94.59231567382812,
        "ortho_loss": 0.5336691737174988
      },
      "step 221": {
        "loss": 127.44511413574219,
        "l1_loss": 783.75439453125,
        "l2_loss": 96.04253387451172,
        "ortho_loss": 0.5240262746810913
      },
      "step 222": {
        "loss": 127.88961791992188,
        "l1_loss": 795.3305053710938,
        "l2_loss": 96.02468872070312,
        "ortho_loss": 0.5171490907669067
      },
      "step 223": {
        "loss": 127.16942596435547,
        "l1_loss": 780.9364013671875,
        "l2_loss": 95.87985229492188,
        "ortho_loss": 0.5211836099624634
      },
      "step 224": {
        "loss": 125.70594787597656,
        "l1_loss": 763.9592895507812,
        "l2_loss": 95.09546661376953,
        "ortho_loss": 0.5210838913917542
      },
      "step 225": {
        "loss": 127.83757019042969,
        "l1_loss": 787.6969604492188,
        "l2_loss": 96.27812194824219,
        "ortho_loss": 0.515724778175354
      },
      "step 226": {
        "loss": 122.61769104003906,
        "l1_loss": 710.127685546875,
        "l2_loss": 94.158935546875,
        "ortho_loss": 0.5365162491798401
      },
      "step 227": {
        "loss": 126.39960479736328,
        "l1_loss": 761.9775390625,
        "l2_loss": 95.86856079101562,
        "ortho_loss": 0.5193843841552734
      },
      "step 228": {
        "loss": 127.99943542480469,
        "l1_loss": 775.557861328125,
        "l2_loss": 96.92567443847656,
        "ortho_loss": 0.5144137740135193
      },
      "step 229": {
        "loss": 127.13819885253906,
        "l1_loss": 768.7486572265625,
        "l2_loss": 96.33651733398438,
        "ortho_loss": 0.517320990562439
      },
      "step 230": {
        "loss": 123.45040893554688,
        "l1_loss": 719.9314575195312,
        "l2_loss": 94.60054016113281,
        "ortho_loss": 0.5261390805244446
      },
      "step 231": {
        "loss": 128.46925354003906,
        "l1_loss": 783.299072265625,
        "l2_loss": 97.0863265991211,
        "ortho_loss": 0.5097183585166931
      },
      "step 232": {
        "loss": 123.36096954345703,
        "l1_loss": 701.4260864257812,
        "l2_loss": 95.25119018554688,
        "ortho_loss": 0.5273316502571106
      },
      "step 233": {
        "loss": 121.54499816894531,
        "l1_loss": 672.8214111328125,
        "l2_loss": 94.57872772216797,
        "ortho_loss": 0.5341615676879883
      },
      "step 234": {
        "loss": 120.91544342041016,
        "l1_loss": 669.7393798828125,
        "l2_loss": 94.07241821289062,
        "ortho_loss": 0.5345019698143005
      },
      "step 235": {
        "loss": 129.42245483398438,
        "l1_loss": 793.43310546875,
        "l2_loss": 97.6346435546875,
        "ortho_loss": 0.5048441886901855
      },
      "step 236": {
        "loss": 126.58780670166016,
        "l1_loss": 748.3828125,
        "l2_loss": 96.60137939453125,
        "ortho_loss": 0.5111859440803528
      },
      "step 237": {
        "loss": 128.16583251953125,
        "l1_loss": 756.342041015625,
        "l2_loss": 97.86116027832031,
        "ortho_loss": 0.5099021196365356
      },
      "step 238": {
        "loss": 121.25450134277344,
        "l1_loss": 665.9823608398438,
        "l2_loss": 94.5619888305664,
        "ortho_loss": 0.5322580337524414
      },
      "step 239": {
        "loss": 117.25346374511719,
        "l1_loss": 609.5032958984375,
        "l2_loss": 92.81907653808594,
        "ortho_loss": 0.5426359176635742
      },
      "step 240": {
        "loss": 115.51819610595703,
        "l1_loss": 582.904296875,
        "l2_loss": 92.14601135253906,
        "ortho_loss": 0.5601840019226074
      },
      "step 241": {
        "loss": 126.26496124267578,
        "l1_loss": 728.309814453125,
        "l2_loss": 97.08116912841797,
        "ortho_loss": 0.5139559507369995
      },
      "step 242": {
        "loss": 117.04879760742188,
        "l1_loss": 598.18798828125,
        "l2_loss": 93.06680297851562,
        "ortho_loss": 0.544719934463501
      },
      "step 243": {
        "loss": 120.51744079589844,
        "l1_loss": 642.923828125,
        "l2_loss": 94.74749755859375,
        "ortho_loss": 0.5299602746963501
      },
      "step 244": {
        "loss": 120.48764038085938,
        "l1_loss": 649.60546875,
        "l2_loss": 94.45079040527344,
        "ortho_loss": 0.526269257068634
      },
      "step 245": {
        "loss": 121.50482177734375,
        "l1_loss": 657.3278198242188,
        "l2_loss": 95.15928649902344,
        "ortho_loss": 0.5241873860359192
      },
      "step 246": {
        "loss": 123.04235076904297,
        "l1_loss": 676.3639526367188,
        "l2_loss": 95.9361572265625,
        "ortho_loss": 0.5163391828536987
      },
      "step 247": {
        "loss": 117.73409271240234,
        "l1_loss": 603.616943359375,
        "l2_loss": 93.53569030761719,
        "ortho_loss": 0.5372897982597351
      },
      "step 248": {
        "loss": 122.99932861328125,
        "l1_loss": 662.760986328125,
        "l2_loss": 96.43685150146484,
        "ortho_loss": 0.520432710647583
      },
      "step 249": {
        "loss": 120.0822982788086,
        "l1_loss": 628.4722900390625,
        "l2_loss": 94.89064025878906,
        "ortho_loss": 0.5276613235473633
      },
      "step 250": {
        "loss": 118.71311950683594,
        "l1_loss": 606.5541381835938,
        "l2_loss": 94.3978042602539,
        "ortho_loss": 0.5314847826957703
      },
      "step 251": {
        "loss": 126.07225799560547,
        "l1_loss": 713.2373657226562,
        "l2_loss": 97.49193572998047,
        "ortho_loss": 0.5082665681838989
      },
      "step 252": {
        "loss": 122.68882751464844,
        "l1_loss": 659.880126953125,
        "l2_loss": 96.24205017089844,
        "ortho_loss": 0.5157794952392578
      },
      "step 253": {
        "loss": 121.61078643798828,
        "l1_loss": 637.0880737304688,
        "l2_loss": 96.07505798339844,
        "ortho_loss": 0.5220965147018433
      },
      "step 254": {
        "loss": 117.11588287353516,
        "l1_loss": 567.7606811523438,
        "l2_loss": 94.3514175415039,
        "ortho_loss": 0.5403914451599121
      },
      "step 255": {
        "loss": 116.8531494140625,
        "l1_loss": 568.0443725585938,
        "l2_loss": 94.07781982421875,
        "ortho_loss": 0.5355772376060486
      },
      "step 256": {
        "loss": 117.51953125,
        "l1_loss": 573.4389038085938,
        "l2_loss": 94.52869415283203,
        "ortho_loss": 0.5328700542449951
      },
      "step 257": {
        "loss": 121.17698669433594,
        "l1_loss": 636.8314208984375,
        "l2_loss": 95.65187072753906,
        "ortho_loss": 0.5185783505439758
      },
      "step 258": {
        "loss": 117.50206756591797,
        "l1_loss": 576.2705078125,
        "l2_loss": 94.39813232421875,
        "ortho_loss": 0.5311654210090637
      },
      "step 259": {
        "loss": 118.2608413696289,
        "l1_loss": 587.9366455078125,
        "l2_loss": 94.69081115722656,
        "ortho_loss": 0.5256547927856445
      },
      "step 260": {
        "loss": 117.10626983642578,
        "l1_loss": 558.6343994140625,
        "l2_loss": 94.70730590820312,
        "ortho_loss": 0.5358848571777344
      },
      "step 261": {
        "loss": 115.14306640625,
        "l1_loss": 532.0843505859375,
        "l2_loss": 93.80545043945312,
        "ortho_loss": 0.5424140095710754
      },
      "step 262": {
        "loss": 114.74962615966797,
        "l1_loss": 524.7078857421875,
        "l2_loss": 93.70701599121094,
        "ortho_loss": 0.5429604649543762
      },
      "step 263": {
        "loss": 116.1849136352539,
        "l1_loss": 540.4500732421875,
        "l2_loss": 94.51325988769531,
        "ortho_loss": 0.536476731300354
      },
      "step 264": {
        "loss": 119.10330963134766,
        "l1_loss": 589.1263427734375,
        "l2_loss": 95.48588562011719,
        "ortho_loss": 0.5236530303955078
      },
      "step 265": {
        "loss": 115.7068862915039,
        "l1_loss": 530.419189453125,
        "l2_loss": 94.43650817871094,
        "ortho_loss": 0.5361089706420898
      },
      "step 266": {
        "loss": 119.0526123046875,
        "l1_loss": 585.5648193359375,
        "l2_loss": 95.5776138305664,
        "ortho_loss": 0.5240467190742493
      },
      "step 267": {
        "loss": 113.30413818359375,
        "l1_loss": 495.6640319824219,
        "l2_loss": 93.42279052734375,
        "ortho_loss": 0.5478419661521912
      },
      "step 268": {
        "loss": 119.45357513427734,
        "l1_loss": 587.6297607421875,
        "l2_loss": 95.89603424072266,
        "ortho_loss": 0.5235150456428528
      },
      "step 269": {
        "loss": 113.72972869873047,
        "l1_loss": 502.4077453613281,
        "l2_loss": 93.57929229736328,
        "ortho_loss": 0.541329562664032
      },
      "step 270": {
        "loss": 113.11581420898438,
        "l1_loss": 480.7703857421875,
        "l2_loss": 93.83006286621094,
        "ortho_loss": 0.5493472218513489
      },
      "step 271": {
        "loss": 115.0223388671875,
        "l1_loss": 516.9725341796875,
        "l2_loss": 94.28990173339844,
        "ortho_loss": 0.535378098487854
      },
      "step 272": {
        "loss": 119.04029846191406,
        "l1_loss": 572.1436767578125,
        "l2_loss": 96.10200500488281,
        "ortho_loss": 0.5255222916603088
      },
      "step 273": {
        "loss": 118.4143295288086,
        "l1_loss": 561.840087890625,
        "l2_loss": 95.88793182373047,
        "ortho_loss": 0.5279455184936523
      },
      "step 274": {
        "loss": 115.74056243896484,
        "l1_loss": 528.2793579101562,
        "l2_loss": 94.5562515258789,
        "ortho_loss": 0.5314071774482727
      },
      "step 275": {
        "loss": 115.37686157226562,
        "l1_loss": 516.5465087890625,
        "l2_loss": 94.66148376464844,
        "ortho_loss": 0.5351801514625549
      },
      "step 276": {
        "loss": 115.65924072265625,
        "l1_loss": 520.9139404296875,
        "l2_loss": 94.76942443847656,
        "ortho_loss": 0.5326274037361145
      },
      "step 277": {
        "loss": 115.80441284179688,
        "l1_loss": 525.749267578125,
        "l2_loss": 94.7213134765625,
        "ortho_loss": 0.5313251614570618
      },
      "step 278": {
        "loss": 110.03136444091797,
        "l1_loss": 426.4012451171875,
        "l2_loss": 92.91935729980469,
        "ortho_loss": 0.5596230626106262
      },
      "step 279": {
        "loss": 111.23710632324219,
        "l1_loss": 454.9501953125,
        "l2_loss": 92.98432922363281,
        "ortho_loss": 0.5477474927902222
      },
      "step 280": {
        "loss": 111.80537414550781,
        "l1_loss": 451.610107421875,
        "l2_loss": 93.6861572265625,
        "ortho_loss": 0.548116147518158
      },
      "step 281": {
        "loss": 111.91278839111328,
        "l1_loss": 457.6695251464844,
        "l2_loss": 93.551513671875,
        "ortho_loss": 0.544992983341217
      },
      "step 282": {
        "loss": 109.59673309326172,
        "l1_loss": 423.6833190917969,
        "l2_loss": 92.59386444091797,
        "ortho_loss": 0.5553459525108337
      },
      "step 283": {
        "loss": 111.1640625,
        "l1_loss": 439.4758605957031,
        "l2_loss": 93.53012084960938,
        "ortho_loss": 0.5490520000457764
      },
      "step 284": {
        "loss": 116.07533264160156,
        "l1_loss": 513.49609375,
        "l2_loss": 95.48228454589844,
        "ortho_loss": 0.5320863127708435
      },
      "step 285": {
        "loss": 112.80706024169922,
        "l1_loss": 455.12432861328125,
        "l2_loss": 94.54769134521484,
        "ortho_loss": 0.5439640283584595
      },
      "step 286": {
        "loss": 109.7314682006836,
        "l1_loss": 418.42010498046875,
        "l2_loss": 92.93931579589844,
        "ortho_loss": 0.5535300374031067
      },
      "step 287": {
        "loss": 110.92424011230469,
        "l1_loss": 443.2149658203125,
        "l2_loss": 93.1409683227539,
        "ortho_loss": 0.5467039942741394
      },
      "step 288": {
        "loss": 112.96424865722656,
        "l1_loss": 469.3648376464844,
        "l2_loss": 94.13572692871094,
        "ortho_loss": 0.5392214059829712
      },
      "step 289": {
        "loss": 109.7356948852539,
        "l1_loss": 427.99658203125,
        "l2_loss": 92.5611801147461,
        "ortho_loss": 0.546501874923706
      },
      "step 290": {
        "loss": 112.61189270019531,
        "l1_loss": 456.47686767578125,
        "l2_loss": 94.2988510131836,
        "ortho_loss": 0.5396707653999329
      },
      "step 291": {
        "loss": 107.79312896728516,
        "l1_loss": 384.30413818359375,
        "l2_loss": 92.36465454101562,
        "ortho_loss": 0.563155472278595
      },
      "step 292": {
        "loss": 114.75435638427734,
        "l1_loss": 498.6119384765625,
        "l2_loss": 94.75665283203125,
        "ortho_loss": 0.5323354005813599
      },
      "step 293": {
        "loss": 111.11962890625,
        "l1_loss": 432.697021484375,
        "l2_loss": 93.75704193115234,
        "ortho_loss": 0.5470148921012878
      },
      "step 294": {
        "loss": 111.09252166748047,
        "l1_loss": 426.3325500488281,
        "l2_loss": 93.9845962524414,
        "ortho_loss": 0.5462661385536194
      },
      "step 295": {
        "loss": 111.3207778930664,
        "l1_loss": 439.98187255859375,
        "l2_loss": 93.66742706298828,
        "ortho_loss": 0.54076087474823
      },
      "step 296": {
        "loss": 112.66069030761719,
        "l1_loss": 462.17333984375,
        "l2_loss": 94.1199951171875,
        "ortho_loss": 0.5376090407371521
      },
      "step 297": {
        "loss": 109.62726593017578,
        "l1_loss": 408.1257019042969,
        "l2_loss": 93.2471694946289,
        "ortho_loss": 0.550715982913971
      },
      "step 298": {
        "loss": 115.66339874267578,
        "l1_loss": 505.97393798828125,
        "l2_loss": 95.37139892578125,
        "ortho_loss": 0.5304856300354004
      },
      "step 299": {
        "loss": 111.4628677368164,
        "l1_loss": 435.38739013671875,
        "l2_loss": 93.99298095703125,
        "ortho_loss": 0.5438898801803589
      },
      "step 300": {
        "loss": 110.36663055419922,
        "l1_loss": 423.3088073730469,
        "l2_loss": 93.37964630126953,
        "ortho_loss": 0.5463520288467407
      },
      "step 301": {
        "loss": 108.64302825927734,
        "l1_loss": 381.0716552734375,
        "l2_loss": 93.34468078613281,
        "ortho_loss": 0.55478435754776
      },
      "step 302": {
        "loss": 106.91500091552734,
        "l1_loss": 357.9559326171875,
        "l2_loss": 92.54059600830078,
        "ortho_loss": 0.5616595149040222
      },
      "step 303": {
        "loss": 110.0816650390625,
        "l1_loss": 409.36065673828125,
        "l2_loss": 93.65251922607422,
        "ortho_loss": 0.5471486449241638
      },
      "step 304": {
        "loss": 110.50285339355469,
        "l1_loss": 416.75579833984375,
        "l2_loss": 93.77804565429688,
        "ortho_loss": 0.5457883477210999
      },
      "step 305": {
        "loss": 115.85479736328125,
        "l1_loss": 501.21563720703125,
        "l2_loss": 95.75291442871094,
        "ortho_loss": 0.5325337648391724
      },
      "step 306": {
        "loss": 115.06892395019531,
        "l1_loss": 489.26324462890625,
        "l2_loss": 95.44490051269531,
        "ortho_loss": 0.534942626953125
      },
      "step 307": {
        "loss": 109.71611785888672,
        "l1_loss": 399.25494384765625,
        "l2_loss": 93.69110107421875,
        "ortho_loss": 0.5481703877449036
      },
      "step 308": {
        "loss": 110.79950714111328,
        "l1_loss": 417.715576171875,
        "l2_loss": 94.03640747070312,
        "ortho_loss": 0.5448328852653503
      },
      "step 309": {
        "loss": 106.10150909423828,
        "l1_loss": 348.27362060546875,
        "l2_loss": 92.114013671875,
        "ortho_loss": 0.5655224919319153
      },
      "step 310": {
        "loss": 107.14771270751953,
        "l1_loss": 358.6936340332031,
        "l2_loss": 92.74417114257812,
        "ortho_loss": 0.557915449142456
      },
      "step 311": {
        "loss": 107.06181335449219,
        "l1_loss": 359.2267150878906,
        "l2_loss": 92.63691711425781,
        "ortho_loss": 0.558343231678009
      },
      "step 312": {
        "loss": 105.86162567138672,
        "l1_loss": 336.6551818847656,
        "l2_loss": 92.33885192871094,
        "ortho_loss": 0.5656248927116394
      },
      "step 313": {
        "loss": 112.41346740722656,
        "l1_loss": 430.43072509765625,
        "l2_loss": 95.14179992675781,
        "ortho_loss": 0.5443955659866333
      },
      "step 314": {
        "loss": 110.25855255126953,
        "l1_loss": 403.2643127441406,
        "l2_loss": 94.07319641113281,
        "ortho_loss": 0.5478944182395935
      },
      "step 315": {
        "loss": 106.90786743164062,
        "l1_loss": 354.1842041015625,
        "l2_loss": 92.6846694946289,
        "ortho_loss": 0.5582963824272156
      },
      "step 316": {
        "loss": 110.46354675292969,
        "l1_loss": 411.4501953125,
        "l2_loss": 93.95085144042969,
        "ortho_loss": 0.5468787550926208
      },
      "step 317": {
        "loss": 108.66992950439453,
        "l1_loss": 369.14764404296875,
        "l2_loss": 93.84843444824219,
        "ortho_loss": 0.555840015411377
      },
      "step 318": {
        "loss": 112.83616638183594,
        "l1_loss": 446.9841613769531,
        "l2_loss": 94.90270233154297,
        "ortho_loss": 0.5409969091415405
      },
      "step 319": {
        "loss": 109.74444580078125,
        "l1_loss": 389.7305908203125,
        "l2_loss": 94.10029602050781,
        "ortho_loss": 0.5492549538612366
      },
      "step 320": {
        "loss": 113.48101806640625,
        "l1_loss": 451.9461669921875,
        "l2_loss": 95.34906768798828,
        "ortho_loss": 0.5411133766174316
      },
      "step 321": {
        "loss": 106.1407241821289,
        "l1_loss": 337.5589599609375,
        "l2_loss": 92.58213806152344,
        "ortho_loss": 0.5622902512550354
      },
      "step 322": {
        "loss": 108.65807342529297,
        "l1_loss": 375.9835510253906,
        "l2_loss": 93.56350708007812,
        "ortho_loss": 0.5522251129150391
      },
      "step 323": {
        "loss": 110.12922668457031,
        "l1_loss": 398.3646240234375,
        "l2_loss": 94.13963317871094,
        "ortho_loss": 0.5500490069389343
      },
      "step 324": {
        "loss": 106.55422973632812,
        "l1_loss": 341.8058166503906,
        "l2_loss": 92.82588958740234,
        "ortho_loss": 0.5610824823379517
      },
      "step 325": {
        "loss": 106.01895904541016,
        "l1_loss": 340.1102294921875,
        "l2_loss": 92.35842895507812,
        "ortho_loss": 0.5611957907676697
      },
      "step 326": {
        "loss": 106.26591491699219,
        "l1_loss": 328.30078125,
        "l2_loss": 93.07740020751953,
        "ortho_loss": 0.5648839473724365
      },
      "step 327": {
        "loss": 105.96452331542969,
        "l1_loss": 335.458740234375,
        "l2_loss": 92.48989868164062,
        "ortho_loss": 0.5627565383911133
      },
      "step 328": {
        "loss": 108.35916137695312,
        "l1_loss": 375.89874267578125,
        "l2_loss": 93.26774597167969,
        "ortho_loss": 0.554692268371582
      },
      "step 329": {
        "loss": 107.60383605957031,
        "l1_loss": 341.23577880859375,
        "l2_loss": 93.89837646484375,
        "ortho_loss": 0.5602790117263794
      },
      "step 330": {
        "loss": 104.68016052246094,
        "l1_loss": 305.59002685546875,
        "l2_loss": 92.399658203125,
        "ortho_loss": 0.5690125823020935
      },
      "step 331": {
        "loss": 112.63410949707031,
        "l1_loss": 420.482421875,
        "l2_loss": 95.75993347167969,
        "ortho_loss": 0.5487610101699829
      },
      "step 332": {
        "loss": 108.72071838378906,
        "l1_loss": 369.9075622558594,
        "l2_loss": 93.86872863769531,
        "ortho_loss": 0.556848406791687
      },
      "step 333": {
        "loss": 108.63935852050781,
        "l1_loss": 373.82037353515625,
        "l2_loss": 93.63104248046875,
        "ortho_loss": 0.555004358291626
      },
      "step 334": {
        "loss": 112.77562713623047,
        "l1_loss": 437.4844055175781,
        "l2_loss": 95.22145080566406,
        "ortho_loss": 0.5480296015739441
      },
      "step 335": {
        "loss": 105.49805450439453,
        "l1_loss": 318.8652648925781,
        "l2_loss": 92.68682861328125,
        "ortho_loss": 0.5661835670471191
      },
      "step 336": {
        "loss": 107.58480072021484,
        "l1_loss": 347.1249084472656,
        "l2_loss": 93.64376831054688,
        "ortho_loss": 0.5603562593460083
      },
      "step 337": {
        "loss": 103.52503967285156,
        "l1_loss": 285.18402099609375,
        "l2_loss": 92.06002807617188,
        "ortho_loss": 0.5764410495758057
      },
      "step 338": {
        "loss": 105.38251495361328,
        "l1_loss": 321.95263671875,
        "l2_loss": 92.44792175292969,
        "ortho_loss": 0.5648654103279114
      },
      "step 339": {
        "loss": 106.04334259033203,
        "l1_loss": 314.260986328125,
        "l2_loss": 93.41606140136719,
        "ortho_loss": 0.5684070587158203
      },
      "step 340": {
        "loss": 102.45869445800781,
        "l1_loss": 272.34442138671875,
        "l2_loss": 91.50674438476562,
        "ortho_loss": 0.581739068031311
      },
      "step 341": {
        "loss": 107.96070861816406,
        "l1_loss": 348.5158996582031,
        "l2_loss": 93.96407318115234,
        "ortho_loss": 0.5599876046180725
      },
      "step 342": {
        "loss": 109.36981201171875,
        "l1_loss": 384.0509338378906,
        "l2_loss": 93.95232391357422,
        "ortho_loss": 0.5544886589050293
      },
      "step 343": {
        "loss": 106.607421875,
        "l1_loss": 327.2015075683594,
        "l2_loss": 93.4627914428711,
        "ortho_loss": 0.5657424926757812
      },
      "step 344": {
        "loss": 106.1871566772461,
        "l1_loss": 324.66845703125,
        "l2_loss": 93.14374542236328,
        "ortho_loss": 0.5667180418968201
      },
      "step 345": {
        "loss": 106.76016998291016,
        "l1_loss": 334.031005859375,
        "l2_loss": 93.34239196777344,
        "ortho_loss": 0.565448522567749
      },
      "step 346": {
        "loss": 103.19227600097656,
        "l1_loss": 280.0685729980469,
        "l2_loss": 91.93180084228516,
        "ortho_loss": 0.5773540139198303
      },
      "step 347": {
        "loss": 114.47968292236328,
        "l1_loss": 451.23382568359375,
        "l2_loss": 96.37521362304688,
        "ortho_loss": 0.5511757731437683
      },
      "step 348": {
        "loss": 106.48698425292969,
        "l1_loss": 321.2812194824219,
        "l2_loss": 93.57878875732422,
        "ortho_loss": 0.569473922252655
      },
      "step 349": {
        "loss": 106.43109130859375,
        "l1_loss": 327.78277587890625,
        "l2_loss": 93.26299285888672,
        "ortho_loss": 0.5678651928901672
      },
      "step 350": {
        "loss": 108.65128326416016,
        "l1_loss": 372.64501953125,
        "l2_loss": 93.68946838378906,
        "ortho_loss": 0.5601731538772583
      },
      "step 351": {
        "loss": 104.84660339355469,
        "l1_loss": 300.90911865234375,
        "l2_loss": 92.75300598144531,
        "ortho_loss": 0.5723857283592224
      },
      "step 352": {
        "loss": 112.36944580078125,
        "l1_loss": 429.69818115234375,
        "l2_loss": 95.12604522705078,
        "ortho_loss": 0.5547665953636169
      },
      "step 353": {
        "loss": 105.1068344116211,
        "l1_loss": 310.28704833984375,
        "l2_loss": 92.63807678222656,
        "ortho_loss": 0.5727361440658569
      },
      "step 354": {
        "loss": 105.91937255859375,
        "l1_loss": 318.1763916015625,
        "l2_loss": 93.13517761230469,
        "ortho_loss": 0.5713711380958557
      },
      "step 355": {
        "loss": 106.54029083251953,
        "l1_loss": 327.05426025390625,
        "l2_loss": 93.40111541748047,
        "ortho_loss": 0.5700380802154541
      },
      "step 356": {
        "loss": 108.9816665649414,
        "l1_loss": 366.7547607421875,
        "l2_loss": 94.25489807128906,
        "ortho_loss": 0.5658190846443176
      },
      "step 357": {
        "loss": 99.89588165283203,
        "l1_loss": 219.55722045898438,
        "l2_loss": 91.052978515625,
        "ortho_loss": 0.6061700582504272
      },
      "step 358": {
        "loss": 105.46083068847656,
        "l1_loss": 311.37725830078125,
        "l2_loss": 92.94822692871094,
        "ortho_loss": 0.5750856399536133
      },
      "step 359": {
        "loss": 110.31938171386719,
        "l1_loss": 388.6126708984375,
        "l2_loss": 94.71833038330078,
        "ortho_loss": 0.5655269026756287
      },
      "step 360": {
        "loss": 106.98200988769531,
        "l1_loss": 329.6742858886719,
        "l2_loss": 93.73764038085938,
        "ortho_loss": 0.5739712119102478
      },
      "step 361": {
        "loss": 106.03532409667969,
        "l1_loss": 310.89691162109375,
        "l2_loss": 93.54158020019531,
        "ortho_loss": 0.5786890983581543
      },
      "step 362": {
        "loss": 103.07511901855469,
        "l1_loss": 269.2703857421875,
        "l2_loss": 92.24560546875,
        "ortho_loss": 0.5870039463043213
      },
      "step 363": {
        "loss": 106.0625,
        "l1_loss": 316.6214599609375,
        "l2_loss": 93.33978271484375,
        "ortho_loss": 0.5786483287811279
      },
      "step 364": {
        "loss": 102.77751159667969,
        "l1_loss": 265.05780029296875,
        "l2_loss": 92.11636352539062,
        "ortho_loss": 0.5883424282073975
      },
      "step 365": {
        "loss": 110.8840103149414,
        "l1_loss": 400.2828369140625,
        "l2_loss": 94.81576538085938,
        "ortho_loss": 0.5693164467811584
      },
      "step 366": {
        "loss": 107.14158630371094,
        "l1_loss": 330.80792236328125,
        "l2_loss": 93.8515396118164,
        "ortho_loss": 0.5773208141326904
      },
      "step 367": {
        "loss": 103.1959228515625,
        "l1_loss": 271.5659484863281,
        "l2_loss": 92.27449035644531,
        "ortho_loss": 0.5879144668579102
      },
      "step 368": {
        "loss": 103.59695434570312,
        "l1_loss": 289.9799499511719,
        "l2_loss": 91.93919372558594,
        "ortho_loss": 0.585638165473938
      },
      "step 369": {
        "loss": 108.27225494384766,
        "l1_loss": 347.1217346191406,
        "l2_loss": 94.32954406738281,
        "ortho_loss": 0.5783780813217163
      },
      "step 370": {
        "loss": 106.05513000488281,
        "l1_loss": 317.4670104980469,
        "l2_loss": 93.2981948852539,
        "ortho_loss": 0.5826042294502258
      },
      "step 371": {
        "loss": 104.69312286376953,
        "l1_loss": 304.12518310546875,
        "l2_loss": 92.4697265625,
        "ortho_loss": 0.5838702321052551
      },
      "step 372": {
        "loss": 110.15363311767578,
        "l1_loss": 380.2131042480469,
        "l2_loss": 94.88745880126953,
        "ortho_loss": 0.5765133500099182
      },
      "step 373": {
        "loss": 103.70774841308594,
        "l1_loss": 273.03363037109375,
        "l2_loss": 92.7272720336914,
        "ortho_loss": 0.5912455916404724
      },
      "step 374": {
        "loss": 104.56587219238281,
        "l1_loss": 301.1997375488281,
        "l2_loss": 92.45928955078125,
        "ortho_loss": 0.5859168171882629
      },
      "step 375": {
        "loss": 101.2437515258789,
        "l1_loss": 236.3695526123047,
        "l2_loss": 91.72865295410156,
        "ortho_loss": 0.6031734943389893
      },
      "step 376": {
        "loss": 101.61962890625,
        "l1_loss": 236.6905059814453,
        "l2_loss": 92.0916976928711,
        "ortho_loss": 0.6031323075294495
      },
      "step 377": {
        "loss": 107.20099639892578,
        "l1_loss": 336.085693359375,
        "l2_loss": 93.69912719726562,
        "ortho_loss": 0.5843738317489624
      },
      "step 378": {
        "loss": 104.16474914550781,
        "l1_loss": 289.14556884765625,
        "l2_loss": 92.53982543945312,
        "ortho_loss": 0.590980589389801
      },
      "step 379": {
        "loss": 100.44751739501953,
        "l1_loss": 228.0684814453125,
        "l2_loss": 91.26435852050781,
        "ortho_loss": 0.6041469573974609
      },
      "step 380": {
        "loss": 107.28144073486328,
        "l1_loss": 342.8775634765625,
        "l2_loss": 93.50780487060547,
        "ortho_loss": 0.5853040218353271
      },
      "step 381": {
        "loss": 104.31047821044922,
        "l1_loss": 274.98651123046875,
        "l2_loss": 93.25154113769531,
        "ortho_loss": 0.5947845578193665
      },
      "step 382": {
        "loss": 101.33984375,
        "l1_loss": 237.7503204345703,
        "l2_loss": 91.76945495605469,
        "ortho_loss": 0.6037588119506836
      },
      "step 383": {
        "loss": 100.52881622314453,
        "l1_loss": 226.79525756835938,
        "l2_loss": 91.39631652832031,
        "ortho_loss": 0.6069445610046387
      },
      "step 384": {
        "loss": 101.76681518554688,
        "l1_loss": 243.57687377929688,
        "l2_loss": 91.96340942382812,
        "ortho_loss": 0.6032949090003967
      },
      "step 385": {
        "loss": 107.49263000488281,
        "l1_loss": 337.89031982421875,
        "l2_loss": 93.9181900024414,
        "ortho_loss": 0.5883097648620605
      },
      "step 386": {
        "loss": 104.06668853759766,
        "l1_loss": 283.07940673828125,
        "l2_loss": 92.68366241455078,
        "ortho_loss": 0.5985283851623535
      },
      "step 387": {
        "loss": 103.40658569335938,
        "l1_loss": 264.5723571777344,
        "l2_loss": 92.76350402832031,
        "ortho_loss": 0.6018542051315308
      },
      "step 388": {
        "loss": 106.6243896484375,
        "l1_loss": 324.2968444824219,
        "l2_loss": 93.59322357177734,
        "ortho_loss": 0.5929746627807617
      },
      "step 389": {
        "loss": 103.89134216308594,
        "l1_loss": 277.1166687011719,
        "l2_loss": 92.74677276611328,
        "ortho_loss": 0.5990884304046631
      },
      "step 390": {
        "loss": 105.49018096923828,
        "l1_loss": 302.427734375,
        "l2_loss": 93.33355712890625,
        "ortho_loss": 0.5951432585716248
      },
      "step 391": {
        "loss": 101.14151000976562,
        "l1_loss": 241.33114624023438,
        "l2_loss": 91.427490234375,
        "ortho_loss": 0.6077259182929993
      },
      "step 392": {
        "loss": 103.54102325439453,
        "l1_loss": 282.52972412109375,
        "l2_loss": 92.17984008789062,
        "ortho_loss": 0.599966287612915
      },
      "step 393": {
        "loss": 103.76488494873047,
        "l1_loss": 276.64166259765625,
        "l2_loss": 92.63893127441406,
        "ortho_loss": 0.6028860211372375
      },
      "step 394": {
        "loss": 105.72294616699219,
        "l1_loss": 312.5706787109375,
        "l2_loss": 93.16022491455078,
        "ortho_loss": 0.5988918542861938
      },
      "step 395": {
        "loss": 101.99829864501953,
        "l1_loss": 255.28573608398438,
        "l2_loss": 91.72607421875,
        "ortho_loss": 0.6079933047294617
      },
      "step 396": {
        "loss": 101.75137329101562,
        "l1_loss": 251.9406280517578,
        "l2_loss": 91.61298370361328,
        "ortho_loss": 0.6076154708862305
      },
      "step 397": {
        "loss": 101.36870574951172,
        "l1_loss": 233.9183349609375,
        "l2_loss": 91.95069885253906,
        "ortho_loss": 0.6127163171768188
      },
      "step 398": {
        "loss": 105.7143783569336,
        "l1_loss": 322.0922546386719,
        "l2_loss": 92.77079772949219,
        "ortho_loss": 0.5988905429840088
      },
      "step 399": {
        "loss": 103.07620239257812,
        "l1_loss": 279.34014892578125,
        "l2_loss": 91.84197235107422,
        "ortho_loss": 0.6061937808990479
      },
      "step 400": {
        "loss": 102.62842559814453,
        "l1_loss": 258.5754699707031,
        "l2_loss": 92.22441101074219,
        "ortho_loss": 0.6099696755409241
      },
      "step 401": {
        "loss": 103.38864135742188,
        "l1_loss": 277.6707763671875,
        "l2_loss": 92.22105407714844,
        "ortho_loss": 0.6075807213783264
      },
      "step 402": {
        "loss": 101.56472778320312,
        "l1_loss": 255.8026123046875,
        "l2_loss": 91.27156066894531,
        "ortho_loss": 0.610662043094635
      },
      "step 403": {
        "loss": 104.30488586425781,
        "l1_loss": 293.3712158203125,
        "l2_loss": 92.50935363769531,
        "ortho_loss": 0.6068100929260254
      },
      "step 404": {
        "loss": 101.96281433105469,
        "l1_loss": 253.1063995361328,
        "l2_loss": 91.77729034423828,
        "ortho_loss": 0.6126527786254883
      },
      "step 405": {
        "loss": 105.5221176147461,
        "l1_loss": 320.872314453125,
        "l2_loss": 92.62657165527344,
        "ortho_loss": 0.6065020561218262
      },
      "step 406": {
        "loss": 105.91302490234375,
        "l1_loss": 326.0823669433594,
        "l2_loss": 92.80929565429688,
        "ortho_loss": 0.6043571829795837
      },
      "step 407": {
        "loss": 105.42755126953125,
        "l1_loss": 315.6655578613281,
        "l2_loss": 92.74018096923828,
        "ortho_loss": 0.607466459274292
      }
    },
    "config": {
      "trainer_class": "CustomTrainer",
      "activation_dim": 2304,
      "dict_size": 2304,
      "lr": 0.0003,
      "l1_penalty": 0.04,
      "warmup_steps": 1000,
      "resample_steps": null,
      "device": "cuda",
      "layer": 5,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "CustomTrainer",
      "submodule_name": "resid_post_layer_5"
    },
    "final_info": {
      "training_steps": 488,
      "final_loss": 94.70309448242188,
      "layer": 5,
      "dict_size": 2304,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "7894d753-6700-4d71-b2d3-ffd3262c4c02",
    "datetime_epoch_millis": 1737876076237,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.006710192260088017,
        "mean_num_split_features": 1.4230769230769231
      }
    },
    "eval_result_details": [
      {
        "first_letter": "a",
        "absorption_rate": 0.0007671653241273494,
        "num_absorption": 2,
        "num_probe_true_positives": 2607,
        "num_split_features": 3
      },
      {
        "first_letter": "b",
        "absorption_rate": 0.01322418136020151,
        "num_absorption": 21,
        "num_probe_true_positives": 1588,
        "num_split_features": 2
      },
      {
        "first_letter": "c",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 2851,
        "num_split_features": 3
      },
      {
        "first_letter": "d",
        "absorption_rate": 0.012582384661473937,
        "num_absorption": 21,
        "num_probe_true_positives": 1669,
        "num_split_features": 1
      },
      {
        "first_letter": "e",
        "absorption_rate": 0.0018656716417910447,
        "num_absorption": 3,
        "num_probe_true_positives": 1608,
        "num_split_features": 1
      },
      {
        "first_letter": "f",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1255,
        "num_split_features": 2
      },
      {
        "first_letter": "g",
        "absorption_rate": 0.004642525533890436,
        "num_absorption": 5,
        "num_probe_true_positives": 1077,
        "num_split_features": 1
      },
      {
        "first_letter": "h",
        "absorption_rate": 0.006349206349206349,
        "num_absorption": 6,
        "num_probe_true_positives": 945,
        "num_split_features": 1
      },
      {
        "first_letter": "i",
        "absorption_rate": 0.000612369871402327,
        "num_absorption": 1,
        "num_probe_true_positives": 1633,
        "num_split_features": 2
      },
      {
        "first_letter": "j",
        "absorption_rate": 0.009174311926605505,
        "num_absorption": 4,
        "num_probe_true_positives": 436,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.005934718100890208,
        "num_absorption": 4,
        "num_probe_true_positives": 674,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.002437043054427295,
        "num_absorption": 3,
        "num_probe_true_positives": 1231,
        "num_split_features": 2
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.009836065573770493,
        "num_absorption": 18,
        "num_probe_true_positives": 1830,
        "num_split_features": 1
      },
      {
        "first_letter": "n",
        "absorption_rate": 0.0035335689045936395,
        "num_absorption": 3,
        "num_probe_true_positives": 849,
        "num_split_features": 1
      },
      {
        "first_letter": "o",
        "absorption_rate": 0.029906542056074768,
        "num_absorption": 32,
        "num_probe_true_positives": 1070,
        "num_split_features": 1
      },
      {
        "first_letter": "p",
        "absorption_rate": 0.000423908435777872,
        "num_absorption": 1,
        "num_probe_true_positives": 2359,
        "num_split_features": 3
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.01092896174863388,
        "num_absorption": 2,
        "num_probe_true_positives": 183,
        "num_split_features": 1
      },
      {
        "first_letter": "r",
        "absorption_rate": 0.008860011813349085,
        "num_absorption": 15,
        "num_probe_true_positives": 1693,
        "num_split_features": 1
      },
      {
        "first_letter": "s",
        "absorption_rate": 0.0010504201680672268,
        "num_absorption": 3,
        "num_probe_true_positives": 2856,
        "num_split_features": 2
      },
      {
        "first_letter": "t",
        "absorption_rate": 0.007597895967270602,
        "num_absorption": 13,
        "num_probe_true_positives": 1711,
        "num_split_features": 1
      },
      {
        "first_letter": "u",
        "absorption_rate": 0.014492753623188406,
        "num_absorption": 11,
        "num_probe_true_positives": 759,
        "num_split_features": 1
      },
      {
        "first_letter": "v",
        "absorption_rate": 0.005820721769499418,
        "num_absorption": 5,
        "num_probe_true_positives": 859,
        "num_split_features": 1
      },
      {
        "first_letter": "w",
        "absorption_rate": 0.004531722054380665,
        "num_absorption": 3,
        "num_probe_true_positives": 662,
        "num_split_features": 1
      },
      {
        "first_letter": "x",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 99,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.012345679012345678,
        "num_absorption": 2,
        "num_probe_true_positives": 162,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.007547169811320755,
        "num_absorption": 2,
        "num_probe_true_positives": 265,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_5_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_5_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.2857142857142857,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 7.1875
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.26973684210526316,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 9.875,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": -0.26171875,
        "mse": 3.5,
        "cossim": 0.4140625
      },
      "shrinkage": {
        "l2_norm_in": 90.0,
        "l2_norm_out": 23.0,
        "l2_ratio": 0.259765625,
        "relative_reconstruction_bias": 0.63671875
      },
      "sparsity": {
        "l0": 108.94010925292969,
        "l1": 133.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr and tpp evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "5e550e3c-fd98-4038-a107-2ef3cdfe89bf",
    "datetime_epoch_millis": 1737876399856,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": -0.017664019645518638,
        "scr_metric_threshold_2": 0.007199029075918071,
        "scr_dir2_threshold_2": 0.007199029075918071,
        "scr_dir1_threshold_5": -0.059708565358126305,
        "scr_metric_threshold_5": 0.009282916965858567,
        "scr_dir2_threshold_5": 0.009282916965858567,
        "scr_dir1_threshold_10": -0.07501989855093254,
        "scr_metric_threshold_10": 0.014098346076795474,
        "scr_dir2_threshold_10": 0.014098346076795474,
        "scr_dir1_threshold_20": -0.1331128525177033,
        "scr_metric_threshold_20": 0.01570339920888572,
        "scr_dir2_threshold_20": 0.01570339920888572,
        "scr_dir1_threshold_50": -0.11593241603753465,
        "scr_metric_threshold_50": 0.01996696619854271,
        "scr_dir2_threshold_50": 0.01996696619854271,
        "scr_dir1_threshold_100": -0.13033023722283257,
        "scr_metric_threshold_100": 0.01775347394161156,
        "scr_dir2_threshold_100": 0.01775347394161156,
        "scr_dir1_threshold_500": -0.235965463937829,
        "scr_metric_threshold_500": 0.016240925473789476,
        "scr_dir2_threshold_500": 0.016240925473789476
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": -0.3103451110805095,
        "scr_metric_threshold_2": 0.018779380822471343,
        "scr_dir2_threshold_2": 0.018779380822471343,
        "scr_dir1_threshold_5": -0.37930977783898107,
        "scr_metric_threshold_5": 0.021126768446028555,
        "scr_dir2_threshold_5": 0.021126768446028555,
        "scr_dir1_threshold_10": -0.37930977783898107,
        "scr_metric_threshold_10": 0.018779380822471343,
        "scr_dir2_threshold_10": 0.018779380822471343,
        "scr_dir1_threshold_20": -0.8275862777701274,
        "scr_metric_threshold_20": 0.021126768446028555,
        "scr_dir2_threshold_20": 0.021126768446028555,
        "scr_dir1_threshold_50": -0.9655176666207642,
        "scr_metric_threshold_50": 0.0563380025504072,
        "scr_dir2_threshold_50": 0.0563380025504072,
        "scr_dir1_threshold_100": -1.0689667220921653,
        "scr_metric_threshold_100": 0.06103291771452845,
        "scr_dir2_threshold_100": 0.06103291771452845,
        "scr_dir1_threshold_500": -1.72413927763242,
        "scr_metric_threshold_500": 0.07511738337287854,
        "scr_dir2_threshold_500": 0.07511738337287854
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": 0.029850374486354535,
        "scr_metric_threshold_2": 0.0,
        "scr_dir2_threshold_2": 0.0,
        "scr_dir1_threshold_5": -0.34328375470276395,
        "scr_metric_threshold_5": -0.005181435159484898,
        "scr_dir2_threshold_5": -0.005181435159484898,
        "scr_dir1_threshold_10": -0.41791058054058766,
        "scr_metric_threshold_10": 0.010362715902782657,
        "scr_dir2_threshold_10": 0.010362715902782657,
        "scr_dir1_threshold_20": -0.46268703189205684,
        "scr_metric_threshold_20": -0.002590640371648879,
        "scr_dir2_threshold_20": -0.002590640371648879,
        "scr_dir1_threshold_50": -0.20895529027029383,
        "scr_metric_threshold_50": 0.005181280743297758,
        "scr_dir2_threshold_50": 0.005181280743297758,
        "scr_dir1_threshold_100": -0.13432846443247012,
        "scr_metric_threshold_100": 0.002590640371648879,
        "scr_dir2_threshold_100": 0.002590640371648879,
        "scr_dir1_threshold_500": -0.20895529027029383,
        "scr_metric_threshold_500": 0.005181280743297758,
        "scr_dir2_threshold_500": 0.005181280743297758
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.023256877745449607,
        "scr_metric_threshold_2": -0.005089144583879204,
        "scr_dir2_threshold_2": -0.005089144583879204,
        "scr_dir1_threshold_5": -0.023255491592288,
        "scr_metric_threshold_5": -0.005089144583879204,
        "scr_dir2_threshold_5": -0.005089144583879204,
        "scr_dir1_threshold_10": -0.069766474776864,
        "scr_metric_threshold_10": -0.005089144583879204,
        "scr_dir2_threshold_10": -0.005089144583879204,
        "scr_dir1_threshold_20": -0.0930233525223136,
        "scr_metric_threshold_20": -0.002544648124819707,
        "scr_dir2_threshold_20": -0.002544648124819707,
        "scr_dir1_threshold_50": -0.046510983184576,
        "scr_metric_threshold_50": -0.005089144583879204,
        "scr_dir2_threshold_50": -0.005089144583879204,
        "scr_dir1_threshold_100": -0.046510983184576,
        "scr_metric_threshold_100": 0.0,
        "scr_dir2_threshold_100": 0.0,
        "scr_dir1_threshold_500": -0.1162788441146016,
        "scr_metric_threshold_500": 0.010178137501998197,
        "scr_dir2_threshold_500": 0.010178137501998197
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": -0.01234551548800365,
        "scr_metric_threshold_2": -0.03225796631216327,
        "scr_dir2_threshold_2": -0.03225796631216327,
        "scr_dir1_threshold_5": 0.1728394244106685,
        "scr_metric_threshold_5": 0.0,
        "scr_dir2_threshold_5": 0.0,
        "scr_dir1_threshold_10": 0.20987670673421857,
        "scr_metric_threshold_10": 0.0,
        "scr_dir2_threshold_10": 0.0,
        "scr_dir1_threshold_20": 0.23456773771022588,
        "scr_metric_threshold_20": 0.0,
        "scr_dir2_threshold_20": 0.0,
        "scr_dir1_threshold_50": 0.2222222222222222,
        "scr_metric_threshold_50": 0.005376434537042601,
        "scr_dir2_threshold_50": 0.005376434537042601,
        "scr_dir1_threshold_100": 0.20987670673421857,
        "scr_metric_threshold_100": -0.008064411464279277,
        "scr_dir2_threshold_100": -0.008064411464279277,
        "scr_dir1_threshold_500": 0.2222222222222222,
        "scr_metric_threshold_500": -0.04301067515872539,
        "scr_dir2_threshold_500": -0.04301067515872539
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": 0.04571437524288891,
        "scr_metric_threshold_2": 0.05936078776117859,
        "scr_dir2_threshold_2": 0.05936078776117859,
        "scr_dir1_threshold_5": 0.04571437524288891,
        "scr_metric_threshold_5": 0.06392693939649612,
        "scr_dir2_threshold_5": 0.06392693939649612,
        "scr_dir1_threshold_10": 0.051428586998763286,
        "scr_metric_threshold_10": 0.07305924266713117,
        "scr_dir2_threshold_10": 0.07305924266713117,
        "scr_dir1_threshold_20": 0.04571437524288891,
        "scr_metric_threshold_20": 0.06849309103181364,
        "scr_dir2_threshold_20": 0.06849309103181364,
        "scr_dir1_threshold_50": 0.04571437524288891,
        "scr_metric_threshold_50": 0.04566206068787576,
        "scr_dir2_threshold_50": 0.04566206068787576,
        "scr_dir1_threshold_100": 0.04571437524288891,
        "scr_metric_threshold_100": 0.06849309103181364,
        "scr_dir2_threshold_100": 0.06849309103181364,
        "scr_dir1_threshold_500": 0.04000016348701453,
        "scr_metric_threshold_500": 0.04566206068787576,
        "scr_dir2_threshold_500": 0.04566206068787576
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.007692208939046454,
        "scr_metric_threshold_2": -0.0323887323562997,
        "scr_dir2_threshold_2": -0.0323887323562997,
        "scr_dir1_threshold_5": -0.007692208939046454,
        "scr_metric_threshold_5": -0.028340201140355195,
        "scr_dir2_threshold_5": -0.028340201140355195,
        "scr_dir1_threshold_10": 0.015384417878092908,
        "scr_metric_threshold_10": -0.012145834962205343,
        "scr_dir2_threshold_10": -0.012145834962205343,
        "scr_dir1_threshold_20": 0.015384417878092908,
        "scr_metric_threshold_20": 0.004048531215944509,
        "scr_dir2_threshold_20": 0.004048531215944509,
        "scr_dir1_threshold_50": 0.030769294253470136,
        "scr_metric_threshold_50": -0.01619436617814985,
        "scr_dir2_threshold_50": -0.01619436617814985,
        "scr_dir1_threshold_100": 0.053845921070609495,
        "scr_metric_threshold_100": -0.04048603610256054,
        "scr_dir2_threshold_100": -0.04048603610256054,
        "scr_dir1_threshold_500": 0.046153712131563045,
        "scr_metric_threshold_500": -0.0323887323562997,
        "scr_dir2_threshold_500": -0.0323887323562997
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.08522750170939969,
        "scr_metric_threshold_2": 0.03418809950102767,
        "scr_dir2_threshold_2": 0.03418809950102767,
        "scr_dir1_threshold_5": 0.06250019049773588,
        "scr_metric_threshold_5": 0.012820600993054022,
        "scr_dir2_threshold_5": 0.012820600993054022,
        "scr_dir1_threshold_10": 0.005682081799897114,
        "scr_metric_threshold_10": 0.012820600993054022,
        "scr_dir2_threshold_10": 0.012820600993054022,
        "scr_dir1_threshold_20": 0.02272731121166381,
        "scr_metric_threshold_20": 0.017094049750513835,
        "scr_dir2_threshold_20": 0.017094049750513835,
        "scr_dir1_threshold_50": 0.0,
        "scr_metric_threshold_50": 0.03846154825848749,
        "scr_dir2_threshold_50": 0.03846154825848749,
        "scr_dir1_threshold_100": -0.10227273112116639,
        "scr_metric_threshold_100": 0.03846154825848749,
        "scr_dir2_threshold_100": 0.03846154825848749,
        "scr_dir1_threshold_500": -0.13636352860734133,
        "scr_metric_threshold_500": 0.03418809950102767,
        "scr_dir2_threshold_500": 0.03418809950102767
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": -0.010362868718775122,
        "scr_metric_threshold_2": 0.014999807775009142,
        "scr_dir2_threshold_2": 0.014999807775009142,
        "scr_dir1_threshold_5": -0.005181279943224265,
        "scr_metric_threshold_5": 0.014999807775009142,
        "scr_dir2_threshold_5": 0.014999807775009142,
        "scr_dir1_threshold_10": -0.015544148661999387,
        "scr_metric_threshold_10": 0.014999807775009142,
        "scr_dir2_threshold_10": 0.014999807775009142,
        "scr_dir1_threshold_20": 0.0,
        "scr_metric_threshold_20": 0.020000041723253828,
        "scr_dir2_threshold_20": 0.020000041723253828,
        "scr_dir1_threshold_50": -0.005181279943224265,
        "scr_metric_threshold_50": 0.029999913573259925,
        "scr_dir2_threshold_50": 0.029999913573259925,
        "scr_dir1_threshold_100": 0.0,
        "scr_metric_threshold_100": 0.020000041723253828,
        "scr_dir2_threshold_100": 0.020000041723253828,
        "scr_dir1_threshold_500": -0.010362868718775122,
        "scr_metric_threshold_500": 0.03499984949826297,
        "scr_dir2_threshold_500": 0.03499984949826297
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "a98bd501-c238-4dae-98de-160083422918",
    "datetime_epoch_millis": 1737876448282,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.940025,
        "llm_top_1_test_accuracy": 0.67890625,
        "llm_top_2_test_accuracy": 0.72346875,
        "llm_top_5_test_accuracy": 0.77423125,
        "llm_top_10_test_accuracy": 0.8231812500000001,
        "llm_top_20_test_accuracy": 0.8601937500000001,
        "llm_top_50_test_accuracy": 0.8994062500000001,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9330437909811736,
        "sae_top_1_test_accuracy": 0.6380125000000001,
        "sae_top_2_test_accuracy": 0.67436875,
        "sae_top_5_test_accuracy": 0.726375,
        "sae_top_10_test_accuracy": 0.77071875,
        "sae_top_20_test_accuracy": 0.8153,
        "sae_top_50_test_accuracy": 0.87174375,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.958,
        "llm_top_1_test_accuracy": 0.6641999999999999,
        "llm_top_2_test_accuracy": 0.6843999999999999,
        "llm_top_5_test_accuracy": 0.7469999999999999,
        "llm_top_10_test_accuracy": 0.8282,
        "llm_top_20_test_accuracy": 0.8606,
        "llm_top_50_test_accuracy": 0.9118,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9470000386238098,
        "sae_top_1_test_accuracy": 0.6460000000000001,
        "sae_top_2_test_accuracy": 0.6796000000000001,
        "sae_top_5_test_accuracy": 0.7314,
        "sae_top_10_test_accuracy": 0.7802,
        "sae_top_20_test_accuracy": 0.8326,
        "sae_top_50_test_accuracy": 0.8906000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9396000000000001,
        "llm_top_1_test_accuracy": 0.6714,
        "llm_top_2_test_accuracy": 0.7222000000000001,
        "llm_top_5_test_accuracy": 0.7607999999999999,
        "llm_top_10_test_accuracy": 0.8038000000000001,
        "llm_top_20_test_accuracy": 0.8455999999999999,
        "llm_top_50_test_accuracy": 0.8880000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9268000364303589,
        "sae_top_1_test_accuracy": 0.669,
        "sae_top_2_test_accuracy": 0.6661999999999999,
        "sae_top_5_test_accuracy": 0.7222,
        "sae_top_10_test_accuracy": 0.7674000000000001,
        "sae_top_20_test_accuracy": 0.7984000000000001,
        "sae_top_50_test_accuracy": 0.8615999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.909,
        "llm_top_1_test_accuracy": 0.6814,
        "llm_top_2_test_accuracy": 0.705,
        "llm_top_5_test_accuracy": 0.751,
        "llm_top_10_test_accuracy": 0.7966,
        "llm_top_20_test_accuracy": 0.8251999999999999,
        "llm_top_50_test_accuracy": 0.8625999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9048000335693359,
        "sae_top_1_test_accuracy": 0.6134000000000001,
        "sae_top_2_test_accuracy": 0.651,
        "sae_top_5_test_accuracy": 0.7030000000000001,
        "sae_top_10_test_accuracy": 0.7442,
        "sae_top_20_test_accuracy": 0.8055999999999999,
        "sae_top_50_test_accuracy": 0.8496,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.8888,
        "llm_top_1_test_accuracy": 0.6068,
        "llm_top_2_test_accuracy": 0.6407999999999999,
        "llm_top_5_test_accuracy": 0.6763999999999999,
        "llm_top_10_test_accuracy": 0.719,
        "llm_top_20_test_accuracy": 0.772,
        "llm_top_50_test_accuracy": 0.8290000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.8760000467300415,
        "sae_top_1_test_accuracy": 0.5886,
        "sae_top_2_test_accuracy": 0.6146,
        "sae_top_5_test_accuracy": 0.6437999999999999,
        "sae_top_10_test_accuracy": 0.6964,
        "sae_top_20_test_accuracy": 0.7356,
        "sae_top_50_test_accuracy": 0.7862,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9245000000000001,
        "llm_top_1_test_accuracy": 0.628,
        "llm_top_2_test_accuracy": 0.686,
        "llm_top_5_test_accuracy": 0.738,
        "llm_top_10_test_accuracy": 0.767,
        "llm_top_20_test_accuracy": 0.8,
        "llm_top_50_test_accuracy": 0.8545,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9155000448226929,
        "sae_top_1_test_accuracy": 0.618,
        "sae_top_2_test_accuracy": 0.677,
        "sae_top_5_test_accuracy": 0.722,
        "sae_top_10_test_accuracy": 0.74,
        "sae_top_20_test_accuracy": 0.755,
        "sae_top_50_test_accuracy": 0.849,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9663999999999999,
        "llm_top_1_test_accuracy": 0.6626,
        "llm_top_2_test_accuracy": 0.706,
        "llm_top_5_test_accuracy": 0.7719999999999999,
        "llm_top_10_test_accuracy": 0.8426,
        "llm_top_20_test_accuracy": 0.8924,
        "llm_top_50_test_accuracy": 0.9362,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9606000423431397,
        "sae_top_1_test_accuracy": 0.6672,
        "sae_top_2_test_accuracy": 0.6928,
        "sae_top_5_test_accuracy": 0.7518,
        "sae_top_10_test_accuracy": 0.7849999999999999,
        "sae_top_20_test_accuracy": 0.8392000000000002,
        "sae_top_50_test_accuracy": 0.8932,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.9345000000000001,
        "llm_top_1_test_accuracy": 0.7082499999999999,
        "llm_top_2_test_accuracy": 0.75075,
        "llm_top_5_test_accuracy": 0.80625,
        "llm_top_10_test_accuracy": 0.8482500000000001,
        "llm_top_20_test_accuracy": 0.89275,
        "llm_top_50_test_accuracy": 0.91475,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9342500418424606,
        "sae_top_1_test_accuracy": 0.6275,
        "sae_top_2_test_accuracy": 0.67775,
        "sae_top_5_test_accuracy": 0.729,
        "sae_top_10_test_accuracy": 0.7797499999999999,
        "sae_top_20_test_accuracy": 0.8190000000000001,
        "sae_top_50_test_accuracy": 0.87275,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9994,
        "llm_top_1_test_accuracy": 0.8086,
        "llm_top_2_test_accuracy": 0.8925999999999998,
        "llm_top_5_test_accuracy": 0.9423999999999999,
        "llm_top_10_test_accuracy": 0.9800000000000001,
        "llm_top_20_test_accuracy": 0.993,
        "llm_top_50_test_accuracy": 0.9984,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9994000434875489,
        "sae_top_1_test_accuracy": 0.6744,
        "sae_top_2_test_accuracy": 0.736,
        "sae_top_5_test_accuracy": 0.8078000000000001,
        "sae_top_10_test_accuracy": 0.8728,
        "sae_top_20_test_accuracy": 0.937,
        "sae_top_50_test_accuracy": 0.9709999999999999,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  }
}