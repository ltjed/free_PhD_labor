{
  "training results": {
    "training_log": {
      "step 0": {
        "loss": 280.3907165527344,
        "l1_loss": 2692.192626953125,
        "l2_loss": 172.6608123779297,
        "ortho_loss": 0.4221651554107666
      },
      "step 1": {
        "loss": 243.6199188232422,
        "l1_loss": 2632.935546875,
        "l2_loss": 138.26095581054688,
        "ortho_loss": 0.4154869616031647
      },
      "step 2": {
        "loss": 249.9102020263672,
        "l1_loss": 2702.074462890625,
        "l2_loss": 141.78500366210938,
        "ortho_loss": 0.42227187752723694
      },
      "step 3": {
        "loss": 249.84877014160156,
        "l1_loss": 2702.375244140625,
        "l2_loss": 141.71153259277344,
        "ortho_loss": 0.4222050607204437
      },
      "step 4": {
        "loss": 252.69287109375,
        "l1_loss": 2733.424560546875,
        "l2_loss": 143.3136749267578,
        "ortho_loss": 0.42206963896751404
      },
      "step 5": {
        "loss": 244.94029235839844,
        "l1_loss": 2648.0673828125,
        "l2_loss": 138.97555541992188,
        "ortho_loss": 0.42033711075782776
      },
      "step 6": {
        "loss": 247.5320281982422,
        "l1_loss": 2678.136962890625,
        "l2_loss": 140.36439514160156,
        "ortho_loss": 0.4215734302997589
      },
      "step 7": {
        "loss": 250.36607360839844,
        "l1_loss": 2709.99560546875,
        "l2_loss": 141.92401123046875,
        "ortho_loss": 0.4225885272026062
      },
      "step 8": {
        "loss": 247.73471069335938,
        "l1_loss": 2682.0224609375,
        "l2_loss": 140.41152954101562,
        "ortho_loss": 0.4228545129299164
      },
      "step 9": {
        "loss": 246.87814331054688,
        "l1_loss": 2673.4326171875,
        "l2_loss": 139.89877319335938,
        "ortho_loss": 0.4206143915653229
      },
      "step 10": {
        "loss": 245.63623046875,
        "l1_loss": 2660.15087890625,
        "l2_loss": 139.1881866455078,
        "ortho_loss": 0.4201294481754303
      },
      "step 11": {
        "loss": 250.17922973632812,
        "l1_loss": 2711.39306640625,
        "l2_loss": 141.68130493164062,
        "ortho_loss": 0.42208826541900635
      },
      "step 12": {
        "loss": 247.0255126953125,
        "l1_loss": 2678.1611328125,
        "l2_loss": 139.85678100585938,
        "ortho_loss": 0.4230251610279083
      },
      "step 13": {
        "loss": 242.2106170654297,
        "l1_loss": 2626.711669921875,
        "l2_loss": 137.10018920898438,
        "ortho_loss": 0.41954585909843445
      },
      "step 14": {
        "loss": 249.42823791503906,
        "l1_loss": 2708.19580078125,
        "l2_loss": 141.0580596923828,
        "ortho_loss": 0.42348456382751465
      },
      "step 15": {
        "loss": 240.16993713378906,
        "l1_loss": 2606.272705078125,
        "l2_loss": 135.87765502929688,
        "ortho_loss": 0.4137392044067383
      },
      "step 16": {
        "loss": 247.02056884765625,
        "l1_loss": 2684.57568359375,
        "l2_loss": 139.59527587890625,
        "ortho_loss": 0.422726035118103
      },
      "step 17": {
        "loss": 246.485107421875,
        "l1_loss": 2679.7392578125,
        "l2_loss": 139.25314331054688,
        "ortho_loss": 0.4238564670085907
      },
      "step 18": {
        "loss": 252.99322509765625,
        "l1_loss": 2753.6337890625,
        "l2_loss": 142.80564880371094,
        "ortho_loss": 0.4223962426185608
      },
      "step 19": {
        "loss": 237.98654174804688,
        "l1_loss": 2588.939453125,
        "l2_loss": 134.38735961914062,
        "ortho_loss": 0.41590991616249084
      },
      "step 20": {
        "loss": 243.49708557128906,
        "l1_loss": 2651.2138671875,
        "l2_loss": 137.4064178466797,
        "ortho_loss": 0.42121854424476624
      },
      "step 21": {
        "loss": 238.7278594970703,
        "l1_loss": 2601.4892578125,
        "l2_loss": 134.62606811523438,
        "ortho_loss": 0.42215582728385925
      },
      "step 22": {
        "loss": 244.75865173339844,
        "l1_loss": 2670.92236328125,
        "l2_loss": 137.87945556640625,
        "ortho_loss": 0.42313385009765625
      },
      "step 23": {
        "loss": 240.25843811035156,
        "l1_loss": 2623.8896484375,
        "l2_loss": 135.26055908203125,
        "ortho_loss": 0.4230283200740814
      },
      "step 24": {
        "loss": 238.20726013183594,
        "l1_loss": 2600.95654296875,
        "l2_loss": 134.12692260742188,
        "ortho_loss": 0.42066049575805664
      },
      "step 25": {
        "loss": 236.88377380371094,
        "l1_loss": 2589.65869140625,
        "l2_loss": 133.25531005859375,
        "ortho_loss": 0.4213530719280243
      },
      "step 26": {
        "loss": 232.59762573242188,
        "l1_loss": 2542.3388671875,
        "l2_loss": 130.8629913330078,
        "ortho_loss": 0.41070330142974854
      },
      "step 27": {
        "loss": 237.78895568847656,
        "l1_loss": 2601.419921875,
        "l2_loss": 133.68984985351562,
        "ortho_loss": 0.42319947481155396
      },
      "step 28": {
        "loss": 237.54669189453125,
        "l1_loss": 2602.43994140625,
        "l2_loss": 133.4066162109375,
        "ortho_loss": 0.4248531460762024
      },
      "step 29": {
        "loss": 242.7250518798828,
        "l1_loss": 2662.43212890625,
        "l2_loss": 136.1852569580078,
        "ortho_loss": 0.42518317699432373
      },
      "step 30": {
        "loss": 245.0502166748047,
        "l1_loss": 2690.788818359375,
        "l2_loss": 137.3761444091797,
        "ortho_loss": 0.4252746105194092
      },
      "step 31": {
        "loss": 240.6953887939453,
        "l1_loss": 2642.8017578125,
        "l2_loss": 134.94058227539062,
        "ortho_loss": 0.4274114966392517
      },
      "step 32": {
        "loss": 230.78932189941406,
        "l1_loss": 2533.22412109375,
        "l2_loss": 129.4184112548828,
        "ortho_loss": 0.4194837510585785
      },
      "step 33": {
        "loss": 237.12591552734375,
        "l1_loss": 2607.98388671875,
        "l2_loss": 132.76385498046875,
        "ortho_loss": 0.42710161209106445
      },
      "step 34": {
        "loss": 244.72402954101562,
        "l1_loss": 2695.02978515625,
        "l2_loss": 136.8802490234375,
        "ortho_loss": 0.42598381638526917
      },
      "step 35": {
        "loss": 239.049072265625,
        "l1_loss": 2633.6142578125,
        "l2_loss": 133.6617431640625,
        "ortho_loss": 0.42756006121635437
      },
      "step 36": {
        "loss": 234.5260009765625,
        "l1_loss": 2585.570556640625,
        "l2_loss": 131.06051635742188,
        "ortho_loss": 0.4266640245914459
      },
      "step 37": {
        "loss": 238.42381286621094,
        "l1_loss": 2630.92578125,
        "l2_loss": 133.14395141601562,
        "ortho_loss": 0.42832639813423157
      },
      "step 38": {
        "loss": 235.66920471191406,
        "l1_loss": 2602.45556640625,
        "l2_loss": 131.52813720703125,
        "ortho_loss": 0.42842307686805725
      },
      "step 39": {
        "loss": 228.67071533203125,
        "l1_loss": 2522.71484375,
        "l2_loss": 127.7196044921875,
        "ortho_loss": 0.4251791536808014
      },
      "step 40": {
        "loss": 233.1425018310547,
        "l1_loss": 2578.728515625,
        "l2_loss": 129.950439453125,
        "ortho_loss": 0.42930448055267334
      },
      "step 41": {
        "loss": 238.63539123535156,
        "l1_loss": 2643.2177734375,
        "l2_loss": 132.86395263671875,
        "ortho_loss": 0.4274059236049652
      },
      "step 42": {
        "loss": 234.51174926757812,
        "l1_loss": 2595.01513671875,
        "l2_loss": 130.6681671142578,
        "ortho_loss": 0.42975929379463196
      },
      "step 43": {
        "loss": 232.88479614257812,
        "l1_loss": 2577.80712890625,
        "l2_loss": 129.7294464111328,
        "ortho_loss": 0.4305441677570343
      },
      "step 44": {
        "loss": 228.0623779296875,
        "l1_loss": 2527.269287109375,
        "l2_loss": 126.92855072021484,
        "ortho_loss": 0.43066808581352234
      },
      "step 45": {
        "loss": 228.65684509277344,
        "l1_loss": 2532.591064453125,
        "l2_loss": 127.31005859375,
        "ortho_loss": 0.4313367009162903
      },
      "step 46": {
        "loss": 223.45237731933594,
        "l1_loss": 2473.06494140625,
        "l2_loss": 124.48709106445312,
        "ortho_loss": 0.42696455121040344
      },
      "step 47": {
        "loss": 226.31729125976562,
        "l1_loss": 2510.07470703125,
        "l2_loss": 125.87115478515625,
        "ortho_loss": 0.431574285030365
      },
      "step 48": {
        "loss": 232.82266235351562,
        "l1_loss": 2586.459228515625,
        "l2_loss": 129.32110595703125,
        "ortho_loss": 0.43174970149993896
      },
      "step 49": {
        "loss": 223.41104125976562,
        "l1_loss": 2478.955078125,
        "l2_loss": 124.209716796875,
        "ortho_loss": 0.43116068840026855
      },
      "step 50": {
        "loss": 221.40957641601562,
        "l1_loss": 2453.626708984375,
        "l2_loss": 123.22134399414062,
        "ortho_loss": 0.4316524267196655
      },
      "step 51": {
        "loss": 220.20208740234375,
        "l1_loss": 2441.0029296875,
        "l2_loss": 122.51902770996094,
        "ortho_loss": 0.4293508231639862
      },
      "step 52": {
        "loss": 221.0068359375,
        "l1_loss": 2450.572021484375,
        "l2_loss": 122.94052124023438,
        "ortho_loss": 0.434211403131485
      },
      "step 53": {
        "loss": 222.6560821533203,
        "l1_loss": 2471.0166015625,
        "l2_loss": 123.77202606201172,
        "ortho_loss": 0.433991938829422
      },
      "step 54": {
        "loss": 225.6357879638672,
        "l1_loss": 2506.218505859375,
        "l2_loss": 125.3435287475586,
        "ortho_loss": 0.43517768383026123
      },
      "step 55": {
        "loss": 216.72190856933594,
        "l1_loss": 2401.363525390625,
        "l2_loss": 120.62411499023438,
        "ortho_loss": 0.43251296877861023
      },
      "step 56": {
        "loss": 225.68772888183594,
        "l1_loss": 2505.92431640625,
        "l2_loss": 125.40718078613281,
        "ortho_loss": 0.4358033537864685
      },
      "step 57": {
        "loss": 220.21844482421875,
        "l1_loss": 2443.62890625,
        "l2_loss": 122.42967224121094,
        "ortho_loss": 0.43608617782592773
      },
      "step 58": {
        "loss": 218.78883361816406,
        "l1_loss": 2424.345947265625,
        "l2_loss": 121.77131652832031,
        "ortho_loss": 0.43692195415496826
      },
      "step 59": {
        "loss": 211.33108520507812,
        "l1_loss": 2338.6591796875,
        "l2_loss": 117.74163818359375,
        "ortho_loss": 0.4309154152870178
      },
      "step 60": {
        "loss": 217.20948791503906,
        "l1_loss": 2408.24853515625,
        "l2_loss": 120.83570861816406,
        "ortho_loss": 0.43835484981536865
      },
      "step 61": {
        "loss": 212.9375,
        "l1_loss": 2355.712158203125,
        "l2_loss": 118.66553497314453,
        "ortho_loss": 0.4349087178707123
      },
      "step 62": {
        "loss": 211.67958068847656,
        "l1_loss": 2341.85302734375,
        "l2_loss": 117.9616928100586,
        "ortho_loss": 0.4377221465110779
      },
      "step 63": {
        "loss": 217.55706787109375,
        "l1_loss": 2410.140625,
        "l2_loss": 121.10752868652344,
        "ortho_loss": 0.43920397758483887
      },
      "step 64": {
        "loss": 210.29234313964844,
        "l1_loss": 2322.2109375,
        "l2_loss": 117.36024475097656,
        "ortho_loss": 0.43660154938697815
      },
      "step 65": {
        "loss": 208.740966796875,
        "l1_loss": 2306.698974609375,
        "l2_loss": 116.42927551269531,
        "ortho_loss": 0.4372701048851013
      },
      "step 66": {
        "loss": 207.96047973632812,
        "l1_loss": 2294.7001953125,
        "l2_loss": 116.12870788574219,
        "ortho_loss": 0.43757960200309753
      },
      "step 67": {
        "loss": 213.3174285888672,
        "l1_loss": 2358.114990234375,
        "l2_loss": 118.94863891601562,
        "ortho_loss": 0.44189801812171936
      },
      "step 68": {
        "loss": 214.48049926757812,
        "l1_loss": 2373.481201171875,
        "l2_loss": 119.49710845947266,
        "ortho_loss": 0.44147640466690063
      },
      "step 69": {
        "loss": 211.53448486328125,
        "l1_loss": 2336.442626953125,
        "l2_loss": 118.032470703125,
        "ortho_loss": 0.4431852400302887
      },
      "step 70": {
        "loss": 208.36285400390625,
        "l1_loss": 2294.0009765625,
        "l2_loss": 116.55853271484375,
        "ortho_loss": 0.4428597390651703
      },
      "step 71": {
        "loss": 216.0298309326172,
        "l1_loss": 2387.291015625,
        "l2_loss": 120.49396514892578,
        "ortho_loss": 0.4423897862434387
      },
      "step 72": {
        "loss": 214.5150146484375,
        "l1_loss": 2364.64306640625,
        "l2_loss": 119.88499450683594,
        "ortho_loss": 0.44290992617607117
      },
      "step 73": {
        "loss": 202.94862365722656,
        "l1_loss": 2224.54833984375,
        "l2_loss": 113.92268371582031,
        "ortho_loss": 0.4401346743106842
      },
      "step 74": {
        "loss": 207.84046936035156,
        "l1_loss": 2283.55712890625,
        "l2_loss": 116.45368194580078,
        "ortho_loss": 0.4451553225517273
      },
      "step 75": {
        "loss": 203.64674377441406,
        "l1_loss": 2231.40283203125,
        "l2_loss": 114.34602355957031,
        "ortho_loss": 0.44597697257995605
      },
      "step 76": {
        "loss": 206.26478576660156,
        "l1_loss": 2259.520263671875,
        "l2_loss": 115.83927917480469,
        "ortho_loss": 0.44698968529701233
      },
      "step 77": {
        "loss": 213.17349243164062,
        "l1_loss": 2346.58349609375,
        "l2_loss": 119.26580810546875,
        "ortho_loss": 0.44341734051704407
      },
      "step 78": {
        "loss": 200.34352111816406,
        "l1_loss": 2182.4296875,
        "l2_loss": 113.00180053710938,
        "ortho_loss": 0.4453415274620056
      },
      "step 79": {
        "loss": 202.51377868652344,
        "l1_loss": 2207.84033203125,
        "l2_loss": 114.15543365478516,
        "ortho_loss": 0.4472654461860657
      },
      "step 80": {
        "loss": 198.74078369140625,
        "l1_loss": 2157.398193359375,
        "l2_loss": 112.40021514892578,
        "ortho_loss": 0.44648051261901855
      },
      "step 81": {
        "loss": 207.14280700683594,
        "l1_loss": 2261.6474609375,
        "l2_loss": 116.63201141357422,
        "ortho_loss": 0.4490000307559967
      },
      "step 82": {
        "loss": 206.31236267089844,
        "l1_loss": 2250.591796875,
        "l2_loss": 116.24374389648438,
        "ortho_loss": 0.4495253264904022
      },
      "step 83": {
        "loss": 200.1201629638672,
        "l1_loss": 2170.81640625,
        "l2_loss": 113.242431640625,
        "ortho_loss": 0.4507293105125427
      },
      "step 84": {
        "loss": 199.4132537841797,
        "l1_loss": 2161.56005859375,
        "l2_loss": 112.90570068359375,
        "ortho_loss": 0.4515187740325928
      },
      "step 85": {
        "loss": 198.15286254882812,
        "l1_loss": 2138.157470703125,
        "l2_loss": 112.58147430419922,
        "ortho_loss": 0.45093363523483276
      },
      "step 86": {
        "loss": 206.85939025878906,
        "l1_loss": 2246.79833984375,
        "l2_loss": 116.94253540039062,
        "ortho_loss": 0.44927701354026794
      },
      "step 87": {
        "loss": 201.562255859375,
        "l1_loss": 2180.5947265625,
        "l2_loss": 114.2931137084961,
        "ortho_loss": 0.45353156328201294
      },
      "step 88": {
        "loss": 193.24342346191406,
        "l1_loss": 2072.8583984375,
        "l2_loss": 110.28399658203125,
        "ortho_loss": 0.4508548974990845
      },
      "step 89": {
        "loss": 198.76034545898438,
        "l1_loss": 2143.705322265625,
        "l2_loss": 112.9667739868164,
        "ortho_loss": 0.45350319147109985
      },
      "step 90": {
        "loss": 196.64743041992188,
        "l1_loss": 2109.17529296875,
        "l2_loss": 112.23486328125,
        "ortho_loss": 0.45563212037086487
      },
      "step 91": {
        "loss": 200.94044494628906,
        "l1_loss": 2162.9130859375,
        "l2_loss": 114.37853240966797,
        "ortho_loss": 0.4540194571018219
      },
      "step 92": {
        "loss": 188.0102996826172,
        "l1_loss": 1994.865478515625,
        "l2_loss": 108.1714859008789,
        "ortho_loss": 0.44205787777900696
      },
      "step 93": {
        "loss": 201.36663818359375,
        "l1_loss": 2163.162109375,
        "l2_loss": 114.79484558105469,
        "ortho_loss": 0.4531361162662506
      },
      "step 94": {
        "loss": 196.97238159179688,
        "l1_loss": 2106.2841796875,
        "l2_loss": 112.67536926269531,
        "ortho_loss": 0.45659780502319336
      },
      "step 95": {
        "loss": 193.13560485839844,
        "l1_loss": 2054.727783203125,
        "l2_loss": 110.9004898071289,
        "ortho_loss": 0.46002620458602905
      },
      "step 96": {
        "loss": 196.22999572753906,
        "l1_loss": 2092.13427734375,
        "l2_loss": 112.49884033203125,
        "ortho_loss": 0.4579812288284302
      },
      "step 97": {
        "loss": 190.82643127441406,
        "l1_loss": 2015.5201416015625,
        "l2_loss": 110.15945434570312,
        "ortho_loss": 0.46174052357673645
      },
      "step 98": {
        "loss": 195.06491088867188,
        "l1_loss": 2069.845703125,
        "l2_loss": 112.22520446777344,
        "ortho_loss": 0.45879045128822327
      },
      "step 99": {
        "loss": 195.7115936279297,
        "l1_loss": 2081.075439453125,
        "l2_loss": 112.4228515625,
        "ortho_loss": 0.45732054114341736
      },
      "step 100": {
        "loss": 188.4266357421875,
        "l1_loss": 1980.8330078125,
        "l2_loss": 109.14692687988281,
        "ortho_loss": 0.4638073742389679
      },
      "step 101": {
        "loss": 185.25730895996094,
        "l1_loss": 1938.6353759765625,
        "l2_loss": 107.66546630859375,
        "ortho_loss": 0.4643803536891937
      },
      "step 102": {
        "loss": 191.5263671875,
        "l1_loss": 2017.140625,
        "l2_loss": 110.7945556640625,
        "ortho_loss": 0.4619317650794983
      },
      "step 103": {
        "loss": 193.43284606933594,
        "l1_loss": 2037.5203857421875,
        "l2_loss": 111.88606262207031,
        "ortho_loss": 0.4596720337867737
      },
      "step 104": {
        "loss": 185.4490203857422,
        "l1_loss": 1929.304931640625,
        "l2_loss": 108.23009490966797,
        "ortho_loss": 0.4673551619052887
      },
      "step 105": {
        "loss": 192.64784240722656,
        "l1_loss": 2019.9097900390625,
        "l2_loss": 111.80533599853516,
        "ortho_loss": 0.4613170027732849
      },
      "step 106": {
        "loss": 191.41522216796875,
        "l1_loss": 2001.9124755859375,
        "l2_loss": 111.2926025390625,
        "ortho_loss": 0.46113473176956177
      },
      "step 107": {
        "loss": 182.83416748046875,
        "l1_loss": 1886.634033203125,
        "l2_loss": 107.32176971435547,
        "ortho_loss": 0.47030147910118103
      },
      "step 108": {
        "loss": 190.34426879882812,
        "l1_loss": 1983.438232421875,
        "l2_loss": 110.96052551269531,
        "ortho_loss": 0.46218159794807434
      },
      "step 109": {
        "loss": 181.0821075439453,
        "l1_loss": 1859.3515625,
        "l2_loss": 106.66081237792969,
        "ortho_loss": 0.4722417891025543
      },
      "step 110": {
        "loss": 193.8629913330078,
        "l1_loss": 2016.13037109375,
        "l2_loss": 113.171875,
        "ortho_loss": 0.45910197496414185
      },
      "step 111": {
        "loss": 187.0230712890625,
        "l1_loss": 1930.8916015625,
        "l2_loss": 109.74075317382812,
        "ortho_loss": 0.46666502952575684
      },
      "step 112": {
        "loss": 185.13858032226562,
        "l1_loss": 1904.1142578125,
        "l2_loss": 108.92720031738281,
        "ortho_loss": 0.46813276410102844
      },
      "step 113": {
        "loss": 181.78363037109375,
        "l1_loss": 1851.32275390625,
        "l2_loss": 107.68331146240234,
        "ortho_loss": 0.4740433692932129
      },
      "step 114": {
        "loss": 177.620849609375,
        "l1_loss": 1795.5615234375,
        "l2_loss": 105.75077819824219,
        "ortho_loss": 0.47608381509780884
      },
      "step 115": {
        "loss": 175.65670776367188,
        "l1_loss": 1763.841064453125,
        "l2_loss": 105.05558776855469,
        "ortho_loss": 0.47480452060699463
      },
      "step 116": {
        "loss": 180.90029907226562,
        "l1_loss": 1833.73046875,
        "l2_loss": 107.50373840332031,
        "ortho_loss": 0.47339674830436707
      },
      "step 117": {
        "loss": 183.50045776367188,
        "l1_loss": 1862.542724609375,
        "l2_loss": 108.95169067382812,
        "ortho_loss": 0.4706282615661621
      },
      "step 118": {
        "loss": 177.0425567626953,
        "l1_loss": 1771.2978515625,
        "l2_loss": 106.14266967773438,
        "ortho_loss": 0.479748010635376
      },
      "step 119": {
        "loss": 179.62283325195312,
        "l1_loss": 1809.9156494140625,
        "l2_loss": 107.17880249023438,
        "ortho_loss": 0.47414156794548035
      },
      "step 120": {
        "loss": 182.36265563964844,
        "l1_loss": 1832.8026123046875,
        "l2_loss": 109.00321960449219,
        "ortho_loss": 0.4733603000640869
      },
      "step 121": {
        "loss": 174.30023193359375,
        "l1_loss": 1729.7158203125,
        "l2_loss": 105.06356811523438,
        "ortho_loss": 0.4803939759731293
      },
      "step 122": {
        "loss": 173.8057098388672,
        "l1_loss": 1720.7518310546875,
        "l2_loss": 104.92745971679688,
        "ortho_loss": 0.48175564408302307
      },
      "step 123": {
        "loss": 175.06236267089844,
        "l1_loss": 1737.162353515625,
        "l2_loss": 105.52792358398438,
        "ortho_loss": 0.4794972836971283
      },
      "step 124": {
        "loss": 180.1365966796875,
        "l1_loss": 1800.689697265625,
        "l2_loss": 108.06187438964844,
        "ortho_loss": 0.4713500738143921
      },
      "step 125": {
        "loss": 181.24832153320312,
        "l1_loss": 1812.016357421875,
        "l2_loss": 108.72067260742188,
        "ortho_loss": 0.4700080156326294
      },
      "step 126": {
        "loss": 171.8125457763672,
        "l1_loss": 1680.128173828125,
        "l2_loss": 104.55902099609375,
        "ortho_loss": 0.48397907614707947
      },
      "step 127": {
        "loss": 175.5006561279297,
        "l1_loss": 1728.0301513671875,
        "l2_loss": 106.33170318603516,
        "ortho_loss": 0.47745221853256226
      },
      "step 128": {
        "loss": 172.7991943359375,
        "l1_loss": 1685.945068359375,
        "l2_loss": 105.31294250488281,
        "ortho_loss": 0.4846021234989166
      },
      "step 129": {
        "loss": 180.8201446533203,
        "l1_loss": 1792.0511474609375,
        "l2_loss": 109.09130096435547,
        "ortho_loss": 0.46794912219047546
      },
      "step 130": {
        "loss": 174.91481018066406,
        "l1_loss": 1709.705322265625,
        "l2_loss": 106.47886657714844,
        "ortho_loss": 0.47728678584098816
      },
      "step 131": {
        "loss": 177.32913208007812,
        "l1_loss": 1733.2626953125,
        "l2_loss": 107.95108032226562,
        "ortho_loss": 0.47550761699676514
      },
      "step 132": {
        "loss": 170.771240234375,
        "l1_loss": 1643.0855712890625,
        "l2_loss": 104.99920654296875,
        "ortho_loss": 0.48620161414146423
      },
      "step 133": {
        "loss": 167.4867706298828,
        "l1_loss": 1598.799072265625,
        "l2_loss": 103.48570251464844,
        "ortho_loss": 0.4909939467906952
      },
      "step 134": {
        "loss": 162.4847412109375,
        "l1_loss": 1533.5032958984375,
        "l2_loss": 101.09527587890625,
        "ortho_loss": 0.4933067858219147
      },
      "step 135": {
        "loss": 164.93637084960938,
        "l1_loss": 1560.591796875,
        "l2_loss": 102.46327209472656,
        "ortho_loss": 0.49428996443748474
      },
      "step 136": {
        "loss": 168.66976928710938,
        "l1_loss": 1603.3812255859375,
        "l2_loss": 104.48568725585938,
        "ortho_loss": 0.4883413016796112
      },
      "step 137": {
        "loss": 171.57908630371094,
        "l1_loss": 1639.169677734375,
        "l2_loss": 105.96420288085938,
        "ortho_loss": 0.4809436500072479
      },
      "step 138": {
        "loss": 164.4706573486328,
        "l1_loss": 1542.1328125,
        "l2_loss": 102.73600006103516,
        "ortho_loss": 0.49339649081230164
      },
      "step 139": {
        "loss": 167.7325897216797,
        "l1_loss": 1582.861083984375,
        "l2_loss": 104.36949157714844,
        "ortho_loss": 0.4865979254245758
      },
      "step 140": {
        "loss": 167.823486328125,
        "l1_loss": 1577.324951171875,
        "l2_loss": 104.68171691894531,
        "ortho_loss": 0.4877222776412964
      },
      "step 141": {
        "loss": 168.30665588378906,
        "l1_loss": 1581.917724609375,
        "l2_loss": 104.98135375976562,
        "ortho_loss": 0.48593834042549133
      },
      "step 142": {
        "loss": 163.52639770507812,
        "l1_loss": 1513.6226806640625,
        "l2_loss": 102.93203735351562,
        "ortho_loss": 0.494602769613266
      },
      "step 143": {
        "loss": 165.34112548828125,
        "l1_loss": 1541.120849609375,
        "l2_loss": 103.64759063720703,
        "ortho_loss": 0.48704200983047485
      },
      "step 144": {
        "loss": 166.80979919433594,
        "l1_loss": 1548.377685546875,
        "l2_loss": 104.82597351074219,
        "ortho_loss": 0.4872625470161438
      },
      "step 145": {
        "loss": 158.3640899658203,
        "l1_loss": 1436.931640625,
        "l2_loss": 100.83651733398438,
        "ortho_loss": 0.5030791759490967
      },
      "step 146": {
        "loss": 165.63385009765625,
        "l1_loss": 1529.2353515625,
        "l2_loss": 104.41557312011719,
        "ortho_loss": 0.4885852038860321
      },
      "step 147": {
        "loss": 159.36209106445312,
        "l1_loss": 1441.0888671875,
        "l2_loss": 101.66857147216797,
        "ortho_loss": 0.4995934069156647
      },
      "step 148": {
        "loss": 158.9978485107422,
        "l1_loss": 1430.3544921875,
        "l2_loss": 101.73344421386719,
        "ortho_loss": 0.5021313428878784
      },
      "step 149": {
        "loss": 155.4697723388672,
        "l1_loss": 1385.027587890625,
        "l2_loss": 100.01789855957031,
        "ortho_loss": 0.5076047778129578
      },
      "step 150": {
        "loss": 161.47483825683594,
        "l1_loss": 1453.810546875,
        "l2_loss": 103.27291107177734,
        "ortho_loss": 0.49508973956108093
      },
      "step 151": {
        "loss": 162.2031707763672,
        "l1_loss": 1465.15478515625,
        "l2_loss": 103.54790496826172,
        "ortho_loss": 0.49072185158729553
      },
      "step 152": {
        "loss": 167.5631561279297,
        "l1_loss": 1536.911865234375,
        "l2_loss": 106.03866577148438,
        "ortho_loss": 0.4802663028240204
      },
      "step 153": {
        "loss": 152.55868530273438,
        "l1_loss": 1330.0377197265625,
        "l2_loss": 99.30601501464844,
        "ortho_loss": 0.5115633606910706
      },
      "step 154": {
        "loss": 159.9542236328125,
        "l1_loss": 1429.343017578125,
        "l2_loss": 102.73129272460938,
        "ortho_loss": 0.49211135506629944
      },
      "step 155": {
        "loss": 159.30506896972656,
        "l1_loss": 1414.010498046875,
        "l2_loss": 102.69520568847656,
        "ortho_loss": 0.4945378005504608
      },
      "step 156": {
        "loss": 160.46929931640625,
        "l1_loss": 1427.1522216796875,
        "l2_loss": 103.33412170410156,
        "ortho_loss": 0.4909195005893707
      },
      "step 157": {
        "loss": 154.58338928222656,
        "l1_loss": 1342.9619140625,
        "l2_loss": 100.81463623046875,
        "ortho_loss": 0.5028125047683716
      },
      "step 158": {
        "loss": 165.13438415527344,
        "l1_loss": 1486.1844482421875,
        "l2_loss": 105.63917541503906,
        "ortho_loss": 0.47830137610435486
      },
      "step 159": {
        "loss": 157.9016876220703,
        "l1_loss": 1389.722412109375,
        "l2_loss": 102.26366424560547,
        "ortho_loss": 0.4914083480834961
      },
      "step 160": {
        "loss": 158.10916137695312,
        "l1_loss": 1381.92138671875,
        "l2_loss": 102.78319549560547,
        "ortho_loss": 0.4910600483417511
      },
      "step 161": {
        "loss": 158.5240478515625,
        "l1_loss": 1382.133544921875,
        "l2_loss": 103.18955993652344,
        "ortho_loss": 0.4914149343967438
      },
      "step 162": {
        "loss": 153.1248321533203,
        "l1_loss": 1310.23779296875,
        "l2_loss": 100.6650390625,
        "ortho_loss": 0.5028237104415894
      },
      "step 163": {
        "loss": 156.48828125,
        "l1_loss": 1344.635986328125,
        "l2_loss": 102.6534194946289,
        "ortho_loss": 0.49429211020469666
      },
      "step 164": {
        "loss": 148.70655822753906,
        "l1_loss": 1242.9869384765625,
        "l2_loss": 98.93582153320312,
        "ortho_loss": 0.512475311756134
      },
      "step 165": {
        "loss": 156.01412963867188,
        "l1_loss": 1336.130126953125,
        "l2_loss": 102.51963806152344,
        "ortho_loss": 0.4928487539291382
      },
      "step 166": {
        "loss": 150.9497833251953,
        "l1_loss": 1266.551513671875,
        "l2_loss": 100.23721313476562,
        "ortho_loss": 0.5051409006118774
      },
      "step 167": {
        "loss": 153.29330444335938,
        "l1_loss": 1294.6678466796875,
        "l2_loss": 101.45706176757812,
        "ortho_loss": 0.49531885981559753
      },
      "step 168": {
        "loss": 152.0301971435547,
        "l1_loss": 1274.08984375,
        "l2_loss": 101.01676940917969,
        "ortho_loss": 0.4983137249946594
      },
      "step 169": {
        "loss": 148.0579071044922,
        "l1_loss": 1215.5015869140625,
        "l2_loss": 99.38671112060547,
        "ortho_loss": 0.5112524032592773
      },
      "step 170": {
        "loss": 147.3278045654297,
        "l1_loss": 1209.318359375,
        "l2_loss": 98.90422058105469,
        "ortho_loss": 0.5085563659667969
      },
      "step 171": {
        "loss": 152.82887268066406,
        "l1_loss": 1280.63232421875,
        "l2_loss": 101.55441284179688,
        "ortho_loss": 0.49171292781829834
      },
      "step 172": {
        "loss": 152.0283660888672,
        "l1_loss": 1262.4056396484375,
        "l2_loss": 101.48267364501953,
        "ortho_loss": 0.49469098448753357
      },
      "step 173": {
        "loss": 151.12533569335938,
        "l1_loss": 1249.976318359375,
        "l2_loss": 101.0768051147461,
        "ortho_loss": 0.4948855936527252
      },
      "step 174": {
        "loss": 149.23304748535156,
        "l1_loss": 1214.43603515625,
        "l2_loss": 100.60552215576172,
        "ortho_loss": 0.5008448362350464
      },
      "step 175": {
        "loss": 146.4508056640625,
        "l1_loss": 1174.99169921875,
        "l2_loss": 99.4002685546875,
        "ortho_loss": 0.508700430393219
      },
      "step 176": {
        "loss": 154.33958435058594,
        "l1_loss": 1281.30615234375,
        "l2_loss": 103.03845977783203,
        "ortho_loss": 0.48873913288116455
      },
      "step 177": {
        "loss": 143.73883056640625,
        "l1_loss": 1130.039794921875,
        "l2_loss": 98.4856948852539,
        "ortho_loss": 0.5155062079429626
      },
      "step 178": {
        "loss": 148.1300048828125,
        "l1_loss": 1184.9835205078125,
        "l2_loss": 100.68035125732422,
        "ortho_loss": 0.5030390620231628
      },
      "step 179": {
        "loss": 143.53575134277344,
        "l1_loss": 1128.385986328125,
        "l2_loss": 98.3492431640625,
        "ortho_loss": 0.5106834173202515
      },
      "step 180": {
        "loss": 142.85736083984375,
        "l1_loss": 1115.4432373046875,
        "l2_loss": 98.18827819824219,
        "ortho_loss": 0.5136637091636658
      },
      "step 181": {
        "loss": 145.315673828125,
        "l1_loss": 1145.14892578125,
        "l2_loss": 99.45918273925781,
        "ortho_loss": 0.5053691267967224
      },
      "step 182": {
        "loss": 143.22605895996094,
        "l1_loss": 1104.859130859375,
        "l2_loss": 98.98052978515625,
        "ortho_loss": 0.5115535259246826
      },
      "step 183": {
        "loss": 139.30621337890625,
        "l1_loss": 1056.47802734375,
        "l2_loss": 96.99506378173828,
        "ortho_loss": 0.5203931331634521
      },
      "step 184": {
        "loss": 144.2209014892578,
        "l1_loss": 1116.800048828125,
        "l2_loss": 99.49852752685547,
        "ortho_loss": 0.503703773021698
      },
      "step 185": {
        "loss": 142.175048828125,
        "l1_loss": 1088.246826171875,
        "l2_loss": 98.59423828125,
        "ortho_loss": 0.5092856287956238
      },
      "step 186": {
        "loss": 143.26422119140625,
        "l1_loss": 1097.711669921875,
        "l2_loss": 99.30528259277344,
        "ortho_loss": 0.5048050284385681
      },
      "step 187": {
        "loss": 139.7967071533203,
        "l1_loss": 1044.0604248046875,
        "l2_loss": 97.98263549804688,
        "ortho_loss": 0.5165333151817322
      },
      "step 188": {
        "loss": 142.97354125976562,
        "l1_loss": 1090.17919921875,
        "l2_loss": 99.31617736816406,
        "ortho_loss": 0.5020706057548523
      },
      "step 189": {
        "loss": 136.22393798828125,
        "l1_loss": 995.1478271484375,
        "l2_loss": 96.36536407470703,
        "ortho_loss": 0.5265362858772278
      },
      "step 190": {
        "loss": 136.652099609375,
        "l1_loss": 992.985595703125,
        "l2_loss": 96.88029479980469,
        "ortho_loss": 0.5238619446754456
      },
      "step 191": {
        "loss": 136.44290161132812,
        "l1_loss": 987.00244140625,
        "l2_loss": 96.91023254394531,
        "ortho_loss": 0.5257885456085205
      },
      "step 192": {
        "loss": 147.25289916992188,
        "l1_loss": 1139.76123046875,
        "l2_loss": 101.61361694335938,
        "ortho_loss": 0.48834991455078125
      },
      "step 193": {
        "loss": 138.037841796875,
        "l1_loss": 1008.809814453125,
        "l2_loss": 97.63398742675781,
        "ortho_loss": 0.5147194862365723
      },
      "step 194": {
        "loss": 140.49856567382812,
        "l1_loss": 1036.748046875,
        "l2_loss": 98.97821044921875,
        "ortho_loss": 0.5043672323226929
      },
      "step 195": {
        "loss": 139.42320251464844,
        "l1_loss": 1019.7593994140625,
        "l2_loss": 98.58213806152344,
        "ortho_loss": 0.506879985332489
      },
      "step 196": {
        "loss": 137.5189666748047,
        "l1_loss": 994.178466796875,
        "l2_loss": 97.70091247558594,
        "ortho_loss": 0.5091691017150879
      },
      "step 197": {
        "loss": 141.19435119628906,
        "l1_loss": 1045.8602294921875,
        "l2_loss": 99.3101806640625,
        "ortho_loss": 0.49756208062171936
      },
      "step 198": {
        "loss": 135.20298767089844,
        "l1_loss": 953.41845703125,
        "l2_loss": 97.01426696777344,
        "ortho_loss": 0.5199010372161865
      },
      "step 199": {
        "loss": 137.36097717285156,
        "l1_loss": 983.7887573242188,
        "l2_loss": 97.95863342285156,
        "ortho_loss": 0.5080119371414185
      },
      "step 200": {
        "loss": 136.07272338867188,
        "l1_loss": 956.1477661132812,
        "l2_loss": 97.77538299560547,
        "ortho_loss": 0.5142238736152649
      },
      "step 201": {
        "loss": 143.71791076660156,
        "l1_loss": 1059.533203125,
        "l2_loss": 101.28721618652344,
        "ortho_loss": 0.493682861328125
      },
      "step 202": {
        "loss": 135.4094696044922,
        "l1_loss": 948.2655639648438,
        "l2_loss": 97.42769622802734,
        "ortho_loss": 0.5114496946334839
      },
      "step 203": {
        "loss": 134.74378967285156,
        "l1_loss": 942.5010986328125,
        "l2_loss": 96.99267578125,
        "ortho_loss": 0.5107531547546387
      },
      "step 204": {
        "loss": 132.29075622558594,
        "l1_loss": 901.9345703125,
        "l2_loss": 96.16120147705078,
        "ortho_loss": 0.5216572880744934
      },
      "step 205": {
        "loss": 140.9574432373047,
        "l1_loss": 1017.3405151367188,
        "l2_loss": 100.21444702148438,
        "ortho_loss": 0.4938167631626129
      },
      "step 206": {
        "loss": 138.36892700195312,
        "l1_loss": 985.4217529296875,
        "l2_loss": 98.90225219726562,
        "ortho_loss": 0.49806666374206543
      },
      "step 207": {
        "loss": 133.40931701660156,
        "l1_loss": 911.2911376953125,
        "l2_loss": 96.90636444091797,
        "ortho_loss": 0.5131825804710388
      },
      "step 208": {
        "loss": 131.4463348388672,
        "l1_loss": 881.173583984375,
        "l2_loss": 96.1478271484375,
        "ortho_loss": 0.5155527591705322
      },
      "step 209": {
        "loss": 130.81982421875,
        "l1_loss": 860.45947265625,
        "l2_loss": 96.34907531738281,
        "ortho_loss": 0.5236184000968933
      },
      "step 210": {
        "loss": 136.30421447753906,
        "l1_loss": 937.528564453125,
        "l2_loss": 98.75262451171875,
        "ortho_loss": 0.5045090913772583
      },
      "step 211": {
        "loss": 131.45437622070312,
        "l1_loss": 870.3230590820312,
        "l2_loss": 96.58975219726562,
        "ortho_loss": 0.5169126391410828
      },
      "step 212": {
        "loss": 129.23171997070312,
        "l1_loss": 836.000244140625,
        "l2_loss": 95.73927307128906,
        "ortho_loss": 0.524339497089386
      },
      "step 213": {
        "loss": 131.36318969726562,
        "l1_loss": 857.4321899414062,
        "l2_loss": 97.01438903808594,
        "ortho_loss": 0.5151264667510986
      },
      "step 214": {
        "loss": 123.75499725341797,
        "l1_loss": 750.833251953125,
        "l2_loss": 93.66644287109375,
        "ortho_loss": 0.5523132681846619
      },
      "step 215": {
        "loss": 128.50437927246094,
        "l1_loss": 823.352783203125,
        "l2_loss": 95.51815795898438,
        "ortho_loss": 0.5210376977920532
      },
      "step 216": {
        "loss": 125.76329040527344,
        "l1_loss": 777.5372314453125,
        "l2_loss": 94.60861206054688,
        "ortho_loss": 0.5319194793701172
      },
      "step 217": {
        "loss": 130.23233032226562,
        "l1_loss": 838.90087890625,
        "l2_loss": 96.6246109008789,
        "ortho_loss": 0.5168137550354004
      },
      "step 218": {
        "loss": 131.1271209716797,
        "l1_loss": 837.5340576171875,
        "l2_loss": 97.57424926757812,
        "ortho_loss": 0.5151243209838867
      },
      "step 219": {
        "loss": 131.8191680908203,
        "l1_loss": 850.339111328125,
        "l2_loss": 97.75469970703125,
        "ortho_loss": 0.5090163946151733
      },
      "step 220": {
        "loss": 124.7777328491211,
        "l1_loss": 753.302490234375,
        "l2_loss": 94.59231567382812,
        "ortho_loss": 0.5332491993904114
      },
      "step 221": {
        "loss": 127.4450912475586,
        "l1_loss": 783.7546997070312,
        "l2_loss": 96.04253387451172,
        "ortho_loss": 0.5236803293228149
      },
      "step 222": {
        "loss": 127.88959503173828,
        "l1_loss": 795.3307495117188,
        "l2_loss": 96.0246810913086,
        "ortho_loss": 0.5168668031692505
      },
      "step 223": {
        "loss": 127.16939544677734,
        "l1_loss": 780.9366455078125,
        "l2_loss": 95.87984466552734,
        "ortho_loss": 0.5208789706230164
      },
      "step 224": {
        "loss": 125.70591735839844,
        "l1_loss": 763.9595947265625,
        "l2_loss": 95.095458984375,
        "ortho_loss": 0.5207934975624084
      },
      "step 225": {
        "loss": 127.83755493164062,
        "l1_loss": 787.6971435546875,
        "l2_loss": 96.27812194824219,
        "ortho_loss": 0.5154693722724915
      },
      "step 226": {
        "loss": 122.61766815185547,
        "l1_loss": 710.1279907226562,
        "l2_loss": 94.158935546875,
        "ortho_loss": 0.536142885684967
      },
      "step 227": {
        "loss": 126.39956665039062,
        "l1_loss": 761.977783203125,
        "l2_loss": 95.86854553222656,
        "ortho_loss": 0.5191024541854858
      },
      "step 228": {
        "loss": 127.99942016601562,
        "l1_loss": 775.5581665039062,
        "l2_loss": 96.92567443847656,
        "ortho_loss": 0.5141851305961609
      },
      "step 229": {
        "loss": 127.13817596435547,
        "l1_loss": 768.7488403320312,
        "l2_loss": 96.33651733398438,
        "ortho_loss": 0.5170626044273376
      },
      "step 230": {
        "loss": 123.45039367675781,
        "l1_loss": 719.9317626953125,
        "l2_loss": 94.60054016113281,
        "ortho_loss": 0.525831401348114
      },
      "step 231": {
        "loss": 128.46925354003906,
        "l1_loss": 783.29931640625,
        "l2_loss": 97.0863265991211,
        "ortho_loss": 0.5095031261444092
      },
      "step 232": {
        "loss": 123.36094665527344,
        "l1_loss": 701.4263916015625,
        "l2_loss": 95.25119018554688,
        "ortho_loss": 0.5270208120346069
      },
      "step 233": {
        "loss": 121.54498291015625,
        "l1_loss": 672.82177734375,
        "l2_loss": 94.57872772216797,
        "ortho_loss": 0.5338350534439087
      },
      "step 234": {
        "loss": 120.91542053222656,
        "l1_loss": 669.7396850585938,
        "l2_loss": 94.07241821289062,
        "ortho_loss": 0.5341628193855286
      },
      "step 235": {
        "loss": 129.4224395751953,
        "l1_loss": 793.433349609375,
        "l2_loss": 97.6346435546875,
        "ortho_loss": 0.5046527981758118
      },
      "step 236": {
        "loss": 126.5877914428711,
        "l1_loss": 748.383056640625,
        "l2_loss": 96.60137939453125,
        "ortho_loss": 0.5109745860099792
      },
      "step 237": {
        "loss": 128.16580200195312,
        "l1_loss": 756.34228515625,
        "l2_loss": 97.86115264892578,
        "ortho_loss": 0.5096939206123352
      },
      "step 238": {
        "loss": 121.25448608398438,
        "l1_loss": 665.982666015625,
        "l2_loss": 94.5619888305664,
        "ortho_loss": 0.5319454669952393
      },
      "step 239": {
        "loss": 117.25344848632812,
        "l1_loss": 609.503662109375,
        "l2_loss": 92.81907653808594,
        "ortho_loss": 0.5422724485397339
      },
      "step 240": {
        "loss": 115.51814270019531,
        "l1_loss": 582.9046020507812,
        "l2_loss": 92.14599609375,
        "ortho_loss": 0.5596473217010498
      },
      "step 241": {
        "loss": 126.26494598388672,
        "l1_loss": 728.31005859375,
        "l2_loss": 97.08116912841797,
        "ortho_loss": 0.5137342810630798
      },
      "step 242": {
        "loss": 117.04876708984375,
        "l1_loss": 598.1883544921875,
        "l2_loss": 93.0667953491211,
        "ortho_loss": 0.5443477630615234
      },
      "step 243": {
        "loss": 120.51741790771484,
        "l1_loss": 642.9241943359375,
        "l2_loss": 94.74748229980469,
        "ortho_loss": 0.5296924710273743
      },
      "step 244": {
        "loss": 120.48762512207031,
        "l1_loss": 649.605712890625,
        "l2_loss": 94.45079040527344,
        "ortho_loss": 0.5260229706764221
      },
      "step 245": {
        "loss": 121.50480651855469,
        "l1_loss": 657.328125,
        "l2_loss": 95.15928649902344,
        "ortho_loss": 0.5239531993865967
      },
      "step 246": {
        "loss": 123.04232788085938,
        "l1_loss": 676.3641967773438,
        "l2_loss": 95.93614959716797,
        "ortho_loss": 0.5161458849906921
      },
      "step 247": {
        "loss": 117.73406982421875,
        "l1_loss": 603.6173095703125,
        "l2_loss": 93.53568267822266,
        "ortho_loss": 0.5369798541069031
      },
      "step 248": {
        "loss": 122.99932861328125,
        "l1_loss": 662.7612915039062,
        "l2_loss": 96.43685150146484,
        "ortho_loss": 0.520226240158081
      },
      "step 249": {
        "loss": 120.08228302001953,
        "l1_loss": 628.4725341796875,
        "l2_loss": 94.89064025878906,
        "ortho_loss": 0.5274192094802856
      },
      "step 250": {
        "loss": 118.71310424804688,
        "l1_loss": 606.5543823242188,
        "l2_loss": 94.3978042602539,
        "ortho_loss": 0.5312385559082031
      },
      "step 251": {
        "loss": 126.07225036621094,
        "l1_loss": 713.237548828125,
        "l2_loss": 97.49193572998047,
        "ortho_loss": 0.5081151127815247
      },
      "step 252": {
        "loss": 122.6888198852539,
        "l1_loss": 659.8802490234375,
        "l2_loss": 96.24205017089844,
        "ortho_loss": 0.5156081914901733
      },
      "step 253": {
        "loss": 121.61077880859375,
        "l1_loss": 637.08837890625,
        "l2_loss": 96.07505798339844,
        "ortho_loss": 0.5218878388404846
      },
      "step 254": {
        "loss": 117.11585998535156,
        "l1_loss": 567.7610473632812,
        "l2_loss": 94.35140991210938,
        "ortho_loss": 0.5401155948638916
      },
      "step 255": {
        "loss": 116.85313415527344,
        "l1_loss": 568.044677734375,
        "l2_loss": 94.07781982421875,
        "ortho_loss": 0.535309374332428
      },
      "step 256": {
        "loss": 117.51952362060547,
        "l1_loss": 573.4391479492188,
        "l2_loss": 94.52869415283203,
        "ortho_loss": 0.5326318740844727
      },
      "step 257": {
        "loss": 121.1769790649414,
        "l1_loss": 636.8316650390625,
        "l2_loss": 95.65187072753906,
        "ortho_loss": 0.5184112191200256
      },
      "step 258": {
        "loss": 117.5020523071289,
        "l1_loss": 576.270751953125,
        "l2_loss": 94.39813232421875,
        "ortho_loss": 0.5309470891952515
      },
      "step 259": {
        "loss": 118.26082611083984,
        "l1_loss": 587.9368286132812,
        "l2_loss": 94.69081115722656,
        "ortho_loss": 0.5254718065261841
      },
      "step 260": {
        "loss": 117.10625457763672,
        "l1_loss": 558.6346435546875,
        "l2_loss": 94.70730590820312,
        "ortho_loss": 0.5356566905975342
      },
      "step 261": {
        "loss": 115.14305114746094,
        "l1_loss": 532.084716796875,
        "l2_loss": 93.80545043945312,
        "ortho_loss": 0.5421603918075562
      },
      "step 262": {
        "loss": 114.7496109008789,
        "l1_loss": 524.7081298828125,
        "l2_loss": 93.70701599121094,
        "ortho_loss": 0.542694628238678
      },
      "step 263": {
        "loss": 116.18489837646484,
        "l1_loss": 540.4503173828125,
        "l2_loss": 94.51325988769531,
        "ortho_loss": 0.5362425446510315
      },
      "step 264": {
        "loss": 119.1032943725586,
        "l1_loss": 589.1265258789062,
        "l2_loss": 95.48588562011719,
        "ortho_loss": 0.5234891176223755
      },
      "step 265": {
        "loss": 115.70686340332031,
        "l1_loss": 530.41943359375,
        "l2_loss": 94.4365005493164,
        "ortho_loss": 0.5358882546424866
      },
      "step 266": {
        "loss": 119.05260467529297,
        "l1_loss": 585.5650634765625,
        "l2_loss": 95.5776138305664,
        "ortho_loss": 0.523874044418335
      },
      "step 267": {
        "loss": 113.30411529541016,
        "l1_loss": 495.664306640625,
        "l2_loss": 93.42279052734375,
        "ortho_loss": 0.5475737452507019
      },
      "step 268": {
        "loss": 119.45355987548828,
        "l1_loss": 587.6299438476562,
        "l2_loss": 95.89602661132812,
        "ortho_loss": 0.5233672857284546
      },
      "step 269": {
        "loss": 113.72972106933594,
        "l1_loss": 502.40802001953125,
        "l2_loss": 93.57929229736328,
        "ortho_loss": 0.5411027073860168
      },
      "step 270": {
        "loss": 113.11579895019531,
        "l1_loss": 480.77069091796875,
        "l2_loss": 93.83006286621094,
        "ortho_loss": 0.5490654110908508
      },
      "step 271": {
        "loss": 115.02233123779297,
        "l1_loss": 516.9727783203125,
        "l2_loss": 94.28990173339844,
        "ortho_loss": 0.5351774096488953
      },
      "step 272": {
        "loss": 119.04029846191406,
        "l1_loss": 572.1438598632812,
        "l2_loss": 96.10200500488281,
        "ortho_loss": 0.5253749489784241
      },
      "step 273": {
        "loss": 118.41432189941406,
        "l1_loss": 561.8402709960938,
        "l2_loss": 95.88793182373047,
        "ortho_loss": 0.5277888178825378
      },
      "step 274": {
        "loss": 115.74056243896484,
        "l1_loss": 528.2796020507812,
        "l2_loss": 94.5562515258789,
        "ortho_loss": 0.5312320590019226
      },
      "step 275": {
        "loss": 115.37684631347656,
        "l1_loss": 516.5467529296875,
        "l2_loss": 94.66148376464844,
        "ortho_loss": 0.5349858999252319
      },
      "step 276": {
        "loss": 115.65923309326172,
        "l1_loss": 520.9141845703125,
        "l2_loss": 94.76942443847656,
        "ortho_loss": 0.5324573516845703
      },
      "step 277": {
        "loss": 115.80441284179688,
        "l1_loss": 525.74951171875,
        "l2_loss": 94.7213134765625,
        "ortho_loss": 0.5311713218688965
      },
      "step 278": {
        "loss": 110.03134155273438,
        "l1_loss": 426.40155029296875,
        "l2_loss": 92.91934967041016,
        "ortho_loss": 0.5593070387840271
      },
      "step 279": {
        "loss": 111.23710632324219,
        "l1_loss": 454.9504699707031,
        "l2_loss": 92.98432922363281,
        "ortho_loss": 0.5475260019302368
      },
      "step 280": {
        "loss": 111.80535888671875,
        "l1_loss": 451.6103820800781,
        "l2_loss": 93.6861572265625,
        "ortho_loss": 0.5479026436805725
      },
      "step 281": {
        "loss": 111.91278839111328,
        "l1_loss": 457.6697998046875,
        "l2_loss": 93.551513671875,
        "ortho_loss": 0.5447903275489807
      },
      "step 282": {
        "loss": 109.59671783447266,
        "l1_loss": 423.68365478515625,
        "l2_loss": 92.59386444091797,
        "ortho_loss": 0.5550757646560669
      },
      "step 283": {
        "loss": 111.1640396118164,
        "l1_loss": 439.47613525390625,
        "l2_loss": 93.53012084960938,
        "ortho_loss": 0.5488194227218628
      },
      "step 284": {
        "loss": 116.07533264160156,
        "l1_loss": 513.4962768554688,
        "l2_loss": 95.48229217529297,
        "ortho_loss": 0.5319461226463318
      },
      "step 285": {
        "loss": 112.80705261230469,
        "l1_loss": 455.1246032714844,
        "l2_loss": 94.54769134521484,
        "ortho_loss": 0.5437828898429871
      },
      "step 286": {
        "loss": 109.73146057128906,
        "l1_loss": 418.42041015625,
        "l2_loss": 92.93931579589844,
        "ortho_loss": 0.5532960295677185
      },
      "step 287": {
        "loss": 110.92423248291016,
        "l1_loss": 443.21527099609375,
        "l2_loss": 93.14097595214844,
        "ortho_loss": 0.5465113520622253
      },
      "step 288": {
        "loss": 112.9642333984375,
        "l1_loss": 469.364990234375,
        "l2_loss": 94.13572692871094,
        "ortho_loss": 0.5390647053718567
      },
      "step 289": {
        "loss": 109.7356948852539,
        "l1_loss": 427.9968566894531,
        "l2_loss": 92.56118774414062,
        "ortho_loss": 0.5463085770606995
      },
      "step 290": {
        "loss": 112.61189270019531,
        "l1_loss": 456.4770812988281,
        "l2_loss": 94.2988510131836,
        "ortho_loss": 0.5395188927650452
      },
      "step 291": {
        "loss": 107.79312133789062,
        "l1_loss": 384.30450439453125,
        "l2_loss": 92.36465454101562,
        "ortho_loss": 0.5628849864006042
      },
      "step 292": {
        "loss": 114.75436401367188,
        "l1_loss": 498.61212158203125,
        "l2_loss": 94.75665283203125,
        "ortho_loss": 0.5322073101997375
      },
      "step 293": {
        "loss": 111.11962127685547,
        "l1_loss": 432.697265625,
        "l2_loss": 93.75704193115234,
        "ortho_loss": 0.5468413829803467
      },
      "step 294": {
        "loss": 111.09252166748047,
        "l1_loss": 426.33282470703125,
        "l2_loss": 93.9845962524414,
        "ortho_loss": 0.5460851788520813
      },
      "step 295": {
        "loss": 111.3207778930664,
        "l1_loss": 439.9820556640625,
        "l2_loss": 93.66743469238281,
        "ortho_loss": 0.54060959815979
      },
      "step 296": {
        "loss": 112.66068267822266,
        "l1_loss": 462.1734924316406,
        "l2_loss": 94.1199951171875,
        "ortho_loss": 0.5374757051467896
      },
      "step 297": {
        "loss": 109.62726593017578,
        "l1_loss": 408.1260070800781,
        "l2_loss": 93.2471694946289,
        "ortho_loss": 0.550527036190033
      },
      "step 298": {
        "loss": 115.66339874267578,
        "l1_loss": 505.9739990234375,
        "l2_loss": 95.37139892578125,
        "ortho_loss": 0.5303835868835449
      },
      "step 299": {
        "loss": 111.4628677368164,
        "l1_loss": 435.3875427246094,
        "l2_loss": 93.99298858642578,
        "ortho_loss": 0.5437451004981995
      },
      "step 300": {
        "loss": 110.36663055419922,
        "l1_loss": 423.3090515136719,
        "l2_loss": 93.37964630126953,
        "ortho_loss": 0.5462034344673157
      },
      "step 301": {
        "loss": 108.64301300048828,
        "l1_loss": 381.0718994140625,
        "l2_loss": 93.34468078613281,
        "ortho_loss": 0.5545929074287415
      },
      "step 302": {
        "loss": 106.91498565673828,
        "l1_loss": 357.9562072753906,
        "l2_loss": 92.54059600830078,
        "ortho_loss": 0.5614365935325623
      },
      "step 303": {
        "loss": 110.08165740966797,
        "l1_loss": 409.3608703613281,
        "l2_loss": 93.65251922607422,
        "ortho_loss": 0.5470147728919983
      },
      "step 304": {
        "loss": 110.50286865234375,
        "l1_loss": 416.7559814453125,
        "l2_loss": 93.77806091308594,
        "ortho_loss": 0.5456392765045166
      },
      "step 305": {
        "loss": 115.85478210449219,
        "l1_loss": 501.2156066894531,
        "l2_loss": 95.75291442871094,
        "ortho_loss": 0.5324421525001526
      },
      "step 306": {
        "loss": 115.06890869140625,
        "l1_loss": 489.26324462890625,
        "l2_loss": 95.44490051269531,
        "ortho_loss": 0.5348439812660217
      },
      "step 307": {
        "loss": 109.71611785888672,
        "l1_loss": 399.2550964355469,
        "l2_loss": 93.69110870361328,
        "ortho_loss": 0.5480288863182068
      },
      "step 308": {
        "loss": 110.79950714111328,
        "l1_loss": 417.7156982421875,
        "l2_loss": 94.03640747070312,
        "ortho_loss": 0.5447050929069519
      },
      "step 309": {
        "loss": 106.10150146484375,
        "l1_loss": 348.2738952636719,
        "l2_loss": 92.114013671875,
        "ortho_loss": 0.5653048753738403
      },
      "step 310": {
        "loss": 107.14769744873047,
        "l1_loss": 358.69384765625,
        "l2_loss": 92.74417114257812,
        "ortho_loss": 0.5577428936958313
      },
      "step 311": {
        "loss": 107.06181335449219,
        "l1_loss": 359.2269287109375,
        "l2_loss": 92.63691711425781,
        "ortho_loss": 0.55817711353302
      },
      "step 312": {
        "loss": 105.86161041259766,
        "l1_loss": 336.6554260253906,
        "l2_loss": 92.33885192871094,
        "ortho_loss": 0.5654320120811462
      },
      "step 313": {
        "loss": 112.4134750366211,
        "l1_loss": 430.4307861328125,
        "l2_loss": 95.14181518554688,
        "ortho_loss": 0.5442782640457153
      },
      "step 314": {
        "loss": 110.25856018066406,
        "l1_loss": 403.264404296875,
        "l2_loss": 94.07320404052734,
        "ortho_loss": 0.5477623343467712
      },
      "step 315": {
        "loss": 106.9078598022461,
        "l1_loss": 354.1844482421875,
        "l2_loss": 92.68467712402344,
        "ortho_loss": 0.558123767375946
      },
      "step 316": {
        "loss": 110.46354675292969,
        "l1_loss": 411.4503479003906,
        "l2_loss": 93.95085144042969,
        "ortho_loss": 0.5467686653137207
      },
      "step 317": {
        "loss": 108.669921875,
        "l1_loss": 369.1478271484375,
        "l2_loss": 93.84843444824219,
        "ortho_loss": 0.5556953549385071
      },
      "step 318": {
        "loss": 112.83616638183594,
        "l1_loss": 446.984130859375,
        "l2_loss": 94.9027099609375,
        "ortho_loss": 0.5409013032913208
      },
      "step 319": {
        "loss": 109.74445343017578,
        "l1_loss": 389.7306823730469,
        "l2_loss": 94.10031127929688,
        "ortho_loss": 0.5491384267807007
      },
      "step 320": {
        "loss": 113.48101806640625,
        "l1_loss": 451.94610595703125,
        "l2_loss": 95.34907531738281,
        "ortho_loss": 0.5410289168357849
      },
      "step 321": {
        "loss": 106.1407241821289,
        "l1_loss": 337.55914306640625,
        "l2_loss": 92.58214569091797,
        "ortho_loss": 0.5621269941329956
      },
      "step 322": {
        "loss": 108.65807342529297,
        "l1_loss": 375.983642578125,
        "l2_loss": 93.56352233886719,
        "ortho_loss": 0.5520989894866943
      },
      "step 323": {
        "loss": 110.12921142578125,
        "l1_loss": 398.36468505859375,
        "l2_loss": 94.13963317871094,
        "ortho_loss": 0.5499288439750671
      },
      "step 324": {
        "loss": 106.5542221069336,
        "l1_loss": 341.80596923828125,
        "l2_loss": 92.82588958740234,
        "ortho_loss": 0.5609480738639832
      },
      "step 325": {
        "loss": 106.01895141601562,
        "l1_loss": 340.1103210449219,
        "l2_loss": 92.35842895507812,
        "ortho_loss": 0.5610549449920654
      },
      "step 326": {
        "loss": 106.26591491699219,
        "l1_loss": 328.30096435546875,
        "l2_loss": 93.07740020751953,
        "ortho_loss": 0.5647093653678894
      },
      "step 327": {
        "loss": 105.96450805664062,
        "l1_loss": 335.4588623046875,
        "l2_loss": 92.48989868164062,
        "ortho_loss": 0.562601625919342
      },
      "step 328": {
        "loss": 108.3591537475586,
        "l1_loss": 375.8988037109375,
        "l2_loss": 93.26774597167969,
        "ortho_loss": 0.5545820593833923
      },
      "step 329": {
        "loss": 107.60384368896484,
        "l1_loss": 341.2358703613281,
        "l2_loss": 93.89839172363281,
        "ortho_loss": 0.5601478219032288
      },
      "step 330": {
        "loss": 104.68016052246094,
        "l1_loss": 305.59014892578125,
        "l2_loss": 92.39966583251953,
        "ortho_loss": 0.5688346028327942
      },
      "step 331": {
        "loss": 112.63410186767578,
        "l1_loss": 420.4822998046875,
        "l2_loss": 95.75994110107422,
        "ortho_loss": 0.5486713647842407
      },
      "step 332": {
        "loss": 108.72071838378906,
        "l1_loss": 369.9075622558594,
        "l2_loss": 93.86874389648438,
        "ortho_loss": 0.5567363500595093
      },
      "step 333": {
        "loss": 108.63935852050781,
        "l1_loss": 373.8203125,
        "l2_loss": 93.63105773925781,
        "ortho_loss": 0.5549087524414062
      },
      "step 334": {
        "loss": 112.77561950683594,
        "l1_loss": 437.4842224121094,
        "l2_loss": 95.2214584350586,
        "ortho_loss": 0.5479534864425659
      },
      "step 335": {
        "loss": 105.498046875,
        "l1_loss": 318.8653564453125,
        "l2_loss": 92.68682861328125,
        "ortho_loss": 0.5660549402236938
      },
      "step 336": {
        "loss": 107.58480072021484,
        "l1_loss": 347.12493896484375,
        "l2_loss": 93.64378356933594,
        "ortho_loss": 0.5602442026138306
      },
      "step 337": {
        "loss": 103.52503204345703,
        "l1_loss": 285.18414306640625,
        "l2_loss": 92.06004333496094,
        "ortho_loss": 0.5762652158737183
      },
      "step 338": {
        "loss": 105.38249969482422,
        "l1_loss": 321.95269775390625,
        "l2_loss": 92.44792175292969,
        "ortho_loss": 0.5647354125976562
      },
      "step 339": {
        "loss": 106.04334259033203,
        "l1_loss": 314.2611083984375,
        "l2_loss": 93.41607666015625,
        "ortho_loss": 0.5682730078697205
      },
      "step 340": {
        "loss": 102.45868682861328,
        "l1_loss": 272.3445739746094,
        "l2_loss": 91.50675201416016,
        "ortho_loss": 0.5815436244010925
      },
      "step 341": {
        "loss": 107.96070098876953,
        "l1_loss": 348.5158996582031,
        "l2_loss": 93.96407318115234,
        "ortho_loss": 0.5598892569541931
      },
      "step 342": {
        "loss": 109.36980438232422,
        "l1_loss": 384.05078125,
        "l2_loss": 93.95233154296875,
        "ortho_loss": 0.5544086694717407
      },
      "step 343": {
        "loss": 106.607421875,
        "l1_loss": 327.2015380859375,
        "l2_loss": 93.46279907226562,
        "ortho_loss": 0.5656242966651917
      },
      "step 344": {
        "loss": 106.1871566772461,
        "l1_loss": 324.6684875488281,
        "l2_loss": 93.14375305175781,
        "ortho_loss": 0.5666065812110901
      },
      "step 345": {
        "loss": 106.76017761230469,
        "l1_loss": 334.0309753417969,
        "l2_loss": 93.3424072265625,
        "ortho_loss": 0.5653411746025085
      },
      "step 346": {
        "loss": 103.19227600097656,
        "l1_loss": 280.0686950683594,
        "l2_loss": 91.93180084228516,
        "ortho_loss": 0.5772115588188171
      },
      "step 347": {
        "loss": 114.47968292236328,
        "l1_loss": 451.23345947265625,
        "l2_loss": 96.37522888183594,
        "ortho_loss": 0.5511133074760437
      },
      "step 348": {
        "loss": 106.48698425292969,
        "l1_loss": 321.28118896484375,
        "l2_loss": 93.57879638671875,
        "ortho_loss": 0.5693753957748413
      },
      "step 349": {
        "loss": 106.43109130859375,
        "l1_loss": 327.7828369140625,
        "l2_loss": 93.26300048828125,
        "ortho_loss": 0.567765474319458
      },
      "step 350": {
        "loss": 108.65127563476562,
        "l1_loss": 372.6448669433594,
        "l2_loss": 93.6894760131836,
        "ortho_loss": 0.5601075887680054
      },
      "step 351": {
        "loss": 104.84661102294922,
        "l1_loss": 300.9090576171875,
        "l2_loss": 92.75302124023438,
        "ortho_loss": 0.5722816586494446
      },
      "step 352": {
        "loss": 112.36943817138672,
        "l1_loss": 429.6978454589844,
        "l2_loss": 95.12605285644531,
        "ortho_loss": 0.554707944393158
      },
      "step 353": {
        "loss": 105.10682678222656,
        "l1_loss": 310.2870788574219,
        "l2_loss": 92.63807678222656,
        "ortho_loss": 0.5726290941238403
      },
      "step 354": {
        "loss": 105.91937255859375,
        "l1_loss": 318.17633056640625,
        "l2_loss": 93.13519287109375,
        "ortho_loss": 0.5712777376174927
      },
      "step 355": {
        "loss": 106.540283203125,
        "l1_loss": 327.05419921875,
        "l2_loss": 93.401123046875,
        "ortho_loss": 0.5699514746665955
      },
      "step 356": {
        "loss": 108.9816665649414,
        "l1_loss": 366.75457763671875,
        "l2_loss": 94.25491333007812,
        "ortho_loss": 0.5657435655593872
      },
      "step 357": {
        "loss": 99.89586639404297,
        "l1_loss": 219.55746459960938,
        "l2_loss": 91.052978515625,
        "ortho_loss": 0.6059452295303345
      },
      "step 358": {
        "loss": 105.46083068847656,
        "l1_loss": 311.377197265625,
        "l2_loss": 92.9482421875,
        "ortho_loss": 0.5750002861022949
      },
      "step 359": {
        "loss": 110.31938171386719,
        "l1_loss": 388.6123962402344,
        "l2_loss": 94.71833801269531,
        "ortho_loss": 0.5654621124267578
      },
      "step 360": {
        "loss": 106.98200225830078,
        "l1_loss": 329.6741943359375,
        "l2_loss": 93.7376480102539,
        "ortho_loss": 0.5738837718963623
      },
      "step 361": {
        "loss": 106.03533172607422,
        "l1_loss": 310.8968505859375,
        "l2_loss": 93.54159545898438,
        "ortho_loss": 0.5785879492759705
      },
      "step 362": {
        "loss": 103.07513427734375,
        "l1_loss": 269.27044677734375,
        "l2_loss": 92.24562072753906,
        "ortho_loss": 0.5868949294090271
      },
      "step 363": {
        "loss": 106.06250762939453,
        "l1_loss": 316.621337890625,
        "l2_loss": 93.33979797363281,
        "ortho_loss": 0.5785659551620483
      },
      "step 364": {
        "loss": 102.77749633789062,
        "l1_loss": 265.057861328125,
        "l2_loss": 92.11636352539062,
        "ortho_loss": 0.5882271528244019
      },
      "step 365": {
        "loss": 110.88400268554688,
        "l1_loss": 400.282470703125,
        "l2_loss": 94.81578063964844,
        "ortho_loss": 0.5692613124847412
      },
      "step 366": {
        "loss": 107.14159393310547,
        "l1_loss": 330.8078308105469,
        "l2_loss": 93.85155487060547,
        "ortho_loss": 0.577238142490387
      },
      "step 367": {
        "loss": 103.19591522216797,
        "l1_loss": 271.56591796875,
        "l2_loss": 92.27449035644531,
        "ortho_loss": 0.5878098607063293
      },
      "step 368": {
        "loss": 103.59696197509766,
        "l1_loss": 289.97991943359375,
        "l2_loss": 91.939208984375,
        "ortho_loss": 0.5855472087860107
      },
      "step 369": {
        "loss": 108.27225494384766,
        "l1_loss": 347.12158203125,
        "l2_loss": 94.32955932617188,
        "ortho_loss": 0.5783066749572754
      },
      "step 370": {
        "loss": 106.05512237548828,
        "l1_loss": 317.466796875,
        "l2_loss": 93.29820251464844,
        "ortho_loss": 0.5825197696685791
      },
      "step 371": {
        "loss": 104.693115234375,
        "l1_loss": 304.1251220703125,
        "l2_loss": 92.4697265625,
        "ortho_loss": 0.5837910771369934
      },
      "step 372": {
        "loss": 110.15363311767578,
        "l1_loss": 380.2127685546875,
        "l2_loss": 94.8874740600586,
        "ortho_loss": 0.5764557719230652
      },
      "step 373": {
        "loss": 103.70773315429688,
        "l1_loss": 273.0335998535156,
        "l2_loss": 92.72727966308594,
        "ortho_loss": 0.5911518335342407
      },
      "step 374": {
        "loss": 104.56587219238281,
        "l1_loss": 301.1995849609375,
        "l2_loss": 92.45930480957031,
        "ortho_loss": 0.5858491659164429
      },
      "step 375": {
        "loss": 101.2437515258789,
        "l1_loss": 236.36959838867188,
        "l2_loss": 91.72866821289062,
        "ortho_loss": 0.603042483329773
      },
      "step 376": {
        "loss": 101.61962890625,
        "l1_loss": 236.69056701660156,
        "l2_loss": 92.09170532226562,
        "ortho_loss": 0.6030029058456421
      },
      "step 377": {
        "loss": 107.20098114013672,
        "l1_loss": 336.08544921875,
        "l2_loss": 93.69912719726562,
        "ortho_loss": 0.5843120813369751
      },
      "step 378": {
        "loss": 104.16474914550781,
        "l1_loss": 289.14544677734375,
        "l2_loss": 92.53984069824219,
        "ortho_loss": 0.590904712677002
      },
      "step 379": {
        "loss": 100.44750213623047,
        "l1_loss": 228.06854248046875,
        "l2_loss": 91.26435852050781,
        "ortho_loss": 0.6040273904800415
      },
      "step 380": {
        "loss": 107.28142547607422,
        "l1_loss": 342.8772888183594,
        "l2_loss": 93.5078125,
        "ortho_loss": 0.5852459669113159
      },
      "step 381": {
        "loss": 104.31048583984375,
        "l1_loss": 274.9864196777344,
        "l2_loss": 93.25155639648438,
        "ortho_loss": 0.5946978330612183
      },
      "step 382": {
        "loss": 101.33984375,
        "l1_loss": 237.75038146972656,
        "l2_loss": 91.76946258544922,
        "ortho_loss": 0.6036399006843567
      },
      "step 383": {
        "loss": 100.52882385253906,
        "l1_loss": 226.79531860351562,
        "l2_loss": 91.39632415771484,
        "ortho_loss": 0.6068298816680908
      },
      "step 384": {
        "loss": 101.76681518554688,
        "l1_loss": 243.576904296875,
        "l2_loss": 91.96341705322266,
        "ortho_loss": 0.6031875610351562
      },
      "step 385": {
        "loss": 107.49262237548828,
        "l1_loss": 337.8900146484375,
        "l2_loss": 93.91819763183594,
        "ortho_loss": 0.5882487297058105
      },
      "step 386": {
        "loss": 104.06668853759766,
        "l1_loss": 283.07928466796875,
        "l2_loss": 92.68367004394531,
        "ortho_loss": 0.5984487533569336
      },
      "step 387": {
        "loss": 103.40657806396484,
        "l1_loss": 264.57232666015625,
        "l2_loss": 92.76351165771484,
        "ortho_loss": 0.6017627120018005
      },
      "step 388": {
        "loss": 106.62438201904297,
        "l1_loss": 324.2965393066406,
        "l2_loss": 93.59323120117188,
        "ortho_loss": 0.592917799949646
      },
      "step 389": {
        "loss": 103.89134216308594,
        "l1_loss": 277.11651611328125,
        "l2_loss": 92.74678039550781,
        "ortho_loss": 0.5990206599235535
      },
      "step 390": {
        "loss": 105.49018096923828,
        "l1_loss": 302.42755126953125,
        "l2_loss": 93.33357238769531,
        "ortho_loss": 0.5950886011123657
      },
      "step 391": {
        "loss": 101.1415023803711,
        "l1_loss": 241.33114624023438,
        "l2_loss": 91.42749786376953,
        "ortho_loss": 0.6076338887214661
      },
      "step 392": {
        "loss": 103.541015625,
        "l1_loss": 282.52960205078125,
        "l2_loss": 92.17984008789062,
        "ortho_loss": 0.5999013185501099
      },
      "step 393": {
        "loss": 103.7648696899414,
        "l1_loss": 276.64154052734375,
        "l2_loss": 92.63893127441406,
        "ortho_loss": 0.6028105616569519
      },
      "step 394": {
        "loss": 105.72293090820312,
        "l1_loss": 312.5704650878906,
        "l2_loss": 93.16023254394531,
        "ortho_loss": 0.5988303422927856
      },
      "step 395": {
        "loss": 101.99829864501953,
        "l1_loss": 255.28567504882812,
        "l2_loss": 91.72608184814453,
        "ortho_loss": 0.6079061031341553
      },
      "step 396": {
        "loss": 101.7513656616211,
        "l1_loss": 251.94053649902344,
        "l2_loss": 91.61299133300781,
        "ortho_loss": 0.6075382828712463
      },
      "step 397": {
        "loss": 101.36869812011719,
        "l1_loss": 233.91831970214844,
        "l2_loss": 91.95069885253906,
        "ortho_loss": 0.6126181483268738
      },
      "step 398": {
        "loss": 105.7143783569336,
        "l1_loss": 322.09197998046875,
        "l2_loss": 92.77081298828125,
        "ortho_loss": 0.5988423824310303
      },
      "step 399": {
        "loss": 103.0761947631836,
        "l1_loss": 279.3399658203125,
        "l2_loss": 91.84197998046875,
        "ortho_loss": 0.6061282753944397
      },
      "step 400": {
        "loss": 102.62841033935547,
        "l1_loss": 258.5753173828125,
        "l2_loss": 92.22441101074219,
        "ortho_loss": 0.6098985075950623
      },
      "step 401": {
        "loss": 103.3886489868164,
        "l1_loss": 277.6706237792969,
        "l2_loss": 92.2210693359375,
        "ortho_loss": 0.6075173616409302
      },
      "step 402": {
        "loss": 101.56473541259766,
        "l1_loss": 255.80250549316406,
        "l2_loss": 91.27157592773438,
        "ortho_loss": 0.6105921268463135
      },
      "step 403": {
        "loss": 104.30486297607422,
        "l1_loss": 293.37091064453125,
        "l2_loss": 92.50935363769531,
        "ortho_loss": 0.6067560315132141
      },
      "step 404": {
        "loss": 101.96280670166016,
        "l1_loss": 253.10626220703125,
        "l2_loss": 91.77729797363281,
        "ortho_loss": 0.6125709414482117
      },
      "step 405": {
        "loss": 105.5220947265625,
        "l1_loss": 320.8719482421875,
        "l2_loss": 92.62657165527344,
        "ortho_loss": 0.6064525246620178
      },
      "step 406": {
        "loss": 105.91302490234375,
        "l1_loss": 326.08197021484375,
        "l2_loss": 92.80931091308594,
        "ortho_loss": 0.6043120622634888
      },
      "step 407": {
        "loss": 105.42755126953125,
        "l1_loss": 315.66522216796875,
        "l2_loss": 92.74019622802734,
        "ortho_loss": 0.607418417930603
      }
    },
    "config": {
      "trainer_class": "CustomTrainer",
      "activation_dim": 2304,
      "dict_size": 2304,
      "lr": 0.0003,
      "l1_penalty": 0.04,
      "warmup_steps": 1000,
      "resample_steps": null,
      "device": "cuda",
      "layer": 5,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "CustomTrainer",
      "submodule_name": "resid_post_layer_5"
    },
    "final_info": {
      "training_steps": 488,
      "final_loss": 94.70306396484375,
      "layer": 5,
      "dict_size": 2304,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "c86cbd5f-ca70-4883-b3d0-5df58902206e",
    "datetime_epoch_millis": 1737876574424,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.006710192260088017,
        "mean_num_split_features": 1.4230769230769231
      }
    },
    "eval_result_details": [
      {
        "first_letter": "a",
        "absorption_rate": 0.0007671653241273494,
        "num_absorption": 2,
        "num_probe_true_positives": 2607,
        "num_split_features": 3
      },
      {
        "first_letter": "b",
        "absorption_rate": 0.01322418136020151,
        "num_absorption": 21,
        "num_probe_true_positives": 1588,
        "num_split_features": 2
      },
      {
        "first_letter": "c",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 2851,
        "num_split_features": 3
      },
      {
        "first_letter": "d",
        "absorption_rate": 0.012582384661473937,
        "num_absorption": 21,
        "num_probe_true_positives": 1669,
        "num_split_features": 1
      },
      {
        "first_letter": "e",
        "absorption_rate": 0.0018656716417910447,
        "num_absorption": 3,
        "num_probe_true_positives": 1608,
        "num_split_features": 1
      },
      {
        "first_letter": "f",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1255,
        "num_split_features": 2
      },
      {
        "first_letter": "g",
        "absorption_rate": 0.004642525533890436,
        "num_absorption": 5,
        "num_probe_true_positives": 1077,
        "num_split_features": 1
      },
      {
        "first_letter": "h",
        "absorption_rate": 0.006349206349206349,
        "num_absorption": 6,
        "num_probe_true_positives": 945,
        "num_split_features": 1
      },
      {
        "first_letter": "i",
        "absorption_rate": 0.000612369871402327,
        "num_absorption": 1,
        "num_probe_true_positives": 1633,
        "num_split_features": 2
      },
      {
        "first_letter": "j",
        "absorption_rate": 0.009174311926605505,
        "num_absorption": 4,
        "num_probe_true_positives": 436,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.005934718100890208,
        "num_absorption": 4,
        "num_probe_true_positives": 674,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.002437043054427295,
        "num_absorption": 3,
        "num_probe_true_positives": 1231,
        "num_split_features": 2
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.009836065573770493,
        "num_absorption": 18,
        "num_probe_true_positives": 1830,
        "num_split_features": 1
      },
      {
        "first_letter": "n",
        "absorption_rate": 0.0035335689045936395,
        "num_absorption": 3,
        "num_probe_true_positives": 849,
        "num_split_features": 1
      },
      {
        "first_letter": "o",
        "absorption_rate": 0.029906542056074768,
        "num_absorption": 32,
        "num_probe_true_positives": 1070,
        "num_split_features": 1
      },
      {
        "first_letter": "p",
        "absorption_rate": 0.000423908435777872,
        "num_absorption": 1,
        "num_probe_true_positives": 2359,
        "num_split_features": 3
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.01092896174863388,
        "num_absorption": 2,
        "num_probe_true_positives": 183,
        "num_split_features": 1
      },
      {
        "first_letter": "r",
        "absorption_rate": 0.008860011813349085,
        "num_absorption": 15,
        "num_probe_true_positives": 1693,
        "num_split_features": 1
      },
      {
        "first_letter": "s",
        "absorption_rate": 0.0010504201680672268,
        "num_absorption": 3,
        "num_probe_true_positives": 2856,
        "num_split_features": 2
      },
      {
        "first_letter": "t",
        "absorption_rate": 0.007597895967270602,
        "num_absorption": 13,
        "num_probe_true_positives": 1711,
        "num_split_features": 1
      },
      {
        "first_letter": "u",
        "absorption_rate": 0.014492753623188406,
        "num_absorption": 11,
        "num_probe_true_positives": 759,
        "num_split_features": 1
      },
      {
        "first_letter": "v",
        "absorption_rate": 0.005820721769499418,
        "num_absorption": 5,
        "num_probe_true_positives": 859,
        "num_split_features": 1
      },
      {
        "first_letter": "w",
        "absorption_rate": 0.004531722054380665,
        "num_absorption": 3,
        "num_probe_true_positives": 662,
        "num_split_features": 1
      },
      {
        "first_letter": "x",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 99,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.012345679012345678,
        "num_absorption": 2,
        "num_probe_true_positives": 162,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.007547169811320755,
        "num_absorption": 2,
        "num_probe_true_positives": 265,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_5_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_5_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.2857142857142857,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 7.1875
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.26973684210526316,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 9.875,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": -0.26171875,
        "mse": 3.5,
        "cossim": 0.4140625
      },
      "shrinkage": {
        "l2_norm_in": 90.0,
        "l2_norm_out": 23.0,
        "l2_ratio": 0.259765625,
        "relative_reconstruction_bias": 0.63671875
      },
      "sparsity": {
        "l0": 108.9400634765625,
        "l1": 133.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr and tpp evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "b5f14f03-b952-41b1-9469-ca62c8429abd",
    "datetime_epoch_millis": 1737876892603,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": -0.017664019645518638,
        "scr_metric_threshold_2": 0.007733210170600548,
        "scr_dir2_threshold_2": 0.007733210170600548,
        "scr_dir1_threshold_5": -0.059708565358126305,
        "scr_metric_threshold_5": 0.009606766314338069,
        "scr_dir2_threshold_5": 0.009606766314338069,
        "scr_dir1_threshold_10": -0.07792700826911374,
        "scr_metric_threshold_10": 0.01359227967480241,
        "scr_dir2_threshold_10": 0.01359227967480241,
        "scr_dir1_threshold_20": -0.1331128525177033,
        "scr_metric_threshold_20": 0.01570339920888572,
        "scr_dir2_threshold_20": 0.01570339920888572,
        "scr_dir1_threshold_50": -0.11593241603753465,
        "scr_metric_threshold_50": 0.01996696619854271,
        "scr_dir2_threshold_50": 0.01996696619854271,
        "scr_dir1_threshold_100": -0.1312917633402134,
        "scr_metric_threshold_100": 0.01775347394161156,
        "scr_dir2_threshold_100": 0.01775347394161156,
        "scr_dir1_threshold_500": -0.235965463937829,
        "scr_metric_threshold_500": 0.016240925473789476,
        "scr_dir2_threshold_500": 0.016240925473789476
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": -0.3103451110805095,
        "scr_metric_threshold_2": 0.018779380822471343,
        "scr_dir2_threshold_2": 0.018779380822471343,
        "scr_dir1_threshold_5": -0.37930977783898107,
        "scr_metric_threshold_5": 0.021126768446028555,
        "scr_dir2_threshold_5": 0.021126768446028555,
        "scr_dir1_threshold_10": -0.37930977783898107,
        "scr_metric_threshold_10": 0.018779380822471343,
        "scr_dir2_threshold_10": 0.018779380822471343,
        "scr_dir1_threshold_20": -0.8275862777701274,
        "scr_metric_threshold_20": 0.021126768446028555,
        "scr_dir2_threshold_20": 0.021126768446028555,
        "scr_dir1_threshold_50": -0.9655176666207642,
        "scr_metric_threshold_50": 0.0563380025504072,
        "scr_dir2_threshold_50": 0.0563380025504072,
        "scr_dir1_threshold_100": -1.0689667220921653,
        "scr_metric_threshold_100": 0.06103291771452845,
        "scr_dir2_threshold_100": 0.06103291771452845,
        "scr_dir1_threshold_500": -1.72413927763242,
        "scr_metric_threshold_500": 0.07511738337287854,
        "scr_dir2_threshold_500": 0.07511738337287854
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": 0.029850374486354535,
        "scr_metric_threshold_2": 0.0,
        "scr_dir2_threshold_2": 0.0,
        "scr_dir1_threshold_5": -0.34328375470276395,
        "scr_metric_threshold_5": -0.002590640371648879,
        "scr_dir2_threshold_5": -0.002590640371648879,
        "scr_dir1_threshold_10": -0.41791058054058766,
        "scr_metric_threshold_10": 0.010362715902782657,
        "scr_dir2_threshold_10": 0.010362715902782657,
        "scr_dir1_threshold_20": -0.46268703189205684,
        "scr_metric_threshold_20": -0.002590640371648879,
        "scr_dir2_threshold_20": -0.002590640371648879,
        "scr_dir1_threshold_50": -0.20895529027029383,
        "scr_metric_threshold_50": 0.005181280743297758,
        "scr_dir2_threshold_50": 0.005181280743297758,
        "scr_dir1_threshold_100": -0.13432846443247012,
        "scr_metric_threshold_100": 0.002590640371648879,
        "scr_dir2_threshold_100": 0.002590640371648879,
        "scr_dir1_threshold_500": -0.20895529027029383,
        "scr_metric_threshold_500": 0.005181280743297758,
        "scr_dir2_threshold_500": 0.005181280743297758
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.023256877745449607,
        "scr_metric_threshold_2": -0.005089144583879204,
        "scr_dir2_threshold_2": -0.005089144583879204,
        "scr_dir1_threshold_5": -0.023255491592288,
        "scr_metric_threshold_5": -0.005089144583879204,
        "scr_dir2_threshold_5": -0.005089144583879204,
        "scr_dir1_threshold_10": -0.0930233525223136,
        "scr_metric_threshold_10": -0.005089144583879204,
        "scr_dir2_threshold_10": -0.005089144583879204,
        "scr_dir1_threshold_20": -0.0930233525223136,
        "scr_metric_threshold_20": -0.002544648124819707,
        "scr_dir2_threshold_20": -0.002544648124819707,
        "scr_dir1_threshold_50": -0.046510983184576,
        "scr_metric_threshold_50": -0.005089144583879204,
        "scr_dir2_threshold_50": -0.005089144583879204,
        "scr_dir1_threshold_100": -0.046510983184576,
        "scr_metric_threshold_100": 0.0,
        "scr_dir2_threshold_100": 0.0,
        "scr_dir1_threshold_500": -0.1162788441146016,
        "scr_metric_threshold_500": 0.010178137501998197,
        "scr_dir2_threshold_500": 0.010178137501998197
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": -0.01234551548800365,
        "scr_metric_threshold_2": -0.03225796631216327,
        "scr_dir2_threshold_2": -0.03225796631216327,
        "scr_dir1_threshold_5": 0.1728394244106685,
        "scr_metric_threshold_5": 0.0,
        "scr_dir2_threshold_5": 0.0,
        "scr_dir1_threshold_10": 0.20987670673421857,
        "scr_metric_threshold_10": 0.0,
        "scr_dir2_threshold_10": 0.0,
        "scr_dir1_threshold_20": 0.23456773771022588,
        "scr_metric_threshold_20": 0.0,
        "scr_dir2_threshold_20": 0.0,
        "scr_dir1_threshold_50": 0.2222222222222222,
        "scr_metric_threshold_50": 0.005376434537042601,
        "scr_dir2_threshold_50": 0.005376434537042601,
        "scr_dir1_threshold_100": 0.20987670673421857,
        "scr_metric_threshold_100": -0.008064411464279277,
        "scr_dir2_threshold_100": -0.008064411464279277,
        "scr_dir1_threshold_500": 0.2222222222222222,
        "scr_metric_threshold_500": -0.04301067515872539,
        "scr_dir2_threshold_500": -0.04301067515872539
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": 0.04571437524288891,
        "scr_metric_threshold_2": 0.05936078776117859,
        "scr_dir2_threshold_2": 0.05936078776117859,
        "scr_dir1_threshold_5": 0.04571437524288891,
        "scr_metric_threshold_5": 0.06392693939649612,
        "scr_dir2_threshold_5": 0.06392693939649612,
        "scr_dir1_threshold_10": 0.051428586998763286,
        "scr_metric_threshold_10": 0.07305924266713117,
        "scr_dir2_threshold_10": 0.07305924266713117,
        "scr_dir1_threshold_20": 0.04571437524288891,
        "scr_metric_threshold_20": 0.06849309103181364,
        "scr_dir2_threshold_20": 0.06849309103181364,
        "scr_dir1_threshold_50": 0.04571437524288891,
        "scr_metric_threshold_50": 0.04566206068787576,
        "scr_dir2_threshold_50": 0.04566206068787576,
        "scr_dir1_threshold_100": 0.04571437524288891,
        "scr_metric_threshold_100": 0.06849309103181364,
        "scr_dir2_threshold_100": 0.06849309103181364,
        "scr_dir1_threshold_500": 0.04000016348701453,
        "scr_metric_threshold_500": 0.04566206068787576,
        "scr_dir2_threshold_500": 0.04566206068787576
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.007692208939046454,
        "scr_metric_threshold_2": -0.0323887323562997,
        "scr_dir2_threshold_2": -0.0323887323562997,
        "scr_dir1_threshold_5": -0.007692208939046454,
        "scr_metric_threshold_5": -0.028340201140355195,
        "scr_dir2_threshold_5": -0.028340201140355195,
        "scr_dir1_threshold_10": 0.015384417878092908,
        "scr_metric_threshold_10": -0.01619436617814985,
        "scr_dir2_threshold_10": -0.01619436617814985,
        "scr_dir1_threshold_20": 0.015384417878092908,
        "scr_metric_threshold_20": 0.004048531215944509,
        "scr_dir2_threshold_20": 0.004048531215944509,
        "scr_dir1_threshold_50": 0.030769294253470136,
        "scr_metric_threshold_50": -0.01619436617814985,
        "scr_dir2_threshold_50": -0.01619436617814985,
        "scr_dir1_threshold_100": 0.046153712131563045,
        "scr_metric_threshold_100": -0.04048603610256054,
        "scr_dir2_threshold_100": -0.04048603610256054,
        "scr_dir1_threshold_500": 0.046153712131563045,
        "scr_metric_threshold_500": -0.0323887323562997,
        "scr_dir2_threshold_500": -0.0323887323562997
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.08522750170939969,
        "scr_metric_threshold_2": 0.03846154825848749,
        "scr_dir2_threshold_2": 0.03846154825848749,
        "scr_dir1_threshold_5": 0.06250019049773588,
        "scr_metric_threshold_5": 0.012820600993054022,
        "scr_dir2_threshold_5": 0.012820600993054022,
        "scr_dir1_threshold_10": 0.005682081799897114,
        "scr_metric_threshold_10": 0.012820600993054022,
        "scr_dir2_threshold_10": 0.012820600993054022,
        "scr_dir1_threshold_20": 0.02272731121166381,
        "scr_metric_threshold_20": 0.017094049750513835,
        "scr_dir2_threshold_20": 0.017094049750513835,
        "scr_dir1_threshold_50": 0.0,
        "scr_metric_threshold_50": 0.03846154825848749,
        "scr_dir2_threshold_50": 0.03846154825848749,
        "scr_dir1_threshold_100": -0.10227273112116639,
        "scr_metric_threshold_100": 0.03846154825848749,
        "scr_dir2_threshold_100": 0.03846154825848749,
        "scr_dir1_threshold_500": -0.13636352860734133,
        "scr_metric_threshold_500": 0.03418809950102767,
        "scr_dir2_threshold_500": 0.03418809950102767
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": -0.010362868718775122,
        "scr_metric_threshold_2": 0.014999807775009142,
        "scr_dir2_threshold_2": 0.014999807775009142,
        "scr_dir1_threshold_5": -0.005181279943224265,
        "scr_metric_threshold_5": 0.014999807775009142,
        "scr_dir2_threshold_5": 0.014999807775009142,
        "scr_dir1_threshold_10": -0.015544148661999387,
        "scr_metric_threshold_10": 0.014999807775009142,
        "scr_dir2_threshold_10": 0.014999807775009142,
        "scr_dir1_threshold_20": 0.0,
        "scr_metric_threshold_20": 0.020000041723253828,
        "scr_dir2_threshold_20": 0.020000041723253828,
        "scr_dir1_threshold_50": -0.005181279943224265,
        "scr_metric_threshold_50": 0.029999913573259925,
        "scr_dir2_threshold_50": 0.029999913573259925,
        "scr_dir1_threshold_100": 0.0,
        "scr_metric_threshold_100": 0.020000041723253828,
        "scr_dir2_threshold_100": 0.020000041723253828,
        "scr_dir1_threshold_500": -0.010362868718775122,
        "scr_metric_threshold_500": 0.03499984949826297,
        "scr_dir2_threshold_500": 0.03499984949826297
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "2ceed3dc-80d1-4094-88bd-bae141ad6618",
    "datetime_epoch_millis": 1737876938265,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.940025,
        "llm_top_1_test_accuracy": 0.67890625,
        "llm_top_2_test_accuracy": 0.72346875,
        "llm_top_5_test_accuracy": 0.77423125,
        "llm_top_10_test_accuracy": 0.8231812500000001,
        "llm_top_20_test_accuracy": 0.8601937500000001,
        "llm_top_50_test_accuracy": 0.8994062500000001,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9336187940090894,
        "sae_top_1_test_accuracy": 0.6359125,
        "sae_top_2_test_accuracy": 0.672825,
        "sae_top_5_test_accuracy": 0.7268125,
        "sae_top_10_test_accuracy": 0.77020625,
        "sae_top_20_test_accuracy": 0.8168875,
        "sae_top_50_test_accuracy": 0.8708812499999999,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.958,
        "llm_top_1_test_accuracy": 0.6641999999999999,
        "llm_top_2_test_accuracy": 0.6843999999999999,
        "llm_top_5_test_accuracy": 0.7469999999999999,
        "llm_top_10_test_accuracy": 0.8282,
        "llm_top_20_test_accuracy": 0.8606,
        "llm_top_50_test_accuracy": 0.9118,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9494000434875488,
        "sae_top_1_test_accuracy": 0.6338,
        "sae_top_2_test_accuracy": 0.6786,
        "sae_top_5_test_accuracy": 0.7162,
        "sae_top_10_test_accuracy": 0.7806,
        "sae_top_20_test_accuracy": 0.8334000000000001,
        "sae_top_50_test_accuracy": 0.8846,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9396000000000001,
        "llm_top_1_test_accuracy": 0.6714,
        "llm_top_2_test_accuracy": 0.7222000000000001,
        "llm_top_5_test_accuracy": 0.7607999999999999,
        "llm_top_10_test_accuracy": 0.8038000000000001,
        "llm_top_20_test_accuracy": 0.8455999999999999,
        "llm_top_50_test_accuracy": 0.8880000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9308000326156616,
        "sae_top_1_test_accuracy": 0.654,
        "sae_top_2_test_accuracy": 0.6646000000000001,
        "sae_top_5_test_accuracy": 0.7352000000000001,
        "sae_top_10_test_accuracy": 0.7634000000000001,
        "sae_top_20_test_accuracy": 0.8096,
        "sae_top_50_test_accuracy": 0.8612,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.909,
        "llm_top_1_test_accuracy": 0.6814,
        "llm_top_2_test_accuracy": 0.705,
        "llm_top_5_test_accuracy": 0.751,
        "llm_top_10_test_accuracy": 0.7966,
        "llm_top_20_test_accuracy": 0.8251999999999999,
        "llm_top_50_test_accuracy": 0.8625999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9070000529289246,
        "sae_top_1_test_accuracy": 0.6237999999999999,
        "sae_top_2_test_accuracy": 0.657,
        "sae_top_5_test_accuracy": 0.7001999999999999,
        "sae_top_10_test_accuracy": 0.7545999999999999,
        "sae_top_20_test_accuracy": 0.8016,
        "sae_top_50_test_accuracy": 0.8538,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.8888,
        "llm_top_1_test_accuracy": 0.6068,
        "llm_top_2_test_accuracy": 0.6407999999999999,
        "llm_top_5_test_accuracy": 0.6763999999999999,
        "llm_top_10_test_accuracy": 0.719,
        "llm_top_20_test_accuracy": 0.772,
        "llm_top_50_test_accuracy": 0.8290000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.8722000598907471,
        "sae_top_1_test_accuracy": 0.5968,
        "sae_top_2_test_accuracy": 0.6088,
        "sae_top_5_test_accuracy": 0.6536,
        "sae_top_10_test_accuracy": 0.6834,
        "sae_top_20_test_accuracy": 0.7318,
        "sae_top_50_test_accuracy": 0.7854,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9245000000000001,
        "llm_top_1_test_accuracy": 0.628,
        "llm_top_2_test_accuracy": 0.686,
        "llm_top_5_test_accuracy": 0.738,
        "llm_top_10_test_accuracy": 0.767,
        "llm_top_20_test_accuracy": 0.8,
        "llm_top_50_test_accuracy": 0.8545,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9145000576972961,
        "sae_top_1_test_accuracy": 0.618,
        "sae_top_2_test_accuracy": 0.677,
        "sae_top_5_test_accuracy": 0.723,
        "sae_top_10_test_accuracy": 0.74,
        "sae_top_20_test_accuracy": 0.755,
        "sae_top_50_test_accuracy": 0.849,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9663999999999999,
        "llm_top_1_test_accuracy": 0.6626,
        "llm_top_2_test_accuracy": 0.706,
        "llm_top_5_test_accuracy": 0.7719999999999999,
        "llm_top_10_test_accuracy": 0.8426,
        "llm_top_20_test_accuracy": 0.8924,
        "llm_top_50_test_accuracy": 0.9362,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9614000439643859,
        "sae_top_1_test_accuracy": 0.6578,
        "sae_top_2_test_accuracy": 0.6952,
        "sae_top_5_test_accuracy": 0.7554000000000001,
        "sae_top_10_test_accuracy": 0.7874,
        "sae_top_20_test_accuracy": 0.8418000000000001,
        "sae_top_50_test_accuracy": 0.8984,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.9345000000000001,
        "llm_top_1_test_accuracy": 0.7082499999999999,
        "llm_top_2_test_accuracy": 0.75075,
        "llm_top_5_test_accuracy": 0.80625,
        "llm_top_10_test_accuracy": 0.8482500000000001,
        "llm_top_20_test_accuracy": 0.89275,
        "llm_top_50_test_accuracy": 0.91475,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9342500418424606,
        "sae_top_1_test_accuracy": 0.6335,
        "sae_top_2_test_accuracy": 0.6689999999999999,
        "sae_top_5_test_accuracy": 0.7244999999999999,
        "sae_top_10_test_accuracy": 0.78025,
        "sae_top_20_test_accuracy": 0.8285,
        "sae_top_50_test_accuracy": 0.86425,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9994,
        "llm_top_1_test_accuracy": 0.8086,
        "llm_top_2_test_accuracy": 0.8925999999999998,
        "llm_top_5_test_accuracy": 0.9423999999999999,
        "llm_top_10_test_accuracy": 0.9800000000000001,
        "llm_top_20_test_accuracy": 0.993,
        "llm_top_50_test_accuracy": 0.9984,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9994000196456909,
        "sae_top_1_test_accuracy": 0.6696,
        "sae_top_2_test_accuracy": 0.7323999999999999,
        "sae_top_5_test_accuracy": 0.8064,
        "sae_top_10_test_accuracy": 0.8719999999999999,
        "sae_top_20_test_accuracy": 0.9334,
        "sae_top_50_test_accuracy": 0.9704,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  }
}