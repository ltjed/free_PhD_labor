{
  "training results": {
    "training_log": {
      "step 0": {
        "loss": 280.3907165527344,
        "l1_loss": 2692.192626953125,
        "l2_loss": 172.6608123779297,
        "ortho_loss": 0.4221651554107666
      },
      "step 1": {
        "loss": 243.6199188232422,
        "l1_loss": 2632.935546875,
        "l2_loss": 138.26095581054688,
        "ortho_loss": 0.415470153093338
      },
      "step 2": {
        "loss": 249.9102020263672,
        "l1_loss": 2702.074462890625,
        "l2_loss": 141.78500366210938,
        "ortho_loss": 0.42224693298339844
      },
      "step 3": {
        "loss": 249.84877014160156,
        "l1_loss": 2702.375244140625,
        "l2_loss": 141.71153259277344,
        "ortho_loss": 0.4221665561199188
      },
      "step 4": {
        "loss": 252.69287109375,
        "l1_loss": 2733.424560546875,
        "l2_loss": 143.3136749267578,
        "ortho_loss": 0.42201998829841614
      },
      "step 5": {
        "loss": 244.94027709960938,
        "l1_loss": 2648.0673828125,
        "l2_loss": 138.97555541992188,
        "ortho_loss": 0.42026135325431824
      },
      "step 6": {
        "loss": 247.53201293945312,
        "l1_loss": 2678.136962890625,
        "l2_loss": 140.36439514160156,
        "ortho_loss": 0.4214893579483032
      },
      "step 7": {
        "loss": 250.36607360839844,
        "l1_loss": 2709.99560546875,
        "l2_loss": 141.92401123046875,
        "ortho_loss": 0.4225001335144043
      },
      "step 8": {
        "loss": 247.7346954345703,
        "l1_loss": 2682.0224609375,
        "l2_loss": 140.41152954101562,
        "ortho_loss": 0.42274388670921326
      },
      "step 9": {
        "loss": 246.8781280517578,
        "l1_loss": 2673.4326171875,
        "l2_loss": 139.89877319335938,
        "ortho_loss": 0.4204729199409485
      },
      "step 10": {
        "loss": 245.63621520996094,
        "l1_loss": 2660.15087890625,
        "l2_loss": 139.1881866455078,
        "ortho_loss": 0.41997435688972473
      },
      "step 11": {
        "loss": 250.17921447753906,
        "l1_loss": 2711.39306640625,
        "l2_loss": 141.68130493164062,
        "ortho_loss": 0.42194128036499023
      },
      "step 12": {
        "loss": 247.02549743652344,
        "l1_loss": 2678.1611328125,
        "l2_loss": 139.85678100585938,
        "ortho_loss": 0.42285290360450745
      },
      "step 13": {
        "loss": 242.21058654785156,
        "l1_loss": 2626.711669921875,
        "l2_loss": 137.10018920898438,
        "ortho_loss": 0.4193384647369385
      },
      "step 14": {
        "loss": 249.42822265625,
        "l1_loss": 2708.19580078125,
        "l2_loss": 141.0580596923828,
        "ortho_loss": 0.42331361770629883
      },
      "step 15": {
        "loss": 240.169921875,
        "l1_loss": 2606.272705078125,
        "l2_loss": 135.87765502929688,
        "ortho_loss": 0.4134571850299835
      },
      "step 16": {
        "loss": 247.0205535888672,
        "l1_loss": 2684.57568359375,
        "l2_loss": 139.59527587890625,
        "ortho_loss": 0.4225010573863983
      },
      "step 17": {
        "loss": 246.48507690429688,
        "l1_loss": 2679.7392578125,
        "l2_loss": 139.25314331054688,
        "ortho_loss": 0.42363137006759644
      },
      "step 18": {
        "loss": 252.9932098388672,
        "l1_loss": 2753.6337890625,
        "l2_loss": 142.80564880371094,
        "ortho_loss": 0.42220497131347656
      },
      "step 19": {
        "loss": 237.9864959716797,
        "l1_loss": 2588.939453125,
        "l2_loss": 134.38735961914062,
        "ortho_loss": 0.415554940700531
      },
      "step 20": {
        "loss": 243.49705505371094,
        "l1_loss": 2651.2138671875,
        "l2_loss": 137.4064178466797,
        "ortho_loss": 0.42090773582458496
      },
      "step 21": {
        "loss": 238.7278289794922,
        "l1_loss": 2601.4892578125,
        "l2_loss": 134.62606811523438,
        "ortho_loss": 0.4218350648880005
      },
      "step 22": {
        "loss": 244.7586212158203,
        "l1_loss": 2670.92236328125,
        "l2_loss": 137.87945556640625,
        "ortho_loss": 0.4228401780128479
      },
      "step 23": {
        "loss": 240.25840759277344,
        "l1_loss": 2623.8896484375,
        "l2_loss": 135.26055908203125,
        "ortho_loss": 0.4227072298526764
      },
      "step 24": {
        "loss": 238.20721435546875,
        "l1_loss": 2600.956787109375,
        "l2_loss": 134.12692260742188,
        "ortho_loss": 0.4202682077884674
      },
      "step 25": {
        "loss": 236.8837432861328,
        "l1_loss": 2589.65869140625,
        "l2_loss": 133.25531005859375,
        "ortho_loss": 0.42093563079833984
      },
      "step 26": {
        "loss": 232.59756469726562,
        "l1_loss": 2542.3388671875,
        "l2_loss": 130.8629913330078,
        "ortho_loss": 0.41017618775367737
      },
      "step 27": {
        "loss": 237.78892517089844,
        "l1_loss": 2601.419921875,
        "l2_loss": 133.68984985351562,
        "ortho_loss": 0.4227852523326874
      },
      "step 28": {
        "loss": 237.54666137695312,
        "l1_loss": 2602.43994140625,
        "l2_loss": 133.4066162109375,
        "ortho_loss": 0.42442598938941956
      },
      "step 29": {
        "loss": 242.7250213623047,
        "l1_loss": 2662.43212890625,
        "l2_loss": 136.1852569580078,
        "ortho_loss": 0.42479416728019714
      },
      "step 30": {
        "loss": 245.05018615722656,
        "l1_loss": 2690.788818359375,
        "l2_loss": 137.3761444091797,
        "ortho_loss": 0.4249325394630432
      },
      "step 31": {
        "loss": 240.69534301757812,
        "l1_loss": 2642.8017578125,
        "l2_loss": 134.94058227539062,
        "ortho_loss": 0.42700180411338806
      },
      "step 32": {
        "loss": 230.7892608642578,
        "l1_loss": 2533.22412109375,
        "l2_loss": 129.4184112548828,
        "ortho_loss": 0.41891753673553467
      },
      "step 33": {
        "loss": 237.12586975097656,
        "l1_loss": 2607.98388671875,
        "l2_loss": 132.76385498046875,
        "ortho_loss": 0.4266658425331116
      },
      "step 34": {
        "loss": 244.72398376464844,
        "l1_loss": 2695.02978515625,
        "l2_loss": 136.8802490234375,
        "ortho_loss": 0.42564043402671814
      },
      "step 35": {
        "loss": 239.0490264892578,
        "l1_loss": 2633.6142578125,
        "l2_loss": 133.6617431640625,
        "ortho_loss": 0.42712289094924927
      },
      "step 36": {
        "loss": 234.5259552001953,
        "l1_loss": 2585.570556640625,
        "l2_loss": 131.06051635742188,
        "ortho_loss": 0.4261588454246521
      },
      "step 37": {
        "loss": 238.42376708984375,
        "l1_loss": 2630.92578125,
        "l2_loss": 133.14395141601562,
        "ortho_loss": 0.4278830587863922
      },
      "step 38": {
        "loss": 235.66915893554688,
        "l1_loss": 2602.45556640625,
        "l2_loss": 131.52813720703125,
        "ortho_loss": 0.4279484748840332
      },
      "step 39": {
        "loss": 228.670654296875,
        "l1_loss": 2522.71484375,
        "l2_loss": 127.7196044921875,
        "ortho_loss": 0.4245374798774719
      },
      "step 40": {
        "loss": 233.1424560546875,
        "l1_loss": 2578.728515625,
        "l2_loss": 129.950439453125,
        "ortho_loss": 0.42879605293273926
      },
      "step 41": {
        "loss": 238.63534545898438,
        "l1_loss": 2643.2177734375,
        "l2_loss": 132.86395263671875,
        "ortho_loss": 0.4269978702068329
      },
      "step 42": {
        "loss": 234.51170349121094,
        "l1_loss": 2595.01513671875,
        "l2_loss": 130.6681671142578,
        "ortho_loss": 0.42926982045173645
      },
      "step 43": {
        "loss": 232.88473510742188,
        "l1_loss": 2577.80712890625,
        "l2_loss": 129.7294464111328,
        "ortho_loss": 0.43001317977905273
      },
      "step 44": {
        "loss": 228.0623321533203,
        "l1_loss": 2527.269287109375,
        "l2_loss": 126.92855072021484,
        "ortho_loss": 0.43007639050483704
      },
      "step 45": {
        "loss": 228.6567840576172,
        "l1_loss": 2532.59130859375,
        "l2_loss": 127.31005859375,
        "ortho_loss": 0.43072134256362915
      },
      "step 46": {
        "loss": 223.45230102539062,
        "l1_loss": 2473.06494140625,
        "l2_loss": 124.48709106445312,
        "ortho_loss": 0.42618826031684875
      },
      "step 47": {
        "loss": 226.31723022460938,
        "l1_loss": 2510.07470703125,
        "l2_loss": 125.87115478515625,
        "ortho_loss": 0.4309341609477997
      },
      "step 48": {
        "loss": 232.82260131835938,
        "l1_loss": 2586.45947265625,
        "l2_loss": 129.32110595703125,
        "ortho_loss": 0.4312407970428467
      },
      "step 49": {
        "loss": 223.4109649658203,
        "l1_loss": 2478.955078125,
        "l2_loss": 124.209716796875,
        "ortho_loss": 0.4304422438144684
      },
      "step 50": {
        "loss": 221.4095001220703,
        "l1_loss": 2453.626953125,
        "l2_loss": 123.22134399414062,
        "ortho_loss": 0.4308261275291443
      },
      "step 51": {
        "loss": 220.20199584960938,
        "l1_loss": 2441.0029296875,
        "l2_loss": 122.51902770996094,
        "ortho_loss": 0.4284909665584564
      },
      "step 52": {
        "loss": 221.0067596435547,
        "l1_loss": 2450.572021484375,
        "l2_loss": 122.94052124023438,
        "ortho_loss": 0.433470219373703
      },
      "step 53": {
        "loss": 222.65599060058594,
        "l1_loss": 2471.0166015625,
        "l2_loss": 123.77201843261719,
        "ortho_loss": 0.4332725405693054
      },
      "step 54": {
        "loss": 225.63572692871094,
        "l1_loss": 2506.218505859375,
        "l2_loss": 125.3435287475586,
        "ortho_loss": 0.4345166087150574
      },
      "step 55": {
        "loss": 216.7218017578125,
        "l1_loss": 2401.363525390625,
        "l2_loss": 120.62411499023438,
        "ortho_loss": 0.43159300088882446
      },
      "step 56": {
        "loss": 225.6876678466797,
        "l1_loss": 2505.92431640625,
        "l2_loss": 125.40718078613281,
        "ortho_loss": 0.43512222170829773
      },
      "step 57": {
        "loss": 220.21836853027344,
        "l1_loss": 2443.62890625,
        "l2_loss": 122.42967224121094,
        "ortho_loss": 0.43530645966529846
      },
      "step 58": {
        "loss": 218.78875732421875,
        "l1_loss": 2424.345947265625,
        "l2_loss": 121.77131652832031,
        "ortho_loss": 0.43609073758125305
      },
      "step 59": {
        "loss": 211.3309783935547,
        "l1_loss": 2338.6591796875,
        "l2_loss": 117.74163818359375,
        "ortho_loss": 0.42985799908638
      },
      "step 60": {
        "loss": 217.2093963623047,
        "l1_loss": 2408.24853515625,
        "l2_loss": 120.83570861816406,
        "ortho_loss": 0.4375365674495697
      },
      "step 61": {
        "loss": 212.93740844726562,
        "l1_loss": 2355.712158203125,
        "l2_loss": 118.66553497314453,
        "ortho_loss": 0.43391481041908264
      },
      "step 62": {
        "loss": 211.6794891357422,
        "l1_loss": 2341.85302734375,
        "l2_loss": 117.96170043945312,
        "ortho_loss": 0.43666085600852966
      },
      "step 63": {
        "loss": 217.55699157714844,
        "l1_loss": 2410.140625,
        "l2_loss": 121.10752868652344,
        "ortho_loss": 0.43839868903160095
      },
      "step 64": {
        "loss": 210.292236328125,
        "l1_loss": 2322.2109375,
        "l2_loss": 117.36024475097656,
        "ortho_loss": 0.4354150891304016
      },
      "step 65": {
        "loss": 208.7408447265625,
        "l1_loss": 2306.698974609375,
        "l2_loss": 116.42927551269531,
        "ortho_loss": 0.4361153841018677
      },
      "step 66": {
        "loss": 207.96035766601562,
        "l1_loss": 2294.7001953125,
        "l2_loss": 116.12870788574219,
        "ortho_loss": 0.4364248812198639
      },
      "step 67": {
        "loss": 213.3173370361328,
        "l1_loss": 2358.114990234375,
        "l2_loss": 118.94863891601562,
        "ortho_loss": 0.44102174043655396
      },
      "step 68": {
        "loss": 214.48040771484375,
        "l1_loss": 2373.48095703125,
        "l2_loss": 119.49710845947266,
        "ortho_loss": 0.4406909644603729
      },
      "step 69": {
        "loss": 211.53439331054688,
        "l1_loss": 2336.44287109375,
        "l2_loss": 118.032470703125,
        "ortho_loss": 0.4422476589679718
      },
      "step 70": {
        "loss": 208.3627471923828,
        "l1_loss": 2294.0009765625,
        "l2_loss": 116.55853271484375,
        "ortho_loss": 0.44175782799720764
      },
      "step 71": {
        "loss": 216.02976989746094,
        "l1_loss": 2387.291015625,
        "l2_loss": 120.49397277832031,
        "ortho_loss": 0.4416448175907135
      },
      "step 72": {
        "loss": 214.51492309570312,
        "l1_loss": 2364.64306640625,
        "l2_loss": 119.88499450683594,
        "ortho_loss": 0.4421222507953644
      },
      "step 73": {
        "loss": 202.94850158691406,
        "l1_loss": 2224.54833984375,
        "l2_loss": 113.92268371582031,
        "ortho_loss": 0.4388023912906647
      },
      "step 74": {
        "loss": 207.8403778076172,
        "l1_loss": 2283.55712890625,
        "l2_loss": 116.45368194580078,
        "ortho_loss": 0.44413161277770996
      },
      "step 75": {
        "loss": 203.64662170410156,
        "l1_loss": 2231.40283203125,
        "l2_loss": 114.34602355957031,
        "ortho_loss": 0.4448446035385132
      },
      "step 76": {
        "loss": 206.2646942138672,
        "l1_loss": 2259.520263671875,
        "l2_loss": 115.83927917480469,
        "ortho_loss": 0.44593891501426697
      },
      "step 77": {
        "loss": 213.1734161376953,
        "l1_loss": 2346.58349609375,
        "l2_loss": 119.26580810546875,
        "ortho_loss": 0.4427059590816498
      },
      "step 78": {
        "loss": 200.34336853027344,
        "l1_loss": 2182.4296875,
        "l2_loss": 113.00180053710938,
        "ortho_loss": 0.443940132856369
      },
      "step 79": {
        "loss": 202.51365661621094,
        "l1_loss": 2207.84033203125,
        "l2_loss": 114.15543365478516,
        "ortho_loss": 0.4460146129131317
      },
      "step 80": {
        "loss": 198.7406463623047,
        "l1_loss": 2157.398193359375,
        "l2_loss": 112.40021514892578,
        "ortho_loss": 0.4450286626815796
      },
      "step 81": {
        "loss": 207.1427001953125,
        "l1_loss": 2261.6474609375,
        "l2_loss": 116.63201141357422,
        "ortho_loss": 0.4480310082435608
      },
      "step 82": {
        "loss": 206.31228637695312,
        "l1_loss": 2250.592041015625,
        "l2_loss": 116.24374389648438,
        "ortho_loss": 0.4485684931278229
      },
      "step 83": {
        "loss": 200.12002563476562,
        "l1_loss": 2170.81640625,
        "l2_loss": 113.24242401123047,
        "ortho_loss": 0.44950002431869507
      },
      "step 84": {
        "loss": 199.4131317138672,
        "l1_loss": 2161.56005859375,
        "l2_loss": 112.90570068359375,
        "ortho_loss": 0.4502623975276947
      },
      "step 85": {
        "loss": 198.15272521972656,
        "l1_loss": 2138.157470703125,
        "l2_loss": 112.58147430419922,
        "ortho_loss": 0.44950637221336365
      },
      "step 86": {
        "loss": 206.8593292236328,
        "l1_loss": 2246.798583984375,
        "l2_loss": 116.94253540039062,
        "ortho_loss": 0.44843366742134094
      },
      "step 87": {
        "loss": 201.56214904785156,
        "l1_loss": 2180.5947265625,
        "l2_loss": 114.2931137084961,
        "ortho_loss": 0.4524626135826111
      },
      "step 88": {
        "loss": 193.24325561523438,
        "l1_loss": 2072.8583984375,
        "l2_loss": 110.28399658203125,
        "ortho_loss": 0.4492027759552002
      },
      "step 89": {
        "loss": 198.76023864746094,
        "l1_loss": 2143.705322265625,
        "l2_loss": 112.9667739868164,
        "ortho_loss": 0.4524397850036621
      },
      "step 90": {
        "loss": 196.64730834960938,
        "l1_loss": 2109.17529296875,
        "l2_loss": 112.23486328125,
        "ortho_loss": 0.4544374942779541
      },
      "step 91": {
        "loss": 200.9403533935547,
        "l1_loss": 2162.9130859375,
        "l2_loss": 114.37853240966797,
        "ortho_loss": 0.45301544666290283
      },
      "step 92": {
        "loss": 188.01011657714844,
        "l1_loss": 1994.86572265625,
        "l2_loss": 108.17149353027344,
        "ortho_loss": 0.43986502289772034
      },
      "step 93": {
        "loss": 201.36654663085938,
        "l1_loss": 2163.162109375,
        "l2_loss": 114.79484558105469,
        "ortho_loss": 0.452231764793396
      },
      "step 94": {
        "loss": 196.9722900390625,
        "l1_loss": 2106.2841796875,
        "l2_loss": 112.67536926269531,
        "ortho_loss": 0.4555807113647461
      },
      "step 95": {
        "loss": 193.13548278808594,
        "l1_loss": 2054.72802734375,
        "l2_loss": 110.90048217773438,
        "ortho_loss": 0.45877015590667725
      },
      "step 96": {
        "loss": 196.2299041748047,
        "l1_loss": 2092.13427734375,
        "l2_loss": 112.49884796142578,
        "ortho_loss": 0.4569207429885864
      },
      "step 97": {
        "loss": 190.8262939453125,
        "l1_loss": 2015.5201416015625,
        "l2_loss": 110.15945434570312,
        "ortho_loss": 0.460417777299881
      },
      "step 98": {
        "loss": 195.06478881835938,
        "l1_loss": 2069.845703125,
        "l2_loss": 112.22520446777344,
        "ortho_loss": 0.4576803147792816
      },
      "step 99": {
        "loss": 195.7115020751953,
        "l1_loss": 2081.07568359375,
        "l2_loss": 112.4228515625,
        "ortho_loss": 0.45628929138183594
      },
      "step 100": {
        "loss": 188.42649841308594,
        "l1_loss": 1980.8330078125,
        "l2_loss": 109.14692687988281,
        "ortho_loss": 0.46245718002319336
      },
      "step 101": {
        "loss": 185.25714111328125,
        "l1_loss": 1938.635498046875,
        "l2_loss": 107.66546630859375,
        "ortho_loss": 0.4627179205417633
      },
      "step 102": {
        "loss": 191.52627563476562,
        "l1_loss": 2017.140869140625,
        "l2_loss": 110.7945556640625,
        "ortho_loss": 0.46084898710250854
      },
      "step 103": {
        "loss": 193.4327392578125,
        "l1_loss": 2037.5205078125,
        "l2_loss": 111.88606262207031,
        "ortho_loss": 0.4586959779262543
      },
      "step 104": {
        "loss": 185.4488983154297,
        "l1_loss": 1929.30517578125,
        "l2_loss": 108.2301025390625,
        "ortho_loss": 0.4658811688423157
      },
      "step 105": {
        "loss": 192.6477813720703,
        "l1_loss": 2019.909912109375,
        "l2_loss": 111.80534362792969,
        "ortho_loss": 0.46029022336006165
      },
      "step 106": {
        "loss": 191.41513061523438,
        "l1_loss": 2001.91259765625,
        "l2_loss": 111.2926025390625,
        "ortho_loss": 0.4601348042488098
      },
      "step 107": {
        "loss": 182.83401489257812,
        "l1_loss": 1886.63427734375,
        "l2_loss": 107.32176971435547,
        "ortho_loss": 0.4687659740447998
      },
      "step 108": {
        "loss": 190.3441925048828,
        "l1_loss": 1983.438232421875,
        "l2_loss": 110.96053314208984,
        "ortho_loss": 0.46120649576187134
      },
      "step 109": {
        "loss": 181.08193969726562,
        "l1_loss": 1859.3515625,
        "l2_loss": 106.66081237792969,
        "ortho_loss": 0.4705354869365692
      },
      "step 110": {
        "loss": 193.8629150390625,
        "l1_loss": 2016.130615234375,
        "l2_loss": 113.17186737060547,
        "ortho_loss": 0.4582182466983795
      },
      "step 111": {
        "loss": 187.02297973632812,
        "l1_loss": 1930.891845703125,
        "l2_loss": 109.74075317382812,
        "ortho_loss": 0.4655861556529999
      },
      "step 112": {
        "loss": 185.13845825195312,
        "l1_loss": 1904.1143798828125,
        "l2_loss": 108.92720031738281,
        "ortho_loss": 0.46692633628845215
      },
      "step 113": {
        "loss": 181.7834930419922,
        "l1_loss": 1851.322998046875,
        "l2_loss": 107.68331146240234,
        "ortho_loss": 0.4725472927093506
      },
      "step 114": {
        "loss": 177.62066650390625,
        "l1_loss": 1795.561767578125,
        "l2_loss": 105.75077819824219,
        "ortho_loss": 0.47423022985458374
      },
      "step 115": {
        "loss": 175.65650939941406,
        "l1_loss": 1763.84130859375,
        "l2_loss": 105.05558776855469,
        "ortho_loss": 0.47271332144737244
      },
      "step 116": {
        "loss": 180.90016174316406,
        "l1_loss": 1833.7308349609375,
        "l2_loss": 107.50373840332031,
        "ortho_loss": 0.4720301926136017
      },
      "step 117": {
        "loss": 183.50035095214844,
        "l1_loss": 1862.54296875,
        "l2_loss": 108.95169067382812,
        "ortho_loss": 0.46946513652801514
      },
      "step 118": {
        "loss": 177.04238891601562,
        "l1_loss": 1771.2982177734375,
        "l2_loss": 106.14266967773438,
        "ortho_loss": 0.47793683409690857
      },
      "step 119": {
        "loss": 179.6227264404297,
        "l1_loss": 1809.9158935546875,
        "l2_loss": 107.17880249023438,
        "ortho_loss": 0.4728322923183441
      },
      "step 120": {
        "loss": 182.362548828125,
        "l1_loss": 1832.8028564453125,
        "l2_loss": 109.00321960449219,
        "ortho_loss": 0.47216859459877014
      },
      "step 121": {
        "loss": 174.30007934570312,
        "l1_loss": 1729.7161865234375,
        "l2_loss": 105.06356811523438,
        "ortho_loss": 0.4786374270915985
      },
      "step 122": {
        "loss": 173.8055419921875,
        "l1_loss": 1720.752197265625,
        "l2_loss": 104.92745971679688,
        "ortho_loss": 0.4799729585647583
      },
      "step 123": {
        "loss": 175.06224060058594,
        "l1_loss": 1737.16259765625,
        "l2_loss": 105.52792358398438,
        "ortho_loss": 0.47805994749069214
      },
      "step 124": {
        "loss": 180.13648986816406,
        "l1_loss": 1800.68994140625,
        "l2_loss": 108.0618667602539,
        "ortho_loss": 0.47025349736213684
      },
      "step 125": {
        "loss": 181.24822998046875,
        "l1_loss": 1812.0166015625,
        "l2_loss": 108.72067260742188,
        "ortho_loss": 0.4689837396144867
      },
      "step 126": {
        "loss": 171.81240844726562,
        "l1_loss": 1680.128662109375,
        "l2_loss": 104.55902862548828,
        "ortho_loss": 0.4822934865951538
      },
      "step 127": {
        "loss": 175.5005340576172,
        "l1_loss": 1728.0303955078125,
        "l2_loss": 106.33170318603516,
        "ortho_loss": 0.4762520492076874
      },
      "step 128": {
        "loss": 172.79905700683594,
        "l1_loss": 1685.9454345703125,
        "l2_loss": 105.31293487548828,
        "ortho_loss": 0.4831196963787079
      },
      "step 129": {
        "loss": 180.820068359375,
        "l1_loss": 1792.051513671875,
        "l2_loss": 109.09130096435547,
        "ortho_loss": 0.46703973412513733
      },
      "step 130": {
        "loss": 174.9147186279297,
        "l1_loss": 1709.7056884765625,
        "l2_loss": 106.4788589477539,
        "ortho_loss": 0.4761791527271271
      },
      "step 131": {
        "loss": 177.3290252685547,
        "l1_loss": 1733.262939453125,
        "l2_loss": 107.95108032226562,
        "ortho_loss": 0.4743965268135071
      },
      "step 132": {
        "loss": 170.7711181640625,
        "l1_loss": 1643.0859375,
        "l2_loss": 104.99920654296875,
        "ortho_loss": 0.48477062582969666
      },
      "step 133": {
        "loss": 167.48660278320312,
        "l1_loss": 1598.799560546875,
        "l2_loss": 103.48570251464844,
        "ortho_loss": 0.48919275403022766
      },
      "step 134": {
        "loss": 162.48451232910156,
        "l1_loss": 1533.50390625,
        "l2_loss": 101.09528350830078,
        "ortho_loss": 0.49075642228126526
      },
      "step 135": {
        "loss": 164.9362030029297,
        "l1_loss": 1560.59228515625,
        "l2_loss": 102.46327209472656,
        "ortho_loss": 0.4923514723777771
      },
      "step 136": {
        "loss": 168.66964721679688,
        "l1_loss": 1603.381591796875,
        "l2_loss": 104.48567962646484,
        "ortho_loss": 0.48701024055480957
      },
      "step 137": {
        "loss": 171.57899475097656,
        "l1_loss": 1639.1700439453125,
        "l2_loss": 105.96420288085938,
        "ortho_loss": 0.47982257604599
      },
      "step 138": {
        "loss": 164.4705047607422,
        "l1_loss": 1542.1331787109375,
        "l2_loss": 102.73600006103516,
        "ortho_loss": 0.4917213022708893
      },
      "step 139": {
        "loss": 167.73248291015625,
        "l1_loss": 1582.861572265625,
        "l2_loss": 104.36949157714844,
        "ortho_loss": 0.4852317273616791
      },
      "step 140": {
        "loss": 167.82337951660156,
        "l1_loss": 1577.325439453125,
        "l2_loss": 104.68171691894531,
        "ortho_loss": 0.48638972640037537
      },
      "step 141": {
        "loss": 168.30654907226562,
        "l1_loss": 1581.918212890625,
        "l2_loss": 104.98135375976562,
        "ortho_loss": 0.48463454842567444
      },
      "step 142": {
        "loss": 163.52626037597656,
        "l1_loss": 1513.623291015625,
        "l2_loss": 102.93203735351562,
        "ortho_loss": 0.49306878447532654
      },
      "step 143": {
        "loss": 165.34100341796875,
        "l1_loss": 1541.121337890625,
        "l2_loss": 103.6475830078125,
        "ortho_loss": 0.4857493042945862
      },
      "step 144": {
        "loss": 166.80970764160156,
        "l1_loss": 1548.378173828125,
        "l2_loss": 104.82597351074219,
        "ortho_loss": 0.485985666513443
      },
      "step 145": {
        "loss": 158.36390686035156,
        "l1_loss": 1436.9322509765625,
        "l2_loss": 100.83651733398438,
        "ortho_loss": 0.5009092688560486
      },
      "step 146": {
        "loss": 165.63375854492188,
        "l1_loss": 1529.23583984375,
        "l2_loss": 104.41558074951172,
        "ortho_loss": 0.48733657598495483
      },
      "step 147": {
        "loss": 159.36195373535156,
        "l1_loss": 1441.0894775390625,
        "l2_loss": 101.6685791015625,
        "ortho_loss": 0.49787232279777527
      },
      "step 148": {
        "loss": 158.9976806640625,
        "l1_loss": 1430.355224609375,
        "l2_loss": 101.73344421386719,
        "ortho_loss": 0.5003958940505981
      },
      "step 149": {
        "loss": 155.46958923339844,
        "l1_loss": 1385.0283203125,
        "l2_loss": 100.01789855957031,
        "ortho_loss": 0.5055476427078247
      },
      "step 150": {
        "loss": 161.47471618652344,
        "l1_loss": 1453.8111572265625,
        "l2_loss": 103.27291107177734,
        "ortho_loss": 0.4936622977256775
      },
      "step 151": {
        "loss": 162.20306396484375,
        "l1_loss": 1465.1552734375,
        "l2_loss": 103.54791259765625,
        "ortho_loss": 0.489502876996994
      },
      "step 152": {
        "loss": 167.56309509277344,
        "l1_loss": 1536.912353515625,
        "l2_loss": 106.03866577148438,
        "ortho_loss": 0.47931939363479614
      },
      "step 153": {
        "loss": 152.5584716796875,
        "l1_loss": 1330.038330078125,
        "l2_loss": 99.30601501464844,
        "ortho_loss": 0.509320080280304
      },
      "step 154": {
        "loss": 159.95411682128906,
        "l1_loss": 1429.343505859375,
        "l2_loss": 102.73129272460938,
        "ortho_loss": 0.4908406436443329
      },
      "step 155": {
        "loss": 159.30496215820312,
        "l1_loss": 1414.01123046875,
        "l2_loss": 102.69519805908203,
        "ortho_loss": 0.4932073652744293
      },
      "step 156": {
        "loss": 160.46920776367188,
        "l1_loss": 1427.15283203125,
        "l2_loss": 103.33412170410156,
        "ortho_loss": 0.48973530530929565
      },
      "step 157": {
        "loss": 154.58326721191406,
        "l1_loss": 1342.9625244140625,
        "l2_loss": 100.81463623046875,
        "ortho_loss": 0.5012053847312927
      },
      "step 158": {
        "loss": 165.1343231201172,
        "l1_loss": 1486.18505859375,
        "l2_loss": 105.63917541503906,
        "ortho_loss": 0.477437287569046
      },
      "step 159": {
        "loss": 157.901611328125,
        "l1_loss": 1389.72314453125,
        "l2_loss": 102.26366424560547,
        "ortho_loss": 0.4902256429195404
      },
      "step 160": {
        "loss": 158.1090850830078,
        "l1_loss": 1381.9219970703125,
        "l2_loss": 102.783203125,
        "ortho_loss": 0.4899480938911438
      },
      "step 161": {
        "loss": 158.52395629882812,
        "l1_loss": 1382.13427734375,
        "l2_loss": 103.18955993652344,
        "ortho_loss": 0.4902779757976532
      },
      "step 162": {
        "loss": 153.12472534179688,
        "l1_loss": 1310.238525390625,
        "l2_loss": 100.6650390625,
        "ortho_loss": 0.5013648271560669
      },
      "step 163": {
        "loss": 156.48818969726562,
        "l1_loss": 1344.6365966796875,
        "l2_loss": 102.65341186523438,
        "ortho_loss": 0.49317115545272827
      },
      "step 164": {
        "loss": 148.70639038085938,
        "l1_loss": 1242.98779296875,
        "l2_loss": 98.9358139038086,
        "ortho_loss": 0.5106409788131714
      },
      "step 165": {
        "loss": 156.01405334472656,
        "l1_loss": 1336.130859375,
        "l2_loss": 102.51963806152344,
        "ortho_loss": 0.4917343258857727
      },
      "step 166": {
        "loss": 150.94967651367188,
        "l1_loss": 1266.55224609375,
        "l2_loss": 100.23721313476562,
        "ortho_loss": 0.5036622881889343
      },
      "step 167": {
        "loss": 153.29322814941406,
        "l1_loss": 1294.668701171875,
        "l2_loss": 101.4570541381836,
        "ortho_loss": 0.49422818422317505
      },
      "step 168": {
        "loss": 152.03009033203125,
        "l1_loss": 1274.0904541015625,
        "l2_loss": 101.01676940917969,
        "ortho_loss": 0.4971391260623932
      },
      "step 169": {
        "loss": 148.05776977539062,
        "l1_loss": 1215.50244140625,
        "l2_loss": 99.38670349121094,
        "ortho_loss": 0.5095963478088379
      },
      "step 170": {
        "loss": 147.32769775390625,
        "l1_loss": 1209.3192138671875,
        "l2_loss": 98.90422058105469,
        "ortho_loss": 0.5070992112159729
      },
      "step 171": {
        "loss": 152.8288116455078,
        "l1_loss": 1280.633056640625,
        "l2_loss": 101.55441284179688,
        "ortho_loss": 0.4907422661781311
      },
      "step 172": {
        "loss": 152.0282745361328,
        "l1_loss": 1262.4063720703125,
        "l2_loss": 101.482666015625,
        "ortho_loss": 0.49358272552490234
      },
      "step 173": {
        "loss": 151.12525939941406,
        "l1_loss": 1249.97705078125,
        "l2_loss": 101.07679748535156,
        "ortho_loss": 0.49378567934036255
      },
      "step 174": {
        "loss": 149.23294067382812,
        "l1_loss": 1214.436767578125,
        "l2_loss": 100.60551452636719,
        "ortho_loss": 0.49962228536605835
      },
      "step 175": {
        "loss": 146.45069885253906,
        "l1_loss": 1174.99267578125,
        "l2_loss": 99.40026092529297,
        "ortho_loss": 0.5073016285896301
      },
      "step 176": {
        "loss": 154.33950805664062,
        "l1_loss": 1281.306884765625,
        "l2_loss": 103.0384521484375,
        "ortho_loss": 0.4878596067428589
      },
      "step 177": {
        "loss": 143.73870849609375,
        "l1_loss": 1130.040771484375,
        "l2_loss": 98.48568725585938,
        "ortho_loss": 0.5138576626777649
      },
      "step 178": {
        "loss": 148.12991333007812,
        "l1_loss": 1184.9844970703125,
        "l2_loss": 100.68034362792969,
        "ortho_loss": 0.5019000768661499
      },
      "step 179": {
        "loss": 143.53564453125,
        "l1_loss": 1128.38671875,
        "l2_loss": 98.34923553466797,
        "ortho_loss": 0.5092902779579163
      },
      "step 180": {
        "loss": 142.85726928710938,
        "l1_loss": 1115.4442138671875,
        "l2_loss": 98.18827819824219,
        "ortho_loss": 0.5121665000915527
      },
      "step 181": {
        "loss": 145.3155975341797,
        "l1_loss": 1145.14990234375,
        "l2_loss": 99.45918273925781,
        "ortho_loss": 0.504128098487854
      },
      "step 182": {
        "loss": 143.2259521484375,
        "l1_loss": 1104.8599853515625,
        "l2_loss": 98.98052215576172,
        "ortho_loss": 0.510189950466156
      },
      "step 183": {
        "loss": 139.3061065673828,
        "l1_loss": 1056.47900390625,
        "l2_loss": 96.99506378173828,
        "ortho_loss": 0.5187352299690247
      },
      "step 184": {
        "loss": 144.2208251953125,
        "l1_loss": 1116.801025390625,
        "l2_loss": 99.49852752685547,
        "ortho_loss": 0.5025874376296997
      },
      "step 185": {
        "loss": 142.17494201660156,
        "l1_loss": 1088.247802734375,
        "l2_loss": 98.59423065185547,
        "ortho_loss": 0.5079565644264221
      },
      "step 186": {
        "loss": 143.26416015625,
        "l1_loss": 1097.712646484375,
        "l2_loss": 99.30528259277344,
        "ortho_loss": 0.5036669373512268
      },
      "step 187": {
        "loss": 139.79660034179688,
        "l1_loss": 1044.0615234375,
        "l2_loss": 97.98263549804688,
        "ortho_loss": 0.5150836706161499
      },
      "step 188": {
        "loss": 142.97348022460938,
        "l1_loss": 1090.18017578125,
        "l2_loss": 99.31616973876953,
        "ortho_loss": 0.5010754466056824
      },
      "step 189": {
        "loss": 136.22378540039062,
        "l1_loss": 995.14892578125,
        "l2_loss": 96.3653564453125,
        "ortho_loss": 0.524817705154419
      },
      "step 190": {
        "loss": 136.65199279785156,
        "l1_loss": 992.9866943359375,
        "l2_loss": 96.88029479980469,
        "ortho_loss": 0.5222712755203247
      },
      "step 191": {
        "loss": 136.4427947998047,
        "l1_loss": 987.0036010742188,
        "l2_loss": 96.91023254394531,
        "ortho_loss": 0.5241220593452454
      },
      "step 192": {
        "loss": 147.2528533935547,
        "l1_loss": 1139.76220703125,
        "l2_loss": 101.61361694335938,
        "ortho_loss": 0.48758262395858765
      },
      "step 193": {
        "loss": 138.0377655029297,
        "l1_loss": 1008.8109741210938,
        "l2_loss": 97.63398742675781,
        "ortho_loss": 0.5134073495864868
      },
      "step 194": {
        "loss": 140.49851989746094,
        "l1_loss": 1036.749267578125,
        "l2_loss": 98.97820281982422,
        "ortho_loss": 0.5033771991729736
      },
      "step 195": {
        "loss": 139.42312622070312,
        "l1_loss": 1019.7605590820312,
        "l2_loss": 98.58212280273438,
        "ortho_loss": 0.5057687759399414
      },
      "step 196": {
        "loss": 137.51890563964844,
        "l1_loss": 994.179443359375,
        "l2_loss": 97.70091247558594,
        "ortho_loss": 0.508100688457489
      },
      "step 197": {
        "loss": 141.19427490234375,
        "l1_loss": 1045.8612060546875,
        "l2_loss": 99.31016540527344,
        "ortho_loss": 0.49672332406044006
      },
      "step 198": {
        "loss": 135.20289611816406,
        "l1_loss": 953.4195556640625,
        "l2_loss": 97.01425170898438,
        "ortho_loss": 0.5186166167259216
      },
      "step 199": {
        "loss": 137.36093139648438,
        "l1_loss": 983.789794921875,
        "l2_loss": 97.95863342285156,
        "ortho_loss": 0.5069739818572998
      },
      "step 200": {
        "loss": 136.0726318359375,
        "l1_loss": 956.14892578125,
        "l2_loss": 97.77537536621094,
        "ortho_loss": 0.5130481719970703
      },
      "step 201": {
        "loss": 143.71788024902344,
        "l1_loss": 1059.5341796875,
        "l2_loss": 101.28721618652344,
        "ortho_loss": 0.4929491877555847
      },
      "step 202": {
        "loss": 135.40939331054688,
        "l1_loss": 948.2666015625,
        "l2_loss": 97.42768859863281,
        "ortho_loss": 0.5103915333747864
      },
      "step 203": {
        "loss": 134.7437286376953,
        "l1_loss": 942.502197265625,
        "l2_loss": 96.99267578125,
        "ortho_loss": 0.5096417665481567
      },
      "step 204": {
        "loss": 132.2906494140625,
        "l1_loss": 901.9357299804688,
        "l2_loss": 96.16119384765625,
        "ortho_loss": 0.5203201174736023
      },
      "step 205": {
        "loss": 140.95742797851562,
        "l1_loss": 1017.3414306640625,
        "l2_loss": 100.21444702148438,
        "ortho_loss": 0.49312227964401245
      },
      "step 206": {
        "loss": 138.36888122558594,
        "l1_loss": 985.4226684570312,
        "l2_loss": 98.90225219726562,
        "ortho_loss": 0.4973233640193939
      },
      "step 207": {
        "loss": 133.4092559814453,
        "l1_loss": 911.292236328125,
        "l2_loss": 96.90635681152344,
        "ortho_loss": 0.5120863318443298
      },
      "step 208": {
        "loss": 131.44625854492188,
        "l1_loss": 881.1746826171875,
        "l2_loss": 96.14781951904297,
        "ortho_loss": 0.5144769549369812
      },
      "step 209": {
        "loss": 130.81971740722656,
        "l1_loss": 860.460693359375,
        "l2_loss": 96.34906005859375,
        "ortho_loss": 0.5223458409309387
      },
      "step 210": {
        "loss": 136.30416870117188,
        "l1_loss": 937.5296020507812,
        "l2_loss": 98.75261688232422,
        "ortho_loss": 0.5036417245864868
      },
      "step 211": {
        "loss": 131.4542999267578,
        "l1_loss": 870.3240966796875,
        "l2_loss": 96.58975219726562,
        "ortho_loss": 0.5158848166465759
      },
      "step 212": {
        "loss": 129.23162841796875,
        "l1_loss": 836.00146484375,
        "l2_loss": 95.73926544189453,
        "ortho_loss": 0.5231152176856995
      },
      "step 213": {
        "loss": 131.36312866210938,
        "l1_loss": 857.4332275390625,
        "l2_loss": 97.01438903808594,
        "ortho_loss": 0.5141282081604004
      },
      "step 214": {
        "loss": 123.75483703613281,
        "l1_loss": 750.8346557617188,
        "l2_loss": 93.66642761230469,
        "ortho_loss": 0.5502113103866577
      },
      "step 215": {
        "loss": 128.50430297851562,
        "l1_loss": 823.3539428710938,
        "l2_loss": 95.51815795898438,
        "ortho_loss": 0.5199125409126282
      },
      "step 216": {
        "loss": 125.76319122314453,
        "l1_loss": 777.5384521484375,
        "l2_loss": 94.60859680175781,
        "ortho_loss": 0.530577540397644
      },
      "step 217": {
        "loss": 130.23228454589844,
        "l1_loss": 838.9020385742188,
        "l2_loss": 96.6246109008789,
        "ortho_loss": 0.5158332586288452
      },
      "step 218": {
        "loss": 131.1270751953125,
        "l1_loss": 837.5352172851562,
        "l2_loss": 97.57424926757812,
        "ortho_loss": 0.5142002105712891
      },
      "step 219": {
        "loss": 131.8191375732422,
        "l1_loss": 850.340087890625,
        "l2_loss": 97.75469970703125,
        "ortho_loss": 0.5082190036773682
      },
      "step 220": {
        "loss": 124.77764129638672,
        "l1_loss": 753.3037109375,
        "l2_loss": 94.5923080444336,
        "ortho_loss": 0.5318772792816162
      },
      "step 221": {
        "loss": 127.44501495361328,
        "l1_loss": 783.755859375,
        "l2_loss": 96.04252624511719,
        "ortho_loss": 0.5225352644920349
      },
      "step 222": {
        "loss": 127.88954162597656,
        "l1_loss": 795.3319091796875,
        "l2_loss": 96.02467346191406,
        "ortho_loss": 0.5159146785736084
      },
      "step 223": {
        "loss": 127.1693344116211,
        "l1_loss": 780.9378051757812,
        "l2_loss": 95.87983703613281,
        "ortho_loss": 0.5198491811752319
      },
      "step 224": {
        "loss": 125.70586395263672,
        "l1_loss": 763.960693359375,
        "l2_loss": 95.095458984375,
        "ortho_loss": 0.5198254585266113
      },
      "step 225": {
        "loss": 127.8375015258789,
        "l1_loss": 787.6983642578125,
        "l2_loss": 96.27810668945312,
        "ortho_loss": 0.5146138072013855
      },
      "step 226": {
        "loss": 122.61758422851562,
        "l1_loss": 710.1292114257812,
        "l2_loss": 94.15892791748047,
        "ortho_loss": 0.5348740220069885
      },
      "step 227": {
        "loss": 126.39952087402344,
        "l1_loss": 761.97900390625,
        "l2_loss": 95.86854553222656,
        "ortho_loss": 0.5181835889816284
      },
      "step 228": {
        "loss": 127.99938201904297,
        "l1_loss": 775.5592041015625,
        "l2_loss": 96.92567443847656,
        "ortho_loss": 0.5134157538414001
      },
      "step 229": {
        "loss": 127.13813018798828,
        "l1_loss": 768.75,
        "l2_loss": 96.33650970458984,
        "ortho_loss": 0.5162062048912048
      },
      "step 230": {
        "loss": 123.45032501220703,
        "l1_loss": 719.932861328125,
        "l2_loss": 94.60053253173828,
        "ortho_loss": 0.5248146057128906
      },
      "step 231": {
        "loss": 128.46920776367188,
        "l1_loss": 783.3003540039062,
        "l2_loss": 97.0863265991211,
        "ortho_loss": 0.5087747573852539
      },
      "step 232": {
        "loss": 123.36089324951172,
        "l1_loss": 701.4276123046875,
        "l2_loss": 95.25118255615234,
        "ortho_loss": 0.5260139107704163
      },
      "step 233": {
        "loss": 121.54491424560547,
        "l1_loss": 672.822998046875,
        "l2_loss": 94.57872009277344,
        "ortho_loss": 0.5327479839324951
      },
      "step 234": {
        "loss": 120.91536712646484,
        "l1_loss": 669.740966796875,
        "l2_loss": 94.07241821289062,
        "ortho_loss": 0.5330455899238586
      },
      "step 235": {
        "loss": 129.4224090576172,
        "l1_loss": 793.434326171875,
        "l2_loss": 97.6346435546875,
        "ortho_loss": 0.5040345191955566
      },
      "step 236": {
        "loss": 126.5877685546875,
        "l1_loss": 748.3841552734375,
        "l2_loss": 96.60137939453125,
        "ortho_loss": 0.5102600455284119
      },
      "step 237": {
        "loss": 128.16580200195312,
        "l1_loss": 756.3433227539062,
        "l2_loss": 97.86116027832031,
        "ortho_loss": 0.5090140104293823
      },
      "step 238": {
        "loss": 121.25443267822266,
        "l1_loss": 665.9839477539062,
        "l2_loss": 94.56198120117188,
        "ortho_loss": 0.5309257507324219
      },
      "step 239": {
        "loss": 117.25338745117188,
        "l1_loss": 609.5050048828125,
        "l2_loss": 92.81907653808594,
        "ortho_loss": 0.5410700440406799
      },
      "step 240": {
        "loss": 115.51802825927734,
        "l1_loss": 582.9060668945312,
        "l2_loss": 92.14599609375,
        "ortho_loss": 0.5579184293746948
      },
      "step 241": {
        "loss": 126.26490783691406,
        "l1_loss": 728.3111572265625,
        "l2_loss": 97.08116149902344,
        "ortho_loss": 0.5130248665809631
      },
      "step 242": {
        "loss": 117.04869079589844,
        "l1_loss": 598.189697265625,
        "l2_loss": 93.06678771972656,
        "ortho_loss": 0.543148398399353
      },
      "step 243": {
        "loss": 120.51737976074219,
        "l1_loss": 642.9254150390625,
        "l2_loss": 94.74748229980469,
        "ortho_loss": 0.5288052558898926
      },
      "step 244": {
        "loss": 120.48757934570312,
        "l1_loss": 649.60693359375,
        "l2_loss": 94.4507827758789,
        "ortho_loss": 0.5252180099487305
      },
      "step 245": {
        "loss": 121.50477600097656,
        "l1_loss": 657.3292236328125,
        "l2_loss": 95.15928649902344,
        "ortho_loss": 0.5231882929801941
      },
      "step 246": {
        "loss": 123.04230499267578,
        "l1_loss": 676.3651733398438,
        "l2_loss": 95.93614959716797,
        "ortho_loss": 0.5155068635940552
      },
      "step 247": {
        "loss": 117.73401641845703,
        "l1_loss": 603.6185302734375,
        "l2_loss": 93.53568267822266,
        "ortho_loss": 0.5359574556350708
      },
      "step 248": {
        "loss": 122.99929809570312,
        "l1_loss": 662.7623291015625,
        "l2_loss": 96.43685150146484,
        "ortho_loss": 0.5195640921592712
      },
      "step 249": {
        "loss": 120.0822525024414,
        "l1_loss": 628.4737548828125,
        "l2_loss": 94.89064025878906,
        "ortho_loss": 0.5266304612159729
      },
      "step 250": {
        "loss": 118.71305847167969,
        "l1_loss": 606.5555419921875,
        "l2_loss": 94.39779663085938,
        "ortho_loss": 0.5304195284843445
      },
      "step 251": {
        "loss": 126.07222747802734,
        "l1_loss": 713.2384033203125,
        "l2_loss": 97.49193572998047,
        "ortho_loss": 0.5076191425323486
      },
      "step 252": {
        "loss": 122.68880462646484,
        "l1_loss": 659.881103515625,
        "l2_loss": 96.24205780029297,
        "ortho_loss": 0.515055239200592
      },
      "step 253": {
        "loss": 121.61074829101562,
        "l1_loss": 637.08935546875,
        "l2_loss": 96.07505798339844,
        "ortho_loss": 0.5212004780769348
      },
      "step 254": {
        "loss": 117.11582946777344,
        "l1_loss": 567.7623291015625,
        "l2_loss": 94.35140991210938,
        "ortho_loss": 0.53922039270401
      },
      "step 255": {
        "loss": 116.85308074951172,
        "l1_loss": 568.0458984375,
        "l2_loss": 94.07780456542969,
        "ortho_loss": 0.5344483852386475
      },
      "step 256": {
        "loss": 117.51947784423828,
        "l1_loss": 573.4402465820312,
        "l2_loss": 94.5286865234375,
        "ortho_loss": 0.5318668484687805
      },
      "step 257": {
        "loss": 121.17696380615234,
        "l1_loss": 636.8326416015625,
        "l2_loss": 95.65187072753906,
        "ortho_loss": 0.5178477764129639
      },
      "step 258": {
        "loss": 117.50202941894531,
        "l1_loss": 576.2719116210938,
        "l2_loss": 94.39813232421875,
        "ortho_loss": 0.5302274227142334
      },
      "step 259": {
        "loss": 118.26079559326172,
        "l1_loss": 587.937744140625,
        "l2_loss": 94.69080352783203,
        "ortho_loss": 0.5248635411262512
      },
      "step 260": {
        "loss": 117.10620880126953,
        "l1_loss": 558.6357421875,
        "l2_loss": 94.70729064941406,
        "ortho_loss": 0.5349134802818298
      },
      "step 261": {
        "loss": 115.14301300048828,
        "l1_loss": 532.0858764648438,
        "l2_loss": 93.80545043945312,
        "ortho_loss": 0.5413430333137512
      },
      "step 262": {
        "loss": 114.74957275390625,
        "l1_loss": 524.7093505859375,
        "l2_loss": 93.70701599121094,
        "ortho_loss": 0.5418428778648376
      },
      "step 263": {
        "loss": 116.18486785888672,
        "l1_loss": 540.451416015625,
        "l2_loss": 94.51325988769531,
        "ortho_loss": 0.5354828834533691
      },
      "step 264": {
        "loss": 119.10327911376953,
        "l1_loss": 589.1273193359375,
        "l2_loss": 95.48588562011719,
        "ortho_loss": 0.5229644775390625
      },
      "step 265": {
        "loss": 115.70684051513672,
        "l1_loss": 530.4205322265625,
        "l2_loss": 94.4365005493164,
        "ortho_loss": 0.535183846950531
      },
      "step 266": {
        "loss": 119.0525894165039,
        "l1_loss": 585.56591796875,
        "l2_loss": 95.57762145996094,
        "ortho_loss": 0.5233147144317627
      },
      "step 267": {
        "loss": 113.3040771484375,
        "l1_loss": 495.6654968261719,
        "l2_loss": 93.42278289794922,
        "ortho_loss": 0.5467077493667603
      },
      "step 268": {
        "loss": 119.45354461669922,
        "l1_loss": 587.6307373046875,
        "l2_loss": 95.89602661132812,
        "ortho_loss": 0.5228810906410217
      },
      "step 269": {
        "loss": 113.72969055175781,
        "l1_loss": 502.4091796875,
        "l2_loss": 93.57928466796875,
        "ortho_loss": 0.5403743982315063
      },
      "step 270": {
        "loss": 113.11575317382812,
        "l1_loss": 480.7718200683594,
        "l2_loss": 93.83006286621094,
        "ortho_loss": 0.548152506351471
      },
      "step 271": {
        "loss": 115.02230834960938,
        "l1_loss": 516.9738159179688,
        "l2_loss": 94.28990173339844,
        "ortho_loss": 0.5345437526702881
      },
      "step 272": {
        "loss": 119.04029846191406,
        "l1_loss": 572.1446533203125,
        "l2_loss": 96.10202026367188,
        "ortho_loss": 0.5248985886573792
      },
      "step 273": {
        "loss": 118.414306640625,
        "l1_loss": 561.841064453125,
        "l2_loss": 95.887939453125,
        "ortho_loss": 0.527286946773529
      },
      "step 274": {
        "loss": 115.74053955078125,
        "l1_loss": 528.280517578125,
        "l2_loss": 94.5562515258789,
        "ortho_loss": 0.5306724905967712
      },
      "step 275": {
        "loss": 115.37682342529297,
        "l1_loss": 516.547607421875,
        "l2_loss": 94.66148376464844,
        "ortho_loss": 0.5343693494796753
      },
      "step 276": {
        "loss": 115.65921783447266,
        "l1_loss": 520.9150390625,
        "l2_loss": 94.76942443847656,
        "ortho_loss": 0.5319157838821411
      },
      "step 277": {
        "loss": 115.80439758300781,
        "l1_loss": 525.750244140625,
        "l2_loss": 94.7213134765625,
        "ortho_loss": 0.5306808352470398
      },
      "step 278": {
        "loss": 110.03128814697266,
        "l1_loss": 426.4027099609375,
        "l2_loss": 92.91934967041016,
        "ortho_loss": 0.5583245754241943
      },
      "step 279": {
        "loss": 111.23706817626953,
        "l1_loss": 454.95159912109375,
        "l2_loss": 92.98432922363281,
        "ortho_loss": 0.5468205809593201
      },
      "step 280": {
        "loss": 111.80533599853516,
        "l1_loss": 451.6113586425781,
        "l2_loss": 93.6861572265625,
        "ortho_loss": 0.5472296476364136
      },
      "step 281": {
        "loss": 111.91275787353516,
        "l1_loss": 457.6707763671875,
        "l2_loss": 93.551513671875,
        "ortho_loss": 0.5441495776176453
      },
      "step 282": {
        "loss": 109.5966796875,
        "l1_loss": 423.684814453125,
        "l2_loss": 92.59386444091797,
        "ortho_loss": 0.554216742515564
      },
      "step 283": {
        "loss": 111.16401672363281,
        "l1_loss": 439.47711181640625,
        "l2_loss": 93.53012084960938,
        "ortho_loss": 0.5480988621711731
      },
      "step 284": {
        "loss": 116.07532501220703,
        "l1_loss": 513.4969482421875,
        "l2_loss": 95.4822998046875,
        "ortho_loss": 0.531499981880188
      },
      "step 285": {
        "loss": 112.80703735351562,
        "l1_loss": 455.1255187988281,
        "l2_loss": 94.54769134521484,
        "ortho_loss": 0.5432097911834717
      },
      "step 286": {
        "loss": 109.7314224243164,
        "l1_loss": 418.4214172363281,
        "l2_loss": 92.93931579589844,
        "ortho_loss": 0.5525461435317993
      },
      "step 287": {
        "loss": 110.92420196533203,
        "l1_loss": 443.216064453125,
        "l2_loss": 93.14097595214844,
        "ortho_loss": 0.545909583568573
      },
      "step 288": {
        "loss": 112.9642105102539,
        "l1_loss": 469.36566162109375,
        "l2_loss": 94.13572692871094,
        "ortho_loss": 0.5385633111000061
      },
      "step 289": {
        "loss": 109.73566436767578,
        "l1_loss": 427.9976806640625,
        "l2_loss": 92.56118774414062,
        "ortho_loss": 0.5456952452659607
      },
      "step 290": {
        "loss": 112.61186981201172,
        "l1_loss": 456.47772216796875,
        "l2_loss": 94.29885864257812,
        "ortho_loss": 0.5390395522117615
      },
      "step 291": {
        "loss": 107.79308319091797,
        "l1_loss": 384.3055114746094,
        "l2_loss": 92.36465454101562,
        "ortho_loss": 0.5620414614677429
      },
      "step 292": {
        "loss": 114.75433349609375,
        "l1_loss": 498.6125793457031,
        "l2_loss": 94.75665283203125,
        "ortho_loss": 0.5317963361740112
      },
      "step 293": {
        "loss": 111.11958312988281,
        "l1_loss": 432.69793701171875,
        "l2_loss": 93.75704193115234,
        "ortho_loss": 0.5462943911552429
      },
      "step 294": {
        "loss": 111.09248352050781,
        "l1_loss": 426.33349609375,
        "l2_loss": 93.9845962524414,
        "ortho_loss": 0.5455247163772583
      },
      "step 295": {
        "loss": 111.32075500488281,
        "l1_loss": 439.9826965332031,
        "l2_loss": 93.66743469238281,
        "ortho_loss": 0.5401386618614197
      },
      "step 296": {
        "loss": 112.6606674194336,
        "l1_loss": 462.17388916015625,
        "l2_loss": 94.12001037597656,
        "ortho_loss": 0.5370603799819946
      },
      "step 297": {
        "loss": 109.62724304199219,
        "l1_loss": 408.12677001953125,
        "l2_loss": 93.24717712402344,
        "ortho_loss": 0.5499300956726074
      },
      "step 298": {
        "loss": 115.66339111328125,
        "l1_loss": 505.97412109375,
        "l2_loss": 95.37141418457031,
        "ortho_loss": 0.5300586223602295
      },
      "step 299": {
        "loss": 111.46284484863281,
        "l1_loss": 435.3880615234375,
        "l2_loss": 93.99299621582031,
        "ortho_loss": 0.5432723760604858
      },
      "step 300": {
        "loss": 110.36661529541016,
        "l1_loss": 423.3096618652344,
        "l2_loss": 93.37965393066406,
        "ortho_loss": 0.5457319021224976
      },
      "step 301": {
        "loss": 108.64298248291016,
        "l1_loss": 381.0727233886719,
        "l2_loss": 93.34468078613281,
        "ortho_loss": 0.5539936423301697
      },
      "step 302": {
        "loss": 106.91495513916016,
        "l1_loss": 357.95709228515625,
        "l2_loss": 92.54059600830078,
        "ortho_loss": 0.5607288479804993
      },
      "step 303": {
        "loss": 110.0816421508789,
        "l1_loss": 409.3614196777344,
        "l2_loss": 93.65252685546875,
        "ortho_loss": 0.54659104347229
      },
      "step 304": {
        "loss": 110.50283813476562,
        "l1_loss": 416.7564697265625,
        "l2_loss": 93.77806091308594,
        "ortho_loss": 0.5451750159263611
      },
      "step 305": {
        "loss": 115.85477447509766,
        "l1_loss": 501.21563720703125,
        "l2_loss": 95.75293731689453,
        "ortho_loss": 0.5321621298789978
      },
      "step 306": {
        "loss": 115.06890106201172,
        "l1_loss": 489.2633361816406,
        "l2_loss": 95.44491577148438,
        "ortho_loss": 0.5345380902290344
      },
      "step 307": {
        "loss": 109.71609497070312,
        "l1_loss": 399.255615234375,
        "l2_loss": 93.69111633300781,
        "ortho_loss": 0.5475893020629883
      },
      "step 308": {
        "loss": 110.79949188232422,
        "l1_loss": 417.7160949707031,
        "l2_loss": 94.03642272949219,
        "ortho_loss": 0.5442971587181091
      },
      "step 309": {
        "loss": 106.1014633178711,
        "l1_loss": 348.2746887207031,
        "l2_loss": 92.114013671875,
        "ortho_loss": 0.5646231770515442
      },
      "step 310": {
        "loss": 107.14766693115234,
        "l1_loss": 358.69451904296875,
        "l2_loss": 92.74417114257812,
        "ortho_loss": 0.5572047829627991
      },
      "step 311": {
        "loss": 107.06179809570312,
        "l1_loss": 359.2275085449219,
        "l2_loss": 92.63693237304688,
        "ortho_loss": 0.55765700340271
      },
      "step 312": {
        "loss": 105.86158752441406,
        "l1_loss": 336.6561584472656,
        "l2_loss": 92.33885955810547,
        "ortho_loss": 0.5648131966590881
      },
      "step 313": {
        "loss": 112.41344451904297,
        "l1_loss": 430.430908203125,
        "l2_loss": 95.14181518554688,
        "ortho_loss": 0.5439178347587585
      },
      "step 314": {
        "loss": 110.25852966308594,
        "l1_loss": 403.2646789550781,
        "l2_loss": 94.07321166992188,
        "ortho_loss": 0.5473455190658569
      },
      "step 315": {
        "loss": 106.9078369140625,
        "l1_loss": 354.18499755859375,
        "l2_loss": 92.68467712402344,
        "ortho_loss": 0.5575984716415405
      },
      "step 316": {
        "loss": 110.46353149414062,
        "l1_loss": 411.4505310058594,
        "l2_loss": 93.95086669921875,
        "ortho_loss": 0.5464293360710144
      },
      "step 317": {
        "loss": 108.66990661621094,
        "l1_loss": 369.14825439453125,
        "l2_loss": 93.84844970703125,
        "ortho_loss": 0.5552452802658081
      },
      "step 318": {
        "loss": 112.83615112304688,
        "l1_loss": 446.9841003417969,
        "l2_loss": 94.90272521972656,
        "ortho_loss": 0.5406107902526855
      },
      "step 319": {
        "loss": 109.74443054199219,
        "l1_loss": 389.73089599609375,
        "l2_loss": 94.1003189086914,
        "ortho_loss": 0.5487751960754395
      },
      "step 320": {
        "loss": 113.48100280761719,
        "l1_loss": 451.9458312988281,
        "l2_loss": 95.34909057617188,
        "ortho_loss": 0.5407625436782837
      },
      "step 321": {
        "loss": 106.14070129394531,
        "l1_loss": 337.5596618652344,
        "l2_loss": 92.5821533203125,
        "ortho_loss": 0.56162428855896
      },
      "step 322": {
        "loss": 108.6580581665039,
        "l1_loss": 375.9838562011719,
        "l2_loss": 93.56353759765625,
        "ortho_loss": 0.5517094135284424
      },
      "step 323": {
        "loss": 110.12919616699219,
        "l1_loss": 398.3648376464844,
        "l2_loss": 94.1396484375,
        "ortho_loss": 0.549560010433197
      },
      "step 324": {
        "loss": 106.55420684814453,
        "l1_loss": 341.8063659667969,
        "l2_loss": 92.82589721679688,
        "ortho_loss": 0.5605269074440002
      },
      "step 325": {
        "loss": 106.01893615722656,
        "l1_loss": 340.1107177734375,
        "l2_loss": 92.35844421386719,
        "ortho_loss": 0.5606176257133484
      },
      "step 326": {
        "loss": 106.26588439941406,
        "l1_loss": 328.3014831542969,
        "l2_loss": 93.07740783691406,
        "ortho_loss": 0.5641794800758362
      },
      "step 327": {
        "loss": 105.9645004272461,
        "l1_loss": 335.4593200683594,
        "l2_loss": 92.48991394042969,
        "ortho_loss": 0.5621312856674194
      },
      "step 328": {
        "loss": 108.3591537475586,
        "l1_loss": 375.89886474609375,
        "l2_loss": 93.26776885986328,
        "ortho_loss": 0.5542393922805786
      },
      "step 329": {
        "loss": 107.60382080078125,
        "l1_loss": 341.23614501953125,
        "l2_loss": 93.89839935302734,
        "ortho_loss": 0.5597482323646545
      },
      "step 330": {
        "loss": 104.68013000488281,
        "l1_loss": 305.59063720703125,
        "l2_loss": 92.39967346191406,
        "ortho_loss": 0.5682967305183411
      },
      "step 331": {
        "loss": 112.63408660888672,
        "l1_loss": 420.4820861816406,
        "l2_loss": 95.75996398925781,
        "ortho_loss": 0.5483912229537964
      },
      "step 332": {
        "loss": 108.72069549560547,
        "l1_loss": 369.90765380859375,
        "l2_loss": 93.8687515258789,
        "ortho_loss": 0.5563902258872986
      },
      "step 333": {
        "loss": 108.63933563232422,
        "l1_loss": 373.8201904296875,
        "l2_loss": 93.63107299804688,
        "ortho_loss": 0.5546126961708069
      },
      "step 334": {
        "loss": 112.77559661865234,
        "l1_loss": 437.4836730957031,
        "l2_loss": 95.22148132324219,
        "ortho_loss": 0.5477174520492554
      },
      "step 335": {
        "loss": 105.49803161621094,
        "l1_loss": 318.8656311035156,
        "l2_loss": 92.68684387207031,
        "ortho_loss": 0.5656445026397705
      },
      "step 336": {
        "loss": 107.58478546142578,
        "l1_loss": 347.12493896484375,
        "l2_loss": 93.643798828125,
        "ortho_loss": 0.5598946213722229
      },
      "step 337": {
        "loss": 103.52500915527344,
        "l1_loss": 285.18463134765625,
        "l2_loss": 92.06005096435547,
        "ortho_loss": 0.5757395029067993
      },
      "step 338": {
        "loss": 105.38248443603516,
        "l1_loss": 321.9527587890625,
        "l2_loss": 92.44793701171875,
        "ortho_loss": 0.564343273639679
      },
      "step 339": {
        "loss": 106.0433120727539,
        "l1_loss": 314.26129150390625,
        "l2_loss": 93.41607666015625,
        "ortho_loss": 0.5678632855415344
      },
      "step 340": {
        "loss": 102.45865631103516,
        "l1_loss": 272.34515380859375,
        "l2_loss": 91.50675964355469,
        "ortho_loss": 0.5809381008148193
      },
      "step 341": {
        "loss": 107.96068572998047,
        "l1_loss": 348.51580810546875,
        "l2_loss": 93.96409606933594,
        "ortho_loss": 0.5595911145210266
      },
      "step 342": {
        "loss": 109.36978912353516,
        "l1_loss": 384.0504150390625,
        "l2_loss": 93.95235443115234,
        "ortho_loss": 0.554165780544281
      },
      "step 343": {
        "loss": 106.6073989868164,
        "l1_loss": 327.2015686035156,
        "l2_loss": 93.46281433105469,
        "ortho_loss": 0.5652729868888855
      },
      "step 344": {
        "loss": 106.1871337890625,
        "l1_loss": 324.66851806640625,
        "l2_loss": 93.14376831054688,
        "ortho_loss": 0.5662689805030823
      },
      "step 345": {
        "loss": 106.76016235351562,
        "l1_loss": 334.0309143066406,
        "l2_loss": 93.34242248535156,
        "ortho_loss": 0.5650168657302856
      },
      "step 346": {
        "loss": 103.19224548339844,
        "l1_loss": 280.0689697265625,
        "l2_loss": 91.93180847167969,
        "ortho_loss": 0.5767782330513
      },
      "step 347": {
        "loss": 114.47965240478516,
        "l1_loss": 451.2325134277344,
        "l2_loss": 96.37525939941406,
        "ortho_loss": 0.5509247183799744
      },
      "step 348": {
        "loss": 106.4869613647461,
        "l1_loss": 321.2811279296875,
        "l2_loss": 93.57881164550781,
        "ortho_loss": 0.5690726637840271
      },
      "step 349": {
        "loss": 106.43107604980469,
        "l1_loss": 327.78271484375,
        "l2_loss": 93.26302337646484,
        "ortho_loss": 0.5674551725387573
      },
      "step 350": {
        "loss": 108.6512680053711,
        "l1_loss": 372.6443176269531,
        "l2_loss": 93.68950653076172,
        "ortho_loss": 0.559908926486969
      },
      "step 351": {
        "loss": 104.84658813476562,
        "l1_loss": 300.90911865234375,
        "l2_loss": 92.7530288696289,
        "ortho_loss": 0.5719665288925171
      },
      "step 352": {
        "loss": 112.36941528320312,
        "l1_loss": 429.69683837890625,
        "l2_loss": 95.12609100341797,
        "ortho_loss": 0.5545237064361572
      },
      "step 353": {
        "loss": 105.10680389404297,
        "l1_loss": 310.2870788574219,
        "l2_loss": 92.63809204101562,
        "ortho_loss": 0.5723108053207397
      },
      "step 354": {
        "loss": 105.91934967041016,
        "l1_loss": 318.1761474609375,
        "l2_loss": 93.13520812988281,
        "ortho_loss": 0.5709934830665588
      },
      "step 355": {
        "loss": 106.54026794433594,
        "l1_loss": 327.053955078125,
        "l2_loss": 93.40113830566406,
        "ortho_loss": 0.5696921348571777
      },
      "step 356": {
        "loss": 108.98163604736328,
        "l1_loss": 366.7540283203125,
        "l2_loss": 94.25492858886719,
        "ortho_loss": 0.565517783164978
      },
      "step 357": {
        "loss": 99.89582824707031,
        "l1_loss": 219.55809020996094,
        "l2_loss": 91.052978515625,
        "ortho_loss": 0.6052519679069519
      },
      "step 358": {
        "loss": 105.46080780029297,
        "l1_loss": 311.37701416015625,
        "l2_loss": 92.94825744628906,
        "ortho_loss": 0.5747395753860474
      },
      "step 359": {
        "loss": 110.3193588256836,
        "l1_loss": 388.6116943359375,
        "l2_loss": 94.71836853027344,
        "ortho_loss": 0.5652719736099243
      },
      "step 360": {
        "loss": 106.98199462890625,
        "l1_loss": 329.6739501953125,
        "l2_loss": 93.7376708984375,
        "ortho_loss": 0.5736193656921387
      },
      "step 361": {
        "loss": 106.03530883789062,
        "l1_loss": 310.8967590332031,
        "l2_loss": 93.54161071777344,
        "ortho_loss": 0.5782825946807861
      },
      "step 362": {
        "loss": 103.07511901855469,
        "l1_loss": 269.2706298828125,
        "l2_loss": 92.24563598632812,
        "ortho_loss": 0.5865540504455566
      },
      "step 363": {
        "loss": 106.06249237060547,
        "l1_loss": 316.62115478515625,
        "l2_loss": 93.33981323242188,
        "ortho_loss": 0.5783161520957947
      },
      "step 364": {
        "loss": 102.77748107910156,
        "l1_loss": 265.0579833984375,
        "l2_loss": 92.11637878417969,
        "ortho_loss": 0.5878827571868896
      },
      "step 365": {
        "loss": 110.88397979736328,
        "l1_loss": 400.2816162109375,
        "l2_loss": 94.81581115722656,
        "ortho_loss": 0.5690881609916687
      },
      "step 366": {
        "loss": 107.1415786743164,
        "l1_loss": 330.8074645996094,
        "l2_loss": 93.85157775878906,
        "ortho_loss": 0.5769949555397034
      },
      "step 367": {
        "loss": 103.1958999633789,
        "l1_loss": 271.5659484863281,
        "l2_loss": 92.2745132446289,
        "ortho_loss": 0.5874922275543213
      },
      "step 368": {
        "loss": 103.59693908691406,
        "l1_loss": 289.97979736328125,
        "l2_loss": 91.93922424316406,
        "ortho_loss": 0.5852702260017395
      },
      "step 369": {
        "loss": 108.2722396850586,
        "l1_loss": 347.12103271484375,
        "l2_loss": 94.32958984375,
        "ortho_loss": 0.5780897736549377
      },
      "step 370": {
        "loss": 106.05511474609375,
        "l1_loss": 317.4664306640625,
        "l2_loss": 93.29823303222656,
        "ortho_loss": 0.5822652578353882
      },
      "step 371": {
        "loss": 104.69310760498047,
        "l1_loss": 304.1248474121094,
        "l2_loss": 92.46975708007812,
        "ortho_loss": 0.5835464000701904
      },
      "step 372": {
        "loss": 110.15361785888672,
        "l1_loss": 380.21185302734375,
        "l2_loss": 94.88751220703125,
        "ortho_loss": 0.5762888193130493
      },
      "step 373": {
        "loss": 103.70772552490234,
        "l1_loss": 273.0334777832031,
        "l2_loss": 92.727294921875,
        "ortho_loss": 0.5908743739128113
      },
      "step 374": {
        "loss": 104.56587219238281,
        "l1_loss": 301.19927978515625,
        "l2_loss": 92.45933532714844,
        "ortho_loss": 0.5856326222419739
      },
      "step 375": {
        "loss": 101.24372863769531,
        "l1_loss": 236.36978149414062,
        "l2_loss": 91.72867584228516,
        "ortho_loss": 0.6026514172554016
      },
      "step 376": {
        "loss": 101.61962127685547,
        "l1_loss": 236.69081115722656,
        "l2_loss": 92.09172058105469,
        "ortho_loss": 0.6026236414909363
      },
      "step 377": {
        "loss": 107.20097351074219,
        "l1_loss": 336.0848388671875,
        "l2_loss": 93.69916534423828,
        "ortho_loss": 0.5841235518455505
      },
      "step 378": {
        "loss": 104.16473388671875,
        "l1_loss": 289.14520263671875,
        "l2_loss": 92.53985595703125,
        "ortho_loss": 0.5906726121902466
      },
      "step 379": {
        "loss": 100.44747924804688,
        "l1_loss": 228.0686492919922,
        "l2_loss": 91.26437377929688,
        "ortho_loss": 0.6036615967750549
      },
      "step 380": {
        "loss": 107.28141784667969,
        "l1_loss": 342.8765869140625,
        "l2_loss": 93.50784301757812,
        "ortho_loss": 0.5850653648376465
      },
      "step 381": {
        "loss": 104.31046295166016,
        "l1_loss": 274.9862060546875,
        "l2_loss": 93.25157165527344,
        "ortho_loss": 0.5944333076477051
      },
      "step 382": {
        "loss": 101.3398208618164,
        "l1_loss": 237.75045776367188,
        "l2_loss": 91.76947784423828,
        "ortho_loss": 0.6032857894897461
      },
      "step 383": {
        "loss": 100.52880096435547,
        "l1_loss": 226.7954559326172,
        "l2_loss": 91.3963394165039,
        "ortho_loss": 0.6064865589141846
      },
      "step 384": {
        "loss": 101.76679992675781,
        "l1_loss": 243.57688903808594,
        "l2_loss": 91.96343994140625,
        "ortho_loss": 0.6028737425804138
      },
      "step 385": {
        "loss": 107.49261474609375,
        "l1_loss": 337.8891906738281,
        "l2_loss": 93.91824340820312,
        "ortho_loss": 0.5880646705627441
      },
      "step 386": {
        "loss": 104.0666732788086,
        "l1_loss": 283.07891845703125,
        "l2_loss": 92.6836929321289,
        "ortho_loss": 0.5982114672660828
      },
      "step 387": {
        "loss": 103.40657043457031,
        "l1_loss": 264.57208251953125,
        "l2_loss": 92.76353454589844,
        "ortho_loss": 0.6014896035194397
      },
      "step 388": {
        "loss": 106.62438201904297,
        "l1_loss": 324.2957763671875,
        "l2_loss": 93.59327697753906,
        "ortho_loss": 0.5927477478981018
      },
      "step 389": {
        "loss": 103.8913345336914,
        "l1_loss": 277.1160888671875,
        "l2_loss": 92.74681091308594,
        "ortho_loss": 0.5988094806671143
      },
      "step 390": {
        "loss": 105.49017333984375,
        "l1_loss": 302.4269104003906,
        "l2_loss": 93.33360290527344,
        "ortho_loss": 0.5949167013168335
      },
      "step 391": {
        "loss": 101.14149475097656,
        "l1_loss": 241.3311309814453,
        "l2_loss": 91.4275131225586,
        "ortho_loss": 0.6073495149612427
      },
      "step 392": {
        "loss": 103.54100799560547,
        "l1_loss": 282.529052734375,
        "l2_loss": 92.17987060546875,
        "ortho_loss": 0.599709153175354
      },
      "step 393": {
        "loss": 103.76486206054688,
        "l1_loss": 276.6410827636719,
        "l2_loss": 92.63896179199219,
        "ortho_loss": 0.6025852560997009
      },
      "step 394": {
        "loss": 105.72293090820312,
        "l1_loss": 312.56976318359375,
        "l2_loss": 93.16027069091797,
        "ortho_loss": 0.5986496806144714
      },
      "step 395": {
        "loss": 101.998291015625,
        "l1_loss": 255.28538513183594,
        "l2_loss": 91.72610473632812,
        "ortho_loss": 0.6076495051383972
      },
      "step 396": {
        "loss": 101.75135040283203,
        "l1_loss": 251.940185546875,
        "l2_loss": 91.6130142211914,
        "ortho_loss": 0.6073053479194641
      },
      "step 397": {
        "loss": 101.3686752319336,
        "l1_loss": 233.91812133789062,
        "l2_loss": 91.95071411132812,
        "ortho_loss": 0.6123292446136475
      },
      "step 398": {
        "loss": 105.71435546875,
        "l1_loss": 322.09088134765625,
        "l2_loss": 92.7708511352539,
        "ortho_loss": 0.5987007021903992
      },
      "step 399": {
        "loss": 103.07617950439453,
        "l1_loss": 279.33935546875,
        "l2_loss": 91.84201049804688,
        "ortho_loss": 0.6059293746948242
      },
      "step 400": {
        "loss": 102.62840270996094,
        "l1_loss": 258.57489013671875,
        "l2_loss": 92.22444152832031,
        "ortho_loss": 0.6096838116645813
      },
      "step 401": {
        "loss": 103.38862609863281,
        "l1_loss": 277.66998291015625,
        "l2_loss": 92.22109985351562,
        "ortho_loss": 0.6073293685913086
      },
      "step 402": {
        "loss": 101.5647201538086,
        "l1_loss": 255.80198669433594,
        "l2_loss": 91.2716064453125,
        "ortho_loss": 0.6103786826133728
      },
      "step 403": {
        "loss": 104.30486297607422,
        "l1_loss": 293.3701477050781,
        "l2_loss": 92.5093994140625,
        "ortho_loss": 0.6065969467163086
      },
      "step 404": {
        "loss": 101.9627914428711,
        "l1_loss": 253.10577392578125,
        "l2_loss": 91.77732849121094,
        "ortho_loss": 0.6123263239860535
      },
      "step 405": {
        "loss": 105.52208709716797,
        "l1_loss": 320.87091064453125,
        "l2_loss": 92.62661743164062,
        "ortho_loss": 0.6063065528869629
      },
      "step 406": {
        "loss": 105.91300201416016,
        "l1_loss": 326.0807189941406,
        "l2_loss": 92.80935668945312,
        "ortho_loss": 0.6041719317436218
      },
      "step 407": {
        "loss": 105.42752838134766,
        "l1_loss": 315.6640625,
        "l2_loss": 92.740234375,
        "ortho_loss": 0.6072688698768616
      }
    },
    "config": {
      "trainer_class": "CustomTrainer",
      "activation_dim": 2304,
      "dict_size": 2304,
      "lr": 0.0003,
      "l1_penalty": 0.04,
      "warmup_steps": 1000,
      "resample_steps": null,
      "device": "cuda",
      "layer": 5,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "CustomTrainer",
      "submodule_name": "resid_post_layer_5"
    },
    "final_info": {
      "training_steps": 488,
      "final_loss": 94.70304870605469,
      "layer": 5,
      "dict_size": 2304,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "94469c1d-f100-45db-aedc-bb942c18f51c",
    "datetime_epoch_millis": 1737877040860,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.006710192260088017,
        "mean_num_split_features": 1.4230769230769231
      }
    },
    "eval_result_details": [
      {
        "first_letter": "a",
        "absorption_rate": 0.0007671653241273494,
        "num_absorption": 2,
        "num_probe_true_positives": 2607,
        "num_split_features": 3
      },
      {
        "first_letter": "b",
        "absorption_rate": 0.01322418136020151,
        "num_absorption": 21,
        "num_probe_true_positives": 1588,
        "num_split_features": 2
      },
      {
        "first_letter": "c",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 2851,
        "num_split_features": 3
      },
      {
        "first_letter": "d",
        "absorption_rate": 0.012582384661473937,
        "num_absorption": 21,
        "num_probe_true_positives": 1669,
        "num_split_features": 1
      },
      {
        "first_letter": "e",
        "absorption_rate": 0.0018656716417910447,
        "num_absorption": 3,
        "num_probe_true_positives": 1608,
        "num_split_features": 1
      },
      {
        "first_letter": "f",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1255,
        "num_split_features": 2
      },
      {
        "first_letter": "g",
        "absorption_rate": 0.004642525533890436,
        "num_absorption": 5,
        "num_probe_true_positives": 1077,
        "num_split_features": 1
      },
      {
        "first_letter": "h",
        "absorption_rate": 0.006349206349206349,
        "num_absorption": 6,
        "num_probe_true_positives": 945,
        "num_split_features": 1
      },
      {
        "first_letter": "i",
        "absorption_rate": 0.000612369871402327,
        "num_absorption": 1,
        "num_probe_true_positives": 1633,
        "num_split_features": 2
      },
      {
        "first_letter": "j",
        "absorption_rate": 0.009174311926605505,
        "num_absorption": 4,
        "num_probe_true_positives": 436,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.005934718100890208,
        "num_absorption": 4,
        "num_probe_true_positives": 674,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.002437043054427295,
        "num_absorption": 3,
        "num_probe_true_positives": 1231,
        "num_split_features": 2
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.009836065573770493,
        "num_absorption": 18,
        "num_probe_true_positives": 1830,
        "num_split_features": 1
      },
      {
        "first_letter": "n",
        "absorption_rate": 0.0035335689045936395,
        "num_absorption": 3,
        "num_probe_true_positives": 849,
        "num_split_features": 1
      },
      {
        "first_letter": "o",
        "absorption_rate": 0.029906542056074768,
        "num_absorption": 32,
        "num_probe_true_positives": 1070,
        "num_split_features": 1
      },
      {
        "first_letter": "p",
        "absorption_rate": 0.000423908435777872,
        "num_absorption": 1,
        "num_probe_true_positives": 2359,
        "num_split_features": 3
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.01092896174863388,
        "num_absorption": 2,
        "num_probe_true_positives": 183,
        "num_split_features": 1
      },
      {
        "first_letter": "r",
        "absorption_rate": 0.008860011813349085,
        "num_absorption": 15,
        "num_probe_true_positives": 1693,
        "num_split_features": 1
      },
      {
        "first_letter": "s",
        "absorption_rate": 0.0010504201680672268,
        "num_absorption": 3,
        "num_probe_true_positives": 2856,
        "num_split_features": 2
      },
      {
        "first_letter": "t",
        "absorption_rate": 0.007597895967270602,
        "num_absorption": 13,
        "num_probe_true_positives": 1711,
        "num_split_features": 1
      },
      {
        "first_letter": "u",
        "absorption_rate": 0.014492753623188406,
        "num_absorption": 11,
        "num_probe_true_positives": 759,
        "num_split_features": 1
      },
      {
        "first_letter": "v",
        "absorption_rate": 0.005820721769499418,
        "num_absorption": 5,
        "num_probe_true_positives": 859,
        "num_split_features": 1
      },
      {
        "first_letter": "w",
        "absorption_rate": 0.004531722054380665,
        "num_absorption": 3,
        "num_probe_true_positives": 662,
        "num_split_features": 1
      },
      {
        "first_letter": "x",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 99,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.012345679012345678,
        "num_absorption": 2,
        "num_probe_true_positives": 162,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.007547169811320755,
        "num_absorption": 2,
        "num_probe_true_positives": 265,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_5_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_5_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.2857142857142857,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 7.1875
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.26973684210526316,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 9.875,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": -0.26171875,
        "mse": 3.5,
        "cossim": 0.4140625
      },
      "shrinkage": {
        "l2_norm_in": 90.0,
        "l2_norm_out": 23.0,
        "l2_ratio": 0.259765625,
        "relative_reconstruction_bias": 0.63671875
      },
      "sparsity": {
        "l0": 108.94181823730469,
        "l1": 133.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr and tpp evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "7877fbe5-693f-4477-8bff-351737ec2b74",
    "datetime_epoch_millis": 1737878910690,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": -0.017664019645518638,
        "scr_metric_threshold_2": 0.007733210170600548,
        "scr_dir2_threshold_2": 0.007733210170600548,
        "scr_dir1_threshold_5": -0.059708565358126305,
        "scr_metric_threshold_5": 0.009930596360794179,
        "scr_dir2_threshold_5": 0.009930596360794179,
        "scr_dir1_threshold_10": -0.07792700826911374,
        "scr_metric_threshold_10": 0.013473354086170092,
        "scr_dir2_threshold_10": 0.013473354086170092,
        "scr_dir1_threshold_20": -0.1331128525177033,
        "scr_metric_threshold_20": 0.01570339920888572,
        "scr_dir2_threshold_20": 0.01570339920888572,
        "scr_dir1_threshold_50": -0.11593241603753465,
        "scr_metric_threshold_50": 0.01996696619854271,
        "scr_dir2_threshold_50": 0.01996696619854271,
        "scr_dir1_threshold_100": -0.13033023722283257,
        "scr_metric_threshold_100": 0.01775347394161156,
        "scr_dir2_threshold_100": 0.01775347394161156,
        "scr_dir1_threshold_500": -0.23885126124859218,
        "scr_metric_threshold_500": 0.016240925473789476,
        "scr_dir2_threshold_500": 0.016240925473789476
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": -0.3103451110805095,
        "scr_metric_threshold_2": 0.018779380822471343,
        "scr_dir2_threshold_2": 0.018779380822471343,
        "scr_dir1_threshold_5": -0.37930977783898107,
        "scr_metric_threshold_5": 0.021126768446028555,
        "scr_dir2_threshold_5": 0.021126768446028555,
        "scr_dir1_threshold_10": -0.37930977783898107,
        "scr_metric_threshold_10": 0.018779380822471343,
        "scr_dir2_threshold_10": 0.018779380822471343,
        "scr_dir1_threshold_20": -0.8275862777701274,
        "scr_metric_threshold_20": 0.021126768446028555,
        "scr_dir2_threshold_20": 0.021126768446028555,
        "scr_dir1_threshold_50": -0.9655176666207642,
        "scr_metric_threshold_50": 0.0563380025504072,
        "scr_dir2_threshold_50": 0.0563380025504072,
        "scr_dir1_threshold_100": -1.0689667220921653,
        "scr_metric_threshold_100": 0.06103291771452845,
        "scr_dir2_threshold_100": 0.06103291771452845,
        "scr_dir1_threshold_500": -1.7586216110116557,
        "scr_metric_threshold_500": 0.07511738337287854,
        "scr_dir2_threshold_500": 0.07511738337287854
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": 0.029850374486354535,
        "scr_metric_threshold_2": 0.0,
        "scr_dir2_threshold_2": 0.0,
        "scr_dir1_threshold_5": -0.34328375470276395,
        "scr_metric_threshold_5": 0.0,
        "scr_dir2_threshold_5": 0.0,
        "scr_dir1_threshold_10": -0.41791058054058766,
        "scr_metric_threshold_10": 0.010362715902782657,
        "scr_dir2_threshold_10": 0.010362715902782657,
        "scr_dir1_threshold_20": -0.46268703189205684,
        "scr_metric_threshold_20": -0.002590640371648879,
        "scr_dir2_threshold_20": -0.002590640371648879,
        "scr_dir1_threshold_50": -0.20895529027029383,
        "scr_metric_threshold_50": 0.005181280743297758,
        "scr_dir2_threshold_50": 0.005181280743297758,
        "scr_dir1_threshold_100": -0.13432846443247012,
        "scr_metric_threshold_100": 0.002590640371648879,
        "scr_dir2_threshold_100": 0.002590640371648879,
        "scr_dir1_threshold_500": -0.20895529027029383,
        "scr_metric_threshold_500": 0.005181280743297758,
        "scr_dir2_threshold_500": 0.005181280743297758
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.023256877745449607,
        "scr_metric_threshold_2": -0.005089144583879204,
        "scr_dir2_threshold_2": -0.005089144583879204,
        "scr_dir1_threshold_5": -0.023255491592288,
        "scr_metric_threshold_5": -0.005089144583879204,
        "scr_dir2_threshold_5": -0.005089144583879204,
        "scr_dir1_threshold_10": -0.0930233525223136,
        "scr_metric_threshold_10": -0.005089144583879204,
        "scr_dir2_threshold_10": -0.005089144583879204,
        "scr_dir1_threshold_20": -0.0930233525223136,
        "scr_metric_threshold_20": -0.002544648124819707,
        "scr_dir2_threshold_20": -0.002544648124819707,
        "scr_dir1_threshold_50": -0.046510983184576,
        "scr_metric_threshold_50": -0.005089144583879204,
        "scr_dir2_threshold_50": -0.005089144583879204,
        "scr_dir1_threshold_100": -0.046510983184576,
        "scr_metric_threshold_100": 0.0,
        "scr_dir2_threshold_100": 0.0,
        "scr_dir1_threshold_500": -0.1162788441146016,
        "scr_metric_threshold_500": 0.010178137501998197,
        "scr_dir2_threshold_500": 0.010178137501998197
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": -0.01234551548800365,
        "scr_metric_threshold_2": -0.03225796631216327,
        "scr_dir2_threshold_2": -0.03225796631216327,
        "scr_dir1_threshold_5": 0.1728394244106685,
        "scr_metric_threshold_5": 0.0,
        "scr_dir2_threshold_5": 0.0,
        "scr_dir1_threshold_10": 0.20987670673421857,
        "scr_metric_threshold_10": 0.0,
        "scr_dir2_threshold_10": 0.0,
        "scr_dir1_threshold_20": 0.23456773771022588,
        "scr_metric_threshold_20": 0.0,
        "scr_dir2_threshold_20": 0.0,
        "scr_dir1_threshold_50": 0.2222222222222222,
        "scr_metric_threshold_50": 0.005376434537042601,
        "scr_dir2_threshold_50": 0.005376434537042601,
        "scr_dir1_threshold_100": 0.20987670673421857,
        "scr_metric_threshold_100": -0.008064411464279277,
        "scr_dir2_threshold_100": -0.008064411464279277,
        "scr_dir1_threshold_500": 0.2222222222222222,
        "scr_metric_threshold_500": -0.04301067515872539,
        "scr_dir2_threshold_500": -0.04301067515872539
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": 0.04571437524288891,
        "scr_metric_threshold_2": 0.05936078776117859,
        "scr_dir2_threshold_2": 0.05936078776117859,
        "scr_dir1_threshold_5": 0.04571437524288891,
        "scr_metric_threshold_5": 0.06392693939649612,
        "scr_dir2_threshold_5": 0.06392693939649612,
        "scr_dir1_threshold_10": 0.051428586998763286,
        "scr_metric_threshold_10": 0.07305924266713117,
        "scr_dir2_threshold_10": 0.07305924266713117,
        "scr_dir1_threshold_20": 0.04571437524288891,
        "scr_metric_threshold_20": 0.06849309103181364,
        "scr_dir2_threshold_20": 0.06849309103181364,
        "scr_dir1_threshold_50": 0.04571437524288891,
        "scr_metric_threshold_50": 0.04566206068787576,
        "scr_dir2_threshold_50": 0.04566206068787576,
        "scr_dir1_threshold_100": 0.04571437524288891,
        "scr_metric_threshold_100": 0.06849309103181364,
        "scr_dir2_threshold_100": 0.06849309103181364,
        "scr_dir1_threshold_500": 0.04571437524288891,
        "scr_metric_threshold_500": 0.04566206068787576,
        "scr_dir2_threshold_500": 0.04566206068787576
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.007692208939046454,
        "scr_metric_threshold_2": -0.0323887323562997,
        "scr_dir2_threshold_2": -0.0323887323562997,
        "scr_dir1_threshold_5": -0.007692208939046454,
        "scr_metric_threshold_5": -0.028340201140355195,
        "scr_dir2_threshold_5": -0.028340201140355195,
        "scr_dir1_threshold_10": 0.015384417878092908,
        "scr_metric_threshold_10": -0.012145834962205343,
        "scr_dir2_threshold_10": -0.012145834962205343,
        "scr_dir1_threshold_20": 0.015384417878092908,
        "scr_metric_threshold_20": 0.004048531215944509,
        "scr_dir2_threshold_20": 0.004048531215944509,
        "scr_dir1_threshold_50": 0.030769294253470136,
        "scr_metric_threshold_50": -0.01619436617814985,
        "scr_dir2_threshold_50": -0.01619436617814985,
        "scr_dir1_threshold_100": 0.053845921070609495,
        "scr_metric_threshold_100": -0.04048603610256054,
        "scr_dir2_threshold_100": -0.04048603610256054,
        "scr_dir1_threshold_500": 0.046153712131563045,
        "scr_metric_threshold_500": -0.0323887323562997,
        "scr_dir2_threshold_500": -0.0323887323562997
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.08522750170939969,
        "scr_metric_threshold_2": 0.03846154825848749,
        "scr_dir2_threshold_2": 0.03846154825848749,
        "scr_dir1_threshold_5": 0.06250019049773588,
        "scr_metric_threshold_5": 0.012820600993054022,
        "scr_dir2_threshold_5": 0.012820600993054022,
        "scr_dir1_threshold_10": 0.005682081799897114,
        "scr_metric_threshold_10": 0.012820600993054022,
        "scr_dir2_threshold_10": 0.012820600993054022,
        "scr_dir1_threshold_20": 0.02272731121166381,
        "scr_metric_threshold_20": 0.017094049750513835,
        "scr_dir2_threshold_20": 0.017094049750513835,
        "scr_dir1_threshold_50": 0.0,
        "scr_metric_threshold_50": 0.03846154825848749,
        "scr_dir2_threshold_50": 0.03846154825848749,
        "scr_dir1_threshold_100": -0.10227273112116639,
        "scr_metric_threshold_100": 0.03846154825848749,
        "scr_dir2_threshold_100": 0.03846154825848749,
        "scr_dir1_threshold_500": -0.13068178547008577,
        "scr_metric_threshold_500": 0.03418809950102767,
        "scr_dir2_threshold_500": 0.03418809950102767
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": -0.010362868718775122,
        "scr_metric_threshold_2": 0.014999807775009142,
        "scr_dir2_threshold_2": 0.014999807775009142,
        "scr_dir1_threshold_5": -0.005181279943224265,
        "scr_metric_threshold_5": 0.014999807775009142,
        "scr_dir2_threshold_5": 0.014999807775009142,
        "scr_dir1_threshold_10": -0.015544148661999387,
        "scr_metric_threshold_10": 0.009999871850006094,
        "scr_dir2_threshold_10": 0.009999871850006094,
        "scr_dir1_threshold_20": 0.0,
        "scr_metric_threshold_20": 0.020000041723253828,
        "scr_dir2_threshold_20": 0.020000041723253828,
        "scr_dir1_threshold_50": -0.005181279943224265,
        "scr_metric_threshold_50": 0.029999913573259925,
        "scr_dir2_threshold_50": 0.029999913573259925,
        "scr_dir1_threshold_100": 0.0,
        "scr_metric_threshold_100": 0.020000041723253828,
        "scr_dir2_threshold_100": 0.020000041723253828,
        "scr_dir1_threshold_500": -0.010362868718775122,
        "scr_metric_threshold_500": 0.03499984949826297,
        "scr_dir2_threshold_500": 0.03499984949826297
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "bd012bc8-5f54-4474-b471-fc20a2a93a77",
    "datetime_epoch_millis": 1737878956304,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.940025,
        "llm_top_1_test_accuracy": 0.67890625,
        "llm_top_2_test_accuracy": 0.72346875,
        "llm_top_5_test_accuracy": 0.77423125,
        "llm_top_10_test_accuracy": 0.8231812500000001,
        "llm_top_20_test_accuracy": 0.8601937500000001,
        "llm_top_50_test_accuracy": 0.8994062500000001,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9338562842458487,
        "sae_top_1_test_accuracy": 0.6355375,
        "sae_top_2_test_accuracy": 0.67341875,
        "sae_top_5_test_accuracy": 0.7261875,
        "sae_top_10_test_accuracy": 0.7687875,
        "sae_top_20_test_accuracy": 0.81420625,
        "sae_top_50_test_accuracy": 0.87241875,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.958,
        "llm_top_1_test_accuracy": 0.6641999999999999,
        "llm_top_2_test_accuracy": 0.6843999999999999,
        "llm_top_5_test_accuracy": 0.7469999999999999,
        "llm_top_10_test_accuracy": 0.8282,
        "llm_top_20_test_accuracy": 0.8606,
        "llm_top_50_test_accuracy": 0.9118,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9488000392913818,
        "sae_top_1_test_accuracy": 0.6494,
        "sae_top_2_test_accuracy": 0.682,
        "sae_top_5_test_accuracy": 0.7144,
        "sae_top_10_test_accuracy": 0.7793999999999999,
        "sae_top_20_test_accuracy": 0.8296000000000001,
        "sae_top_50_test_accuracy": 0.8834,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9396000000000001,
        "llm_top_1_test_accuracy": 0.6714,
        "llm_top_2_test_accuracy": 0.7222000000000001,
        "llm_top_5_test_accuracy": 0.7607999999999999,
        "llm_top_10_test_accuracy": 0.8038000000000001,
        "llm_top_20_test_accuracy": 0.8455999999999999,
        "llm_top_50_test_accuracy": 0.8880000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9298000454902648,
        "sae_top_1_test_accuracy": 0.6592,
        "sae_top_2_test_accuracy": 0.671,
        "sae_top_5_test_accuracy": 0.727,
        "sae_top_10_test_accuracy": 0.7612,
        "sae_top_20_test_accuracy": 0.7978000000000001,
        "sae_top_50_test_accuracy": 0.8622,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.909,
        "llm_top_1_test_accuracy": 0.6814,
        "llm_top_2_test_accuracy": 0.705,
        "llm_top_5_test_accuracy": 0.751,
        "llm_top_10_test_accuracy": 0.7966,
        "llm_top_20_test_accuracy": 0.8251999999999999,
        "llm_top_50_test_accuracy": 0.8625999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9086000323295593,
        "sae_top_1_test_accuracy": 0.6192,
        "sae_top_2_test_accuracy": 0.6567999999999999,
        "sae_top_5_test_accuracy": 0.7084,
        "sae_top_10_test_accuracy": 0.7496,
        "sae_top_20_test_accuracy": 0.8054,
        "sae_top_50_test_accuracy": 0.8546000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.8888,
        "llm_top_1_test_accuracy": 0.6068,
        "llm_top_2_test_accuracy": 0.6407999999999999,
        "llm_top_5_test_accuracy": 0.6763999999999999,
        "llm_top_10_test_accuracy": 0.719,
        "llm_top_20_test_accuracy": 0.772,
        "llm_top_50_test_accuracy": 0.8290000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.8676000237464905,
        "sae_top_1_test_accuracy": 0.5788,
        "sae_top_2_test_accuracy": 0.6088,
        "sae_top_5_test_accuracy": 0.6559999999999999,
        "sae_top_10_test_accuracy": 0.6896000000000001,
        "sae_top_20_test_accuracy": 0.7438,
        "sae_top_50_test_accuracy": 0.7964,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9245000000000001,
        "llm_top_1_test_accuracy": 0.628,
        "llm_top_2_test_accuracy": 0.686,
        "llm_top_5_test_accuracy": 0.738,
        "llm_top_10_test_accuracy": 0.767,
        "llm_top_20_test_accuracy": 0.8,
        "llm_top_50_test_accuracy": 0.8545,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.917000025510788,
        "sae_top_1_test_accuracy": 0.618,
        "sae_top_2_test_accuracy": 0.677,
        "sae_top_5_test_accuracy": 0.722,
        "sae_top_10_test_accuracy": 0.739,
        "sae_top_20_test_accuracy": 0.755,
        "sae_top_50_test_accuracy": 0.849,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9663999999999999,
        "llm_top_1_test_accuracy": 0.6626,
        "llm_top_2_test_accuracy": 0.706,
        "llm_top_5_test_accuracy": 0.7719999999999999,
        "llm_top_10_test_accuracy": 0.8426,
        "llm_top_20_test_accuracy": 0.8924,
        "llm_top_50_test_accuracy": 0.9362,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9630000472068787,
        "sae_top_1_test_accuracy": 0.6608,
        "sae_top_2_test_accuracy": 0.6910000000000001,
        "sae_top_5_test_accuracy": 0.7468,
        "sae_top_10_test_accuracy": 0.7786,
        "sae_top_20_test_accuracy": 0.8362,
        "sae_top_50_test_accuracy": 0.8968,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.9345000000000001,
        "llm_top_1_test_accuracy": 0.7082499999999999,
        "llm_top_2_test_accuracy": 0.75075,
        "llm_top_5_test_accuracy": 0.80625,
        "llm_top_10_test_accuracy": 0.8482500000000001,
        "llm_top_20_test_accuracy": 0.89275,
        "llm_top_50_test_accuracy": 0.91475,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9362500458955765,
        "sae_top_1_test_accuracy": 0.6305,
        "sae_top_2_test_accuracy": 0.66775,
        "sae_top_5_test_accuracy": 0.7225,
        "sae_top_10_test_accuracy": 0.7755000000000001,
        "sae_top_20_test_accuracy": 0.8132499999999999,
        "sae_top_50_test_accuracy": 0.86475,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9994,
        "llm_top_1_test_accuracy": 0.8086,
        "llm_top_2_test_accuracy": 0.8925999999999998,
        "llm_top_5_test_accuracy": 0.9423999999999999,
        "llm_top_10_test_accuracy": 0.9800000000000001,
        "llm_top_20_test_accuracy": 0.993,
        "llm_top_50_test_accuracy": 0.9984,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9998000144958497,
        "sae_top_1_test_accuracy": 0.6684,
        "sae_top_2_test_accuracy": 0.7329999999999999,
        "sae_top_5_test_accuracy": 0.8124,
        "sae_top_10_test_accuracy": 0.8774,
        "sae_top_20_test_accuracy": 0.9326000000000001,
        "sae_top_50_test_accuracy": 0.9722,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "d8a9fbf2e09c6353944addaddfb5ca77a0714984",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 5,
      "hook_name": "blocks.5.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "l2",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  }
}