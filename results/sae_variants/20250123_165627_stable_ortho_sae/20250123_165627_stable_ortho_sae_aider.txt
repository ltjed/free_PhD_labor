
# aider chat started at 2025-01-23 16:56:27


#### Your goal is to implement the following idea: Stable Orthogonal Sparse Autoencoders with Native QR and Adaptive Sparsity.  
#### The proposed experiment is as follows: 1. Implement W_enc via native PyTorch QR decomposition  
#### 2. Use orthogonal initialization for W_raw  
#### 3. Track feature activation percentiles via EMA  
#### 4. Compare gradient stability against manual QR implementations  
#### 5. Evaluate training speed and final metrics.  
#### The implementation plan is as follows: 1. In CustomSAE:  
####    a. self.W_raw = nn.Parameter(torch.empty(d_in, d_sae))  
####    b. nn.init.orthogonal_(self.W_raw)  
####    c. def forward(): W_enc = torch.linalg.qr(self.W_raw).Q  
#### 2. In Trainer:  
####    a. Update ε(t) via EMA of feature percentiles  
####    b. Compute M(t) from threshold  
#### 3. Remove custom QR backward pass.  
####   
#### You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.  
####   
#### First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.  
####   
#### Note that we already provide the vanilla baseline results, so you do not need to re-run it.  
####   
#### For reference, the baseline results are as follows:  
####   
#### {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'e823bbbb-62c9-41ec-840b-cacb8ca4230d', 'datetime_epoch_millis': 1737147895673, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.939325, 'llm_top_1_test_accuracy': 0.6842749999999999, 'llm_top_2_test_accuracy': 0.7260625, 'llm_top_5_test_accuracy': 0.7746249999999999, 'llm_top_10_test_accuracy': 0.82099375, 'llm_top_20_test_accuracy': 0.8589374999999999, 'llm_top_50_test_accuracy': 0.90028125, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9576, 'llm_top_1_test_accuracy': 0.6648000000000001, 'llm_top_2_test_accuracy': 0.6844, 'llm_top_5_test_accuracy': 0.7466, 'llm_top_10_test_accuracy': 0.8286, 'llm_top_20_test_accuracy': 0.8602000000000001, 'llm_top_50_test_accuracy': 0.9118, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9385999999999999, 'llm_top_1_test_accuracy': 0.6869999999999999, 'llm_top_2_test_accuracy': 0.7228000000000001, 'llm_top_5_test_accuracy': 0.7626, 'llm_top_10_test_accuracy': 0.806, 'llm_top_20_test_accuracy': 0.8484, 'llm_top_50_test_accuracy': 0.8892, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9038, 'llm_top_1_test_accuracy': 0.6799999999999999, 'llm_top_2_test_accuracy': 0.7066000000000001, 'llm_top_5_test_accuracy': 0.7432000000000001, 'llm_top_10_test_accuracy': 0.7984, 'llm_top_20_test_accuracy': 0.8173999999999999, 'llm_top_50_test_accuracy': 0.8709999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8832000000000001, 'llm_top_1_test_accuracy': 0.6068, 'llm_top_2_test_accuracy': 0.6446, 'llm_top_5_test_accuracy': 0.6818, 'llm_top_10_test_accuracy': 0.7076, 'llm_top_20_test_accuracy': 0.7714000000000001, 'llm_top_50_test_accuracy': 0.8346, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9255, 'llm_top_1_test_accuracy': 0.629, 'llm_top_2_test_accuracy': 0.685, 'llm_top_5_test_accuracy': 0.737, 'llm_top_10_test_accuracy': 0.766, 'llm_top_20_test_accuracy': 0.8, 'llm_top_50_test_accuracy': 0.854, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.969, 'llm_top_1_test_accuracy': 0.6644, 'llm_top_2_test_accuracy': 0.7016, 'llm_top_5_test_accuracy': 0.7836000000000001, 'llm_top_10_test_accuracy': 0.834, 'llm_top_20_test_accuracy': 0.8939999999999999, 'llm_top_50_test_accuracy': 0.931, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9375, 'llm_top_1_test_accuracy': 0.733, 'llm_top_2_test_accuracy': 0.7685000000000001, 'llm_top_5_test_accuracy': 0.8, 'llm_top_10_test_accuracy': 0.84575, 'llm_top_20_test_accuracy': 0.8865000000000001, 'llm_top_50_test_accuracy': 0.91225, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.8092, 'llm_top_2_test_accuracy': 0.8949999999999999, 'llm_top_5_test_accuracy': 0.9422, 'llm_top_10_test_accuracy': 0.9816, 'llm_top_20_test_accuracy': 0.9936, 'llm_top_50_test_accuracy': 0.9984, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'bcb003afd6045deaee4be8dd883ae42863da9163', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_5_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 5, 'hook_name': 'blocks.5.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None, 'training_steps': 0, 'final_loss': None, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}  
####   
#### After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.  
#### YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.  
#### You can then implement the next thing on your list.  

Let me plan the experiments to implement and evaluate the orthogonal SAE approach. Given the baseline results, here's my proposed experiment sequence:

Run 1-3: Core Implementation & Basic Ablations
1. Run 1: Native QR + Orthogonal Init + Remove Manual Constraints
   - Implement W_raw with QR decomposition
   - Remove ConstrainedAdam in favor of vanilla Adam
   - Add EMA percentile tracking
   
2. Run 2: Adaptive Sparsity Scheduling
   - Implement ε(t) EMA-based sparsity adjustment
   - Add M(t) threshold masking
   
3. Run 3: Initialization Ablation
   - Compare orthogonal vs. default init for W_raw

Let's start with Run 1. I'll need to modify:
1. CustomSAE to use W_raw + QR decomposition
2. Update Trainer for EMA tracking
3. Remove constrained optimizer

Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
        self.W_enc = nn.Parameter(torch.zeros(d_in, d_sae))
        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))
=======
        # Orthogonal initialization with QR decomposition
        self.W_raw = nn.Parameter(torch.empty(d_in, d_sae))
        nn.init.orthogonal_(self.W_raw)
        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
    def encode(self, input_acts):
        pre_acts = (input_acts - self.b_dec) @ self.W_enc + self.b_enc
=======
    def encode(self, input_acts):
        # Compute orthogonal weights via QR decomposition
        W_enc = torch.linalg.qr(self.W_raw).Q
        pre_acts = (input_acts - self.b_dec) @ W_enc + self.b_enc
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
class ConstrainedAdam(torch.optim.Adam):
    """A variant of Adam where some parameters are constrained to have unit norm."""
    def __init__(self, params, constrained_params, lr):
        super().__init__(params, lr=lr)
        self.constrained_params = list(constrained_params)
    
    def step(self, closure=None):
        with torch.no_grad():
            for p in self.constrained_params:
                normed_p = p / p.norm(dim=0, keepdim=True)
                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p
        super().step(closure=closure)
        with torch.no_grad():
            for p in self.constrained_params:
                p /= p.norm(dim=0, keepdim=True)


=======
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        self.optimizer = ConstrainedAdam(
            self.ae.parameters(),
            [self.ae.W_dec],  # Constrain decoder weights
            lr=lr
        )
=======
        # Use vanilla Adam since we handle orthogonality via QR
        self.optimizer = torch.optim.Adam(self.ae.parameters(), lr=lr)
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
    def loss(self, x, logging=False, **kwargs):
        x_hat, f = self.ae(x, output_features=True)
        l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()
        l1_loss = f.norm(p=1, dim=-1).mean()
=======
    def loss(self, x, logging=False, **kwargs):
        x_hat, f = self.ae(x, output_features=True)
        l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()
        
        # Track feature activation percentiles via EMA
        with torch.no_grad():
            l1_norms = f.norm(p=1, dim=0)
            if not hasattr(self, 'ema_percentile'):
                self.register_buffer('ema_percentile', l1_norms.quantile(0.1).item())
            else:
                self.ema_percentile = 0.9 * self.ema_percentile + 0.1 * l1_norms.quantile(0.1).item()
                
        l1_loss = f.norm(p=1, dim=-1).mean()
>>>>>>> REPLACE
```

This implements:
1. Orthogonal initialization via QR decomposition
2. Removes manual weight constraints
3. Adds EMA tracking of feature activation percentiles
4. Updates optimizer to vanilla Adam

The next command to run will be:
```bash
python experiment.py --out_dir=run_1
```

After we see these results, we can proceed with Run 2 to implement the adaptive sparsity scheduling. Would you like me to proceed with running this first set of changes?

> Tokens: 15k sent, 7.9k cache hit, 1.6k received. Cost: $0.01 message, $0.01 session.  
> Applied edit to experiment.py  
> python experiment.py --out_dir=run_1  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.47s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.47s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.24s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.43s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.27s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:31<00:00, 31.37s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:31<00:00, 31.37s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.27s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.25s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.17s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.63s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.18s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.18s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.26s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.22s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.18s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.64s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.79s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.79s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.31s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.27s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.16s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.63s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.03s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.03s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.27s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.32s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.20s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.67s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.93s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.93s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.27s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.35s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.22s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.69s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:31<00:00, 31.75s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:31<00:00, 31.75s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.28s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.27s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.18s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.64s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:33<00:00, 33.13s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:33<00:00, 33.13s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.40s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.33s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.21s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.69s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  

#### Run failed with the following error   
#### Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]Failed to load model google/gemma-2-2b: 'CustomSAE' object has no attribute 'W_enc'  
####   
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.68s/it]  
#### Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:32<00:00, 32.68s/it]  
####   
#### Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]  
#### Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.33s/it]  
#### Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.39s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.23s/it]  
#### Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.71s/it]  
> litellm.BadRequestError: DeepseekException - Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[7] and messages[8] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}  
