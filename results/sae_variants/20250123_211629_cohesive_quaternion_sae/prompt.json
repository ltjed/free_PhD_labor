{
    "system": "You are an expert AI Researcher deeply immersed in the field of mechanistic interpretability, specifically focusing on improving the interpretability of sparse autoencoders (SAEs). Your goal is to publish a paper to advance progress on a specific benchmark. Reason step-by-step how your proposed variants of SAE could improve on the target benchmark",
    "task_description": "Your current research focuses on addressing the challenge of limited interpretability in the latent space of sparse autoencoders. The core problem is polysemanticity, where individual latent features appear to represent multiple, semantically distinct concepts. This hinders our ability to understand what neural networks are truly learning.\n\nYour task is to propose novel and feasible variants of sparse autoencoders that result in more interpretable latents compared to the internal activations they are trained to reconstruct. Consider the insights from the following abstract of a key paper in this area:\n\n\"One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task wang2022interpretability to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\"\n\nSpecifically, you should focus on the following benchmarks: absorption, core.\n\nCore contains a suite of 6 metrics detailed below. 1. L0 Sparsity Measures the average number of non-zero feature activations. A lower number indicates higher sparsity. Measures the change in model performance when replacing activations with the corresponding SAE reconstruction during a forward pass. 2. Cross Entropy Loss Score. This metric is quantified as (H^* - H0) / (H_{orig} - H0), where H_{orig} is the cross-entropy loss of the original model for next-token prediction, H^* is the cross-entropy loss after substituting the model activation x with its SAE reconstruction during the forward pass, and H_0 is the cross-entropy loss when zero-ablating x. Higher Cross Entropy Loss Score is better. The range of the metric is 0-1. 3. Feature Density Statistics. We track the activation frequency for every SAE latent. This can be informative for both the proportion of dead features that never activate (indicating wasted capacity) and dense features that activate with a very high frequency (which tend to be less interpretable). These can also be visualized as a log-scale histogram. 4. L2 Ratio Compares the L2 norm (Euclidean distance) of different components, typically between the original and reconstructed representations. Helps assess how well the model preserves the magnitude of the input signals. 5. Explained Variance Quantifies how much of the variability in the data is captured by the model. Higher values of Explained Variance (closer to 1) indicate the model better explains the patterns in the input data. 6. KL Divergence Kullback-Leibler divergence measures the difference between two probability distributions, often used to compare the model's output distribution to the target distribution. Lower values of KL divergence indicate better alignment."
}