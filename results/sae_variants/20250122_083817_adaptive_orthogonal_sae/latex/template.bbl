\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bau et~al.(2017)Bau, Zhou, Khosla, Oliva, and
  Torralba]{Bau2017NetworkDQ}
David Bau, Bolei Zhou, A.~Khosla, A.~Oliva, and A.~Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual
  representations.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  3319--3327, 2017.

\bibitem[Bengio(2007)]{Bengio2007LearningDA}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock \emph{Found. Trends Mach. Learn.}, 2:\penalty0 1--127, 2007.

\bibitem[Cao et~al.(2021)Cao, Aziz, and Titov]{Cao2021EditingFK}
Nicola~De Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock pp.\  6491--6506, 2021.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Dunefsky et~al.(2024)Dunefsky, Chlenski, and
  Nanda]{Dunefsky2024TranscodersFI}
Jacob Dunefsky, Philippe Chlenski, and Neel Nanda.
\newblock Transcoders find interpretable llm feature circuits.
\newblock \emph{ArXiv}, abs/2406.11944, 2024.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish, Kaplan, Amodei,
  Wattenberg, and Olah]{Elhage2022ToyMO}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, T.~Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, R.~Lasenby, Dawn Drain, Carol Chen,
  R.~Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, M.~Wattenberg, and
  C.~Olah.
\newblock Toy models of superposition.
\newblock \emph{ArXiv}, abs/2209.10652, 2022.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Makelov et~al.(2024)Makelov, Lange, and Nanda]{Makelov2024TowardsPE}
Aleksandar Makelov, Georg Lange, and Neel Nanda.
\newblock Towards principled evaluations of sparse autoencoders for
  interpretability and control.
\newblock \emph{ArXiv}, abs/2405.08366, 2024.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Paulo et~al.(2024)Paulo, Mallen, Juang, and
  Belrose]{Paulo2024AutomaticallyIM}
Gonccalo Paulo, Alex~Troy Mallen, Caden Juang, and Nora Belrose.
\newblock Automatically interpreting millions of features in large language
  models.
\newblock \emph{ArXiv}, abs/2410.13928, 2024.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{Ribeiro2016WhySI}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock \emph{“Why Should I Trust You?”: Explaining the Predictions of
  Any Classifier}.
\newblock 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zeiler \& Fergus(2013)Zeiler and Fergus]{Zeiler2013VisualizingAU}
Matthew~D. Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock \emph{ArXiv}, abs/1311.2901, 2013.

\end{thebibliography}
