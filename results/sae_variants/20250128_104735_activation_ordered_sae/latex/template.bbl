\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharon et~al.(2006)Aharon, Elad, and Bruckstein]{Aharon2006rmKA}
M.~Aharon, Michael Elad, and A.~Bruckstein.
\newblock $rm k$-svd: An algorithm for designing overcomplete dictionaries for
  sparse representation.
\newblock \emph{IEEE Transactions on Signal Processing}, 54:\penalty0
  4311--4322, 2006.

\bibitem[Bell \& Sejnowski(1997)Bell and Sejnowski]{Bell1997THEI}
A.~J. Bell and T.~Sejnowski.
\newblock The " independent components " of natural scenes are edge filters
  3329 recover the causes.
\newblock 1997.

\bibitem[Chanin et~al.(2024)Chanin, {Wilken-Smith}, Dulka, Bhatnagar, and
  Bloom]{chaninAbsorptionStudyingFeature2024}
David Chanin, James {Wilken-Smith}, Tom{\'a}{\v s} Dulka, Hardik Bhatnagar, and
  Joseph Bloom.
\newblock A is for {{Absorption}}: {{Studying Feature Splitting}} and
  {{Absorption}} in {{Sparse Autoencoders}}, September 2024.

\bibitem[{De-Arteaga} et~al.(2019){De-Arteaga}, Romanov, Wallach, Chayes,
  Borgs, Chouldechova, Geyik, Kenthapadi, and
  Kalai]{de-arteagaBiasBiosCase2019}
Maria {De-Arteaga}, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian
  Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and
  Adam~Tauman Kalai.
\newblock Bias in {{Bios}}: {{A Case Study}} of {{Semantic Representation
  Bias}} in a {{High-Stakes Setting}}.
\newblock In \emph{Proceedings of the {{Conference}} on {{Fairness}},
  {{Accountability}}, and {{Transparency}}}, pp.\  120--128, January 2019.
\newblock \doi{10.1145/3287560.3287572}.
\newblock Comment: Accepted at ACM Conference on Fairness, Accountability, and
  Transparency (ACM FAT*), 2019.

\bibitem[Farrell et~al.(2024)Farrell, Lau, and
  Conmy]{farrellApplyingSparseAutoencoders2024}
Eoin Farrell, Yeu-Tong Lau, and Arthur Conmy.
\newblock Applying sparse autoencoders to unlearn knowledge in language models,
  November 2024.

\bibitem[Gao et~al.()Gao, Goh, and Sutskever]{gaoScalingEvaluatingSparse}
Leo Gao, Gabriel Goh, and Ilya Sutskever.
\newblock Scaling and evaluating sparse autoencoders.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineLF}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009.

\bibitem[Marks et~al.(2024)Marks, Rager, Michaud, Belinkov, Bau, and
  Mueller]{marksSparseFeatureCircuits2024}
Samuel Marks, Can Rager, Eric~J. Michaud, Yonatan Belinkov, David Bau, and
  Aaron Mueller.
\newblock Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing
  Interpretable Causal Graphs}} in {{Language Models}}, March 2024.
\newblock Comment: Code and data at
  https://github.com/saprmarks/feature-circuits. Demonstration at
  https://feature-circuits.xyz.

\bibitem[Mudide et~al.(2024)Mudide, Engels, Michaud, Tegmark, and
  de~Witt]{mudideEfficientDictionaryLearning2024a}
Anish Mudide, Joshua Engels, Eric~J. Michaud, Max Tegmark, and
  Christian~Schroeder de~Witt.
\newblock Efficient {{Dictionary Learning}} with {{Switch Sparse
  Autoencoders}}, October 2024.
\newblock Comment: Code available at https://github.com/amudide/switch\_sae.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Paulo et~al.(2024)Paulo, Mallen, Juang, and
  Belrose]{pauloAutomaticallyInterpretingMillions2024}
Gon{\c c}alo Paulo, Alex Mallen, Caden Juang, and Nora Belrose.
\newblock Automatically {{Interpreting Millions}} of {{Features}} in {{Large
  Language Models}}, December 2024.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{a}})Rajamanoharan, Conmy, Smith,
  Lieberum, Varma, Kram{\'a}r, Shah, and
  Nanda]{rajamanoharanImprovingDictionaryLearning2024}
Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant
  Varma, J{\'a}nos Kram{\'a}r, Rohin Shah, and Neel Nanda.
\newblock Improving {{Dictionary Learning}} with {{Gated Sparse Autoencoders}},
  April 2024{\natexlab{a}}.
\newblock Comment: 15 main text pages, 22 appendix pages.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{b}})Rajamanoharan, Lieberum,
  Sonnerat, Conmy, Varma, Kram{\'a}r, and
  Nanda]{rajamanoharanJumpingAheadImproving2024}
Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant
  Varma, J{\'a}nos Kram{\'a}r, and Neel Nanda.
\newblock Jumping {{Ahead}}: {{Improving Reconstruction Fidelity}} with
  {{JumpReLU Sparse Autoencoders}}, August 2024{\natexlab{b}}.
\newblock Comment: v2: new appendix H comparing kernel functions \& bug-fixes
  to pseudo-code in Appendix J v3: further bug-fix to pseudo-code in Appendix
  J.

\end{thebibliography}
