{
    "Summary": "The paper proposes a memory-efficient sparse autoencoder with selective orthogonality constraints to achieve feature disentanglement in large language models. The method dynamically identifies and constrains the most correlated feature pairs, maintaining O(n) memory complexity. Experimental results show a significant reduction in GPU memory usage while maintaining model performance.",
    "Strengths": [
        "Addresses a significant challenge in feature disentanglement for large language models.",
        "Introduces a novel selective orthogonality constraint mechanism.",
        "Experimental results show a 47% reduction in peak GPU memory usage with stable training dynamics."
    ],
    "Weaknesses": [
        "Lacks detailed explanations for some methodological aspects, particularly the autoencoder aggregator.",
        "Additional ablation studies are needed to fully validate the contributions of different components.",
        "Insufficient detail for reproducibility, particularly regarding complex tensor management and incremental updates.",
        "The clarity of the paper is suboptimal, with several sections being overly complex and lacking clear explanations.",
        "The experimental results, while extensive, lack sufficient comparative analysis with other state-of-the-art methods.",
        "The paper does not provide a thorough analysis of the limitations and potential negative impacts of the proposed method.",
        "Limited discussion on scalability to larger models.",
        "Sensitivity to hyperparameters is not deeply analyzed.",
        "Validation is limited to a single dataset/model (Pythia-70M)."
    ],
    "Originality": 3,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can you provide more details about the autoencoder aggregator?",
        "What is the performance impact of changing the type of aggregators?",
        "Could you perform additional ablation studies to clarify the contribution of different components of the method?",
        "Can the authors provide a clearer explanation of the selective orthogonality constraint mechanism?",
        "How does the proposed method compare to other state-of-the-art feature disentanglement methods in terms of performance and memory efficiency?",
        "What are the potential limitations and negative impacts of the proposed method?",
        "How does the approach scale to even larger models?",
        "Can the authors analyze the sensitivity of the method to different hyperparameters in more detail?",
        "How does the performance of the proposed method compare with other state-of-the-art feature disentanglement methods on different datasets?",
        "Can the authors provide more details on the adaptive penalty mechanism and how it scales with correlation strength?",
        "How does the method perform on other datasets besides Pythia-70M?",
        "What are the exact training configurations and parameter settings used in the experiments?"
    ],
    "Limitations": [
        "The paper does not adequately address the potential limitations and challenges of the proposed method in different settings or with larger models.",
        "The paper lacks a detailed analysis of potential limitations and negative societal impacts of the proposed method. It would be beneficial for the authors to discuss these aspects to provide a balanced view of their contribution.",
        "The paper acknowledges sensitivity to hyperparameters but does not deeply analyze this aspect.",
        "The method is validated on a single dataset/model, limiting generalizability.",
        "The dependency on specific hyperparameters (e.g., pair selection threshold and orthogonality scaling) is concerning and should be investigated further."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}