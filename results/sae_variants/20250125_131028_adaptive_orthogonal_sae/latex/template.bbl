\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bell \& Sejnowski(1995)Bell and Sejnowski]{Bell1995AnIA}
A.~J. Bell and T.~Sejnowski.
\newblock An information-maximization approach to blind separation and blind
  deconvolution.
\newblock \emph{Neural Computation}, 7:\penalty0 1129--1159, 1995.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{Higgins2016betaVAELB}
I.~Higgins, L.~Matthey, Arka Pal, Christopher~P. Burgess, Xavier Glorot,
  M.~Botvinick, S.~Mohamed, and Alexander Lerchner.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Kim \& Yun(2022)Kim and Yun]{Kim2022RevisitingOR}
Taehyeon Kim and Se-Young Yun.
\newblock Revisiting orthogonality regularization: A study for convolutional
  neural networks in image classification.
\newblock \emph{IEEE Access}, PP:\penalty0 1--1, 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee et~al.(2006)Lee, Battle, Raina, and Ng]{Lee2006EfficientSC}
Honglak Lee, Alexis Battle, Rajat Raina, and A.~Ng.
\newblock Efficient sparse coding algorithms.
\newblock pp.\  801--808, 2006.

\bibitem[Mudide et~al.(2024)Mudide, Engels, Michaud, Tegmark, and
  de~Witt]{Mudide2024EfficientDL}
Anish Mudide, Joshua Engels, Eric~J. Michaud, Max Tegmark, and
  Christian~Schroeder de~Witt.
\newblock Efficient dictionary learning with switch sparse autoencoders.
\newblock \emph{ArXiv}, abs/2410.08201, 2024.

\bibitem[Ngiam et~al.(2011)Ngiam, Koh, Chen, Bhaskar, and Ng]{Ngiam2011SparseF}
Jiquan Ngiam, Pang~Wei Koh, Zhenghao Chen, Sonia~A. Bhaskar, and Andrew~Y. Ng.
\newblock Sparse filtering.
\newblock pp.\  1125--1133, 2011.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Park et~al.(2024)Park, Ahn, Kim, and Kang]{Park2024MonetMO}
Jungwoo Park, Y.~Ahn, Kee-Eung Kim, and Jaewoo Kang.
\newblock Monet: Mixture of monosemantic experts for transformers.
\newblock \emph{ArXiv}, abs/2412.04139, 2024.

\bibitem[Vorontsov et~al.(2017)Vorontsov, Trabelsi, Kadoury, and
  Pal]{Vorontsov2017OnOA}
Eugene Vorontsov, C.~Trabelsi, S.~Kadoury, and C.~Pal.
\newblock On orthogonality and learning recurrent networks with long term
  dependencies.
\newblock pp.\  3570--3578, 2017.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{Zbontar2021BarlowTS}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St√©phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock \emph{ArXiv}, abs/2103.03230, 2021.

\end{thebibliography}
