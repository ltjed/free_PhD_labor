{
    "system": "You are an expert AI Researcher deeply immersed in the field of mechanistic interpretability, specifically focusing on improving the interpretability of sparse autoencoders (SAEs). Your goal is to publish a paper to advance progress on a specific benchmark. Reason step-by-step how your proposed variants of SAE could improve on the target benchmark",
    "task_description": "Your current research focuses on addressing the challenge of limited interpretability in the latent space of sparse autoencoders. The core problem, as highlighted in the paper 'Applying sparse autoencoders to unlearn knowledge in language models', is polysemanticity, where individual latent features appear to represent multiple, semantically distinct concepts. This hinders our ability to understand what neural networks are truly learning.\n\nYour task is to propose novel and feasible variants of sparse autoencoders that result in more interpretable latents compared to the internal activations they are trained to reconstruct. Consider the insights from the following abstract of a key paper in this area:\n\n\"One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task wang2022interpretability to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\"\n\nSpecifically, you should focus on the following benchmarks: absorption, core, scr, tpp, sparse_probing, unlearning.\n\nCore contains a suite of 6 metrics detailed below. 1. L0 Sparsity Measures the average number of non-zero feature activations. A lower number indicates higher sparsity. Measures the change in model performance when replacing activations with the corresponding SAE reconstruction during a forward pass. 2. Cross Entropy Loss Score. This metric is quantified as (H^* - H0) / (H_{orig} - H0), where H_{orig} is the cross-entropy loss of the original model for next-token prediction, H^* is the cross-entropy loss after substituting the model activation x with its SAE reconstruction during the forward pass, and H_0 is the cross-entropy loss when zero-ablating x. Higher Cross Entropy Loss Score is better. The range of the metric is 0-1. 3. Feature Density Statistics. We track the activation frequency for every SAE latent. This can be informative for both the proportion of dead features that never activate (indicating wasted capacity) and dense features that activate with a very high frequency (which tend to be less interpretable). These can also be visualized as a log-scale histogram. 4. L2 Ratio Compares the L2 norm (Euclidean distance) of different components, typically between the original and reconstructed representations. Helps assess how well the model preserves the magnitude of the input signals. 5. Explained Variance Quantifies how much of the variability in the data is captured by the model. Higher values of Explained Variance (closer to 1) indicate the model better explains the patterns in the input data. 6. KL Divergence Kullback-Leibler divergence measures the difference between two probability distributions, often used to compare the model's output distribution to the target distribution. Lower values of KL divergence indicate better alignment.**Unlearning Benchmark Details:**\nWe evaluate SAEs on their ability to selectively remove knowledge while maintaining model performance on unrelated tasks, following the methodology in Applying sparse autoencoders to unlearn knowledge in language models.\n\nThis SAE unlearning evaluation uses the WMDP-bio dataset, which contains multiple-choice questions containing dangerous biology knowledge. The intervention methodology involves clamping selected SAE feature activations to negative values whenever the features activate during inference. Feature selection utilizes a dual-dataset approach: calculating feature sparsity across a \"forget\" dataset (WMDP-bio corpus) and a \"retain\" dataset (WikiText). The selection and intervention process involves three key hyperparameters:\n\n* `retain_threshold`: maximum allowable sparsity on the retain set\n* `n_features`: number of top features to select\n* `multiplier`: magnitude of negative clamping\n\nThe procedure first discards features with retain set sparsity above `retain_threshold`, then selects the top `n_features` by forget set sparsity, and finally clamps their activations to negative `multiplier` when activated.\n\nWe quantify unlearning effectiveness through two metrics:\n\n1. **Accuracy on WMDP-bio questions:** Lower accuracy indicates successful unlearning.\n2. **Accuracy on biology-unrelated MMLU subsets:** Including High school US history, Geography, College computer science, and Human aging. Higher accuracy demonstrates preserved general capabilities.\n\nBoth metrics only evaluate on questions that the base model answers correctly across all option permutations, to reduce noise from uncertain model knowledge.\n\nWe sweep the three hyperparameters to obtain multiple evaluation results per SAE. To derive a single evaluation metric, we filter for results maintaining MMLU accuracy above 0.99 and select the minimum achieved WMDP-bio accuracy, thereby measuring optimal unlearning performance within acceptable side effect constraints.\nSpurious Correlation Removal (SCR): In the SHIFT method, a human evaluator debiases a classifier by ablating SAE latents. We automate SHIFT and use it to measure whether an SAE has found separate latents for distinct concepts \u2013 the concept of gender, and the concepts for someone's profession for example. Distinct features will enable a more precise removal of spurious correlations, thereby effectively debiasing the classifier. First, we filter datasets (Bias in Bios and Amazon Reviews) for two binary labels. For example, we select text samples of two professions (professor, nurse) and the gender labels (male, female) from the Bias in Bios dataset. We partition this dataset into a balanced set\u2014containing all combinations of professor/nurse and male/female\u2014and a biased set that only includes male+professor and female+nurse combinations. We then train a linear classifier C_b on the biased dataset. The linear classifier picks up on both signals, such as gender and profession. During the evaluation, we attempt to debias the classifier C_b by selecting SAE latents related to one class (eg. gender) to increase classification accuracy for the other class (eg. profession). We select set L containing the top n SAE latents according to their absolute probe attribution score with a probe trained specifically to predict the spurious signal (eg. gender). Karvonen et al. found that the scores obtained with feature selection through probe attribution had a strong correlation with scores obtained with feature selection using an LLM judge. Thus, we select features using probe attribution to avoid the cost and potential biases associated with an LLM judge. For each original and spurious-feature-informed set L of selected features, we remove the spurious signal by defining a modified classifier C_m = C_b L where all selected unrelated yet with high attribution latents are zero-ablated. The accuracy with which the modified classifier C_m predicts the desired class when evaluated on the balanced dataset indicates SAE quality. A higher accuracy suggests that the SAE was more effective in isolating and removing the spurious correlation of e.g. gender, allowing the classifier to focus on the intended task of e.g. profession classification. We consider a normalized evaluation score: S_{SHIFT} = (A_{abl} - A_{base}) / (A_{oracle} - A_{base}) where A_{abl} is the probe accuracy after ablation, A_{base} is the baseline accuracy (spurious probe before ablation), and A_{oracle} is the skyline accuracy (probe trained directly on the desired concept). This score represents the proportion of improvement achieved through ablation relative to the maximum possible improvement, allowing fair comparison across different classes and models.In Targeted Probe Perturbation, SHIFT requires datasets with correlated labels. We generalize SHIFT to all multiclass NLP datasets by introducing the targeted probe perturbation (TPP) metric. At a high level, we aim to find sets of SAE latents that disentangle the dataset classes. Inspired by SHIFT, we train probes on the model activations and measure the effect of ablating sets of latents on the probe accuracy. Ablating a disentangled set of latents should only have an isolated causal effect on one class probe, while leaving other class probes unaffected. We consider a dataset mapping text to exactly one of m concepts c in C. For each class with index i = 1, ..., m we select the set L_i of the most relevant SAE latents as described in section [ref]. Note that we select the top signed importance scores, as we are only interested in latents that actively contribute to the targeted class. For each concept c_i, we partition the dataset into samples of the targeted concept and a random mix of all other labels. We define the model with probe corresponding to concept c_j with j = 1, ..., m as a linear classifier C_j which is able to classify concept c_j with accuracy A_j. Further, C_{i,j} denotes a classifier for c_j where latents L_i are ablated. Then, we iteratively evaluate the accuracy A_{i,j} of all linear classifiers C_{i,j} on the dataset partitioned for the corresponding class c_j. The targeted probe perturbation score: S_{TPP} = mean_{(i=j)} (A_{i,j} - A_j) - mean_{(i\u2260j)} (A_{i,j} - A_j) represents the effectiveness of causally isolating a single probe. Ablating a disentangled set of latents should only show a significant accuracy decrease if i = j, namely if the latents selected for class i are ablated in the classifier of the same class i, and remain constant if i \u2260 j.We evaluate our sparse autoencoders' ability to learn specified features through a series of targeted probing tasks across diverse domains, including language identification, profession classification, and sentiment analysis. For each task, we encode inputs through the SAE, apply mean pooling over non-padding tokens, and select the top-K latents using maximum mean difference. We train a logistic regression probe on the resulting representations and evaluate classification performance on held-out test data. Our evaluation spans 35 distinct binary classification tasks derived from five datasets. Our probing evaluation encompasses five datasets spanning different domains and tasks: bias_in_bios for Profession Classification (predicting professional roles from biographical text), Amazon Reviews for Product Classification & Sentiment (dual tasks: category prediction and sentiment analysis), Europarl for Language Identification (detecting document language), GitHub for Programming Language Classification (identifying coding language from source code), and AG News for Topic Categorization (classifying news articles by subject). To ensure consistent computational requirements across tasks, we sample 4,000 training and 1,000 test examples per binary classification task and truncate all inputs to 128 tokens. For GitHub data, we follow Gurnee et al. by excluding the first 150 characters (approximately 50 tokens) as a crude attempt to avoid license headers. We evaluated both mean pooling and max pooling across non-padding tokens, and used mean pooling as it obtained slightly higher accuracy. From each dataset, we select subsets containing up to five classes. Multiple subsets may be drawn from the same dataset to maintain positive ratios \u22650.2.The following is an example good target benchmark and proposed solution: **benchmark: Feature absorption: Sparsity incentivizes an undesirable phenomenon called feature absorption. Imagine an SAE learned two distinct latents tracking the features \"starts with S\" and \"short\". Since \"short\" always starts with S, the SAE can increase sparsity by absorbing the \"starts with S\" feature into the \"short\" latent and then no longer needs to fire the \"starts with S\" latent when the token \"short\" is present, as it already includes the \"starts with S\" feature direction.\n\nIn general, feature absorption is incentivised any time there's a pair of concepts, A & B, where A implies B (i.e. if A activates then B will always also be active, but not necessarily the other way round). This will happen with categories/hierarchies, e.g. India => Asia, pig => mammal, red => color, etc. If the SAE learns a latent for A and a latent for B, then both will fire on inputs with A. But this is redundant\u2013A implies B, so there's no need for the B latent to light up on A. And if the model learns a latent for A and a latent for \"B except for A\", then only one activates. This is sparser, but clearly less interpretable!\n\nFeature absorption often happens in an unpredictable manner, resulting in unusual gerrymandered features. For example, the \"starts with S\" feature may fire on 95% of tokens beginning with S, yet fail to fire on an arbitrary 5% as the \"starts with S\" feature has been absorbed for this 5% of tokens. This is an undesirable property that we would like to minimize.\n\nTo quantify feature absorption, we follow the example in Chanin et al. and use a first letter classification task. First, tokens consisting of only English letters and an optional leading space are split into a train and test set, and a supervised logistic regression probe is trained on the train set using residual stream activations from the model. This probe is used as ground truth for the feature direction in the model. Next, k-sparse probing is performed on SAE latents from the train set to find which latents are most relevant for the task. The k=1 sparse probing latent is considered as a main SAE latent for the first letter task. To account for feature splitting, as k is increased from k=n to k=n+1, if the F1 score for the k=n+1 sparse probe represents an increase of more than \u03c4_{fs} than the F1 of the k=n probe, the k=n+1 feature is considered a feature split and is added to the set of main SAE latents performing the first letter task. We use \u03c4_fs=0.03 in line with Chanin et al.\n\nAfter the main feature split latents for the first letter task are found, we look for test set examples where the main feature split latents fail to correctly classify the token, but the logistic regression probe is able to correctly classify the sample. We then look for a different SAE latent that fires on this sample that has a cosine similarity with the probe of at least \u03c4_{ps}, and where the SAE latent accounts for at least \u03c4_{pa} portion of the probe projection in the activation. We use \u03c4_{ps}=0.025 and \u03c4_{pa}=0.4 in line with Chanin et al.. Proposed_solution: Matryoshka SAE: Matryoshka representation learning aims to learn nested representations where lower-dimensional features are embedded within higher-dimensional ones, similar to Russian Matryoshka dolls. The key idea is to explicitly optimize for good representation at multiple scales simultaneously.\n\nWe adapt this approach to the context of sparse autoencoders. This means we can nest multiple sizes of dictionaries within each other. The largest autoencoder uses all latents for reconstruction, another uses the first half, a third uses the first quarter, and so on.\n\nThe losses of these nested autoencoders are summed. This incentivizes the initial latents to represent broadly applicable and general features, as they contribute to multiple reconstruction objectives, while later latents can focus on more specific, less frequent features."
}