{
    "Summary": "The paper introduces 'CompositionalLens,' a hierarchical sparse autoencoder designed to uncover multi-tier features within language models. It employs an adaptive architecture with dynamic feature allocation and curriculum learning to progressively build representations from simpler components. Experiments on the Gemma-2B model demonstrate high reconstruction fidelity and minimal disruption to model behavior, providing insights into how language models organize information hierarchically.",
    "Strengths": [
        "The problem of understanding hierarchical representations in language models is well-motivated and crucial for interpretability.",
        "The hierarchical autoencoder approach with dynamic feature allocation and curriculum learning is novel and well-conceived.",
        "The results show effective reconstruction fidelity and model behavior preservation, indicating the success of the proposed method.",
        "Comprehensive experiments demonstrating the method's effectiveness in maintaining reconstruction fidelity and preserving model behavior.",
        "The hierarchical approach provides a deeper understanding of how language models compose simple features into complex concepts."
    ],
    "Weaknesses": [
        "The paper lacks clarity in explaining some components, particularly the autoencoder aggregator and its implementation details.",
        "Ablation studies are limited and do not sufficiently explore variations in the proposed architecture, such as changes in aggregation methods or modulation functions.",
        "The evaluation could benefit from more diverse datasets or additional qualitative examples to strengthen the claims.",
        "Limited discussion on the robustness and generalizability of the method across different models and datasets.",
        "Certain sections, such as the autoencoder aggregator's implementation details and the choice of hyperparameters, require more detailed explanations.",
        "The computational overhead introduced by the hierarchical structure and curriculum learning is not adequately addressed, which could limit the practicality of the approach.",
        "Potential limitations and negative societal impacts are not adequately addressed."
    ],
    "Originality": 3,
    "Quality": 3,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can you provide more details on the autoencoder aggregator and its implementation?",
        "Have you considered ablation studies involving different types of aggregators or modulation functions?",
        "Can you provide more qualitative examples of the hierarchical features discovered by CompositionalLens?",
        "How does the method perform on other language models or datasets beyond Gemma-2B?",
        "What are the practical applications or scenarios where CompositionalLens would be particularly beneficial?",
        "Can the authors include additional ablation studies to isolate the contributions of each component, such as different initialization methods and the specific contributions of each tier?",
        "What is the computational overhead introduced by the hierarchical structure and curriculum learning, and how can it be mitigated?",
        "What steps were taken to address the noted training stability issues and how reproducible are these steps across different setups?",
        "Can the impact of different composition weights and sparsity penalties on model performance be elaborated?"
    ],
    "Limitations": [
        "The paper needs to address the clarity of some components, such as the autoencoder aggregator. Additional ablation studies and examples during the rebuttal period would enhance the evaluation.",
        "The method requires careful initialization and gradient management to ensure stable training, which may limit its practical applicability.",
        "Memory usage and computational overhead due to hierarchical computation are significant and may hinder scalability.",
        "The paper does not sufficiently discuss the limitations of the proposed method. Additionally, potential negative societal impacts, particularly related to the interpretability and control of language models, are not addressed."
    ],
    "Ethical Concerns": false,
    "Soundness": 3,
    "Presentation": 2,
    "Contribution": 3,
    "Overall": 5,
    "Confidence": 4,
    "Decision": "Reject"
}