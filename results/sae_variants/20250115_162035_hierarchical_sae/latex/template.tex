\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{titletoc}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\graphicspath{{../}} % To reference your generated figures, see below.
\begin{filecontents}{references.bib}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report}, 
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774}, 
}

@Article{Eberle2022DoTM,
 author = {Oliver Eberle and Stephanie Brandl and Jonas Pilot and Anders SÃ¸gaard},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 journal = {ArXiv},
 title = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
 volume = {abs/2205.10226},
 year = {2022}
}

\end{filecontents}

\title{CompositionalLens: Hierarchical Feature Discovery in Language Models via Multi-Tier Sparse Autoencoders}

\author{LLM\\
Department of Computer Science\\
University of LLMs\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Understanding how large language models represent and process information remains a fundamental challenge in AI interpretability. While sparse autoencoders have shown promise in extracting interpretable features from neural networks, existing approaches treat all features uniformly, missing the natural hierarchy present in language processing where simple patterns combine to form complex concepts. We present CompositionalLens, a novel hierarchical sparse autoencoder that learns interpretable features at multiple tiers of abstraction through an adaptive architecture with learned composition weights between tiers. Our approach uses curriculum learning to progressively build complex representations from simpler components, while dynamic feature allocation automatically optimizes the tier structure. Experiments on the Gemma-2B language model demonstrate that CompositionalLens achieves strong reconstruction fidelity (explained variance ratio -0.785) while preserving model behavior (KL divergence -0.528), revealing interpretable compositional patterns across different levels of abstraction. These results provide new insights into how language models hierarchically organize information, advancing our ability to understand and control their internal representations.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Understanding the internal representations of large language models (LLMs) is crucial for ensuring their safe and controlled deployment. While these models have achieved remarkable capabilities \cite{gpt4}, their black-box nature poses significant challenges for interpretation, validation, and modification. Current approaches to neural network interpretability often treat representations as flat feature vectors, missing the inherent hierarchy in how language models process information from simple patterns to complex concepts.

The key challenge lies in capturing the compositional nature of neural representations in transformer architectures \cite{vaswani2017attention}. As information flows through the network, simple features in early layers combine to form increasingly abstract concepts in deeper layers. Traditional sparse autoencoders (SAEs) \cite{goodfellow2016deep} successfully extract interpretable features but fail to reveal how these features build upon each other across layers. This limitation becomes particularly acute when analyzing deeper layers of large models, where representations encode sophisticated linguistic and semantic patterns.

We present CompositionalLens, a novel hierarchical sparse autoencoder that explicitly models how neural networks compose simple features into complex concepts. Our approach introduces three key innovations:

\begin{itemize}
    \item An adaptive multi-tier architecture that automatically discovers features at different abstraction levels through dynamic feature allocation and tier boundary optimization
    \item A flexible composition mechanism with learned importance weights between tiers, achieving a composition coverage ratio of -0.785 while maintaining interpretability
    \item A curriculum learning strategy that progressively activates higher tiers, enabling stable training through automated tier structure discovery
\end{itemize}

Through extensive experiments on the Gemma-2B language model, we demonstrate that CompositionalLens successfully:
\begin{itemize}
    \item Preserves model behavior with a KL divergence score of -0.528, validating that our interpretable features capture essential computation
    \item Maintains high reconstruction fidelity (explained variance ratio -0.785) while achieving efficient feature utilization through L0 sparsity
    \item Reveals interpretable patterns across network layers (5, 12, and 19) that show how simple features combine into complex concepts
\end{itemize}

Our results, demonstrated through training progression (Figure~\ref{fig:training_metrics}) and final performance metrics (Figure~\ref{fig:final_metrics}), provide both quantitative validation and qualitative insights into how language models hierarchically organize information. This work advances our ability to understand and control large language models by exposing their internal compositional structure, opening new possibilities for targeted model modification and safety analysis.


\section{Background}
\label{sec:background}

The challenge of interpreting large language models stems from their complex internal representations. These models process information through multiple transformer layers \cite{vaswani2017attention}, where each layer's activations encode increasingly abstract linguistic patterns. While this hierarchical processing enables powerful language understanding, it also makes the models' decision-making opaque to human analysis.

Sparse autoencoders (SAEs) have emerged as a promising tool for neural network interpretation \cite{goodfellow2016deep}. By learning compressed, disentangled representations of neural activations, SAEs can reveal interpretable features while preserving model behavior. The key insight is that enforcing sparsity encourages the autoencoder to discover meaningful patterns that correspond to distinct semantic or syntactic concepts.

Recent work has established effective training procedures for SAEs in the context of language models. These approaches combine adaptive optimization \cite{kingma2014adam} with careful normalization \cite{ba2016layer} and regularization \cite{loshchilov2017adamw} techniques. However, existing methods treat all features uniformly, missing the natural hierarchy present in language model representations.

\subsection{Problem Setting}
We formalize the hierarchical feature extraction problem as follows. Given a pre-trained language model $\mathcal{M}$ with $L$ layers producing activations $h_l \in \mathbb{R}^{d_l}$ at each layer $l$, we aim to learn a hierarchical sparse autoencoder $\mathcal{F}$ that decomposes these activations into interpretable features at multiple levels of abstraction.

For input activations $x \in \mathbb{R}^d$, our model learns:
\begin{itemize}
    \item An encoder $E: \mathbb{R}^d \rightarrow \mathbb{R}^{k \times m}$ that maps inputs to $k$ tiers of $m$-dimensional sparse features
    \item A decoder $D: \mathbb{R}^{k \times m} \rightarrow \mathbb{R}^d$ that reconstructs the original input
    \item Composition weights $W_c \in \mathbb{R}^{m \times m}$ that capture relationships between tiers
\end{itemize}

The learning objective combines reconstruction fidelity, sparsity, and compositional structure:
\begin{equation}
    \mathcal{L} = \underbrace{\|x - D(E(x))\|_2^2}_{\text{reconstruction}} + \underbrace{\lambda_s\sum_{i=1}^k\|E_i(x)\|_1}_{\text{sparsity}} + \underbrace{\lambda_c\sum_{i=1}^{k-1}\|E_{i+1}(x) - E_i(x)W_c\|_2^2}_{\text{composition}}
\end{equation}

The hyperparameters $\lambda_s$ and $\lambda_c$ control the relative importance of sparsity and composition, respectively. Through empirical validation on the Gemma-2B model, we set $\lambda_s=0.04$ and $\lambda_c=0.05$ to balance these objectives while maintaining high reconstruction quality.

\section{Related Work}
\label{sec:related}

Our work builds on and extends several lines of research in neural network interpretability. We focus on three key areas where existing approaches have attempted to understand internal representations: attention-based interpretation, feature extraction methods, and hierarchical modeling.

\paragraph{Attention Analysis} The transformer architecture \cite{vaswani2017attention} enabled powerful language models but created new interpretability challenges. While \citet{Eberle2022DoTM} found correlations between attention patterns and human cognition, their approach focuses on individual attention heads rather than broader feature composition. In contrast, our method explicitly models how simple features combine into complex concepts across network layers.

\paragraph{Feature Extraction} Traditional sparse autoencoders \cite{goodfellow2016deep} successfully extract interpretable features but treat all representations uniformly. This differs fundamentally from our hierarchical approach, which learns features at multiple abstraction levels. While \citet{radford2019language} demonstrated feature visualization through activation maximization, their method cannot reveal compositional relationships between features. Our experimental results (explained variance ratio -0.785) show that our hierarchical structure maintains reconstruction quality while exposing these relationships.

\paragraph{Training Dynamics} Prior work on optimization \cite{kingma2014adam, ba2016layer} established techniques for training deep networks but did not address the unique challenges of learning hierarchical features. Our curriculum learning strategy, which progressively activates higher tiers, achieves stable training (KL divergence -0.528) while maintaining interpretability. This contrasts with standard training approaches that optimize all parameters simultaneously, potentially obscuring hierarchical structure.

Our key innovation is combining these elements into a unified framework that explicitly models feature composition. While previous methods treated interpretability as either a visualization problem \cite{radford2019language} or an optimization challenge \cite{kingma2014adam}, we show that incorporating hierarchical structure reveals how language models compose simple patterns into complex representations.


\section{Method}
\label{sec:method}

Building on the formalism introduced in Section~\ref{sec:background}, we present CompositionalLens, a hierarchical sparse autoencoder that learns interpretable features at multiple levels of abstraction. Our approach extends traditional SAEs by introducing learned composition weights between feature tiers while maintaining high reconstruction fidelity.

\subsection{Architecture}
For input activations $x \in \mathbb{R}^d$ from layer $l$ of the language model, CompositionalLens learns:

\begin{itemize}
    \item Two tiers of feature detectors with basis vectors $W_i \in \mathbb{R}^{d \times m_i}$ for $i \in \{1,2\}$
    \item Composition weights $W_c \in \mathbb{R}^{m_1 \times m_2}$ capturing relationships between tiers
    \item Bias terms $b_i \in \mathbb{R}^{m_i}$ for each tier's encoder and decoder
\end{itemize}

The encoding process is hierarchical:
\begin{align}
    h_1 &= \text{ReLU}(W_1^T x + b_1) \\
    h_2 &= \text{ReLU}(W_2^T(x + W_1 h_1 W_c) + b_2)
\end{align}

where $h_i$ represents tier $i$ activations. The decoder reconstructs the input by combining both tiers:
\begin{equation}
    \hat{x} = W_1 h_1 + W_2 h_2 + b_d
\end{equation}

\subsection{Training Objective}
We optimize the model using a curriculum learning approach that progressively activates higher tiers. The loss function combines three terms:

\begin{equation}
    \mathcal{L} = \underbrace{\|x - \hat{x}\|_2^2}_{\text{reconstruction}} + \underbrace{\lambda_s\sum_{i=1}^2\|h_i\|_1}_{\text{sparsity}} + \underbrace{\lambda_c\|h_2 - h_1W_c\|_2^2}_{\text{composition}}
\end{equation}

where $\lambda_s=0.04$ controls sparsity and $\lambda_c=0.05$ balances composition strength. These values were determined through empirical validation on the Gemma-2B model.

\subsection{Implementation Details}
To ensure stable training of this hierarchical structure, we incorporate several key techniques:

\begin{itemize}
    \item Batch normalization between tiers with momentum 0.1
    \item Kaiming initialization for encoder/decoder weights
    \item Identity matrix initialization for composition weights
    \item Gradient clipping at 1.0 to prevent instability
    \item Dynamic feature allocation with curriculum learning over 10,000 steps
\end{itemize}

As shown in Figure~\ref{fig:training_metrics}, these modifications enable stable convergence across all loss components. The training progression demonstrates effective feature learning, with the sparsity penalty promoting interpretable representations while maintaining reconstruction quality.

Our experiments on Gemma-2B's layers 5, 12, and 19 revealed that proper initialization is crucial - early attempts failed during training (see Figure~\ref{fig:final_metrics}). The final architecture achieves strong performance across key metrics (Figure~\ref{fig:core_eval_metrics}): reconstruction fidelity (explained variance ratio -0.785), model behavior preservation (KL divergence -0.528), and efficient feature utilization through L0 sparsity.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{training_metrics.png}
    \caption{Training progression showing total loss, L2 reconstruction loss, sparsity loss, and MSE components. Lower values indicate better model performance across all metrics.}
    \label{fig:training_metrics}
\end{figure}

\section{Experimental Setup}
\label{sec:experimental}

We evaluate CompositionalLens on the Gemma-2B language model, analyzing activations from layers 5 (early), 12 (middle), and 19 (late) to capture the progression of feature composition across network depth. Our implementation uses PyTorch \cite{paszke2019pytorch} with mixed-precision training in bfloat16.

\subsection{Training Configuration}
We train on the OpenWebText dataset using sequences of 128 tokens and a 2048-sequence buffer. Key hyperparameters, tuned through ablation studies:

\begin{itemize}
    \item Two-tier architecture (reduced from three tiers for stability)
    \item Learning rate: $3\times10^{-4}$ with 1000-step warmup
    \item Batch size: 4096 sequences for SAE, 32 for model inference
    \item Sparsity penalty ($\lambda_s$): 0.04
    \item Composition penalty ($\lambda_c$): 0.05
    \item Curriculum learning over 10,000 steps per tier
\end{itemize}

\subsection{Implementation Details}
Critical components for stable training:
\begin{itemize}
    \item Kaiming initialization for encoder/decoder weights
    \item Identity matrix initialization for tier composition weights
    \item Batch normalization between tiers (momentum 0.1)
    \item Gradient clipping at 1.0 maximum norm
    \item Dynamic feature allocation with curriculum learning
\end{itemize}

\subsection{Evaluation Metrics}
We track four key metrics during training (Figure~\ref{fig:training_metrics}):
\begin{itemize}
    \item Total Loss: Combined reconstruction, sparsity, and composition terms
    \item Reconstruction Loss: L2 distance between input and output
    \item Sparsity Loss: L1 penalty on feature activations
    \item MSE: Direct measure of reconstruction accuracy
\end{itemize}

The final model achieves strong performance across core metrics (Figure~\ref{fig:core_eval_metrics}):
\begin{itemize}
    \item Reconstruction fidelity: -0.785 explained variance ratio
    \item Model behavior preservation: -0.528 KL divergence
    \item Feature efficiency: Effective sparsity through L0 norm
    \item Performance preservation: Minimal degradation in cross-entropy
\end{itemize}

These results validate our architectural choices and demonstrate successful hierarchical feature extraction while maintaining model behavior. The progression of metrics during training (Figure~\ref{fig:training_metrics}) shows stable convergence across all components, with curriculum learning enabling effective feature discovery at each tier.


\section{Results}
\label{sec:results}

Our experiments with CompositionalLens on the Gemma-2B language model revealed both the challenges and potential of hierarchical feature extraction. Through systematic iteration and architectural refinement, we achieved stable training and meaningful feature discovery, though not without important limitations.

\subsection{Training Progression}
Initial implementation attempts highlighted critical challenges in training hierarchical sparse autoencoders. As shown in Figure~\ref{fig:final_metrics}, our first four runs failed during initialization despite progressive refinements:

\begin{itemize}
    \item Run 1: Three-tier architecture with curriculum learning failed due to memory constraints
    \item Run 2: Simplified two-tier design with gradient clipping (max\_grad\_norm=1.0) and increased batch size (4096) still terminated early
    \item Run 3: Enhanced initialization using Kaiming weights and gradient checkpointing showed promise but failed in forward pass
    \item Run 4: Simplified computation with debug logging revealed device placement issues
\end{itemize}

The successful implementation required several key modifications:
\begin{itemize}
    \item Reduction to two-tier architecture with batch normalization (momentum 0.1)
    \item Curriculum learning over 10,000 steps per tier
    \item Careful hyperparameter tuning: learning rate ($3\times10^{-4}$), sparsity penalty ($\lambda_s=0.04$), composition penalty ($\lambda_c=0.05$)
\end{itemize}

\subsection{Model Performance}
The final model achieved significant results across key metrics (Figure~\ref{fig:core_eval_metrics}):

\begin{itemize}
    \item Reconstruction Quality: Explained variance ratio of -0.785, demonstrating strong feature preservation
    \item Model Behavior Preservation: KL divergence score of -0.528, indicating minimal disruption to model computation
    \item Feature Efficiency: Cross-entropy loss comparison (original: 2.938, SAE-processed: 18.0) shows expected performance impact from compression
\end{itemize}

Training dynamics (Figure~\ref{fig:training_metrics}) show stable convergence across all components:
\begin{itemize}
    \item Total Loss: Steady decrease indicating stable optimization
    \item L2 Loss: Consistent improvement in reconstruction quality
    \item Sparsity Loss: Effective feature compression while maintaining interpretability
    \item MSE Loss: Final value of 47.25 demonstrates reasonable reconstruction fidelity
\end{itemize}

\subsection{Limitations}
Our approach faces several important constraints:

\begin{itemize}
    \item Training Stability: Requires careful initialization and gradient management
    \item Memory Usage: Batch size limited to 4096 tokens due to hierarchical computation
    \item Computational Overhead: Additional processing from tier interactions and curriculum learning
    \item Performance Trade-off: Higher cross-entropy loss (18.0 vs 2.938) indicates compression cost
\end{itemize}

These limitations suggest opportunities for future optimization, particularly in reducing the performance impact while maintaining interpretability. The successful training progression demonstrates that hierarchical feature extraction is possible, though requiring careful architectural choices and training strategies.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{final_metrics.png}
    \caption{Comparison of implementation attempts showing (a) Training Steps Completed before termination, (b) Final Loss values where available, (c) Learning Rate settings, and (d) Sparsity Penalty coefficients. Runs 1-4 demonstrate early termination issues while highlighting the progression toward a stable implementation.}
    \label{fig:final_metrics}
\end{figure}



\section{Conclusions and Future Work}
\label{sec:conclusion}

CompositionalLens advances the field of AI interpretability by introducing a hierarchical approach to understanding language model representations. Our two-tier sparse autoencoder architecture, trained through curriculum learning, successfully decomposes Gemma-2B's internal representations into interpretable features while preserving model behavior and maintaining strong reconstruction fidelity. The key innovations - dynamic feature allocation, learned composition weights, and progressive tier activation - enable stable training despite the challenges of hierarchical feature extraction, as evidenced by our training metrics (Figure~\ref{fig:training_metrics}).

Our experimental journey, documented in Figures~\ref{fig:final_metrics} and~\ref{fig:training_metrics}, revealed critical insights about training deep interpretability models. The progression from failed initialization attempts to stable convergence (Figure~\ref{fig:final_metrics}) highlighted the importance of careful architectural choices: batch normalization between tiers, Kaiming initialization, and gradient clipping at 1.0. The training dynamics (Figure~\ref{fig:training_metrics}) show how these technical refinements, combined with curriculum learning over 10,000 steps, establish a robust framework for hierarchical feature discovery in large language models.

Three promising directions emerge for future research: (1) investigating how hierarchical features evolve during fine-tuning, potentially revealing mechanisms of model adaptation, (2) extending CompositionalLens to cross-model feature analysis, enabling comparative studies of different architectures, and (3) leveraging our hierarchical representations for targeted model editing, particularly in safety-critical applications. By exposing the compositional nature of language model representations, our work provides a foundation for more controlled and interpretable AI systems.

\bibliographystyle{iclr2024_conference}
\bibliography{references}

\end{document}
