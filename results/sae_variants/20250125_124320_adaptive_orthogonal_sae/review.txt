{
    "Summary": "The paper proposes a method for feature disentanglement in sparse autoencoders, aimed at reducing computational overhead. The method combines selective orthogonality constraints, local competition, and adaptive feature management. The paper demonstrates the effectiveness of the approach on the Gemma-2B language model.",
    "Strengths": [
        "Addresses a relevant problem in feature disentanglement for large language models.",
        "Proposes an innovative combination of selective constraints, local competition, and adaptive feature management.",
        "Extensive experiments demonstrate the effectiveness of the approach in reducing computational overhead."
    ],
    "Weaknesses": [
        "The paper lacks clarity in certain sections, making it difficult to follow the proposed methodology.",
        "The novelty of the approach is somewhat incremental, building on existing concepts without a substantial leap.",
        "The evaluation could benefit from more diverse datasets and additional baselines.",
        "The adaptive mechanisms require careful tuning, which may limit the practical applicability of the method.",
        "Insufficient comparative analysis with other state-of-the-art methods in disentanglement.",
        "Sensitivity to hyperparameters is not thoroughly explored, which could be crucial for practical applications."
    ],
    "Originality": 3,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can the authors provide more details and clarity on the adaptive mechanisms?",
        "How does the proposed approach perform on a wider range of datasets and additional baselines?",
        "Can the authors discuss the practical implications of the careful tuning required for the adaptive mechanisms?",
        "How does the method compare with other state-of-the-art disentanglement methods in terms of both computational efficiency and feature quality?",
        "Can the authors provide more detailed explanations and possibly pseudo-code or algorithmic steps for the proposed mechanisms?",
        "How sensitive are the results to the choice of hyperparameters, particularly the momentum coefficient and neighborhood radius?",
        "What additional measures can be taken to mitigate the parameter sensitivity and training stability issues mentioned in the limitations?",
        "Are there any specific failure cases or limitations that were observed during the experiments but not discussed in the paper?",
        "Have you tested the approach on other language models or datasets to evaluate its generalizability?",
        "Can you provide more detailed ablation studies and comparisons with other state-of-the-art methods?"
    ],
    "Limitations": [
        "The adaptive mechanisms require careful tuning, which may limit the practical applicability of the method.",
        "The evaluation is limited to a specific model and dataset, which may not generalize well to other settings.",
        "Parameter sensitivity and training stability issues need further exploration and mitigation strategies.",
        "Potential limitations and failure cases are not discussed in detail, which could impact the practical applicability of the proposed methods.",
        "The dynamic pair selection may miss important correlations below the statistical threshold."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}