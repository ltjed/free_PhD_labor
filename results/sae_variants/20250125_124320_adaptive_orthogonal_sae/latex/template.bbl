\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bell \& Sejnowski(1995)Bell and Sejnowski]{Bell1995AnIA}
A.~J. Bell and T.~Sejnowski.
\newblock An information-maximization approach to blind separation and blind
  deconvolution.
\newblock \emph{Neural Computation}, 7:\penalty0 1129--1159, 1995.

\bibitem[Bengio(2007)]{Bengio2007LearningDA}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock \emph{Found. Trends Mach. Learn.}, 2:\penalty0 1--127, 2007.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee et~al.(2006)Lee, Battle, Raina, and Ng]{Lee2006EfficientSC}
Honglak Lee, Alexis Battle, Rajat Raina, and A.~Ng.
\newblock Efficient sparse coding algorithms.
\newblock pp.\  801--808, 2006.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineLF}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009.

\bibitem[Merullo et~al.(2024)Merullo, Eickhoff, and
  Pavlick]{Merullo2024TalkingHU}
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.
\newblock Talking heads: Understanding inter-layer communication in transformer
  language models.
\newblock \emph{ArXiv}, abs/2406.09519, 2024.

\bibitem[Mondorf et~al.(2024)Mondorf, Wold, and Plank]{Mondorf2024CircuitCE}
Philipp Mondorf, Sondre Wold, and Barbara Plank.
\newblock Circuit compositions: Exploring modular structures in
  transformer-based language models.
\newblock \emph{ArXiv}, abs/2410.01434, 2024.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[Ranzato et~al.(2006)Ranzato, Poultney, Chopra, and
  LeCun]{Ranzato2006EfficientLO}
Marc'Aurelio Ranzato, Christopher~S. Poultney, S.~Chopra, and Yann LeCun.
\newblock Efficient learning of sparse representations with an energy-based
  model.
\newblock pp.\  1137--1144, 2006.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Yosinski et~al.(2015)Yosinski, Clune, Nguyen, Fuchs, and
  Lipson]{Yosinski2015UnderstandingNN}
J.~Yosinski, J.~Clune, Anh~Totti Nguyen, Thomas~J. Fuchs, and Hod Lipson.
\newblock Understanding neural networks through deep visualization.
\newblock \emph{ArXiv}, abs/1506.06579, 2015.

\end{thebibliography}
