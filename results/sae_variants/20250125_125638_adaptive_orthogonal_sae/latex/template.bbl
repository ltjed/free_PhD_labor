\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bell \& Sejnowski(1995)Bell and Sejnowski]{Bell1995AnIA}
A.~J. Bell and T.~Sejnowski.
\newblock An information-maximization approach to blind separation and blind
  deconvolution.
\newblock \emph{Neural Computation}, 7:\penalty0 1129--1159, 1995.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{Burgess2018UnderstandingDI}
Christopher~P. Burgess, I.~Higgins, Arka Pal, L.~Matthey, Nicholas Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in β-vae.
\newblock \emph{ArXiv}, abs/1804.03599, 2018.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and
  Goldberg]{Geva2022TransformerFL}
Mor Geva, Avi Caciularu, Ke~Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting
  concepts in the vocabulary space.
\newblock \emph{ArXiv}, abs/2203.14680, 2022.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Hyvärinen \& Oja(2000)Hyvärinen and
  Oja]{Hyvärinen2000IndependentCA}
Aapo Hyvärinen and E.~Oja.
\newblock Independent component analysis : Algorithms and applications.
\newblock 2000.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{Kingma2013AutoEncodingVB}
Diederik~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{CoRR}, abs/1312.6114, 2013.

\bibitem[Lee et~al.(2024)Lee, Lum, Liu, and Yogatama]{Lee2024CausalIO}
Isabelle Lee, Joshua Lum, Ziyi Liu, and Dani Yogatama.
\newblock Causal interventions on causal paths: Mapping gpt-2's reasoning from
  syntax to semantics.
\newblock \emph{ArXiv}, abs/2410.21353, 2024.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[Shao et~al.(2020)Shao, Lin, Yang, Yao, Zhao, and
  Abdelzaher]{Shao2020DynamicVAEDR}
Huajie Shao, Haohong Lin, Qinmin Yang, Shuochao Yao, Han Zhao, and
  T.~Abdelzaher.
\newblock Dynamicvae: Decoupling reconstruction error and disentangled
  representation learning.
\newblock \emph{arXiv: Learning}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\end{thebibliography}
