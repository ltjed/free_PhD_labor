{
    "Summary": "The paper proposes a selective orthogonality method for efficient feature disentanglement in sparse autoencoders, specifically targeting the most entangled feature pairs. This reduces computational complexity and maintains effective feature disentanglement. The method is validated on the Gemma-2B language model, showing improvements in training time, memory usage, and feature utilization.",
    "Strengths": [
        "The selective orthogonality approach reduces computational complexity significantly.",
        "Comprehensive experiments validate the method, including layer-wise analysis and ablation studies.",
        "Practical improvements in training time and memory usage make the method feasible for consumer hardware."
    ],
    "Weaknesses": [
        "The robustness of the top-k correlated pairs selection mechanism is not thoroughly explored.",
        "The method is only validated on the Gemma-2B language model; generalization to other architectures or tasks is unclear.",
        "The paper acknowledges sensitivity to batch composition and layer dependence but does not offer concrete solutions.",
        "Lacks detailed comparison with other state-of-the-art disentanglement methods beyond sparse autoencoders.",
        "The explanation of the autoencoder architecture and hyperparameter choices is somewhat lacking in detail.",
        "Empirical analysis could benefit from more diverse datasets and broader comparison."
    ],
    "Originality": 3,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "How robust is the top-k correlated pairs selection mechanism across different datasets and models?",
        "Can the method be generalized to other architectures or tasks?",
        "What strategies can be employed to mitigate the sensitivity to batch composition and layer dependence?",
        "How does the method perform compared to other disentanglement techniques like \u03b2-VAE in terms of feature interpretability and computational efficiency?",
        "Can the authors provide more details on the choice of hyperparameters, especially \u03bb2, and how it impacts the balance between reconstruction quality and feature disentanglement?",
        "Can the authors provide more detailed implementation steps to improve reproducibility?",
        "How does the method perform on different types of datasets other than Gemma-2B?",
        "Can the authors discuss potential strategies to mitigate the sensitivity to parameter tuning and batch composition?"
    ],
    "Limitations": [
        "The method's efficiency heavily depends on the selection of top-k correlated pairs.",
        "Validation is limited to the Gemma-2B language model.",
        "Sensitivity to batch composition and layer dependence is acknowledged but not addressed.",
        "The method's sensitivity to batch composition and parameter tuning suggests that it may not be robust across different datasets or model architectures.",
        "The selective orthogonality method might not generalize well to other types of neural networks or tasks beyond language models."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}