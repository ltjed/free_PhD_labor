{
    "Summary": "The paper introduces Hierarchical Sparse Autoencoders (HSAEs) to enhance the interpretability of large language models by capturing hierarchical compositionality in activations. The method includes a dynamic tier structure, a learned composition loss, and curriculum-based training. Experiments on Gemma-2B activations show that HSAEs achieve better reconstruction quality and model fidelity compared to baseline SAEs. However, the experiments also reveal significant optimization challenges, including persistent gradient explosion and complete feature inactivity.",
    "Strengths": [
        "Addresses a significant problem in language model interpretability.",
        "Introduces a novel hierarchical approach to sparse autoencoders.",
        "Theoretical framework is well-motivated and could be promising."
    ],
    "Weaknesses": [
        "The experiments show that the method faces severe optimization challenges, including persistent gradient explosion and complete feature inactivity.",
        "The training process consistently produces NaN losses, indicating fundamental issues with the implementation.",
        "The paper does not provide sufficient solutions or mitigations for the identified challenges.",
        "The results fail to demonstrate the effectiveness of the proposed method in practice.",
        "Incomplete ablation studies and lack of detailed analysis on failure cases.",
        "The paper lacks clarity and organization, making it difficult to follow."
    ],
    "Originality": 3,
    "Quality": 1,
    "Clarity": 2,
    "Significance": 2,
    "Questions": [
        "Can the authors provide more details on the initialization schemes and why they might fail?",
        "Are there any alternative approaches that could be used to mitigate the gradient explosion issue?",
        "Can the authors provide more insights into the failure modes and how they plan to address them in future work?",
        "What specific modifications can be made to the activation functions or optimization techniques to address the gradient explosion?",
        "Can the authors conduct more ablation studies to isolate the impact of each component of the HSAE?",
        "How does the hierarchical approach compare to flat SAEs in more detail, particularly in cases where the training is stable?",
        "Can the authors provide more details on the autoencoder aggregator and the dynamic tier allocation mechanism?",
        "What alternative strategies were considered for addressing the observed training stability issues?",
        "How does the approach perform with different initialization schemes and activation functions?"
    ],
    "Limitations": [
        "The paper's primary limitation is the failure of the proposed method to train successfully, as evidenced by NaN losses and zero feature activity. The authors acknowledge these issues but do not offer concrete solutions.",
        "The potential negative societal impacts of the work are not discussed.",
        "The current implementation fails to provide actionable results due to the training failures."
    ],
    "Ethical Concerns": false,
    "Soundness": 1,
    "Presentation": 2,
    "Contribution": 1,
    "Overall": 2,
    "Confidence": 4,
    "Decision": "Reject"
}