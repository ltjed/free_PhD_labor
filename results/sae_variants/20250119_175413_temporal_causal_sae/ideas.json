[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "cosine_unlearning_sae",
        "Title": "Knowledge-Separating Sparse Autoencoders via Cosine Feature Alignment",
        "Experiment": "1. Implement cosine similarity loss between retain/forget features\n2. Add activation caching and early stopping\n3. Train on WikiText/WMDP-bio with different loss weights\n4. Evaluate using unlearning metrics and feature separation analysis\n5. Compare against standard SAE and other baselines\n6. Analyze feature activation patterns across datasets",
        "Technical_Details": "The model uses a simplified objective: L = L_reconstruct + \u03bb_1 * L_sparse + \u03bb_2 * L_cosine, where L_cosine = mean(cos_sim(f_retain, f_forget)) for l2-normalized features. Implementation caches activations per batch to avoid recomputation. Early stopping triggers when feature separation plateaus. Uses batch size 256 with gradient accumulation. Features are analyzed using activation pattern visualization and clustering. Evaluation includes standard unlearning metrics plus new separation quality measures based on feature activation distributions.",
        "Research_Impact": "A key challenge in targeted unlearning is cleanly separating desirable from undesirable knowledge in model representations. Current approaches rely on post-hoc feature clamping without guarantees of clean knowledge separation. This research directly addresses the challenge through explicit feature alignment during training, producing more interpretable and effective unlearning while maintaining model capabilities. The simple yet principled approach enables widespread adoption and analysis of knowledge organization in neural networks.",
        "Implementation_Plan": "1. Add CosineSimilarityLoss class\n2. Modify CustomSAE to cache and normalize features\n3. Update CustomTrainer with loss calculation\n4. Add early stopping and monitoring\n5. Implement separation quality metrics\n6. Create analysis and visualization tools\n7. Integrate with unlearning benchmark",
        "Interestingness_Evaluation": "The elegant combination of cosine feature alignment with sparse autoencoders provides a powerful new lens for understanding and controlling knowledge representation in neural networks.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The stripped-down implementation focusing on cosine similarity is highly feasible, requiring minimal code changes and computation. Caching optimizations and gradient accumulation ensure it fits within time constraints. The core concepts are simple and well-understood, making implementation straightforward.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While cosine similarity is a standard technique, its application to knowledge separation in SAEs and the focus on interpretability represents a novel contribution.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": false
    },
    {
        "Name": "momentum_contrastive_sae",
        "Title": "Momentum Contrastive Learning for Knowledge-Separating Sparse Autoencoders",
        "Experiment": "1. Implement efficient pattern matching with batch matmul\n2. Add momentum-based pattern cache updates\n3. Train with varying momentum values\n4. Compare unlearning performance metrics\n5. Analyze pattern evolution over training\n6. Evaluate knowledge separation quality\n7. Measure computational overhead",
        "Technical_Details": "Two-component approach with momentum updates:\n1. Pattern Management:\n   - Initialize retain/forget pattern caches from WikiText/WMDP-bio\n   - Update patterns with momentum: p_new = m*p_old + (1-m)*p_batch\n   - Maintain running statistics for pattern importance\n2. Training:\n   - Compute similarities using batch matrix multiplication\n   - Contrastive loss: L_cont = max(0, margin + pos_sim - neg_sim)\n   - Dynamic margin based on pattern statistics\n   - Final loss: L = L_reconstruct + \u03bb1*L_sparse + \u03bb2*L_cont\nHyperparameters: batch_size=512, momentum=0.99, initial_margin=0.1, cache_size=10K per domain",
        "Research_Impact": "A critical challenge in selective unlearning is maintaining stable feature separation across training while adapting to changing patterns. This research addresses the challenge through momentum-based pattern updates that provide consistent learning signals while allowing adaptation. The method achieves better unlearning performance by maintaining cleaner knowledge separation throughout training.",
        "Implementation_Plan": "1. Add MomentumPatternCache class\n2. Implement efficient batch similarity computation\n3. Modify CustomTrainer with momentum updates\n4. Add pattern statistics tracking\n5. Create visualization tools\n6. Integrate with benchmarks",
        "Interestingness_Evaluation": "Using momentum-based pattern evolution to guide knowledge separation presents an elegant and theoretically-grounded approach to improving SAE interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The approach is extremely feasible because: 1) Uses simple matrix operations available in PyTorch 2) Momentum updates add minimal overhead 3) Memory usage is lower than previous version 4) Implementation requires only modest changes to existing code 5) Runtime well within 30-min constraint due to efficient batch operations",
        "Feasibility": 10,
        "Novelty_Evaluation": "While building on contrastive learning, the addition of momentum-based pattern evolution for knowledge separation in SAEs is novel.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": true
    },
    {
        "Name": "graph_attention_sae",
        "Title": "Two-Stage Graph Attention for Efficient Knowledge Separation in Sparse Autoencoders",
        "Experiment": "1. Implement two-stage feature selection\n2. Add domain-specific attention masks\n3. Create efficient attention computation\n4. Train on WikiText/WMDP-bio datasets\n5. Compare unlearning metrics vs baselines\n6. Analyze attention patterns\n7. Measure speed and memory usage",
        "Technical_Details": "Two-stage approach: 1) Select candidate features using sparsity threshold \u03b8=0.1, typically reducing to ~20% of features. 2) Apply single-head GAT to candidates. Node features v_i are l2-normalized activation vectors. Edge weights initialized as e_ij = ReLU(cos_sim(v_i,v_j) - 0.5). Attention scores \u03b1_ij = softmax(LeakyReLU(W[v_i||v_j])). Domain masks m_retain, m_forget from activation patterns on respective datasets. Final importance score s_i = \u03a3_j \u03b1_ij * (m_retain_j - m_forget_j). Features with s_i < 0 selected for unlearning. Uses batch size 1024 with gradient accumulation. Attention computed every 50 steps.",
        "Research_Impact": "A critical challenge in selective unlearning is efficiently identifying groups of features that encode related knowledge while maintaining tractable computation. Current methods either ignore feature relationships or become computationally intractable for large feature sets. This research addresses both challenges through a novel two-stage approach that focuses attention computation on relevant feature subsets, enabling scalable analysis of feature relationships while improving unlearning performance. The domain-specific attention masks provide interpretable visualization of knowledge separation.",
        "Implementation_Plan": "1. Add TwoStageAttention class\n2. Implement efficient candidate selection\n3. Add domain-specific masking\n4. Create cached attention computation\n5. Modify CustomTrainer integration\n6. Add visualization tools\n7. Update benchmark integration",
        "Interestingness_Evaluation": "The two-stage attention approach elegantly combines efficiency with interpretability while providing theoretical justification for feature group identification.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The approach is extremely feasible because: 1) Two-stage approach reduces computation by ~80% 2) Single attention head with simple architecture 3) Implementation requires minimal new code 4) Memory usage very low due to candidate selection 5) Runtime well within 30-min constraint as attention only computed every 50 steps on reduced feature set",
        "Feasibility": 10,
        "Novelty_Evaluation": "The combination of two-stage selection and domain-specific attention masks for knowledge separation in SAEs is highly novel.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": true
    },
    {
        "Name": "temporal_causal_sae",
        "Title": "Temporal Causal Sparse Autoencoders for Precise Knowledge Unlearning",
        "Experiment": "1. Implement sparse causal dictionary\n2. Add circular sequence buffer\n3. Create LSH-based top-k selection\n4. Train with optimized architecture\n5. Evaluate using enhanced metrics\n6. Generate causal visualizations\n7. Analyze intervention impacts",
        "Technical_Details": "Architecture uses sparse dictionary D_i storing top-k causal successors for each feature i. LSH-based approximate top-k selection (k=2) reduces computation. Circular buffer of size 1024 stores recent activations with window_size=3. Causality scores s_ij = exp(cos_sim(v_i, v_j))/Z computed for candidate pairs from LSH. Temporal consistency uses exponential moving average: D_t = 0.9*D_{t-1} + 0.1*D_batch. Activation pruning threshold \u03c4=0.1 reduces feature set. Intervention triggers on dangerous feature detection, clamping features j where s_ij > 0.8. Implementation uses PyTorch sparse operations and efficient LSH library. New visualization tools track feature trajectories and intervention effects.",
        "Research_Impact": "A key challenge in selective unlearning is precisely targeting dangerous knowledge while minimizing impact on beneficial knowledge. Current approaches that intervene on individual features often disrupt related beneficial knowledge or miss dangerous knowledge that emerges from feature combinations. This research addresses the challenge by modeling temporal causal relationships between features, enabling more precise interventions that target specific causal chains while preserving beneficial feature interactions. The temporal modeling helps distinguish dangerous from benign activation patterns.",
        "Implementation_Plan": "1. Add CircularBuffer class\n2. Implement SparseCausalDict\n3. Create LSHTopK selector\n4. Add visualization tools\n5. Implement intervention logic\n6. Create evaluation suite\n7. Update benchmark integration",
        "Interestingness_Evaluation": "The combination of temporal modeling and efficient causal tracking provides a novel and practical approach to understanding and controlling knowledge emergence in neural networks.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The approach is extremely feasible because: 1) Sparse dictionary and LSH drastically reduce computation 2) Simple circular buffer manages memory efficiently 3) Implementation requires minimal new code 4) No complex operations like gradient checkpointing needed 5) Runtime well under 30-min constraint due to approximation techniques",
        "Feasibility": 10,
        "Novelty_Evaluation": "While building on existing techniques, the combination of sparse causal modeling with efficient approximations for knowledge separation in SAEs is highly novel.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": true
    },
    {
        "Name": "dual_temporal_sae",
        "Title": "Dual-Expert Temporal Modeling for Efficient Knowledge Unlearning",
        "Experiment": "1. Implement dual-expert architecture\n2. Add pattern queue mechanism\n3. Create momentum-based specialization\n4. Train on WikiText/WMDP-bio datasets\n5. Compare unlearning metrics vs baselines\n6. Analyze expert specialization\n7. Measure computational overhead",
        "Technical_Details": "Two-expert architecture specialized for dangerous vs. benign patterns. Fixed-size queue Q of size 256 stores recent patterns. Pattern matching uses cosine similarity with threshold \u03c4=0.7. Experts share dictionary D with momentum-based updates: D_t = mD_{t-1} + (1-m)D_batch, m=0.9. Gating network g uses softmax(Wx) with temperature 0.1. Expert specialization enforced through contrastive loss L_c between experts. Implementation uses efficient PyTorch operations. Batch size 1024 with updates every 25 steps.",
        "Research_Impact": "A critical challenge in selective unlearning is precisely targeting dangerous knowledge while preserving beneficial knowledge. Current approaches that focus on static feature patterns often miss dangerous knowledge that emerges from temporal interactions. This research addresses the challenge through an efficient dual-expert architecture that explicitly models and separates dangerous and benign temporal patterns. The momentum-based specialization ensures stable learning of pattern categories while maintaining model capabilities.",
        "Implementation_Plan": "1. Add DualExpertSAE class\n2. Implement PatternQueue\n3. Create MomentumUpdater\n4. Add contrastive loss\n5. Modify CustomTrainer integration\n6. Create visualization tools\n7. Update benchmark integration",
        "Interestingness_Evaluation": "The dual-expert approach with momentum-based specialization provides an elegant and practical solution to temporal knowledge separation in neural networks.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The approach is extremely feasible because: 1) Simple two-expert architecture 2) Fixed-size queue bounds memory usage 3) Implementation requires minimal new code 4) No complex operations needed 5) Runtime well under 30-min constraint due to efficient design",
        "Feasibility": 10,
        "Novelty_Evaluation": "While building on existing techniques, the combination of dual experts with momentum-based temporal specialization for knowledge separation in SAEs represents a novel approach.",
        "Novelty": 9,
        "Overall_Score": 9.5,
        "novel": true
    }
]