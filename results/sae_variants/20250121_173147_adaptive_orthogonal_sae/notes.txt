# Title: Adaptive Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders
# Experiment description: 1. Implement adaptive orthogonality loss with controlled feature sharing
2. Add batch-wise feature grouping with periodic updates
3. Train on Pythia-70M using WMDP-bio and WikiText datasets
4. Compare unlearning performance against baseline and fixed orthogonal SAE
5. Analyze condition numbers of feature subspaces
6. Evaluate impact of different α values for controlled sharing

## Run 0: Baseline
Results: {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'e823bbbb-62c9-41ec-840b-cacb8ca4230d', 'datetime_epoch_millis': 1737147895673, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.939325, 'llm_top_1_test_accuracy': 0.6842749999999999, 'llm_top_2_test_accuracy': 0.7260625, 'llm_top_5_test_accuracy': 0.7746249999999999, 'llm_top_10_test_accuracy': 0.82099375, 'llm_top_20_test_accuracy': 0.8589374999999999, 'llm_top_50_test_accuracy': 0.90028125, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9576, 'llm_top_1_test_accuracy': 0.6648000000000001, 'llm_top_2_test_accuracy': 0.6844, 'llm_top_5_test_accuracy': 0.7466, 'llm_top_10_test_accuracy': 0.8286, 'llm_top_20_test_accuracy': 0.8602000000000001, 'llm_top_50_test_accuracy': 0.9118, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9385999999999999, 'llm_top_1_test_accuracy': 0.6869999999999999, 'llm_top_2_test_accuracy': 0.7228000000000001, 'llm_top_5_test_accuracy': 0.7626, 'llm_top_10_test_accuracy': 0.806, 'llm_top_20_test_accuracy': 0.8484, 'llm_top_50_test_accuracy': 0.8892, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9038, 'llm_top_1_test_accuracy': 0.6799999999999999, 'llm_top_2_test_accuracy': 0.7066000000000001, 'llm_top_5_test_accuracy': 0.7432000000000001, 'llm_top_10_test_accuracy': 0.7984, 'llm_top_20_test_accuracy': 0.8173999999999999, 'llm_top_50_test_accuracy': 0.8709999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8832000000000001, 'llm_top_1_test_accuracy': 0.6068, 'llm_top_2_test_accuracy': 0.6446, 'llm_top_5_test_accuracy': 0.6818, 'llm_top_10_test_accuracy': 0.7076, 'llm_top_20_test_accuracy': 0.7714000000000001, 'llm_top_50_test_accuracy': 0.8346, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9255, 'llm_top_1_test_accuracy': 0.629, 'llm_top_2_test_accuracy': 0.685, 'llm_top_5_test_accuracy': 0.737, 'llm_top_10_test_accuracy': 0.766, 'llm_top_20_test_accuracy': 0.8, 'llm_top_50_test_accuracy': 0.854, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.969, 'llm_top_1_test_accuracy': 0.6644, 'llm_top_2_test_accuracy': 0.7016, 'llm_top_5_test_accuracy': 0.7836000000000001, 'llm_top_10_test_accuracy': 0.834, 'llm_top_20_test_accuracy': 0.8939999999999999, 'llm_top_50_test_accuracy': 0.931, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9375, 'llm_top_1_test_accuracy': 0.733, 'llm_top_2_test_accuracy': 0.7685000000000001, 'llm_top_5_test_accuracy': 0.8, 'llm_top_10_test_accuracy': 0.84575, 'llm_top_20_test_accuracy': 0.8865000000000001, 'llm_top_50_test_accuracy': 0.91225, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.8092, 'llm_top_2_test_accuracy': 0.8949999999999999, 'llm_top_5_test_accuracy': 0.9422, 'llm_top_10_test_accuracy': 0.9816, 'llm_top_20_test_accuracy': 0.9936, 'llm_top_50_test_accuracy': 0.9984, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'bcb003afd6045deaee4be8dd883ae42863da9163', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_5_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 5, 'hook_name': 'blocks.5.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None, 'training_steps': 0, 'final_loss': None, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}
Description: Baseline results.

## Run 1: Basic Orthogonality Loss (α=0.1)
Results: {'training_steps': 0, 'final_loss': None, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5279503105590062, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.375}, 'model_performance_preservation': {'ce_loss_score': -0.5855263157894737, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.0, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -0.78515625, 'mse': 47.25, 'cossim': nan}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 0.0, 'l2_ratio': 0.0, 'relative_reconstruction_bias': nan}, 'sparsity': {'l0': 0.0, 'l1': 0.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Initial implementation of orthogonality loss with α=0.1 showed poor performance. The model failed to learn meaningful features, with zero sparsity (L0=0.0, L1=0.0) and negative KL divergence and cross-entropy scores. The high MSE (47.25) and negative explained variance (-0.79) suggest the model is not reconstructing inputs properly. This indicates potential issues with the loss function implementation or training dynamics.

## Run 2: Attempted Training Loop Fix
Results: {'training_steps': 48, 'final_loss': nan, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': nan, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': nan}, 'model_performance_preservation': {'ce_loss_score': nan, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': nan, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': nan, 'mse': nan, 'cossim': nan}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': nan, 'l2_ratio': nan, 'relative_reconstruction_bias': nan}, 'sparsity': {'l0': 0.0, 'l1': 0.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Training loop modifications did not resolve the issues. The model only trained for 48 steps before encountering numerical instabilities (NaN values). The sparsity metrics remained at zero (L0=0.0, L1=0.0), indicating no feature activation. This suggests potential issues with:
1. Gradient instability possibly due to the interaction between orthogonality loss and L1 sparsity
2. Learning rate may be too high for stable training
3. Weight initialization might be contributing to the instability

## Run 3: Gradient Stabilization Attempt
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.639751552795031, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 16.5}, 'model_performance_preservation': {'ce_loss_score': -0.6907894736842105, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 19.0, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -4.3125, 'mse': 140.0, 'cossim': -0.00311279296875}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 430.0, 'l2_ratio': 1.3984375, 'relative_reconstruction_bias': -948.0}, 'sparsity': {'l0': 1150.35205078125, 'l1': 8256.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: The gradient stabilization attempts showed mixed results. While we avoided NaN values, the model's performance is still problematic:
1. High MSE (140.0) and negative explained variance (-4.3125) indicate poor reconstruction quality
2. Negative cosine similarity (-0.003) suggests misaligned feature learning
3. High L0 (1150.35) and L1 (8256.0) sparsity metrics indicate insufficient feature sparsity
4. Large reconstruction bias (-948.0) points to systematic errors in the learned representations
5. The high final loss (1000000.0) suggests the model hit our error ceiling

Next steps should focus on:
1. Reducing learning rate further
2. Implementing proper weight normalization
3. Adjusting the balance between reconstruction and sparsity losses

## Run 4: Reduced Learning Rate with Weight Normalization
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5527950310559007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.625}, 'model_performance_preservation': {'ce_loss_score': -0.5986842105263158, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -1.671875, 'mse': 70.5, 'cossim': -0.0032196044921875}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 215.0, 'l2_ratio': 0.69921875, 'relative_reconstruction_bias': 960.0}, 'sparsity': {'l0': 1150.32568359375, 'l1': 5856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: The reduced learning rate (0.0001) and weight normalization showed some improvements but still faces challenges:
1. MSE improved to 70.5 (down from 140.0) showing better reconstruction
2. Explained variance improved to -1.67 (from -4.31) but still negative
3. L1 sparsity improved to 5856.0 (from 8256.0) showing better feature compression
4. L2 ratio of 0.699 indicates better controlled feature magnitudes
5. However, the high final loss (1000000.0) and negative cosine similarity (-0.0032) suggest fundamental optimization issues

Next steps:
1. Implement adaptive learning rates per feature
2. Add gradient accumulation for more stable updates
3. Introduce feature-wise normalization

## Run 5: Adaptive Learning with Gradient Accumulation
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5527950310559007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.625}, 'model_performance_preservation': {'ce_loss_score': -0.5986842105263158, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -1.671875, 'mse': 70.5, 'cossim': -0.0032196044921875}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 215.0, 'l2_ratio': 0.69921875, 'relative_reconstruction_bias': 960.0}, 'sparsity': {'l0': 1150.32568359375, 'l1': 5856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Run 5 implemented several significant changes to improve training stability:
1. Added per-feature adaptive learning rates with bounds (0.1x to 10x base lr)
2. Implemented gradient accumulation over 4 steps for more stable updates
3. Improved optimizer parameter grouping with explicit constraints
4. Added gradient clipping (max norm 1.0)

Results analysis:
- No improvement in final metrics compared to Run 4
- Training still hits the loss ceiling (1000000.0)
- Reconstruction metrics remain poor (MSE: 70.5, explained variance: -1.671875)
- Sparsity metrics unchanged (L0: ~1150, L1: 5856.0)
- Model behavior preservation metrics static (KL div: -0.553)

The lack of improvement despite these stability measures suggests deeper issues:
1. Possible architectural limitations in the basic SAE design
2. Loss landscape may be too complex for current optimization approach
3. Feature initialization might be suboptimal

Next steps:
1. Implement progressive feature learning (start with smaller dict_size and gradually increase)
2. Add feature dropout during training for better generalization
3. Modify architecture to include skip connections for improved gradient flow

## Run 6: Skip Connections and Feature Dropout
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5527950310559007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.625}, 'model_performance_preservation': {'ce_loss_score': -0.5986842105263158, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -1.671875, 'mse': 70.5, 'cossim': -0.0032196044921875}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 215.0, 'l2_ratio': 0.69921875, 'relative_reconstruction_bias': 960.0}, 'sparsity': {'l0': 1150.32568359375, 'l1': 5856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Run 6 focused on architectural improvements:
1. Added residual skip connections between encoder and decoder
2. Implemented feature dropout (30% during training)
3. Retained all previous stability measures from Run 5

Results analysis:
- Metrics remain largely unchanged from Run 5
- Still hitting loss ceiling (1000000.0)
- No improvement in reconstruction (MSE: 70.5, explained variance: -1.671875)
- Sparsity metrics static (L0: ~1150, L1: 5856.0)
- KL divergence and CE loss scores unchanged

The consistent poor performance across multiple architectural changes suggests:
1. The optimization problem may be fundamentally ill-conditioned
2. Current approach to orthogonality constraints might be too rigid
3. Need to rethink the basic assumptions about feature learning

Next steps:
1. Implement curriculum learning with gradually increasing orthogonality constraints
2. Add layer normalization between encoder and decoder
3. Experiment with alternative sparsity mechanisms

## Run 7: Curriculum Learning for Orthogonality
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5527950310559007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.625}, 'model_performance_preservation': {'ce_loss_score': -0.5986842105263158, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -1.671875, 'mse': 70.5, 'cossim': -0.0032196044921875}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 215.0, 'l2_ratio': 0.69921875, 'relative_reconstruction_bias': 960.0}, 'sparsity': {'l0': 1150.32568359375, 'l1': 5856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Run 7 implemented curriculum learning for orthogonality constraints:
1. Added gradual increase in orthogonality loss weight (alpha) from 0.01 to 0.1 over 5000 steps
2. Maintained feature dropout and skip connections from Run 6
3. Added proper initialization of orthogonality curriculum parameters
4. Improved loss calculation stability with epsilon terms

Results analysis:
- No improvement in overall metrics despite curriculum learning
- Loss still hits ceiling (1000000.0)
- Reconstruction quality remains poor (MSE: 70.5, explained variance: -1.671875)
- Sparsity metrics unchanged (L0: ~1150, L1: 5856.0)
- Model behavior preservation metrics static

The lack of improvement with curriculum learning suggests:
1. The training dynamics might be unstable from the start
2. Need for better initialization strategy
3. Possible issues with the basic architecture design

Next steps:
1. Add layer normalization between encoder and decoder
2. Implement warm-up period for reconstruction loss before introducing constraints
3. Experiment with smaller initial dictionary size and progressive growth

## Run 8: Layer Normalization and Loss Warm-up
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.7142857142857143, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 17.25}, 'model_performance_preservation': {'ce_loss_score': -0.7828947368421053, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 19.875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -0.80859375, 'mse': 48.0, 'cossim': -0.003204345703125}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 33.75, 'l2_ratio': 0.11181640625, 'relative_reconstruction_bias': -35.75}, 'sparsity': {'l0': 1154.838134765625, 'l1': 920.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Run 8 focused on stabilizing training dynamics through architectural and loss function modifications:
1. Added layer normalization (nn.LayerNorm) to the encoder stage to control activation magnitudes
2. Implemented warm-up factors for all constraint losses (L1 sparsity and orthogonality)
3. Doubled the warm-up period to 2000 steps for more gradual introduction of constraints
4. Reduced initial learning rate by 10x and added adaptive per-feature learning rates
5. Maintained previous improvements (feature dropout, skip connections)

Results analysis:
- Some improvements in reconstruction metrics:
  * MSE decreased to 48.0 (from 70.5)
  * L1 sparsity improved significantly to 920.0 (from 5856.0)
  * Better l2_ratio at 0.112 (previously 0.699)
- However, persistent issues remain:
  * Still hitting loss ceiling (1000000.0)
  * Explained variance still negative (-0.809)
  * Negative cosine similarity (-0.003)
  * Poor model behavior preservation (KL div: -0.714)

The results suggest that while layer normalization and loss warm-up helped with some metrics:
1. The fundamental optimization challenge remains
2. Need to address the negative explained variance
3. Model behavior preservation needs significant improvement

Next steps:
1. Implement progressive feature unlocking (start with 10% active features)
2. Add gradient checkpointing for memory efficiency
3. Experiment with alternative sparsity mechanisms (e.g., k-winners-take-all)

## Run 9: Progressive Feature Unlocking
Results: {'training_steps': 48, 'final_loss': 1000000.0, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0001, 'sparsity_penalty': 0.04, 'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.7142857142857143, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 17.25}, 'model_performance_preservation': {'ce_loss_score': -0.7828947368421053, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 19.875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -0.80859375, 'mse': 48.0, 'cossim': -0.003204345703125}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 33.75, 'l2_ratio': 0.11181640625, 'relative_reconstruction_bias': -35.75}, 'sparsity': {'l0': 1154.838134765625, 'l1': 920.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}
Description: Run 9 implemented progressive feature activation with the following key changes:
1. Started with 25% active features (576 of 2304)
2. Added feature growth rate of 10% every 1000 steps
3. Implemented feature masking mechanism in forward pass
4. Maintained all stability improvements from Run 8
5. Added gradient checkpointing for memory efficiency

Results analysis:
- No significant improvements over Run 8:
  * MSE remained at 48.0
  * L1 sparsity stable at 920.0
  * L2 ratio unchanged at 0.112
  * KL divergence score static at -0.714
- Progressive feature activation did not address core issues:
  * Loss still hits ceiling (1000000.0)
  * Explained variance remains negative (-0.809)
  * Cosine similarity still negative (-0.003)
  * Model behavior preservation metrics unchanged

The results suggest that progressive feature activation, while theoretically promising, did not help with:
1. Training stability (still hitting loss ceiling)
2. Feature learning quality (negative explained variance)
3. Model behavior preservation (poor KL divergence)

Next steps:
1. Implement k-winners-take-all activation for better sparsity control
2. Add feature-wise normalization
3. Consider alternative initialization strategies for features
