{
  "eval_type_id": "sparse_probing",
  "eval_config": {
    "random_seed": 42,
    "dataset_names": [
      "LabHC/bias_in_bios_class_set1",
      "LabHC/bias_in_bios_class_set2",
      "LabHC/bias_in_bios_class_set3",
      "canrager/amazon_reviews_mcauley_1and5",
      "canrager/amazon_reviews_mcauley_1and5_sentiment",
      "codeparrot/github-code",
      "fancyzhx/ag_news",
      "Helsinki-NLP/europarl"
    ],
    "probe_train_set_size": 4000,
    "probe_test_set_size": 1000,
    "context_length": 128,
    "sae_batch_size": 125,
    "llm_batch_size": 32,
    "llm_dtype": "bfloat16",
    "model_name": "google/gemma-2-2b",
    "k_values": [
      1,
      2,
      5,
      10,
      20,
      50
    ],
    "lower_vram_usage": false
  },
  "eval_id": "e823bbbb-62c9-41ec-840b-cacb8ca4230d",
  "datetime_epoch_millis": 1737147895673,
  "eval_result_metrics": {
    "llm": {
      "llm_test_accuracy": 0.939325,
      "llm_top_1_test_accuracy": 0.6842749999999999,
      "llm_top_2_test_accuracy": 0.7260625,
      "llm_top_5_test_accuracy": 0.7746249999999999,
      "llm_top_10_test_accuracy": 0.82099375,
      "llm_top_20_test_accuracy": 0.8589374999999999,
      "llm_top_50_test_accuracy": 0.90028125,
      "llm_top_100_test_accuracy": null
    },
    "sae": {
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  },
  "eval_result_details": [
    {
      "dataset_name": "LabHC/bias_in_bios_class_set1_results",
      "llm_test_accuracy": 0.9576,
      "llm_top_1_test_accuracy": 0.6648000000000001,
      "llm_top_2_test_accuracy": 0.6844,
      "llm_top_5_test_accuracy": 0.7466,
      "llm_top_10_test_accuracy": 0.8286,
      "llm_top_20_test_accuracy": 0.8602000000000001,
      "llm_top_50_test_accuracy": 0.9118,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set2_results",
      "llm_test_accuracy": 0.9385999999999999,
      "llm_top_1_test_accuracy": 0.6869999999999999,
      "llm_top_2_test_accuracy": 0.7228000000000001,
      "llm_top_5_test_accuracy": 0.7626,
      "llm_top_10_test_accuracy": 0.806,
      "llm_top_20_test_accuracy": 0.8484,
      "llm_top_50_test_accuracy": 0.8892,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "LabHC/bias_in_bios_class_set3_results",
      "llm_test_accuracy": 0.9038,
      "llm_top_1_test_accuracy": 0.6799999999999999,
      "llm_top_2_test_accuracy": 0.7066000000000001,
      "llm_top_5_test_accuracy": 0.7432000000000001,
      "llm_top_10_test_accuracy": 0.7984,
      "llm_top_20_test_accuracy": 0.8173999999999999,
      "llm_top_50_test_accuracy": 0.8709999999999999,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
      "llm_test_accuracy": 0.8832000000000001,
      "llm_top_1_test_accuracy": 0.6068,
      "llm_top_2_test_accuracy": 0.6446,
      "llm_top_5_test_accuracy": 0.6818,
      "llm_top_10_test_accuracy": 0.7076,
      "llm_top_20_test_accuracy": 0.7714000000000001,
      "llm_top_50_test_accuracy": 0.8346,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
      "llm_test_accuracy": 0.9255,
      "llm_top_1_test_accuracy": 0.629,
      "llm_top_2_test_accuracy": 0.685,
      "llm_top_5_test_accuracy": 0.737,
      "llm_top_10_test_accuracy": 0.766,
      "llm_top_20_test_accuracy": 0.8,
      "llm_top_50_test_accuracy": 0.854,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "codeparrot/github-code_results",
      "llm_test_accuracy": 0.969,
      "llm_top_1_test_accuracy": 0.6644,
      "llm_top_2_test_accuracy": 0.7016,
      "llm_top_5_test_accuracy": 0.7836000000000001,
      "llm_top_10_test_accuracy": 0.834,
      "llm_top_20_test_accuracy": 0.8939999999999999,
      "llm_top_50_test_accuracy": 0.931,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "fancyzhx/ag_news_results",
      "llm_test_accuracy": 0.9375,
      "llm_top_1_test_accuracy": 0.733,
      "llm_top_2_test_accuracy": 0.7685000000000001,
      "llm_top_5_test_accuracy": 0.8,
      "llm_top_10_test_accuracy": 0.84575,
      "llm_top_20_test_accuracy": 0.8865000000000001,
      "llm_top_50_test_accuracy": 0.91225,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    },
    {
      "dataset_name": "Helsinki-NLP/europarl_results",
      "llm_test_accuracy": 0.9994,
      "llm_top_1_test_accuracy": 0.8092,
      "llm_top_2_test_accuracy": 0.8949999999999999,
      "llm_top_5_test_accuracy": 0.9422,
      "llm_top_10_test_accuracy": 0.9816,
      "llm_top_20_test_accuracy": 0.9936,
      "llm_top_50_test_accuracy": 0.9984,
      "llm_top_100_test_accuracy": null,
      "sae_test_accuracy": 0.5,
      "sae_top_1_test_accuracy": 0.5,
      "sae_top_2_test_accuracy": 0.5,
      "sae_top_5_test_accuracy": 0.5,
      "sae_top_10_test_accuracy": 0.5,
      "sae_top_20_test_accuracy": 0.5,
      "sae_top_50_test_accuracy": 0.5,
      "sae_top_100_test_accuracy": null
    }
  ],
  "sae_bench_commit_hash": "bcb003afd6045deaee4be8dd883ae42863da9163",
  "sae_lens_id": "custom_sae",
  "sae_lens_release_id": "google/gemma-2-2b_layer_5_sae",
  "sae_lens_version": "5.3.0",
  "sae_cfg_dict": {
    "model_name": "google/gemma-2-2b",
    "d_in": 2304,
    "d_sae": 2304,
    "hook_layer": 5,
    "hook_name": "blocks.5.hook_resid_post",
    "context_size": null,
    "hook_head_index": null,
    "architecture": "Custom",
    "apply_b_dec_to_input": true,
    "finetuning_scaling_factor": null,
    "activation_fn_str": "relu",
    "prepend_bos": true,
    "normalize_activations": "none",
    "dtype": "bfloat16",
    "device": "",
    "dataset_path": "",
    "dataset_trust_remote_code": true,
    "seqpos_slice": [
      null
    ],
    "training_tokens": -100000,
    "sae_lens_training_version": null,
    "neuronpedia_id": null
  },
  "eval_result_unstructured": null,
  "training_steps": 0,
  "final_loss": null,
  "layer": 19,
  "dict_size": 2304,
  "learning_rate": 0.0003,
  "sparsity_penalty": 0.04,
  "unique_id": "google/gemma-2-2b_layer_5_sae_custom_sae",
  "sae_set": "google/gemma-2-2b_layer_5_sae",
  "sae_id": "custom_sae",
  "eval_cfg": {
    "model_name": "google/gemma-2-2b",
    "llm_dtype": "bfloat16",
    "batch_size_prompts": 16,
    "n_eval_reconstruction_batches": 200,
    "n_eval_sparsity_variance_batches": 2000,
    "dataset": "Skylion007/openwebtext",
    "context_size": 128,
    "compute_kl": true,
    "compute_ce_loss": true,
    "compute_l2_norms": true,
    "compute_sparsity_metrics": true,
    "compute_variance_metrics": true,
    "compute_featurewise_density_statistics": false,
    "compute_featurewise_weight_based_metrics": false,
    "exclude_special_tokens_from_reconstruction": true,
    "verbose": false
  },
  "metrics": {
    "model_behavior_preservation": {
      "kl_div_score": 0.006211180124223602,
      "kl_div_with_ablation": 10.0625,
      "kl_div_with_sae": 10.0
    },
    "model_performance_preservation": {
      "ce_loss_score": -0.019736842105263157,
      "ce_loss_with_ablation": 12.4375,
      "ce_loss_with_sae": 12.625,
      "ce_loss_without_sae": 2.9375
    },
    "reconstruction_quality": {
      "explained_variance": -0.498046875,
      "mse": 4.0625,
      "cossim": NaN
    },
    "shrinkage": {
      "l2_norm_in": 90.0,
      "l2_norm_out": 0.0,
      "l2_ratio": 0.0,
      "relative_reconstruction_bias": NaN
    },
    "sparsity": {
      "l0": 0.0,
      "l1": 0.0
    },
    "token_stats": {
      "total_tokens_eval_reconstruction": 409600,
      "total_tokens_eval_sparsity_variance": 4096000
    }
  }
}