# Title: Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders

# Visualization Analysis:

## Figure: sae_performance_metrics.png
This figure presents a 2x2 grid of plots showing key metrics across different learning rates:

1. Reconstruction Quality (Top Left):
   - Shows both explained variance and cosine similarity metrics
   - Notable trend of improvement up to 9e-4 learning rate
   - Unexpected sharp decline at 1.1e-3, suggesting we found the upper stability limit
   - Peak performance at 9e-4 with ~0.97 explained variance and ~0.99 cosine similarity

2. Feature Utilization (Top Right):
   - Measures percentage of features actively used by the model
   - Steady increase from ~80% to ~90% utilization as learning rate increases
   - Plateaus around 90% after 9e-4, indicating optimal feature engagement
   - Surprisingly robust even at higher learning rates

3. Model Behavior Preservation (Bottom Left):
   - KL divergence score showing how well the SAE preserves original model behavior
   - Maintains very high preservation (>0.99) until 9e-4
   - Sharp degradation at 1.1e-3, confirming this as the stability threshold
   - Critical metric showing clear upper bound for safe learning rates

4. Combined Performance (Bottom Right):
   - Aggregate metric combining all key performance indicators
   - Clear peak at 9e-4 learning rate
   - Validates 9e-4 as optimal learning rate
   - Sharp decline after peak suggests clear cutoff point

## Figure: sae_metrics_summary.png
This bar plot provides a comparative view across all experimental runs:

- X-axis shows runs with corresponding learning rates
- Four metrics plotted side by side for each run:
  * Explained Variance (red)
  * Cosine Similarity (blue)
  * KL Divergence Score (green)
  * Feature Utilization (orange)

Key Observations:
1. Metric Correlation:
   - Strong positive correlation between explained variance and cosine similarity
   - Feature utilization increases don't compromise other metrics until the final run
   - KL divergence remains stable until critical threshold

2. Optimal Configuration:
   - Run 7 (lr=9e-4) shows best overall performance
   - Balanced optimization across all metrics
   - High feature utilization without compromising stability

3. Performance Cliff:
   - Run 8 (lr=1.1e-3) shows clear performance degradation
   - Sharp decline in KL divergence and reconstruction metrics
   - Feature utilization remains high but quality suffers

Recommendations for Future Work:
1. Focus on learning rates between 7e-4 and 9e-4 for fine-tuning
2. Consider adaptive learning rate schemes around the 9e-4 sweet spot
3. Investigate methods to maintain stability above 9e-4 while preserving high feature utilization

# Experiment description: 
The goal is to develop an orthogonal sparse autoencoder that learns disentangled features while maintaining strong reconstruction performance. Key components:

1. Adaptive orthogonality loss with controlled feature sharing
2. Group-specific biases for better feature separation
3. Constrained decoder weights for improved stability
4. Evaluation on Gemma-2B model activations

## Run 0: Baseline
Results: Standard SAE implementation without orthogonality constraints.
Performance shows room for improvement in feature disentanglement.

## Run 1: Initial Orthogonal Implementation
Configuration:
- Orthogonality penalty max: 0.1
- Group-specific biases with 8 groups
- Constrained decoder weights
- Learning rate: 3e-4
- Sparsity penalty: 0.04

Results:
- Final loss: 4081.94
- KL divergence score: 0.996 (excellent model behavior preservation)
- Reconstruction quality:
  * Cosine similarity: 0.98
  * Explained variance: 0.93
  * MSE: 1.87
- Sparsity metrics:
  * L0: 1835.54 (79.7% feature utilization)
  * L1: 20992.0
- Strong performance preservation (CE loss score: 0.997)

Analysis:
The initial implementation shows very promising results with excellent reconstruction quality and model behavior preservation. The high feature utilization (79.7%) suggests room for increasing sparsity. The strong performance metrics indicate that the orthogonal constraints are not harming the model's core functionality.

## Run 2: Increased Orthogonality
Adjusting orthogonality penalty to 0.3 to promote stronger feature disentanglement while maintaining the successful aspects of Run 1.

## Run 3: Optimized Sparsity
Configuration:
- Increased sparsity penalty to 0.06 (from 0.04)
- Maintained other hyperparameters from Run 1
- Learning rate: 3e-4
- Orthogonality penalty max: 0.3

Results:
- Final loss: 4089.29
- KL divergence score: 0.997 (exceptional model behavior preservation)
- Reconstruction quality:
  * Cosine similarity: 0.98
  * Explained variance: 0.938
  * MSE: 1.67
- Sparsity metrics:
  * L0: 1870.64 (81.2% feature utilization)
  * L1: 19456.0
- Outstanding performance preservation (CE loss score: 0.998)

Analysis:
Run 3 achieved an excellent balance between sparsity and reconstruction quality. The increased sparsity penalty led to slightly lower feature utilization (81.2% vs 79.7% in Run 1) while maintaining or improving other metrics. The exceptional KL divergence and CE loss scores suggest the model's behavior is very well preserved. The lower MSE (1.67 vs 1.87 in Run 1) indicates better reconstruction despite increased sparsity.

## Run 4: Enhanced Orthogonality
Configuration:
- Increased orthogonality penalty max to 0.5 (from 0.3)
- Maintained sparsity penalty at 0.06
- Learning rate: 3e-4
- Other hyperparameters unchanged

Results:
- Final loss: 3945.75 (improved from Run 3)
- KL divergence score: 0.997 (maintained exceptional preservation)
- Reconstruction quality:
  * Cosine similarity: 0.98
  * Explained variance: 0.941
  * MSE: 1.58 (improved)
- Sparsity metrics:
  * L0: 1891.51 (82.1% feature utilization)
  * L1: 19584.0
- Outstanding performance preservation (CE loss score: 0.998)

Analysis:
Run 4 demonstrated that increasing the orthogonality penalty further led to improved results across multiple metrics. The lower final loss and MSE suggest better overall reconstruction quality, while maintaining the excellent model behavior preservation. The slightly higher feature utilization (82.1%) indicates that stronger orthogonality constraints may help prevent feature collapse. The consistent high CE loss score confirms that the model's core functionality remains intact despite the stronger constraints.

## Run 5: Optimized Learning Rate
Configuration:
- Increased learning rate to 5e-4 (from 3e-4)
- Maintained orthogonality penalty max at 0.5
- Maintained sparsity penalty at 0.06
- Other hyperparameters unchanged

Results:
- Final loss: 3307.28 (significant improvement from Run 4)
- KL divergence score: 0.998 (slightly improved preservation)
- Reconstruction quality:
  * Cosine similarity: 0.984 (improved)
  * Explained variance: 0.953 (improved)
  * MSE: 1.242 (substantial improvement)
- Sparsity metrics:
  * L0: 1968.30 (85.4% feature utilization)
  * L1: 18688.0
- Outstanding performance preservation (CE loss score: 0.998)

Analysis:
Run 5 shows that increasing the learning rate while maintaining the strong orthogonality constraints led to significant improvements across all metrics. The lower final loss (3307.28 vs 3945.75) and substantially improved MSE (1.242 vs 1.58) indicate better reconstruction quality. The higher feature utilization (85.4% vs 82.1%) suggests the model is making better use of its capacity while maintaining sparsity. The improved cosine similarity (0.984) and explained variance (0.953) demonstrate enhanced fidelity in representing the original activations. The KL divergence and CE loss scores remain excellent, confirming that the model's behavior is well-preserved despite the more aggressive training regime.

## Run 6: Further Learning Rate Optimization
Configuration:
- Increased learning rate to 7e-4 (from 5e-4)
- Maintained orthogonality penalty max at 0.5
- Maintained sparsity penalty at 0.06
- Other hyperparameters unchanged

Results:
- Final loss: 2761.61 (substantial improvement from Run 5)
- KL divergence score: 0.998 (maintained excellent preservation)
- Reconstruction quality:
  * Cosine similarity: 0.988 (improved)
  * Explained variance: 0.965 (improved)
  * MSE: 0.945 (dramatic improvement)
- Sparsity metrics:
  * L0: 2034.75 (88.3% feature utilization)
  * L1: 18176.0
- Outstanding performance preservation (CE loss score: 0.998)

Analysis:
Run 6 demonstrates that further increasing the learning rate led to even better results across all metrics. The substantial drop in final loss (2761.61 vs 3307.28) and MSE (0.945 vs 1.242) shows significantly improved reconstruction quality. The higher feature utilization (88.3% vs 85.4%) indicates better capacity utilization while maintaining good sparsity levels. The improved cosine similarity (0.988) and explained variance (0.965) show enhanced fidelity in activation representation. Most importantly, the model maintains excellent behavior preservation with high KL divergence and CE loss scores, suggesting that the higher learning rate enables better optimization without compromising the model's core functionality.

## Run 7: Final Learning Rate Optimization
Configuration:
- Increased learning rate to 9e-4 (from 7e-4)
- Maintained orthogonality penalty max at 0.5
- Maintained sparsity penalty at 0.06
- Other hyperparameters unchanged

Results:
- Final loss: 2439.78 (further improvement from Run 6)
- KL divergence score: 0.9986 (slightly improved preservation)
- Reconstruction quality:
  * Cosine similarity: 0.992 (improved)
  * Explained variance: 0.973 (improved)
  * MSE: 0.75 (substantial improvement)
- Sparsity metrics:
  * L0: 2095.25 (90.9% feature utilization)
  * L1: 18432.0
- Outstanding performance preservation (CE loss score: 0.9984)

Analysis:
Run 7 shows that a final increase in learning rate to 9e-4 yielded further improvements across all metrics. The lower final loss (2439.78 vs 2761.61) and MSE (0.75 vs 0.945) demonstrate even better reconstruction quality. The higher feature utilization (90.9% vs 88.3%) suggests more efficient use of the model's capacity while maintaining appropriate sparsity. The improved cosine similarity (0.992) and explained variance (0.973) indicate superior fidelity in representing the original activations. The KL divergence and CE loss scores remain excellent and even show slight improvements, confirming that the model's behavior is exceptionally well-preserved despite the more aggressive training regime. This appears to be an optimal configuration, as the improvements are beginning to show diminishing returns while maintaining strong performance across all metrics.

## Run 8: Final Learning Rate Verification
Configuration:
- Increased learning rate to 1.1e-3 (from 9e-4)
- Maintained orthogonality penalty max at 0.5
- Maintained sparsity penalty at 0.06
- Other hyperparameters unchanged

Results:
- Final loss: 3936.69 (increased from Run 7)
- KL divergence score: 0.9979 (slight decrease from 0.9986)
- Reconstruction quality:
  * Cosine similarity: 0.988 (slight decrease)
  * Explained variance: 0.961 (slight decrease)
  * MSE: 1.055 (increased from 0.75)
- Sparsity metrics:
  * L0: 2090.19 (90.7% feature utilization)
  * L1: 17536.0
- Outstanding performance preservation (CE loss score: 0.9984)

Analysis:
Run 8 confirms that we have reached the optimal learning rate range. While the model still maintains excellent performance across all metrics, we observe slight degradation in several key areas compared to Run 7. The increased MSE (1.055 vs 0.75) and slightly lower cosine similarity and explained variance suggest that pushing the learning rate higher begins to impact reconstruction quality. The marginally lower KL divergence score (0.9979 vs 0.9986) indicates a small decrease in model behavior preservation. Feature utilization remains strong at 90.7%, comparable to Run 7's 90.9%. These results validate that 9e-4 represents the sweet spot for the learning rate, optimally balancing training dynamics with model performance.
Results: {'training results for layer 19': {'config': {'trainer_class': 'JumpReLUTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'sparsity_penalty': 1.0, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'JumpReLUTrainer', 'jump_coeff': 0.1, 'target_l0': 20.0, 'bandwidth': 0.001}, 'final_info': {'training_steps': 4882, 'final_loss': 15402.853515625, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b9515a27-7342-4830-a642-b73e4584890b', 'datetime_epoch_millis': 1738044699812, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives': 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives': 980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 'bcd583862026537f9d24d335de9957ea3c0032ec', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9869953416149069, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.130859375}, 'model_performance_preservation': {'ce_loss_score': 0.9868421052631579, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.0625, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.8203125, 'mse': 4.84375, 'cossim': 0.9453125}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 290.0, 'l2_ratio': 0.9375, 'relative_reconstruction_bias': 0.9921875}, 'sparsity': {'l0': 1639.4825439453125, 'l1': 8576.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '5b599dea-14bc-4457-b2d4-21cb76f447fb', 'datetime_epoch_millis': 1738044896029, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': -0.03266818599837234, 'scr_metric_threshold_2': 0.010666901599584704, 'scr_dir2_threshold_2': 0.010666901599584704, 'scr_dir1_threshold_5': -0.018543904112656242, 'scr_metric_threshold_5': 0.0046776323502625995, 'scr_dir2_threshold_5': 0.0046776323502625995, 'scr_dir1_threshold_10': 0.007057370176879058, 'scr_metric_threshold_10': 0.013445115064759015, 'scr_dir2_threshold_10': 0.013445115064759015, 'scr_dir1_threshold_20': 0.005191064835835884, 'scr_metric_threshold_20': 0.032262100590487494, 'scr_dir2_threshold_20': 0.032262100590487494, 'scr_dir1_threshold_50': -0.012725667783962178, 'scr_metric_threshold_50': 0.0932423154352256, 'scr_dir2_threshold_50': 0.0932423154352256, 'scr_dir1_threshold_100': -0.14053964567259286, 'scr_metric_threshold_100': 0.14346394459832132, 'scr_dir2_threshold_100': 0.14346394459832132, 'scr_dir1_threshold_500': -0.31347739579980494, 'scr_metric_threshold_500': 0.33492377854657673, 'scr_dir2_threshold_500': 0.33492377854657673}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': -0.1492536516756474, 'scr_metric_threshold_2': 0.017676839894409477, 'scr_dir2_threshold_2': 0.017676839894409477, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.03030308959751515, 'scr_dir2_threshold_5': 0.03030308959751515, 'scr_dir1_threshold_10': 0.0, 'scr_metric_threshold_10': 0.03282830943478244, 'scr_dir2_threshold_10': 0.03282830943478244, 'scr_dir1_threshold_20': -0.0746268258378237, 'scr_metric_threshold_20': 0.0530303691664592, 'scr_dir2_threshold_20': 0.0530303691664592, 'scr_dir1_threshold_50': -0.05970163859464643, 'scr_metric_threshold_50': 0.10606058781614919, 'scr_dir2_threshold_50': 0.10606058781614919, 'scr_dir1_threshold_100': -0.3283585674595867, 'scr_metric_threshold_100': 0.1035353679788819, 'scr_dir2_threshold_100': 0.1035353679788819, 'scr_dir1_threshold_500': -1.4029862829193478, 'scr_metric_threshold_500': 0.23989904539254622, 'scr_dir2_threshold_500': 0.23989904539254622}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': -0.09090899238902972, 'scr_metric_threshold_2': -0.00293268818155329, 'scr_dir2_threshold_2': -0.00293268818155329, 'scr_dir1_threshold_5': -0.09999978325586537, 'scr_metric_threshold_5': -0.014662741733089717, 'scr_dir2_threshold_5': -0.014662741733089717, 'scr_dir1_threshold_10': -0.06363607792818617, 'scr_metric_threshold_10': 0.011730228345205611, 'scr_dir2_threshold_10': 0.011730228345205611, 'scr_dir1_threshold_20': -0.0727274106553584, 'scr_metric_threshold_20': 0.03225799685406354, 'scr_dir2_threshold_20': 0.03225799685406354, 'scr_dir1_threshold_50': -0.11818190684987326, 'scr_metric_threshold_50': 0.0850439370106542, 'scr_dir2_threshold_50': 0.0850439370106542, 'scr_dir1_threshold_100': -0.5, 'scr_metric_threshold_100': 0.12316713543415514, 'scr_dir2_threshold_100': 0.12316713543415514, 'scr_dir1_threshold_500': -0.318181473361604, 'scr_metric_threshold_500': 0.4398826015800299, 'scr_dir2_threshold_500': 0.4398826015800299}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.018182665454344472, 'scr_metric_threshold_2': 0.0, 'scr_dir2_threshold_2': 0.0, 'scr_dir1_threshold_5': 0.01818158173367131, 'scr_metric_threshold_5': 0.007370912377219193, 'scr_dir2_threshold_5': 0.007370912377219193, 'scr_dir1_threshold_10': 0.03636316346734262, 'scr_metric_threshold_10': 0.0, 'scr_dir2_threshold_10': 0.0, 'scr_dir1_threshold_20': 0.03636316346734262, 'scr_metric_threshold_20': 0.007370912377219193, 'scr_dir2_threshold_20': 0.007370912377219193, 'scr_dir1_threshold_50': 0.01818158173367131, 'scr_metric_threshold_50': 0.07862409049830214, 'scr_dir2_threshold_50': 0.07862409049830214, 'scr_dir1_threshold_100': 0.09090899238902972, 'scr_metric_threshold_100': 0.08353803208311494, 'scr_dir2_threshold_100': 0.08353803208311494, 'scr_dir1_threshold_500': -1.3454543878224476, 'scr_metric_threshold_500': 0.20393121184735294, 'scr_dir2_threshold_500': 0.20393121184735294}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.015503675388504437, 'scr_metric_threshold_2': 0.002985035855303425, 'scr_dir2_threshold_2': 0.002985035855303425, 'scr_dir1_threshold_5': -0.023255975134237508, 'scr_metric_threshold_5': -0.06268646465854193, 'scr_dir2_threshold_5': -0.06268646465854193, 'scr_dir1_threshold_10': -0.10852711387397361, 'scr_metric_threshold_10': -0.035820964036518604, 'scr_dir2_threshold_10': -0.035820964036518604, 'scr_dir1_threshold_20': -0.22480606544219944, 'scr_metric_threshold_20': 0.0, 'scr_dir2_threshold_20': 0.0, 'scr_dir1_threshold_50': -0.2790698534049267, 'scr_metric_threshold_50': 0.03283575025692268, 'scr_dir2_threshold_50': 0.03283575025692268, 'scr_dir1_threshold_100': -0.8062012853348094, 'scr_metric_threshold_100': 0.1761194284787046, 'scr_dir2_threshold_100': 0.1761194284787046, 'scr_dir1_threshold_500': -1.0930229764339883, 'scr_metric_threshold_500': 0.3880596430696353, 'scr_dir2_threshold_500': 0.3880596430696353}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': -0.018072414612760133, 'scr_metric_threshold_2': 0.021897746705048578, 'scr_dir2_threshold_2': 0.021897746705048578, 'scr_dir1_threshold_5': -0.04819286625795596, 'scr_metric_threshold_5': -0.0036498057301819443, 'scr_dir2_threshold_5': -0.0036498057301819443, 'scr_dir1_threshold_10': -0.012048396096542284, 'scr_metric_threshold_10': 0.0, 'scr_dir2_threshold_10': 0.0, 'scr_dir1_threshold_20': 0.042168488677631524, 'scr_metric_threshold_20': 0.021897746705048578, 'scr_dir2_threshold_20': 0.021897746705048578, 'scr_dir1_threshold_50': -0.0783133179031518, 'scr_metric_threshold_50': 0.06569345765035435, 'scr_dir2_threshold_50': 0.06569345765035435, 'scr_dir1_threshold_100': -0.18072306893528156, 'scr_metric_threshold_100': 0.1532846620057573, 'scr_dir2_threshold_100': 0.1532846620057573, 'scr_dir1_threshold_500': 0.35542140122613214, 'scr_metric_threshold_500': 0.5510947785676481, 'scr_dir2_threshold_500': 0.5510947785676481}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.047058914276958914, 'scr_metric_threshold_2': 0.007518923352021404, 'scr_dir2_threshold_2': 0.007518923352021404, 'scr_dir1_threshold_5': 0.052941190907698006, 'scr_metric_threshold_5': 0.007518923352021404, 'scr_dir2_threshold_5': 0.007518923352021404, 'scr_dir1_threshold_10': 0.08823555192317874, 'scr_metric_threshold_10': 0.02255654597847959, 'scr_dir2_threshold_10': 0.02255654597847959, 'scr_dir1_threshold_20': 0.13529411558461457, 'scr_metric_threshold_20': 0.04135351824215616, 'scr_dir2_threshold_20': 0.04135351824215616, 'scr_dir1_threshold_50': 0.058823818153960185, 'scr_metric_threshold_50': 0.14285730291256044, 'scr_dir2_threshold_50': 0.14285730291256044, 'scr_dir1_threshold_100': 0.15294129609235493, 'scr_metric_threshold_100': 0.18796994671435038, 'scr_dir2_threshold_100': 0.18796994671435038, 'scr_dir1_threshold_500': 0.5058824519385007, 'scr_metric_threshold_500': 0.22932346495650655, 'scr_dir2_threshold_500': 0.22932346495650655}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': -0.03571381054991115, 'scr_metric_threshold_2': 0.00607912922882654, 'scr_dir2_threshold_2': 0.00607912922882654, 'scr_dir1_threshold_5': -0.026785357912433364, 'scr_metric_threshold_5': 0.027355628606963597, 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.05357178019306575, 'scr_metric_threshold_10': 0.030395283805928035, 'scr_dir2_threshold_10': 0.030395283805928035, 'scr_dir1_threshold_20': 0.08928612292707641, 'scr_metric_threshold_20': 0.042553179925376446, 'scr_dir2_threshold_20': 0.042553179925376446, 'scr_dir1_threshold_50': 0.24107141431649734, 'scr_metric_threshold_50': 0.10638304039799229, 'scr_dir2_threshold_50': 0.10638304039799229, 'scr_dir1_threshold_100': 0.31250009978451865, 'scr_metric_threshold_100': 0.16413377164178158, 'scr_dir2_threshold_100': 0.16413377164178158, 'scr_dir1_threshold_500': 0.5982143094725044, 'scr_metric_threshold_500': 0.4528876090298304, 'scr_dir2_threshold_500': 0.4528876090298304}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.01923080780625969, 'scr_metric_threshold_2': 0.03211022594262151, 'scr_dir2_threshold_2': 0.03211022594262151, 'scr_dir1_threshold_5': 0.03846161561251938, 'scr_metric_threshold_5': 0.04587151699019504, 'scr_dir2_threshold_5': 0.04587151699019504, 'scr_dir1_threshold_10': 0.06250005373014743, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 0.04587151699019504, 'scr_dir1_threshold_20': 0.11057692996540351, 'scr_metric_threshold_20': 0.05963308145357687, 'scr_dir2_threshold_20': 0.05963308145357687, 'scr_dir1_threshold_50': 0.11538456027677187, 'scr_metric_threshold_50': 0.12844035693886943, 'scr_dir2_threshold_50': 0.12844035693886943, 'scr_dir1_threshold_100': 0.13461536808303157, 'scr_metric_threshold_100': 0.15596321244982478, 'scr_dir2_threshold_100': 0.15596321244982478, 'scr_dir1_threshold_500': 0.19230779150181063, 'scr_metric_threshold_500': 0.17431187392906444, 'scr_dir2_threshold_500': 0.17431187392906444}], 'sae_bench_commit_hash': 'bcd583862026537f9d24d335de9957ea3c0032ec', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'caf0b1b2-6d6a-42d3-8dd0-925f5695dcce', 'datetime_epoch_millis': 1738044936713, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95108125, 'llm_top_1_test_accuracy': 0.7010624999999999, 'llm_top_2_test_accuracy': 0.75919375, 'llm_top_5_test_accuracy': 0.816825, 'llm_top_10_test_accuracy': 0.86908125, 'llm_top_20_test_accuracy': 0.9019437499999999, 'llm_top_50_test_accuracy': 0.9339937500000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9605937860906124, 'sae_top_1_test_accuracy': 0.7770750000000001, 'sae_top_2_test_accuracy': 0.8246125, 'sae_top_5_test_accuracy': 0.872575, 'sae_top_10_test_accuracy': 0.8967999999999999, 'sae_top_20_test_accuracy': 0.9164375, 'sae_top_50_test_accuracy': 0.93406875, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9714000344276428, 'sae_top_1_test_accuracy': 0.7886, 'sae_top_2_test_accuracy': 0.8544, 'sae_top_5_test_accuracy': 0.8789999999999999, 'sae_top_10_test_accuracy': 0.9112, 'sae_top_20_test_accuracy': 0.9344000000000001, 'sae_top_50_test_accuracy': 0.9486000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9507999999999999, 'llm_top_1_test_accuracy': 0.6746000000000001, 'llm_top_2_test_accuracy': 0.7064, 'llm_top_5_test_accuracy': 0.7628, 'llm_top_10_test_accuracy': 0.8340000000000002, 'llm_top_20_test_accuracy': 0.8882, 'llm_top_50_test_accuracy': 0.9222000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.957200038433075, 'sae_top_1_test_accuracy': 0.7562000000000001, 'sae_top_2_test_accuracy': 0.8064, 'sae_top_5_test_accuracy': 0.8452, 'sae_top_10_test_accuracy': 0.861, 'sae_top_20_test_accuracy': 0.8949999999999999, 'sae_top_50_test_accuracy': 0.9255999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6839999999999999, 'llm_top_2_test_accuracy': 0.7380000000000001, 'llm_top_5_test_accuracy': 0.7819999999999999, 'llm_top_10_test_accuracy': 0.8346, 'llm_top_20_test_accuracy': 0.8736, 'llm_top_50_test_accuracy': 0.905, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9366000294685364, 'sae_top_1_test_accuracy': 0.7592, 'sae_top_2_test_accuracy': 0.806, 'sae_top_5_test_accuracy': 0.836, 'sae_top_10_test_accuracy': 0.858, 'sae_top_20_test_accuracy': 0.8812, 'sae_top_50_test_accuracy': 0.9024000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9084, 'llm_top_1_test_accuracy': 0.632, 'llm_top_2_test_accuracy': 0.701, 'llm_top_5_test_accuracy': 0.7496, 'llm_top_10_test_accuracy': 0.8114000000000001, 'llm_top_20_test_accuracy': 0.8526, 'llm_top_50_test_accuracy': 0.8902000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9274000406265259, 'sae_top_1_test_accuracy': 0.704, 'sae_top_2_test_accuracy': 0.7428, 'sae_top_5_test_accuracy': 0.7942000000000001, 'sae_top_10_test_accuracy': 0.8300000000000001, 'sae_top_20_test_accuracy': 0.8539999999999999, 'sae_top_50_test_accuracy': 0.8868, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9704999999999999, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9730000495910645, 'sae_top_1_test_accuracy': 0.766, 'sae_top_2_test_accuracy': 0.797, 'sae_top_5_test_accuracy': 0.885, 'sae_top_10_test_accuracy': 0.893, 'sae_top_20_test_accuracy': 0.925, 'sae_top_50_test_accuracy': 0.934, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6275999999999999, 'llm_top_2_test_accuracy': 0.6854, 'llm_top_5_test_accuracy': 0.8081999999999999, 'llm_top_10_test_accuracy': 0.8766, 'llm_top_20_test_accuracy': 0.9144, 'llm_top_50_test_accuracy': 0.9374, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9654000520706176, 'sae_top_1_test_accuracy': 0.776, 'sae_top_2_test_accuracy': 0.7972, 'sae_top_5_test_accuracy': 0.8582000000000001, 'sae_top_10_test_accuracy': 0.9207999999999998, 'sae_top_20_test_accuracy': 0.9309999999999998, 'sae_top_50_test_accuracy': 0.9494, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9367500000000001, 'llm_top_1_test_accuracy': 0.6964999999999999, 'llm_top_2_test_accuracy': 0.7887500000000001, 'llm_top_5_test_accuracy': 0.844, 'llm_top_10_test_accuracy': 0.8702500000000001, 'llm_top_20_test_accuracy': 0.89975, 'llm_top_50_test_accuracy': 0.9247500000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9537500441074371, 'sae_top_1_test_accuracy': 0.743, 'sae_top_2_test_accuracy': 0.8335, 'sae_top_5_test_accuracy': 0.891, 'sae_top_10_test_accuracy': 0.903, 'sae_top_20_test_accuracy': 0.9125000000000001, 'sae_top_50_test_accuracy': 0.92675, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9992000000000001, 'llm_top_1_test_accuracy': 0.9404, 'llm_top_2_test_accuracy': 0.9864, 'llm_top_5_test_accuracy': 0.9927999999999999, 'llm_top_10_test_accuracy': 0.9986, 'llm_top_20_test_accuracy': 0.9996, 'llm_top_50_test_accuracy': 0.999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 1.0, 'sae_top_1_test_accuracy': 0.9236000000000001, 'sae_top_2_test_accuracy': 0.9596, 'sae_top_5_test_accuracy': 0.992, 'sae_top_10_test_accuracy': 0.9974000000000001, 'sae_top_20_test_accuracy': 0.9984000000000002, 'sae_top_50_test_accuracy': 0.999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'bcd583862026537f9d24d335de9957ea3c0032ec', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': '0caeafe4-4d41-4e23-af94-ea10dba73189', 'datetime_epoch_millis': 1738044945555, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'bcd583862026537f9d24d335de9957ea3c0032ec', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}}
Description: Baseline results.
