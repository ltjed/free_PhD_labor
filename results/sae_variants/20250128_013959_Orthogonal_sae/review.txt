{
    "Summary": "The paper presents OrthoSAE, a novel architecture that enforces feature independence in sparse autoencoders (SAEs) using adaptive orthogonality constraints and group-specific biases. The proposed method aims to address the feature entanglement issue in SAEs, thereby improving interpretability and reconstruction quality. Experimental results on the Gemma-2b dataset demonstrate significant improvements in reconstruction error, feature utilization, and interpretability metrics.",
    "Strengths": [
        "The paper addresses a critical issue in SAEs: feature entanglement, which is important for improving interpretability of neural models.",
        "The proposed method, combining adaptive orthogonality constraints and group-specific biases, appears novel and well-motivated.",
        "Extensive experiments and ablation studies are performed, demonstrating significant improvements in key metrics."
    ],
    "Weaknesses": [
        "Certain methodological details are unclear, such as the mechanics of the autoencoder aggregator and the exact implementation of group-specific biases.",
        "The analysis is limited to a single dataset (Gemma-2b), which may affect the generalizability of the findings.",
        "The optimal hyperparameters derived from the experiments may be overfitting to the specific dataset used, and it's unclear how well these would generalize.",
        "The novelty of the approach is somewhat limited, as orthogonality constraints have been explored in prior work.",
        "The paper lacks rigorous theoretical analysis to support the empirical findings.",
        "Increased training time due to orthogonality computations needs to be addressed for practical deployment."
    ],
    "Originality": 3,
    "Quality": 3,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can you provide more detailed explanations of the autoencoder aggregator and group-specific biases?",
        "Have you tested OrthoSAE on any other datasets? If so, please share the results.",
        "How do you ensure that the optimal hyperparameters (\u03b1, orthogonality penalty) are not overfitting to the Gemma-2b dataset?",
        "Can the authors provide a more detailed theoretical justification for the chosen values of \u03b1 and the orthogonality penalty?",
        "Can the authors elaborate on the practical implications of improved KL divergence and feature utilization?",
        "Can the authors include additional ablation studies, such as the effect of different modulation functions and aggregators?",
        "Can the authors provide more qualitative analysis with multiple visualizations?",
        "What is the impact of varying the type of aggregators used in the model?",
        "How does the proposed method compare with more recent advancements in feature disentanglement that are not covered in the paper?"
    ],
    "Limitations": [
        "The clarity of the methodological description needs improvement.",
        "The generalizability of the results is uncertain due to the limited dataset used.",
        "Potential overfitting on hyperparameters needs to be addressed.",
        "The training time increased by 32% compared to standard SAEs due to orthogonality computations.",
        "Feature absorption still occurs for 0.87% of probed concepts.",
        "Performance on sparse probing tasks shows 2.3% degradation for highly correlated features."
    ],
    "Ethical Concerns": false,
    "Soundness": 3,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}