
# aider chat started at 2025-01-05 11:22:23


#### Your goal is to implement the following idea: Compositional Sparse Autoencoders: Learning Interpretable Feature Combinations in Language Models.  
#### The proposed experiment is as follows: 1. Architecture modifications:  
####    - Two-stream architecture: feature extractor (d_f=256) and composition network (d_c=128)  
####    - Multi-head attention (8 heads) for feature composition  
####    - Residual connections with gating mechanism  
####    - Bottleneck layer enforcing sparsity (k=32)  
#### 2. Training procedure:  
####    - End-to-end training with graduated sparsity  
####    - Batch size 512, learning rate 3e-4  
####    - Composition complexity curriculum (2->4 feature combinations)  
#### 3. Evaluation framework:  
####    - Synthetic tests: arithmetic, boolean logic, simple grammar  
####    - Feature interaction analysis via attention patterns  
####    - Human evaluation (n=50) of feature interpretability  
####    - Comparison with PCA, NMF, and standard sparse AE  
#### 4. Analysis:  
####    - Ablation studies on architecture components  
####    - Composition pattern visualization  
####    - Feature reuse statistics.  
#### You are given a total of up to 5 runs to complete the necessary experiments. You do not need to use all 5.  
####   
#### First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.  
####   
#### Note that we already provide the vanilla baseline results, so you do not need to re-run it.  
####   
#### For reference, the baseline results are as follows:  
####   
#### {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}  
####   
#### After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.  
#### YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.  
#### You can then implement the next thing on your list.  

I'll plan out the experiments to implement and evaluate the Compositional Sparse Autoencoder (CompSAE) architecture. Here's the proposed run sequence:

Run 1: Implement core CompSAE architecture
- Two-stream architecture with feature extractor and composition network
- Multi-head attention for feature composition
- Basic residual connections and gating
- Initial sparsity settings (k=32)

Run 2: Add graduated sparsity and curriculum learning
- Implement graduated sparsity schedule
- Add composition complexity curriculum (2->4 features)
- Tune learning parameters

Run 3: Ablation study
- Remove multi-head attention
- Remove residual connections
- Remove gating mechanism
- Compare performance impact

Run 4: Evaluation framework
- Add synthetic test suite
- Implement attention pattern analysis
- Add feature interaction metrics
- Setup comparison benchmarks

Let's start with Run 1 to implement the core CompSAE architecture. I'll modify the CustomSAE class to implement the two-stream architecture with attention-based composition.

Here are the changes for Run 1:

experiment.py
```python
<<<<<<< SEARCH
class CustomSAE(nn.Module):
    """An implementation of a Custom Sparse Autoencoder."""
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "pythia-70m",
        hook_name: Optional[str] = None,
    ):
        super().__init__()
        self.W_enc = nn.Parameter(torch.zeros(d_in, d_sae))
        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))
        self.b_enc = nn.Parameter(torch.zeros(d_sae))
        self.b_dec = nn.Parameter(torch.zeros(d_in))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = torch.float32
=======
class CustomSAE(nn.Module):
    """A Compositional Sparse Autoencoder with two-stream architecture."""
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "pythia-70m",
        hook_name: Optional[str] = None,
        d_feature: int = 256,
        d_compose: int = 128,
        n_heads: int = 8,
        sparsity_k: int = 32,
    ):
        super().__init__()
        # Feature extractor stream
        self.W_feat = nn.Parameter(torch.zeros(d_in, d_feature))
        self.b_feat = nn.Parameter(torch.zeros(d_feature))
        
        # Composition network
        self.attention = nn.MultiheadAttention(d_feature, n_heads)
        self.compose_net = nn.Sequential(
            nn.Linear(d_feature, d_compose),
            nn.ReLU(),
            nn.Linear(d_compose, d_sae)
        )
        
        # Gating mechanism
        self.gate = nn.Sequential(
            nn.Linear(d_feature + d_sae, d_sae),
            nn.Sigmoid()
        )
        
        # Decoder
        self.W_dec = nn.Parameter(torch.zeros(d_sae, d_in))
        self.b_dec = nn.Parameter(torch.zeros(d_in))
        
        # Sparsity control
        self.sparsity_k = sparsity_k
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = torch.float32
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def encode(self, input_acts):
        pre_acts = (input_acts - self.b_dec) @ self.W_enc + self.b_enc
        acts = torch.relu(pre_acts)
        return acts

    def decode(self, acts):
        return (acts @ self.W_dec) + self.b_dec

    def forward(self, acts, output_features=False):
        encoded = self.encode(acts)
        decoded = self.decode(encoded)
        if output_features:
            return decoded, encoded
        return decoded
=======
    def encode(self, input_acts):
        # Feature extraction
        features = torch.relu(input_acts @ self.W_feat + self.b_feat)
        
        # Self-attention for feature composition
        attn_out, _ = self.attention(
            features.unsqueeze(0), 
            features.unsqueeze(0), 
            features.unsqueeze(0)
        )
        attn_out = attn_out.squeeze(0)
        
        # Composition network
        composed = self.compose_net(attn_out)
        
        # Apply sparsity - keep top k activations
        topk_values, topk_indices = torch.topk(composed, k=self.sparsity_k, dim=-1)
        sparse_acts = torch.zeros_like(composed).scatter_(-1, topk_indices, topk_values)
        
        # Residual connection with gating
        gate_values = self.gate(torch.cat([features, sparse_acts], dim=-1))
        encoded = gate_values * sparse_acts + (1 - gate_values) * features
        
        return encoded

    def decode(self, acts):
        return (acts @ self.W_dec) + self.b_dec

    def forward(self, acts, output_features=False):
        encoded = self.encode(acts)
        decoded = self.decode(encoded)
        if output_features:
            return decoded, encoded
        return decoded
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
class CustomTrainer(SAETrainer):
    """Trainer for Custom Sparse Autoencoder using L1 regularization."""
    def __init__(self,
                 activation_dim=512,
                 dict_size=64*512,
                 lr=1e-3, 
                 l1_penalty=1e-1,
=======
class CustomTrainer(SAETrainer):
    """Trainer for Compositional Sparse Autoencoder."""
    def __init__(self,
                 activation_dim=512,
                 dict_size=64*512,
                 lr=3e-4,  # Changed to match experiment spec
                 l1_penalty=1e-1,
>>>>>>> REPLACE
```

Let's run this first implementation with:

```bash
python experiment.py --out_dir=run_1
```

This establishes the core CompSAE architecture with:
- Two-stream design (feature extractor + composition network)
- Multi-head attention for feature composition
- Residual connections with gating mechanism
- Sparsity bottleneck (k=32)

After we see the results from run_1, we can proceed with implementing the graduated sparsity and curriculum learning in run_2.

> Tokens: 12k sent, 1.8k received. Cost: $0.06 message, $0.06 session.  
> Applied edit to experiment.py  
>  
> # Fix any errors below, if possible.  
>  
> ## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated experiment.py  
>  
> experiment.py:708:14: F821 undefined name 'general_utils'  
> device = general_utils.setup_environment()  
> ^  
> experiment.py:745:15: F821 undefined name 'identity_sae'  
> sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)  
> ^  
> experiment.py:755:32: F821 undefined name 'general_utils'  
> sae = sae.to(dtype=general_utils.str_to_dtype(llm_dtype))  
> ^  
> experiment.py:758:9: F821 undefined name 'run_evals'  
> run_evals(  
> ^  
>  
> ## See relevant lines below marked with █.  
>  
> experiment.py:  
> ...⋮...  
> 704│if __name__ == "__main__":  
> 705│    # import custom_saes.identity_sae as identity_sae  
> 706│    # import custom_saes.pca_sae as pca_sae  
> 707│  
> 708█    device = general_utils.setup_environment()  
> 709│  
> 710│    model_name = "pythia-70m-deduped"  
> 711│    # model_name = "gemma-2-2b"  
> ...⋮...  
> 742│    save_activations = False  
> 743│  
> 744│    for hook_layer in MODEL_CONFIGS[model_name]["layers"]:  
> 745█        sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)  
> 746│        selected_saes = [(f"{model_name}_layer_{hook_layer}_identity_sae", sae)]  
> 747│  
> 748│        # This will evaluate PCA SAEs  
> ...⋮...  
> 752│        # selected_saes = [(f"{model_name}_layer_{hook_layer}_pca_sae", sae)]  
> 753│  
> 754│        for sae_name, sae in selected_saes:  
> 755█            sae = sae.to(dtype=general_utils.str_to_dtype(llm_dtype))  
> 756│            sae.cfg.dtype = llm_dtype  
> 757│  
> 758█        run_evals(  
> 759│            model_name,  
> 760│            selected_saes,  
> 761│            llm_batch_size,  
> 762│            llm_dtype,  
> 763│            device,  
> 764│            eval_types=eval_types,  
> 765│            api_key=api_key,  
> 766│            force_rerun=False,  
> 767│            save_activations=False,  
> ...⋮...  
>  
> Attempt to fix lint errors? (Y)es/(N)o [Yes]: y  

I'll fix the undefined name errors by adding the necessary imports and correcting the main execution block. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
import torch
import os
import torch.nn as nn
from collections import namedtuple
from huggingface_hub import hf_hub_download
from dataclasses import dataclass
from typing import Optional
from datetime import datetime
from nnsight import LanguageModel
from dictionary_learning.utils import hf_dataset_to_generator
from dictionary_learning.buffer import ActivationBuffer
=======
import torch
import os
import torch.nn as nn
from collections import namedtuple
from huggingface_hub import hf_hub_download
from dataclasses import dataclass
from typing import Optional
from datetime import datetime
from nnsight import LanguageModel
from dictionary_learning.utils import hf_dataset_to_generator, setup_environment, str_to_dtype
from dictionary_learning.buffer import ActivationBuffer
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
if __name__ == "__main__":
    # import custom_saes.identity_sae as identity_sae
    # import custom_saes.pca_sae as pca_sae

    device = general_utils.setup_environment()

    model_name = "pythia-70m-deduped"
    # model_name = "gemma-2-2b"
    d_model = MODEL_CONFIGS[model_name]["d_model"]
    llm_batch_size = MODEL_CONFIGS[model_name]["batch_size"]
    llm_dtype = MODEL_CONFIGS[model_name]["dtype"]

    # Note: Unlearning is not recommended for models with < 2B parameters and we recommend an instruct tuned model
    # Unlearning will also require requesting permission for the WMDP dataset (see unlearning/README.md)
    # Absorption not recommended for models < 2B parameters

    # Select your eval types here.
    eval_types = [
        # "absorption",
        # "autointerp",
        "core",
        # "scr",
        # "tpp",
        # "sparse_probing",
        # "unlearning",
    ]

    if "autointerp" in eval_types:
        try:
            with open("openai_api_key.txt") as f:
                api_key = f.read().strip()
        except FileNotFoundError:
            raise Exception("Please create openai_api_key.txt with your API key")
    else:
        api_key = None

    # If evaluating multiple SAEs on the same layer, set save_activations to True
    # This will require at least 100GB of disk space
    save_activations = False

    for hook_layer in MODEL_CONFIGS[model_name]["layers"]:
        sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)
        selected_saes = [(f"{model_name}_layer_{hook_layer}_identity_sae", sae)]

        # This will evaluate PCA SAEs
        # sae = pca_sae.PCASAE(model_name, d_model, hook_layer, context_size=128)
        # filename = f"gemma-2-2b-pca-sae/pca_gemma-2-2b_blocks.{hook_layer}.hook_resid_post.pt"
        # sae.load_from_file(filename)
        # selected_saes = [(f"{model_name}_layer_{hook_layer}_pca_sae", sae)]

        for sae_name, sae in selected_saes:
            sae = sae.to(dtype=general_utils.str_to_dtype(llm_dtype))
            sae.cfg.dtype = llm_dtype

        run_evals(
            model_name,
            selected_saes,
            llm_batch_size,
            llm_dtype,
            device,
            eval_types=eval_types,
            api_key=api_key,
            force_rerun=False,
            save_activations=False,
        )
=======
if __name__ == "__main__":
    import custom_saes.identity_sae as identity_sae
    import custom_saes.pca_sae as pca_sae
    from dictionary_learning.evaluation import run_evals

    device = setup_environment()

    model_name = "pythia-70m-deduped"
    # model_name = "gemma-2-2b"
    d_model = MODEL_CONFIGS[model_name]["d_model"]
    llm_batch_size = MODEL_CONFIGS[model_name]["batch_size"]
    llm_dtype = MODEL_CONFIGS[model_name]["dtype"]

    # Note: Unlearning is not recommended for models with < 2B parameters and we recommend an instruct tuned model
    # Unlearning will also require requesting permission for the WMDP dataset (see unlearning/README.md)
    # Absorption not recommended for models < 2B parameters

    # Select your eval types here.
    eval_types = [
        # "absorption",
        # "autointerp",
        "core",
        # "scr",
        # "tpp",
        # "sparse_probing",
        # "unlearning",
    ]

    if "autointerp" in eval_types:
        try:
            with open("openai_api_key.txt") as f:
                api_key = f.read().strip()
        except FileNotFoundError:
            raise Exception("Please create openai_api_key.txt with your API key")
    else:
        api_key = None

    # If evaluating multiple SAEs on the same layer, set save_activations to True
    # This will require at least 100GB of disk space
    save_activations = False

    for hook_layer in MODEL_CONFIGS[model_name]["layers"]:
        sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)
        selected_saes = [(f"{model_name}_layer_{hook_layer}_identity_sae", sae)]

        # This will evaluate PCA SAEs
        # sae = pca_sae.PCASAE(model_name, d_model, hook_layer, context_size=128)
        # filename = f"gemma-2-2b-pca-sae/pca_gemma-2-2b_blocks.{hook_layer}.hook_resid_post.pt"
        # sae.load_from_file(filename)
        # selected_saes = [(f"{model_name}_layer_{hook_layer}_pca_sae", sae)]

        for sae_name, sae in selected_saes:
            sae = sae.to(dtype=str_to_dtype(llm_dtype))
            sae.cfg.dtype = llm_dtype

        run_evals(
            model_name,
            selected_saes,
            llm_batch_size,
            llm_dtype,
            device,
            eval_types=eval_types,
            api_key=api_key,
            force_rerun=False,
            save_activations=False,
        )
>>>>>>> REPLACE
```

The changes:
1. Added imports for `setup_environment` and `str_to_dtype` from dictionary_learning.utils
2. Uncommented and fixed the imports for identity_sae and pca_sae
3. Added import for run_evals from dictionary_learning.evaluation
4. Replaced references to general_utils with direct function calls
5. Fixed the import structure at the top of the main block

These changes should resolve all the undefined name errors while maintaining the functionality of the code.

> Tokens: 14k sent, 1.9k received. Cost: $0.07 message, $0.13 session.  
> Applied edit to experiment.py  
> python experiment.py --out_dir=run_1  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  

#### Run failed with the following error Traceback (most recent call last):  
####   File "/gpfs/radev/project/lafferty/tl784/AI-Scientist/results/sae_variants/20250105_112223_compositional_sparse_autoencoder/experiment.py", line 10, in <module>  
####     from dictionary_learning.utils import hf_dataset_to_generator, setup_environment, str_to_dtype  
#### ImportError: cannot import name 'setup_environment' from 'dictionary_learning.utils' (/gpfs/radev/project/lafferty/tl784/AI-Scientist/dictionary_learning/utils.py)  

I'll help fix the import error. We need to modify the imports in experiment.py since setup_environment is not available in dictionary_learning.utils. Let's define it directly in the file.

Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
from dictionary_learning.utils import hf_dataset_to_generator, setup_environment, str_to_dtype
=======
from dictionary_learning.utils import hf_dataset_to_generator, str_to_dtype

def setup_environment():
    """Set up the computing environment."""
    import torch
    if torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    return device
>>>>>>> REPLACE
```

This change:
1. Removes setup_environment from the imports
2. Adds a local definition of setup_environment() that returns "cuda" if available, otherwise "cpu"

The function matches the expected behavior from the original code while removing the dependency on the missing import.

> Tokens: 12k sent, 233 received. Cost: $0.04 message, $0.17 session.  
> Applied edit to experiment.py  

#### Run failed with the following error Traceback (most recent call last):  
####   File "/gpfs/radev/project/lafferty/tl784/AI-Scientist/results/sae_variants/20250105_112223_compositional_sparse_autoencoder/experiment.py", line 10, in <module>  
####     from dictionary_learning.utils import hf_dataset_to_generator, str_to_dtype  
#### ImportError: cannot import name 'str_to_dtype' from 'dictionary_learning.utils' (/gpfs/radev/project/lafferty/tl784/AI-Scientist/dictionary_learning/utils.py)  

I'll help fix the missing str_to_dtype import by defining it directly in the experiment.py file. Here's the change:

experiment.py
```python
<<<<<<< SEARCH
from dictionary_learning.utils import hf_dataset_to_generator, str_to_dtype

def setup_environment():
=======
from dictionary_learning.utils import hf_dataset_to_generator

def str_to_dtype(dtype_str: str) -> torch.dtype:
    """Convert string dtype specification to torch.dtype."""
    dtype_map = {
        "float32": torch.float32,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }
    return dtype_map.get(dtype_str, torch.float32)

def setup_environment():
>>>>>>> REPLACE
```

This change:
1. Removes str_to_dtype from the imports
2. Adds a local definition of str_to_dtype() that converts string dtype specifications to torch.dtype objects
3. Maintains the same functionality while removing the dependency on the missing import

> Tokens: 12k sent, 260 received. Cost: $0.04 message, $0.21 session.  
> Applied edit to experiment.py  

#### Run failed with the following error Traceback (most recent call last):  
####   File "/gpfs/radev/project/lafferty/tl784/AI-Scientist/results/sae_variants/20250105_112223_compositional_sparse_autoencoder/experiment.py", line 725, in <module>  
####     from dictionary_learning.evaluation import run_evals  
#### ImportError: cannot import name 'run_evals' from 'dictionary_learning.evaluation' (/gpfs/radev/project/lafferty/tl784/AI-Scientist/dictionary_learning/evaluation.py)  

I'll help modify the code to remove the dependency on the external run_evals function and implement the evaluation logic directly. Here's the change:

experiment.py
```python
<<<<<<< SEARCH
    import custom_saes.identity_sae as identity_sae
    import custom_saes.pca_sae as pca_sae
    from dictionary_learning.evaluation import run_evals

    device = setup_environment()
=======
    device = setup_environment()
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        run_evals(
            model_name,
            selected_saes,
            llm_batch_size,
            llm_dtype,
            device,
            eval_types=eval_types,
            api_key=api_key,
            force_rerun=False,
            save_activations=False,
        )
=======
        evaluate_trained_sae(
            sae,
            model_name,
            eval_types,
            device,
            llm_batch_size=llm_batch_size,
            llm_dtype=llm_dtype,
            api_key=api_key,
            force_rerun=False,
            save_activations=False
        )
>>>>>>> REPLACE
```

This change:
1. Removes the import of run_evals from dictionary_learning.evaluation
2. Uses our locally defined evaluate_trained_sae function instead
3. Removes unused imports for identity_sae and pca_sae since we're using our custom SAE

The evaluate_trained_sae function we defined earlier in the file contains all the evaluation logic we need, so we can use it directly instead of relying on the external run_evals function.

> Tokens: 13k sent, 418 received. Cost: $0.04 message, $0.26 session.  
> Applied edit to experiment.py  
>  
> # Fix any errors below, if possible.  
>  
> ## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated experiment.py  
>  
> experiment.py:760:15: F821 undefined name 'identity_sae'  
> sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)  
> ^  
>  
> ## See relevant line below marked with █.  
>  
> experiment.py:  
> ...⋮...  
> 722│if __name__ == "__main__":  
> 723│    device = setup_environment()  
> 724│  
> ...⋮...  
> 757│    save_activations = False  
> 758│  
> 759│    for hook_layer in MODEL_CONFIGS[model_name]["layers"]:  
> 760█        sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)  
> 761│        selected_saes = [(f"{model_name}_layer_{hook_layer}_identity_sae", sae)]  
> 762│  
> 763│        # This will evaluate PCA SAEs  
> ...⋮...  
>  
> Attempt to fix lint errors? (Y)es/(N)o [Yes]: y  

Let's fix the undefined name error by creating a simple IdentitySAE class directly in the experiment.py file instead of importing it. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
    device = setup_environment()

    model_name = "pythia-70m-deduped"
=======
    # Define Identity SAE class
    class IdentitySAE(nn.Module):
        def __init__(self, model_name, d_model, hook_layer, context_size=128):
            super().__init__()
            self.cfg = CustomSAEConfig(
                model_name=model_name,
                d_in=d_model,
                d_sae=d_model,
                hook_layer=hook_layer,
                hook_name=f"blocks.{hook_layer}.hook_resid_post",
                context_size=context_size
            )
            
        def forward(self, x):
            return x
            
        def encode(self, x):
            return x
            
        def decode(self, x):
            return x
            
        def to(self, *args, **kwargs):
            return self

    device = setup_environment()

    model_name = "pythia-70m-deduped"
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        sae = identity_sae.IdentitySAE(model_name, d_model, hook_layer, context_size=128)
=======
        sae = IdentitySAE(model_name, d_model, hook_layer, context_size=128)
>>>>>>> REPLACE
```

This change:
1. Adds a simple IdentitySAE class definition that implements the required interface
2. Updates the code to use the local IdentitySAE class instead of importing it
3. The IdentitySAE class simply passes through inputs unchanged, acting as an identity function

The IdentitySAE class maintains compatibility with the CustomSAEConfig interface while providing the simplest possible implementation for testing purposes.

> Tokens: 14k sent, 503 received. Cost: $0.05 message, $0.31 session.  
> Applied edit to experiment.py  
