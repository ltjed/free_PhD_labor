\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achour et~al.(2021)Achour, Malgouyres, and
  Mamalet]{Achour2021ExistenceSA}
E.~M. Achour, Franccois Malgouyres, and Franck Mamalet.
\newblock Existence, stability and scalability of orthogonal convolutional
  neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 23:\penalty0 347:1--347:56, 2021.

\bibitem[Braun et~al.(2024)Braun, Taylor, Goldowsky-Dill, and
  Sharkey]{Braun2024IdentifyingFI}
Dan Braun, Jordan~K. Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey.
\newblock Identifying functionally important features with end-to-end sparse
  dictionary learning.
\newblock \emph{ArXiv}, abs/2405.12241, 2024.

\bibitem[Ferrando \& Voita(2024)Ferrando and Voita]{Ferrando2024InformationFR}
Javier Ferrando and Elena Voita.
\newblock Information flow routes: Automatically interpreting language models
  at scale.
\newblock \emph{ArXiv}, abs/2403.00824, 2024.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li et~al.(2024)Li, Feng, and Zhou]{Li2024ConvergenceAF}
Jianfei Li, Han Feng, and Ding-Xuan Zhou.
\newblock Convergence analysis for deep sparse coding via convolutional neural
  networks.
\newblock \emph{ArXiv}, abs/2408.05540, 2024.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineLF}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009.

\bibitem[Nguyen et~al.(2016)Nguyen, Yosinski, and
  Clune]{Nguyen2016MultifacetedFV}
Anh~Totti Nguyen, J.~Yosinski, and J.~Clune.
\newblock Multifaceted feature visualization: Uncovering the different types of
  features learned by each neuron in deep neural networks.
\newblock \emph{ArXiv}, abs/1602.03616, 2016.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Syed et~al.(2023)Syed, Rager, and Conmy]{Syed2023AttributionPO}
Aaquib Syed, Can Rager, and Arthur Conmy.
\newblock Attribution patching outperforms automated circuit discovery.
\newblock \emph{ArXiv}, abs/2310.10348, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zeiler \& Fergus(2013)Zeiler and Fergus]{Zeiler2013VisualizingAU}
Matthew~D. Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock \emph{ArXiv}, abs/1311.2901, 2013.

\end{thebibliography}
