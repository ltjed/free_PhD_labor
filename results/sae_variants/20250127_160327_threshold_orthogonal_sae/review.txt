{
    "Summary": "The paper proposes a threshold-based orthogonality approach to improve feature disentanglement in sparse autoencoders (SAEs) for large language models, specifically focusing on transformer architectures. The method introduces dynamic threshold adjustment, importance-weighted learning rates, and correlation-aware orthogonality constraints. Experiments on the Gemma-2B model demonstrate improvements in feature sparsity, reconstruction quality, semantic concept recognition, and feature specialization.",
    "Strengths": [
        "The paper addresses a significant challenge in the interpretability of large language models by improving feature disentanglement.",
        "The proposed methodology is innovative, combining dynamic threshold adjustment, importance-weighted learning rates, and correlation-aware orthogonality constraints.",
        "Experimental results on the Gemma-2B model show promising improvements in several key metrics, including feature sparsity and semantic concept recognition."
    ],
    "Weaknesses": [
        "The clarity of the paper is compromised by the dense and complex presentation of the methodology. Key concepts such as the autoencoder aggregator and the specifics of the dynamic threshold adjustment could be explained more clearly.",
        "The evaluation is limited to a single model (Gemma-2B) and a specific layer (layer 19), raising concerns about the generalizability of the results.",
        "The paper lacks sufficient ablation studies to isolate the impact of each proposed mechanism. For instance, it would be beneficial to see the effects of excluding dynamic threshold adjustment or the orthogonality constraints.",
        "The method introduces significant computational overhead, particularly with the gradient-based importance scoring, which is not thoroughly addressed in the context of large-scale models."
    ],
    "Originality": 3,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can you provide more details on the autoencoder aggregator and its role in the proposed method?",
        "How does the method perform on other models or layers? Is the observed improvement specific to the Gemma-2B model and layer 19?",
        "Could you conduct additional ablation studies to better understand the contribution of each component of the proposed method?",
        "How do you plan to address the computational overhead introduced by gradient-based importance scoring in large-scale models?"
    ],
    "Limitations": [
        "The paper does not adequately address the potential computational overhead associated with the proposed method, particularly in the context of large-scale models.",
        "The generalizability of the results is questionable as the experiments are limited to a single model and specific layer."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 3,
    "Confidence": 4,
    "Decision": "Reject"
}