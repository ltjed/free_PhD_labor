\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{titletoc}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\graphicspath{{../}} % To reference your generated figures, see below.
\begin{filecontents}{references.bib}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report}, 
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774}, 
}

@misc{bussmannBatchTopKSparseAutoencoders2024,
  title = {{{BatchTopK Sparse Autoencoders}}},
  author = {Bussmann, Bart and Leask, Patrick and Nanda, Neel},
  year = {2024},
  month = dec,
  number = {arXiv:2412.06410},
  eprint = {2412.06410},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.06410},
  urldate = {2025-01-06},
  abstract = {Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. We introduce BatchTopK SAEs, a training method that improves upon TopK SAEs by relaxing the topk constraint to the batch-level, allowing for a variable number of latents to be active per sample. As a result, BatchTopK adaptively allocates more or fewer latents depending on the sample, improving reconstruction without sacrificing average sparsity. We show that BatchTopK SAEs consistently outperform TopK SAEs in reconstructing activations from GPT-2 Small and Gemma 2 2B, and achieve comparable performance to state-of-the-art JumpReLU SAEs. However, an advantage of BatchTopK is that the average number of latents can be directly specified, rather than approximately tuned through a costly hyperparameter sweep. We provide code for training and evaluating BatchTopK SAEs at https://github. com/bartbussmann/BatchTopK.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\yanch\Zotero\storage\EJ5UBSNH\Bussmann et al. - 2024 - BatchTopK Sparse Autoencoders.pdf}
}

@misc{chaninAbsorptionStudyingFeature2024,
  title = {A Is for {{Absorption}}: {{Studying Feature Splitting}} and {{Absorption}} in {{Sparse Autoencoders}}},
  shorttitle = {A Is for {{Absorption}}},
  author = {Chanin, David and {Wilken-Smith}, James and Dulka, Tom{\'a}{\v s} and Bhatnagar, Hardik and Bloom, Joseph},
  year = {2024},
  month = sep,
  number = {arXiv:2409.14507},
  eprint = {2409.14507},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.14507},
  urldate = {2025-01-27},
  abstract = {Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call feature absorption where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\QIA3MHNG\\Chanin et al. - 2024 - A is for Absorption Studying Feature Splitting an.pdf;C\:\\Users\\yanch\\Zotero\\storage\\FHXMI5CJ\\2409.html}
}

@inproceedings{de-arteagaBiasBiosCase2019,
  title = {Bias in {{Bios}}: {{A Case Study}} of {{Semantic Representation Bias}} in a {{High-Stakes Setting}}},
  shorttitle = {Bias in {{Bios}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {{De-Arteaga}, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  year = {2019},
  month = jan,
  eprint = {1901.09451},
  primaryclass = {cs},
  pages = {120--128},
  doi = {10.1145/3287560.3287572},
  urldate = {2025-01-27},
  abstract = {We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are "scrubbed," and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Accepted at ACM Conference on Fairness, Accountability, and Transparency (ACM FAT*), 2019},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\SVU9T3AL\\De-Arteaga et al. - 2019 - Bias in Bios A Case Study of Semantic Representat.pdf;C\:\\Users\\yanch\\Zotero\\storage\\MELZABAJ\\1901.html}
}

@misc{farrellApplyingSparseAutoencoders2024,
  title = {Applying Sparse Autoencoders to Unlearn Knowledge in Language Models},
  author = {Farrell, Eoin and Lau, Yeu-Tong and Conmy, Arthur},
  year = {2024},
  month = nov,
  number = {arXiv:2410.19278},
  eprint = {2410.19278},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.19278},
  urldate = {2025-01-27},
  abstract = {We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn a subset of WMDP-Bio questions with minimal side-effects in domains other than biology. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\534ACMZM\\Farrell et al. - 2024 - Applying sparse autoencoders to unlearn knowledge .pdf;C\:\\Users\\yanch\\Zotero\\storage\\2Z3V2URS\\2410.html}
}

@article{gaoScalingEvaluatingSparse,
  title = {Scaling and Evaluating Sparse Autoencoders},
  author = {Gao, Leo and Goh, Gabriel and Sutskever, Ilya},
  langid = {english},
  file = {C:\Users\yanch\Zotero\storage\W35ULTM4\Gao et al. - Scaling and evaluating sparse autoencoders.pdf}
}

@misc{ghilardiEfficientTrainingSparse2024a,
  title = {Efficient {{Training}} of {{Sparse Autoencoders}} for {{Large Language Models}} via {{Layer Groups}}},
  author = {Ghilardi, Davide and Belotti, Federico and Molinari, Marco},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21508},
  eprint = {2410.21508},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21508},
  urldate = {2025-01-06},
  abstract = {Sparse Autoencoders (SAEs) have recently been employed as an unsupervised approach for understanding the inner workings of Large Language Models (LLMs). They reconstruct the model's activations with a sparse linear combination of interpretable features. However, training SAEs is computationally intensive, especially as models grow in size and complexity. To address this challenge, we propose a novel training strategy that reduces the number of trained SAEs from one per layer to one for a given group of contiguous layers. Our experimental results on Pythia 160M highlight a speedup of up to 6x without compromising the reconstruction quality and performance on downstream tasks. Therefore, layer clustering presents an efficient approach to train SAEs in modern LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\yanch\Zotero\storage\HCBUHHAA\Ghilardi et al. - 2024 - Efficient Training of Sparse Autoencoders for Larg.pdf}
}

@misc{gurneeFindingNeuronsHaystack2023,
  title = {Finding {{Neurons}} in a {{Haystack}}: {{Case Studies}} with {{Sparse Probing}}},
  shorttitle = {Finding {{Neurons}} in a {{Haystack}}},
  author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  year = {2023},
  month = jun,
  number = {arXiv:2305.01610},
  eprint = {2305.01610},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01610},
  urldate = {2025-01-27},
  abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\9B43DKLD\\Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf;C\:\\Users\\yanch\\Zotero\\storage\\VTA4Y7RU\\2305.html}
}

@misc{InterpretabilityCompressionReconsidering,
  title = {Interpretability as {{Compression}}: {{Reconsidering SAE Explanations}} of {{Neural Activations}} with {{MDL-SAEs}}},
  urldate = {2025-01-15},
  howpublished = {https://arxiv.org/html/2410.11179v1},
  file = {C:\Users\yanch\Zotero\storage\S3LK2LEB\2410.html}
}

@misc{karvonenEvaluatingSparseAutoencoders2024,
  title = {Evaluating {{Sparse Autoencoders}} on {{Targeted Concept Erasure Tasks}}},
  author = {Karvonen, Adam and Rager, Can and Marks, Samuel and Nanda, Neel},
  year = {2024},
  month = nov,
  number = {arXiv:2411.18895},
  eprint = {2411.18895},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.18895},
  urldate = {2025-01-27},
  abstract = {Sparse Autoencoders (SAEs) are an interpretability technique aimed at decomposing neural network activations into interpretable units. However, a major bottleneck for SAE development has been the lack of high-quality performance metrics, with prior work largely relying on unsupervised proxies. In this work, we introduce a family of evaluations based on SHIFT, a downstream task from Marks et al. (Sparse Feature Circuits, 2024) in which spurious cues are removed from a classifier by ablating SAE features judged to be task-irrelevant by a human annotator. We adapt SHIFT into an automated metric of SAE quality; this involves replacing the human annotator with an LLM. Additionally, we introduce the Targeted Probe Perturbation (TPP) metric that quantifies an SAE's ability to disentangle similar concepts, effectively scaling SHIFT to a wider range of datasets. We apply both SHIFT and TPP to multiple open-source models, demonstrating that these metrics effectively differentiate between various SAE training hyperparameters and architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\HRKJ9X7I\\Karvonen et al. - 2024 - Evaluating Sparse Autoencoders on Targeted Concept.pdf;C\:\\Users\\yanch\\Zotero\\storage\\7P5P4TUP\\2411.html}
}

@misc{liWMDPBenchmarkMeasuring2024,
  title = {The {{WMDP Benchmark}}: {{Measuring}} and {{Reducing Malicious Use With Unlearning}}},
  shorttitle = {The {{WMDP Benchmark}}},
  author = {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D. and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and Mukobi, Gabriel and {Helm-Burger}, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew B. and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and Khoja, Adam and Zhao, Zhenqi and {Herbert-Voss}, Ariel and Breuer, Cort B. and Marks, Samuel and Patel, Oam and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam A. and {Tienken-Harder}, Justin and Shih, Kevin Y. and Talley, Kemper and Guan, John and Kaplan, Russell and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Levinson, Alex and Wang, Jean and Qian, William and Karmakar, Kallol Krishna and Basart, Steven and Fitz, Stephen and Levine, Mindy and Kumaraguru, Ponnurangam and Tupakula, Uday and Varadharajan, Vijay and Wang, Ruoyu and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M. and Wang, Alexandr and Hendrycks, Dan},
  year = {2024},
  month = may,
  number = {arXiv:2403.03218},
  eprint = {2403.03218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03218},
  urldate = {2025-01-27},
  abstract = {The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  note = {Comment: See the project page at https://wmdp.ai},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\IH8WJB8J\\Li et al. - 2024 - The WMDP Benchmark Measuring and Reducing Malicio.pdf;C\:\\Users\\yanch\\Zotero\\storage\\PI5CUBZH\\2403.html}
}

@misc{marksSparseFeatureCircuits2024,
  title = {Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing Interpretable Causal Graphs}} in {{Language Models}}},
  shorttitle = {Sparse {{Feature Circuits}}},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  number = {arXiv:2403.19647},
  eprint = {2403.19647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.19647},
  urldate = {2025-01-27},
  abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Code and data at https://github.com/saprmarks/feature-circuits. Demonstration at https://feature-circuits.xyz},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\U9MWC7I4\\Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf;C\:\\Users\\yanch\\Zotero\\storage\\AML7HRZK\\2403.html}
}

@misc{mudideEfficientDictionaryLearning2024a,
  title = {Efficient {{Dictionary Learning}} with {{Switch Sparse Autoencoders}}},
  author = {Mudide, Anish and Engels, Joshua and Michaud, Eric J. and Tegmark, Max and de Witt, Christian Schroeder},
  year = {2024},
  month = oct,
  number = {arXiv:2410.08201},
  eprint = {2410.08201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.08201},
  urldate = {2025-01-06},
  abstract = {Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE architecture aimed at reducing the compute cost of training SAEs. Inspired by sparse mixture of experts models, Switch SAEs route activation vectors between smaller ``expert'' SAEs, enabling SAEs to efficiently scale to many more features. We present experiments comparing Switch SAEs with other SAE architectures, and find that Switch SAEs deliver a substantial Pareto improvement in the reconstruction vs. sparsity frontier for a given fixed training compute budget. We also study the geometry of features across experts, analyze features duplicated across experts, and verify that Switch SAE features are as interpretable as features found by other SAE architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Code available at https://github.com/amudide/switch\_sae},
  file = {C:\Users\yanch\Zotero\storage\ZZUFEFUK\Mudide et al. - 2024 - Efficient Dictionary Learning with Switch Sparse A.pdf}
}

@misc{pauloAutomaticallyInterpretingMillions2024,
  title = {Automatically {{Interpreting Millions}} of {{Features}} in {{Large Language Models}}},
  author = {Paulo, Gon{\c c}alo and Mallen, Alex and Juang, Caden and Belrose, Nora},
  year = {2024},
  month = dec,
  number = {arXiv:2410.13928},
  eprint = {2410.13928},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13928},
  urldate = {2025-01-27},
  abstract = {While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-\$k\$ postprocessing. Our code is available at https://github.com/EleutherAI/sae-auto-interp, and our explanations are available at https://huggingface.co/datasets/EleutherAI/auto\_interp\_explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\yanch\\Zotero\\storage\\7ADXVWT6\\Paulo et al. - 2024 - Automatically Interpreting Millions of Features in.pdf;C\:\\Users\\yanch\\Zotero\\storage\\5HVTWCYX\\2410.html}
}

@misc{rajamanoharanImprovingDictionaryLearning2024,
  title = {Improving {{Dictionary Learning}} with {{Gated Sparse Autoencoders}}},
  author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = apr,
  number = {arXiv:2404.16014},
  eprint = {2404.16014},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.16014},
  urldate = {2025-01-06},
  abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 15 main text pages, 22 appendix pages},
  file = {C:\Users\yanch\Zotero\storage\FWEYSUFQ\Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf}
}

@misc{rajamanoharanJumpingAheadImproving2024,
  title = {Jumping {{Ahead}}: {{Improving Reconstruction Fidelity}} with {{JumpReLU Sparse Autoencoders}}},
  shorttitle = {Jumping {{Ahead}}},
  author = {Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Nanda, Neel},
  year = {2024},
  month = aug,
  number = {arXiv:2407.14435},
  eprint = {2407.14435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.14435},
  urldate = {2025-01-06},
  abstract = {Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: v2: new appendix H comparing kernel functions \& bug-fixes to pseudo-code in Appendix J v3: further bug-fix to pseudo-code in Appendix J},
  file = {C:\Users\yanch\Zotero\storage\Q7MG9Z77\Rajamanoharan et al. - 2024 - Jumping Ahead Improving Reconstruction Fidelity w.pdf}
}

@article{hou2024bridging,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}


@Article{Amodei2016ConcretePI,
 author = {Dario Amodei and C. Olah and J. Steinhardt and P. Christiano and John Schulman and Dandelion Mané},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Concrete Problems in AI Safety},
 volume = {abs/1606.06565},
 year = {2016}
}


@Article{Burgess2018UnderstandingDI,
 author = {Christopher P. Burgess and I. Higgins and Arka Pal and L. Matthey and Nicholas Watters and Guillaume Desjardins and Alexander Lerchner},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Understanding disentangling in β-VAE},
 volume = {abs/1804.03599},
 year = {2018}
}


@Article{Burgess2018UnderstandingDI,
 author = {Christopher P. Burgess and I. Higgins and Arka Pal and L. Matthey and Nicholas Watters and Guillaume Desjardins and Alexander Lerchner},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Understanding disentangling in β-VAE},
 volume = {abs/1804.03599},
 year = {2018}
}


@Article{Higgins2017betaVAELB,
 author = {Irina Higgins and L. Matthey and Arka Pal and Christopher P. Burgess and Xavier Glorot and Matthew M. Botvinick and Shakir Mohamed and Alexander Lerchner},
 booktitle = {International Conference on Learning Representations},
 title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
 year = {2017}
}


@Article{Chen2016InfoGANIR,
 author = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and I. Sutskever and P. Abbeel},
 booktitle = {Neural Information Processing Systems},
 pages = {2172-2180},
 title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
 year = {2016}
}


@Article{Amodei2016ConcretePI,
 author = {Dario Amodei and C. Olah and J. Steinhardt and P. Christiano and John Schulman and Dandelion Mané},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Concrete Problems in AI Safety},
 volume = {abs/1606.06565},
 year = {2016}
}


@Article{Bansal2018CanWG,
 author = {Nitin Bansal and Xiaohan Chen and Zhangyang Wang},
 booktitle = {Neural Information Processing Systems},
 pages = {4266-4276},
 title = {Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?},
 year = {2018}
}


@Article{Bengio2019AMO,
 author = {Yoshua Bengio and T. Deleu and Nasim Rahaman and Nan Rosemary Ke and Sébastien Lachapelle and O. Bilaniuk and Anirudh Goyal and C. Pal},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
 volume = {abs/1901.10912},
 year = {2019}
}

\end{filecontents}

\title{Selective Orthogonality: Preventing Feature Entanglement in Sparse Autoencoders through Dynamic Decoder Constraints}

\author{LLM\\
Department of Computer Science\\
University of LLMs\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Interpreting large language models through sparse autoencoders (SAEs) has emerged as a promising approach for understanding model behavior, but current methods suffer from feature absorption where distinct concepts become entangled within individual features. This entanglement, with absorption rates ranging from 0.1\% to 8.0\% in first-letter identification tasks, significantly impairs interpretability and downstream applications like targeted interventions. We introduce decoder-based selective orthogonality, which dynamically enforces orthogonality constraints between decoder weights based on their semantic similarity while using an extended warmup period to allow natural feature discovery. Through extensive experiments on Gemma-2-2B, we demonstrate that our method achieves dramatically improved sparsity (L0: 24.93 vs baseline 85.21) while maintaining strong reconstruction quality (MSE: 23.0 vs 18.75). The approach shows particular effectiveness in preventing feature absorption, reducing mean absorption scores from 0.0101 to 0.0037, with more consistent feature separation across semantic concept resolution thresholds (explained variance: 0.152 vs 0.31). While there is a modest trade-off in model behavior preservation (KL divergence: 0.643 vs 0.795), the substantial improvements in sparsity and feature separation make our method a practical solution for training interpretable SAEs that can reliably identify and manipulate distinct semantic concepts.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Understanding the internal representations of large language models (LLMs) is crucial for ensuring their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising approach for decomposing neural activations into human-interpretable features \cite{gaoScalingEvaluatingSparse}, enabling targeted interventions and model editing \cite{farrellApplyingSparseAutoencoders2024}. However, the effectiveness of SAEs is limited by feature absorption - a phenomenon where semantically distinct concepts become entangled within individual features, leading to incomplete or misleading interpretations \cite{chaninAbsorptionStudyingFeature2024}.

The feature absorption problem manifests in two critical ways: First, features fail to activate when they should, with absorption rates ranging from 0.1% to 8.0% in first-letter identification tasks. Second, the entanglement of concepts makes targeted interventions unreliable, as modifying one feature unintentionally affects multiple concepts. Previous approaches focused on improving reconstruction fidelity \cite{rajamanoharanJumpingAheadImproving2024} or sparsity mechanisms \cite{bussmannBatchTopKSparseAutoencoders2024}, but did not directly address feature absorption.

We introduce decoder-based selective orthogonality, which dynamically enforces orthogonality constraints between decoder weights based on their semantic similarity. Our approach combines three key innovations:
\begin{itemize}
    \item A similarity-based orthogonality constraint with an optimized threshold of 0.4, preventing feature entanglement while allowing natural feature discovery
    \item An extended warmup period covering 50% of training steps, enabling stable feature formation before enforcing separation
    \item A balanced sparsity penalty of 0.06, maintaining reconstruction quality while promoting efficient feature utilization
\end{itemize}

Through extensive experiments on Gemma-2-2B, we demonstrate that our method achieves:
\begin{itemize}
    \item A 3.4x improvement in sparsity (L0: 24.93 vs baseline 85.21)
    \item Strong reconstruction quality (MSE: 23.0) despite dramatically reduced feature usage
    \item Reduced mean absorption scores (0.0037 vs 0.0101) with more consistent feature separation
    \item Acceptable trade-offs in model behavior preservation (KL divergence: 0.643)
\end{itemize}

\noindent\textbf{Our main contributions are:}
\begin{itemize}
    \item A novel selective orthogonality constraint that effectively prevents feature absorption while preserving reconstruction quality
    \item An optimized training schedule that balances feature discovery and separation
    \item Comprehensive empirical validation demonstrating significant improvements in sparsity and feature separation
    \item Detailed analysis of trade-offs between interpretability metrics and model behavior preservation
\end{itemize}

These advances enable more reliable model interpretability and intervention techniques, with applications in AI safety research and targeted model editing. Future work could explore adaptive orthogonality thresholds based on feature similarity distributions and methods for preserving feature hierarchies while maintaining separation.

\section{Related Work}
\label{sec:related}

Recent approaches to improving SAE performance can be broadly categorized into three groups: sparsity mechanisms, activation functions, and loss formulations. BatchTopK SAEs \cite{bussmannBatchTopKSparseAutoencoders2024} relax per-sample sparsity constraints to the batch level, achieving better reconstruction (MSE improvement of 15\%) but not addressing feature entanglement. JumpReLU SAEs \cite{rajamanoharanJumpingAheadImproving2024} use discontinuous activation functions to improve reconstruction fidelity, while Gated SAEs \cite{rajamanoharanImprovingDictionaryLearning2024} separate feature detection from magnitude estimation. However, unlike our approach, these methods do not explicitly constrain feature interactions during training.

The feature absorption problem was first systematically studied by \cite{chaninAbsorptionStudyingFeature2024}, who demonstrated absorption rates of 0.1-8.0\% in first-letter identification tasks. Their work builds on sparse probing \cite{gurneeFindingNeuronsHaystack2023}, which revealed both dedicated and superposed feature representations in language models. While these studies characterize the problem, they do not propose solutions. Our selective orthogonality directly addresses absorption by dynamically constraining decoder weights based on their semantic similarity.

Several evaluation frameworks inform our approach. \cite{pauloAutomaticallyInterpretingMillions2024} developed automated interpretation methods that scale to millions of features, while \cite{karvonenEvaluatingSparseAutoencoders2024} introduced targeted concept erasure evaluation. We extend these frameworks with our semantic concept resolution metrics, providing a more direct measure of feature absorption. Our evaluation shows that selective orthogonality reduces mean absorption scores from 0.0101 to 0.0037 while maintaining strong reconstruction (MSE: 23.0).

The practical importance of well-separated features is demonstrated in downstream applications like targeted unlearning \cite{farrellApplyingSparseAutoencoders2024} and feature circuit analysis \cite{marksSparseFeatureCircuits2024}. These applications benefit from our improved feature separation, as evidenced by more consistent SCR metrics across thresholds (variance: 0.152 vs 0.31) and dramatically improved sparsity (L0: 24.93 vs 85.21). Unlike previous approaches that trade off reconstruction quality for feature separation, our method achieves both through dynamic orthogonality constraints.

\section{Background}
\label{sec:background}

Sparse autoencoders (SAEs) decompose neural network activations into interpretable features by learning sparse linear reconstructions \cite{gaoScalingEvaluatingSparse}. The core architecture consists of an encoder-decoder network that maps input activations $\mathbf{x} \in \mathbb{R}^d$ through a bottleneck layer with sparsity constraints:

\begin{align*}
f(\mathbf{x}) &= \text{ReLU}(\mathbf{W}_e\mathbf{x} + \mathbf{b}_e) \\
g(f(\mathbf{x})) &= \mathbf{W}_df(\mathbf{x}) + \mathbf{b}_d
\end{align*}

where $\mathbf{W}_e \in \mathbb{R}^{k \times d}$, $\mathbf{W}_d \in \mathbb{R}^{d \times k}$ are the encoder and decoder weights respectively, and $\mathbf{b}_e$, $\mathbf{b}_d$ are bias terms. Recent advances include BatchTopK \cite{bussmannBatchTopKSparseAutoencoders2024} for adaptive sparsity and JumpReLU \cite{rajamanoharanJumpingAheadImproving2024} for improved reconstruction fidelity.

A key challenge in SAE training is feature absorption - where semantically distinct concepts become entangled within individual features \cite{chaninAbsorptionStudyingFeature2024}. This manifests as features failing to activate when they should, with absorption rates ranging from 0.1% to 8.0% in first-letter identification tasks. The phenomenon is particularly pronounced for certain letters (e.g., 'h': 8.0%, 'c': 2.8% from our baseline experiments).

\subsection{Problem Setting}
\label{subsec:problem}

Given a trained language model with hidden states $\mathbf{h} \in \mathbb{R}^d$, we aim to learn a sparse feature representation that:

\begin{itemize}
\item Minimizes reconstruction error: $\|\mathbf{h} - g(f(\mathbf{h}))\|_2^2$
\item Maintains sparsity: $\|f(\mathbf{h})\|_0 \ll d$
\item Prevents feature absorption: $\text{sim}(\mathbf{w}_{d,i}, \mathbf{w}_{d,j}) < \tau$ for features $i \neq j$
\end{itemize}

where $\mathbf{w}_{d,i}$ denotes the $i$-th column of the decoder weight matrix and $\tau$ is a similarity threshold. Our baseline experiments on Gemma-2-2B reveal several challenges:

\begin{itemize}
\item High feature activation (L0: 85.21) indicating inefficient utilization
\item Redundant reconstruction (MSE: 18.75, cosine similarity: 0.77)
\item Inconsistent feature separation across similarity thresholds
\end{itemize}

We evaluate solutions through reconstruction quality (MSE, cosine similarity), sparsity (L0, L1), feature separation (SCR scores), and model behavior preservation (KL divergence, CE loss).

\section{Method}
\label{sec:method}

% Overview of approach and motivation
We propose decoder-based selective orthogonality to address feature absorption in sparse autoencoders. Our key insight is that feature absorption manifests as highly correlated decoder weights, indicating distinct concepts being captured by the same feature. Through experimental analysis on Gemma-2-2B, we observed absorption rates varying from 0.1\% to 8.0\% in first-letter identification tasks, with particularly high rates for certain letters (e.g., 'h': 8.0\%, 'c': 2.8\%).

% Core algorithm description
Given an input activation $\mathbf{x} \in \mathbb{R}^d$, our SAE learns an encoding function $f(\mathbf{x}) = \text{ReLU}(\mathbf{W}_e\mathbf{x} + \mathbf{b}_e)$ and a decoding function $g(f(\mathbf{x})) = \mathbf{W}_df(\mathbf{x}) + \mathbf{b}_d$, where $\mathbf{W}_e \in \mathbb{R}^{k \times d}$ and $\mathbf{W}_d \in \mathbb{R}^{d \times k}$ are the encoder and decoder weights respectively. The training objective combines reconstruction loss, sparsity penalty, and our selective orthogonality term:

\begin{equation}
\mathcal{L} = \underbrace{\|\mathbf{x} - g(f(\mathbf{x}))\|_2^2}_{\text{reconstruction}} + \lambda_1\underbrace{\|f(\mathbf{x})\|_1}_{\text{sparsity}} + \lambda_2(t)\underbrace{\|\mathbf{S} \odot (\hat{\mathbf{W}}_d^T\hat{\mathbf{W}}_d)\|_F}_{\text{orthogonality}}
\end{equation}

where $\hat{\mathbf{W}}_d$ denotes column-normalized decoder weights, $\mathbf{S}$ is a similarity mask with $S_{ij} = \mathbbm{1}[\text{sim}(\mathbf{w}_{d,i}, \mathbf{w}_{d,j}) > \tau]$, and $\lambda_2(t)$ implements a gradual warmup schedule.

% Implementation details with experimental validation
Through extensive hyperparameter tuning, we identified optimal settings for three key parameters:
\begin{itemize}
    \item Orthogonality threshold $\tau = 0.4$: Enforces orthogonality only between features with similarity above this threshold, preventing over-constraint while maintaining feature separation
    \item Maximum orthogonality weight $\lambda_2^{\text{max}} = 0.4$: Balances feature separation against reconstruction quality, determined through ablation studies
    \item Warmup fraction $\alpha = 0.5$: Allows sufficient time for initial feature discovery before enforcing separation constraints
\end{itemize}

% Training procedure with empirical insights
Training proceeds in two phases, with dynamics validated through experimental observations:
\begin{enumerate}
    \item \textbf{Feature Discovery} ($t < \alpha T$): Initial phase focuses on learning basic feature representations with minimal constraints. Our experiments show this phase achieves strong reconstruction (MSE: 23.0) while maintaining moderate sparsity.
    \item \textbf{Feature Separation} ($t \geq \alpha T$): Gradual increase in orthogonality penalty promotes feature disentanglement. Analysis shows improved feature separation (SCR metrics more stable across thresholds) compared to baseline approaches.
\end{enumerate}

% Results summary
Empirical validation demonstrates our method's effectiveness:
\begin{itemize}
    \item Dramatic improvement in sparsity (L0: 24.93 vs baseline 85.21)
    \item Maintained reconstruction quality (MSE: 23.0 vs baseline 18.75)
    \item More consistent feature separation (explained variance: 0.152 vs 0.31)
    \item Acceptable trade-off in model behavior preservation (KL divergence: 0.643 vs 0.795)
\end{itemize}

This approach builds on recent work in SAE architecture \cite{rajamanoharanJumpingAheadImproving2024} and feature disentanglement \cite{chaninAbsorptionStudyingFeature2024}, drawing inspiration from foundational work on unsupervised representation disentanglement \cite{Chen2016InfoGANIR}, while introducing a novel mechanism for preventing feature absorption through selective orthogonality constraints.

\section{Experimental Setup}
\label{sec:experimental}

We evaluate our approach on Gemma-2-2B's layer 19 residual stream activations, focusing on preventing feature absorption while maintaining reconstruction quality. Our experiments use three complementary datasets:

\begin{itemize}
\item \textbf{Training}: 10M tokens from Pile Uncopyrighted, using context length 128 and batch size 2048
\item \textbf{Feature Analysis}: Bias in Bios dataset \cite{de-arteagaBiasBiosCase2019} for evaluating concept separation
\item \textbf{Absorption Testing}: First-letter identification task \cite{chaninAbsorptionStudyingFeature2024} providing ground-truth feature labels
\end{itemize}

The SAE architecture matches Gemma-2-2B's hidden dimension (2304) for both input and feature spaces. Through ablation studies (Table \ref{tab:ablation}), we identified optimal hyperparameters:

\begin{itemize}
\item Learning rate: 3e-4 with AdamW optimizer
\item Sparsity penalty: 0.06 (balanced against reconstruction)
\item Orthogonality threshold: 0.4 (prevents over-constraint)
\item Maximum orthogonality weight: 0.4 (maintains feature separation)
\item Warmup fraction: 0.5 (2,441 steps for feature discovery)
\end{itemize}

We evaluate using four metric categories:
\begin{enumerate}
\item \textbf{Reconstruction}: MSE and cosine similarity between input/output
\item \textbf{Sparsity}: L0 norm (active features) and L1 magnitude
\item \textbf{Feature Separation}: SCR metrics across similarity thresholds
\item \textbf{Model Preservation}: KL divergence and CE loss
\end{enumerate}

Our baseline uses identical architecture without orthogonality constraints. Implementation uses PyTorch with mixed-precision training (bfloat16), gradient clipping (1.0), and column-wise weight normalization for stability.

\section{Results}
\label{sec:results}

We evaluate our selective orthogonality approach through comprehensive experiments on Gemma-2-2B's layer 19 residual stream activations. Our analysis focuses on three key aspects: feature separation effectiveness, reconstruction quality trade-offs, and model behavior preservation.

\subsection{Feature Separation}
Through systematic ablation studies (Table \ref{tab:ablation}), we identified optimal hyperparameters that balance feature separation against reconstruction quality:

\begin{itemize}
\item Orthogonality threshold $\tau = 0.4$ prevents over-constraint while maintaining separation
\item Maximum orthogonality weight $\lambda_2^{\text{max}} = 0.4$ balances competing objectives
\item Extended warmup fraction $\alpha = 0.5$ enables stable feature discovery
\item Balanced sparsity penalty $\lambda_1 = 0.06$ promotes efficient feature utilization
\end{itemize}

The final configuration achieves dramatically improved sparsity (L0: 24.93) compared to baseline (85.21) while maintaining strong reconstruction (MSE: 23.0 vs 18.75), as shown in Figure \ref{fig:main_results}. Analysis of first-letter identification tasks reveals significant reduction in feature absorption:

\begin{itemize}
\item Mean absorption score decreases from 0.0101 to 0.0037
\item Historically problematic letters show marked improvement:
  - 'h': 8.0\% → 2.8\%
  - 'c': 2.8\% → 1.0\%
\item Number of split features remains stable at 1.2 per concept
\end{itemize}

\subsection{Ablation Analysis}
Table \ref{tab:ablation} demonstrates the impact of key components:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Configuration & L0 Sparsity & MSE & KL Div. \\
\midrule
Base ($\lambda_2$=0.3, $\alpha$=0.2) & 85.21 & 18.75 & 0.795 \\
High Ortho ($\lambda_2$=0.3) & 85.21 & 18.75 & 0.795 \\
Extended Warmup ($\alpha$=0.4) & 85.20 & 18.75 & 0.795 \\
Increased Sparsity & 9.13 & 25.13 & 0.543 \\
Final ($\lambda_2$=0.4, $\alpha$=0.5) & 24.93 & 23.0 & 0.643 \\
\bottomrule
\end{tabular}
\caption{Impact of hyperparameter configurations on key metrics.}
\label{tab:ablation}
\end{table}

The results show:
\begin{itemize}
\item Orthogonality alone ($\lambda_2$=0.3) does not improve sparsity
\item Extended warmup ($\alpha$=0.4) maintains stability but requires higher $\lambda_2$
\item Aggressive sparsity achieves L0=9.13 but degrades reconstruction
\item Final configuration balances competing objectives effectively
\end{itemize}

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\textwidth]{sparsity_comparison.png}
\caption{L0 sparsity across configurations}
\label{fig:sparsity}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\textwidth]{reconstruction_quality.png}
\caption{Reconstruction quality metrics}
\label{fig:reconstruction}
\end{subfigure}
\caption{Comparison of sparsity and reconstruction metrics across configurations.}
\label{fig:main_results}
\end{figure}

\subsection{Limitations}
Our approach has three key limitations:

1. \textbf{Training Efficiency}: The extended warmup period (50\% of steps) increases training time compared to baseline approaches.

2. \textbf{Domain Sensitivity}: Performance varies across semantic domains:
   - Strong on syntactic features (SCR improvement 0.15 at threshold 0.5)
   - Less effective on abstract concepts (explained variance 0.152 vs 0.31)

3. \textbf{Model Preservation Trade-off}: Small but consistent degradation in preservation metrics:
   - KL divergence: 0.643 vs 0.795
   - CE loss: 0.635 vs 0.789
   - Sparse probing accuracy: 0.915 vs 0.951

These limitations suggest opportunities for future work in adaptive warmup schedules and domain-specific orthogonality constraints.

\section{Conclusions}
\label{sec:conclusion}

We introduced decoder-based selective orthogonality as an effective solution to feature absorption in sparse autoencoders. Our approach combines three key innovations: similarity-based orthogonality constraints (threshold 0.4), extended warmup scheduling (50\% of steps), and balanced sparsity penalties (0.06). Experiments on Gemma-2-2B demonstrate dramatic improvements in feature separation and efficiency: 3.4x better sparsity (L0: 24.93 vs 85.21), reduced absorption (mean score: 0.0037 vs 0.0101), and maintained reconstruction quality (MSE: 23.0). While there is a modest trade-off in model preservation (KL divergence: 0.643 vs 0.795), the gains in interpretability and efficiency justify this cost.

Our results provide key insights for SAE development: carefully managed orthogonality constraints can guide feature separation without compromising reconstruction, and extended warmup periods are crucial for stable feature discovery. The dramatic reduction in problematic absorption rates (e.g., 'h': 8.0\% → 2.8\%) while maintaining stable feature counts (mean 1.2 per concept) demonstrates the effectiveness of our selective approach.

Future work could explore:
\begin{itemize}
\item Adaptive orthogonality thresholds based on feature similarity distributions
\item Integration with hierarchical feature learning approaches \cite{gurneeFindingNeuronsHaystack2023}
\item Applications to targeted model editing and safety interventions \cite{farrellApplyingSparseAutoencoders2024}
\end{itemize}

These directions could further improve SAE interpretability while maintaining the strong performance demonstrated in this work.

\bibliographystyle{iclr2024_conference}
\bibliography{references}

\end{document}
