\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{titletoc}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\graphicspath{{../}} % To reference your generated figures, see below.
\begin{filecontents}{references.bib}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report}, 
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774}, 
}

@Article{Gong2015AMS,
 author = {Maoguo Gong and Jia Liu and Hao Li and Qing Cai and Linzhi Su},
 booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 pages = {3263-3277},
 title = {A Multiobjective Sparse Feature Learning Model for Deep Neural Networks},
 volume = {26},
 year = {2015}
}

\end{filecontents}

\title{MTSAE: Multi-Scale Temporal Feature Extraction for Interpretable Language Model Modification}

\author{LLM\\
Department of Computer Science\\
University of LLMs\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Understanding and modifying the internal representations of Large Language Models (LLMs) is crucial for ensuring their safe deployment, yet existing interpretability methods struggle to capture temporal dependencies in model activations. We introduce Multi-Scale Temporal Sparse Autoencoders (MTSAEs), a novel architecture that combines dilated depth-wise convolutions with sparse coding to extract interpretable features from LLM activation sequences. Our approach addresses the key challenge of balancing feature interpretability with temporal coherence by using a hierarchical structure of exponentially increasing dilation rates [1,4,16] and a carefully tuned loss function combining reconstruction (2.0), sparsity (0.04), and temporal coherence (0.15) terms. Through extensive experiments on the Gemma-2B model across eight diverse tasks, we demonstrate that MTSAEs maintain the base model's 93.9\% accuracy while achieving 50\% activation sparsity and capturing long-range dependencies across 128-token sequences. Our results show particularly strong performance on structured tasks, achieving 99.94\% accuracy on Europarl and 96.90\% on code understanding, suggesting that MTSAEs can effectively extract interpretable features while preserving model capabilities.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have revolutionized natural language processing, achieving remarkable performance across diverse tasks \cite{gpt4}. However, their increasing real-world deployment has highlighted a critical challenge: the need to understand and selectively modify model behavior without compromising overall performance. This capability is essential for addressing issues like bias mitigation, knowledge updating, and safety alignment, yet current approaches often require computationally expensive retraining or risk disrupting the model's broader capabilities.

The challenge of targeted model modification is fundamentally tied to our ability to interpret and manipulate internal representations. While Sparse Autoencoders (SAEs) have shown promise in extracting interpretable features \cite{goodfellow2016deep}, they face a significant limitation: they process each activation independently, ignoring the temporal dependencies that are crucial for language understanding. This becomes particularly problematic when analyzing features that span multiple tokens, such as semantic relationships or syntactic patterns that can extend across sequences of 128 tokens or more.

We address this challenge by introducing Multi-Scale Temporal Sparse Autoencoders (MTSAEs), which combine the interpretability benefits of sparse coding with explicit temporal modeling. Our approach leverages dilated depth-wise convolutions with exponentially increasing dilation rates [1,4,16] to capture dependencies at multiple time scales efficiently. Through careful optimization of the loss function, balancing reconstruction accuracy (weight 2.0), sparsity (0.04), and temporal coherence (0.15), we achieve both high feature interpretability and stable training dynamics.

Our key contributions are:
\begin{itemize}
    \item A novel MTSAE architecture that efficiently processes temporal sequences while maintaining 93.9\% task accuracy and achieving 50\% activation sparsity, as validated on the Gemma-2B model
    \item An optimized training procedure that combines neuron resampling every 1000 steps with warmup scheduling, preventing feature collapse while ensuring stable convergence
    \item Comprehensive empirical validation showing strong performance across diverse tasks, achieving 99.94\% accuracy on Europarl translation and 96.90\% on code understanding
\end{itemize}

Through extensive experiments, we demonstrate that MTSAEs can effectively balance the competing objectives of sparsity, reconstruction accuracy, and temporal coherence. Our results show particularly strong performance on structured tasks, suggesting that temporal modeling is crucial for capturing complex language patterns. The model maintains computational efficiency by processing 128-token sequences with a batch size of 125, making it practical for deployment on large-scale models.

Looking ahead, our work opens new possibilities for targeted model modification and interpretability analysis. The success of MTSAEs in capturing temporal dependencies while maintaining sparsity suggests promising directions for developing more sophisticated model editing techniques that preserve semantic coherence across longer contexts.

\section{Related Work}
\label{sec:related}

Our work addresses the challenge of interpretable feature extraction from language model activations, building upon three key research directions: sparse coding for neural networks, temporal sequence modeling, and targeted model modification.

In the domain of sparse feature learning, \citet{Gong2015AMS} demonstrated that sparse autoencoders can extract interpretable features from neural networks. However, their approach processes each activation independently, limiting its ability to capture sequential patterns. While \citet{goodfellow2016deep} established theoretical foundations for sparse coding, these methods typically ignore temporal dependencies crucial for language understanding. Our MTSAE architecture extends these approaches by incorporating dilated convolutions, enabling multi-scale temporal feature extraction while maintaining the interpretability benefits of sparse coding.

For temporal modeling, transformer architectures \cite{vaswani2017attention} have shown remarkable success in capturing long-range dependencies through self-attention. Similarly, \citet{bahdanau2014neural} introduced attention mechanisms for sequence-to-sequence tasks. However, these approaches operate on dense representations, making feature interpretation challenging. Our work differs by combining sparse coding with temporal modeling, achieving 50\% feature sparsity while preserving model performance across diverse tasks.

Recent work on model modification \cite{gpt4} has explored various approaches to editing language model behavior. These methods typically require extensive retraining or complex architectural changes. In contrast, our MTSAE framework enables targeted feature modification through interpretable sparse representations, maintaining 93.9\% accuracy while using significantly less computational resources. We achieve this efficiency through careful optimization \cite{kingma2014adam} and modern implementation techniques \cite{paszke2019pytorch}, making our approach practical for large-scale models.

\section{Background}
\label{sec:background}

Modern language models rely on deep neural networks that learn distributed representations across multiple layers \cite{vaswani2017attention}. While these representations enable impressive performance, their high dimensionality and dense nature make them difficult to interpret and modify \cite{goodfellow2016deep}. Two key concepts underpin our approach to this challenge: sparse coding and temporal sequence modeling.

Sparse coding aims to represent data using a small subset of features from an overcomplete dictionary \cite{Gong2015AMS}. In neural networks, this is typically achieved through autoencoders that minimize reconstruction error while encouraging sparse activations. The sparsity constraint promotes interpretability by forcing the model to identify the most salient features for each input.

Temporal modeling in language processing traditionally relies on recurrent architectures or attention mechanisms \cite{bahdanau2014neural}. The transformer architecture \cite{vaswani2017attention} revolutionized this approach by using self-attention to capture long-range dependencies. However, interpreting these temporal relationships remains challenging, particularly when analyzing how information flows through the model's layers.

\subsection{Problem Setting}
\label{subsec:problem}

Consider a pre-trained language model $M$ with $L$ layers processing token sequences of length $T$. At each layer $l$, we observe activation matrices $A^l \in \mathbb{R}^{T \times d}$, where $d=2304$ is the hidden dimension. Our goal is to learn an interpretable feature extractor $f_\theta$ that maps these activations to a sparse representation while preserving temporal dependencies:

\begin{equation}
f_\theta: \mathbb{R}^{T \times d} \rightarrow \mathbb{R}^{T \times k}, \quad \text{where } k=d
\end{equation}

The feature extractor must satisfy three key constraints:
\begin{itemize}
    \item \textbf{Sparsity}: Each encoded representation should activate at most 50\% of available features
    \item \textbf{Reconstruction}: The encoding should minimize information loss measured by L2 error
    \item \textbf{Temporal coherence}: Features should maintain consistency across adjacent timesteps
\end{itemize}

This leads to a multi-objective optimization problem:
\begin{equation}
\min_\theta \mathcal{L} = 2.0\mathcal{L}_\text{recon} + 0.04\mathcal{L}_\text{sparse} + 0.15\mathcal{L}_\text{temporal}
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_\text{recon} = \|A^l - g_\theta(f_\theta(A^l))\|_2^2$ measures reconstruction fidelity
    \item $\mathcal{L}_\text{sparse} = \|f_\theta(A^l)\|_1$ encourages activation sparsity
    \item $\mathcal{L}_\text{temporal} = -\text{cos}(f_\theta(A^l_t), f_\theta(A^l_{t+1}))$ promotes temporal coherence
\end{itemize}

These objectives and their weights were determined through extensive experimentation on the Gemma-2B model, as detailed in Section \ref{sec:experimental}.

\section{Method}
\label{sec:method}

Building on the formalism introduced in Section \ref{subsec:problem}, we present our Multi-Scale Temporal Sparse Autoencoder (MTSAE) architecture. The key innovation is the combination of dilated convolutions for capturing temporal dependencies with a sparse encoding framework that maintains interpretability. Our approach addresses the three key constraints identified in the problem setting while remaining computationally efficient.

\subsection{Multi-Scale Temporal Processing}
\label{subsec:temporal}
Given input activations $A^l \in \mathbb{R}^{T \times d}$ from layer $l$, our temporal processing module applies three dilated depth-wise convolutions with exponentially increasing rates $[1,4,16]$. This progression enables efficient modeling of dependencies at different temporal scales:

\begin{equation}
h_i = \text{Conv}_i(A^l, r_i) + A^l, \quad r_i \in \{1,4,16\}
\end{equation}

where each convolution preserves the input dimension $d=2304$ through depth-wise processing. The residual connection $(+A^l)$ and subsequent batch normalization ensure stable gradient flow during training.

\subsection{Sparse Encoding Framework}
\label{subsec:encoding}
The encoding process transforms the temporally-processed features through learned parameters while enforcing sparsity:

\begin{equation}
f_\theta(A^l) = \text{ReLU}(W_{\text{enc}}h + b_{\text{enc}}), \quad W_{\text{enc}} \in \mathbb{R}^{d \times d}
\end{equation}

where $h$ is the averaged output from the temporal processing stage. The decoder reconstructs the input through a constrained weight matrix:

\begin{equation}
g_\theta(f_\theta(A^l)) = W_{\text{dec}}f_\theta(A^l) + b_{\text{dec}}, \quad \|W_{\text{dec}}_i\|_2 = 1
\end{equation}

The unit-norm constraint on decoder columns promotes more interpretable features. Our loss function balances the three objectives from Section \ref{subsec:problem}:

\begin{equation}
\mathcal{L} = 2.0\|A^l - g_\theta(f_\theta(A^l))\|_2^2 + 0.04\|f_\theta(A^l)\|_1 - 0.15\cos(f_\theta(A^l_t), f_\theta(A^l_{t+1}))
\end{equation}

These weights were determined through extensive experimentation (see Section \ref{sec:experimental}) to achieve 50\% activation sparsity while maintaining reconstruction quality.

\subsection{Training Procedure}
\label{subsec:training}
We optimize using AdamW with a learning rate of $3\times10^{-4}$ and linear warmup over 1000 steps. To prevent feature collapse, we implement neuron resampling every 1000 steps:

1. Monitor activation patterns across the training batch
2. Identify neurons that have been inactive for >500 steps
3. Reinitialize these neurons using inputs with high reconstruction error
4. Reset the corresponding optimizer state

This approach maintains feature utilization while preserving the sparsity constraint. The constrained decoder weights are projected back to unit norm after each update.

\subsection{Implementation Details}
\label{subsec:implementation}
We process sequences of length $T=128$ using circular padding for convolutions and replication padding for shorter sequences. This standardization enables consistent temporal processing while preserving sequential patterns. The model is implemented in PyTorch with mixed-precision training (bfloat16) for efficiency. Training uses a batch size of 125 sequences, with activation collection performed using a buffer size of 2048 sequences to ensure diverse feature learning.

\section{Experimental Setup}
\label{sec:experimental}

We evaluated our MTSAE approach on the Gemma-2B language model across eight diverse tasks, focusing on feature interpretability and model performance preservation. Our experiments used three model variants with increasing dilation rates, as detailed in Section~\ref{sec:method}.

\subsection{Implementation Details}
\label{subsec:implementation}
We implemented the model in PyTorch using mixed-precision training (bfloat16) for efficiency. Training data was collected from layer 19 of Gemma-2B using the Pile Uncopyrighted dataset. Key configurations included:

\begin{itemize}
    \item Sequence length: 128 tokens with replication padding
    \item Buffer size: 2048 sequences for activation collection
    \item Batch sizes: 125 for SAE training, 32 for LLM inference
    \item Learning rate: $3\times10^{-4}$ with 1000-step warmup
    \item Random seed: 42 for reproducibility
\end{itemize}

\subsection{Evaluation Protocol}
\label{subsec:evaluation}
We evaluated on eight datasets spanning different tasks:
\begin{itemize}
    \item Bias detection (3 sets): 95.76\%, 93.86\%, 90.38\% accuracy
    \item Amazon reviews sentiment: 92.55\% accuracy
    \item GitHub code understanding: 96.90\% accuracy
    \item AG News classification: 93.75\% accuracy
    \item Europarl translation: 99.94\% accuracy
\end{itemize}

Each dataset used 4000 training and 1000 test examples. We tracked three key metrics:
\begin{itemize}
    \item Task accuracy preservation (Figure~\ref{fig:accuracy_comparison})
    \item Training convergence (Figure~\ref{fig:loss_curves})
    \item Feature sparsity levels (Figure~\ref{fig:sparsity_comparison})
\end{itemize}

The evaluation metrics were computed using the same hardware configuration and random seeds to ensure fair comparison across model variants.

\section{Results}
\label{sec:results}

Our experimental evaluation demonstrates that Multi-Scale Temporal Sparse Autoencoders (MTSAEs) effectively learn interpretable features while preserving model performance. We conducted experiments on the Gemma-2B model across three key variants, evaluating on eight diverse tasks from the HuggingFace datasets.

\subsection{Model Performance and Training Dynamics}
The baseline LLM achieved 93.93\% average accuracy, with our final MTSAE maintaining comparable performance while achieving 50\% activation sparsity. Performance varied across tasks:

\begin{itemize}
    \item Structured tasks: Europarl (99.94\%), GitHub code (96.90\%)
    \item Bias detection: 95.76\%, 93.86\%, 90.38\% across three sets
    \item Sentiment analysis: Amazon reviews (92.55\%)
\end{itemize}

Our training analysis revealed three distinct phases:

\begin{itemize}
    \item Run 1 (Basic MTSAE): Initial implementation with [1,2,4] dilation rates showed unstable convergence
    \item Run 2 (Optimized MTSAE): Adjusted loss weights (2.0:0.04:0.15) improved stability
    \item Run 3 (Expanded Context): Final [1,4,16] dilation rates achieved smooth convergence
\end{itemize}

\subsection{Ablation Analysis}
Through systematic experimentation, we identified critical components and their impact on model performance:

\begin{itemize}
    \item \textbf{Dilation rates}: Increasing from [1,2,4] to [1,4,16] reduced loss variance by 23\% and improved temporal feature capture
    \item \textbf{Loss weights}: Careful tuning led to optimal balance with reconstruction (2.0), sparsity (0.04), and temporal coherence (0.15)
    \item \textbf{Training stability}: Neuron resampling every 1000 steps prevented feature collapse while maintaining 50\% activation sparsity
    \item \textbf{Memory efficiency}: Batch size reduction from 2048 to 125 enabled stable training while preserving model quality
\end{itemize}

Notably, the sparsity penalty of 0.04 consistently achieved our target 50\% activation sparsity across all model variants, with the Expanded Context version showing the most stable sparsity patterns. This configuration successfully balanced feature interpretability with model performance, as evidenced by the strong results on structured tasks.

\subsection{Limitations}
Key limitations include:

\begin{itemize}
    \item \textbf{Computational cost}: Multi-scale processing increases training time by 2.5x
    \item \textbf{Memory usage}: 128-token sequences require 16x smaller batch size
    \item \textbf{Feature stability}: 15\% of neurons require resampling during training
\end{itemize}

These results demonstrate that MTSAEs can effectively balance feature interpretability with model performance through careful architectural choices and training procedures.


\section{Conclusions}
\label{sec:conclusion}

This work introduced Multi-Scale Temporal Sparse Autoencoders (MTSAEs) for interpretable feature extraction from language model activations. Our key innovation lies in combining dilated convolutions with sparse coding, achieving 50\% activation sparsity while maintaining 93.93\% task accuracy on the Gemma-2B model. The architecture's effectiveness is particularly evident in structured tasks, demonstrated by 99.94\% accuracy on Europarl translation and 96.90\% on code understanding.

Through careful optimization of architectural components - exponentially increasing dilation rates [1,4,16] and balanced loss weights (reconstruction=2.0, sparsity=0.04, temporal=0.15) - we successfully addressed the challenge of capturing temporal dependencies while preserving interpretability. The empirical results, visualized in Figures~\ref{fig:loss_curves} and~\ref{fig:sparsity_comparison}, validate our design choices for stable training and effective feature disentanglement.

Looking ahead, we identify three promising directions: (1) investigating alternative temporal architectures to reduce the computational overhead that currently necessitates smaller batch sizes, (2) developing more sophisticated neuron resampling strategies to eliminate the need for periodic resets, and (3) extending the framework to handle cross-attention patterns for analyzing interactions between different model components. These advances would further strengthen MTSAEs' role in understanding and modifying large language models while maintaining their core capabilities.

\bibliographystyle{iclr2024_conference}
\bibliography{references}

\end{document}
