\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bergstra et~al.(2011)Bergstra, Courville, and
  Bengio]{Bergstra2011TheSI}
J.~Bergstra, Aaron~C. Courville, and Yoshua Bengio.
\newblock The statistical inefficiency of sparse coding for images (or, one
  gabor to rule them all).
\newblock \emph{ArXiv}, abs/1109.6638, 2011.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{Burgess2018UnderstandingDI}
Christopher~P. Burgess, I.~Higgins, Arka Pal, L.~Matthey, Nicholas Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in Î²-vae.
\newblock \emph{ArXiv}, abs/1804.03599, 2018.

\bibitem[DeRose et~al.(2020)DeRose, Wang, and Berger]{DeRose2020AttentionFA}
Joseph~F DeRose, Jiayao Wang, and M.~Berger.
\newblock Attention flows: Analyzing and comparing attention mechanisms in
  language models.
\newblock \emph{IEEE Transactions on Visualization and Computer Graphics},
  27:\penalty0 1160--1170, 2020.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{Gao2020ThePA}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv}, abs/2101.00027, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kissane et~al.(2024)Kissane, Krzyzanowski, Bloom, Conmy, and
  Nanda]{Kissane2024InterpretingAL}
Connor Kissane, Robert Krzyzanowski, J.~Bloom, Arthur Conmy, and Neel Nanda.
\newblock Interpreting attention layer outputs with sparse autoencoders.
\newblock \emph{ArXiv}, abs/2406.17759, 2024.

\bibitem[Lee(2010)]{Lee2010UnsupervisedFL}
Honglak Lee.
\newblock Unsupervised feature learning via sparse hierarchical
  representations.
\newblock 2010.

\bibitem[Ngiam et~al.(2011)Ngiam, Koh, Chen, Bhaskar, and Ng]{Ngiam2011SparseF}
Jiquan Ngiam, Pang~Wei Koh, Zhenghao Chen, Sonia~A. Bhaskar, and Andrew~Y. Ng.
\newblock Sparse filtering.
\newblock pp.\  1125--1133, 2011.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{Tishby2000TheIB}
Naftali Tishby, Fernando~C Pereira, and W.~Bialek.
\newblock The information bottleneck method.
\newblock \emph{ArXiv}, physics/0004057, 2000.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol]{Vincent2010StackedDA}
Pascal Vincent, H.~Larochelle, Isabelle Lajoie, Yoshua Bengio, and
  Pierre-Antoine Manzagol.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 3371--3408, 2010.

\end{thebibliography}
