{
    "Summary": "The paper proposes AdaptiveCompress, a method for dynamic feature selection in large language model compression using exponential moving average (EMA) importance tracking and a gradual compression schedule. The method aims to balance compression effectiveness with the preservation of model performance. Experimental results on the Gemma-2-2B model show improvements in task performance and significant compression, but also reveal fundamental challenges in the compression mechanism.",
    "Strengths": [
        "Addresses a significant and relevant problem in the field of language model compression.",
        "The use of exponential moving averages for importance tracking is a novel approach.",
        "Comprehensive experimental setup with diverse tasks for evaluation."
    ],
    "Weaknesses": [
        "Significant issues with compression mechanisms, including complete signal blocking and high reconstruction error.",
        "Lack of detailed explanation of key components, such as the autoencoder aggregator and compression function.",
        "Insufficient theoretical justification for the proposed methods.",
        "The paper lacks clarity in some sections and does not provide enough ablation studies.",
        "The method shows sensitivity to hyperparameters, and the paper does not provide sufficient analysis or guidelines for selecting these hyperparameters."
    ],
    "Originality": 2,
    "Quality": 2,
    "Clarity": 2,
    "Significance": 2,
    "Questions": [
        "Can you provide more details on the autoencoder aggregator and its role in the method?",
        "How does the proposed method compare with other state-of-the-art dynamic compression methods in terms of reconstruction error and signal blocking?",
        "Can you provide more ablation studies to understand the impact of different components of the method?",
        "Can the authors provide a more rigorous theoretical justification for the use of EMA in importance tracking and its impact on model compression?",
        "How do the authors plan to address the issue of complete signal blocking and high reconstruction error in future work?",
        "Can the authors clarify the implementation details of the autoencoder aggregator and the compression/decompression functions?",
        "How does the method ensure the interpretability of the compressed model? Can the authors provide specific examples or case studies to illustrate this?",
        "What guidelines can the authors provide for selecting the hyperparameters, particularly the learning rate, sparsity penalty, and compression schedule?"
    ],
    "Limitations": [
        "The paper acknowledges several critical limitations, such as signal blocking, high reconstruction error, and sensitivity to hyperparameters. However, the proposed solutions to these issues are not convincing or adequately detailed.",
        "The results highlight the need for more robust and stable compression mechanisms, which are not sufficiently addressed in the current work."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 2,
    "Overall": 3,
    "Confidence": 4,
    "Decision": "Reject"
}