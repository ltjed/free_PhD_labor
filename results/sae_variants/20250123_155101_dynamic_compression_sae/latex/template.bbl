\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{Guo2016DynamicNS}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock \emph{ArXiv}, abs/1608.04493, 2016.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{Han2015DeepCC}
Song Han, Huizi Mao, and W.~Dally.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv: Computer Vision and Pattern Recognition},
  2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{Han2015LearningBW}
Song Han, Jeff Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock pp.\  1135--1143, 2015{\natexlab{b}}.

\bibitem[Kundu et~al.(2020)Kundu, Nazemi, Beerel, and Pedram]{Kundu2020DNRAT}
Souvik Kundu, M.~Nazemi, P.~Beerel, and M.~Pedram.
\newblock Dnr: A tunable robust pruning framework through dynamic network
  rewiring of dnns.
\newblock \emph{2021 26th Asia and South Pacific Design Automation Conference
  (ASP-DAC)}, pp.\  344--350, 2020.

\bibitem[Li et~al.(2020)Li, Kong, Zhang, Li, Li, Liu, and
  Ding]{Li2020EfficientTL}
Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji~Li, Z.~Li, Hang Liu, and Caiwen
  Ding.
\newblock Efficient transformer-based large scale language representations
  using hardware-friendly block structured pruning.
\newblock pp.\  3187--3199, 2020.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{Michel2019AreSH}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock \emph{ArXiv}, abs/1905.10650, 2019.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Sarti et~al.(2023)Sarti, Feldhus, Sickert, van~der Wal, Nissim, and
  Bisazza]{Sarti2023InseqAI}
Gabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van~der Wal, M.~Nissim, and
  Arianna Bisazza.
\newblock Inseq: An interpretability toolkit for sequence generation models.
\newblock \emph{ArXiv}, abs/2302.13942, 2023.

\bibitem[Tang et~al.(2021)Tang, Wang, Xu, Deng, Xu, Tao, and
  Xu]{Tang2021ManifoldRD}
Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, D.~Tao, and Chang Xu.
\newblock Manifold regularized dynamic network pruning.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  5016--5026, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{Voita2019AnalyzingMS}
Elena Voita, David Talbot, F.~Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{ArXiv}, abs/1905.09418, 2019.

\end{thebibliography}
