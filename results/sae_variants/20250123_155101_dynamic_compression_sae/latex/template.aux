\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gpt4}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{Han2015LearningBW}
\citation{Li2020EfficientTL}
\citation{Michel2019AreSH,Voita2019AnalyzingMS}
\citation{Sarti2023InseqAI}
\citation{Kundu2020DNRAT,Guo2016DynamicNS}
\citation{Tang2021ManifoldRD}
\citation{Li2020EfficientTL}
\citation{Michel2019AreSH}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\citation{Han2015DeepCC}
\citation{Michel2019AreSH}
\citation{Guo2016DynamicNS}
\citation{Han2015LearningBW}
\citation{Voita2019AnalyzingMS}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{3}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{3}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Setting}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}{}{}{}}
\citation{paszke2019pytorch}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][4][]4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Compression metrics across different architectural variants: (A) Explained variance showing information preservation, (B) Reconstruction MSE indicating compression quality, (C) L0 sparsity measuring feature utilization, and (D) KL divergence tracking distribution shifts. The Additive Mix configuration (rightmost) achieves the best balance of metrics.}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:compression_metrics}{{1}{5}{Compression metrics across different architectural variants: (A) Explained variance showing information preservation, (B) Reconstruction MSE indicating compression quality, (C) L0 sparsity measuring feature utilization, and (D) KL divergence tracking distribution shifts. The Additive Mix configuration (rightmost) achieves the best balance of metrics}{figure.caption.1}{}}
\newlabel{fig:compression_metrics@cref}{{[figure][1][]1}{[1][4][]5}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{5}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Compression Quality}{5}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Task Performance}{6}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Ablation Studies}{6}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Limitations}{6}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{6}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{6}{Conclusions and Future Work}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][6][]6}{}{}{}}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{Guo2016DynamicNS}{{1}{2016}{{Guo et~al.}}{{Guo, Yao, and Chen}}}
\bibcite{Han2015DeepCC}{{2}{2015{a}}{{Han et~al.}}{{Han, Mao, and Dally}}}
\bibcite{Han2015LearningBW}{{3}{2015{b}}{{Han et~al.}}{{Han, Pool, Tran, and Dally}}}
\bibcite{Kundu2020DNRAT}{{4}{2020}{{Kundu et~al.}}{{Kundu, Nazemi, Beerel, and Pedram}}}
\bibcite{Li2020EfficientTL}{{5}{2020}{{Li et~al.}}{{Li, Kong, Zhang, Li, Li, Liu, and Ding}}}
\bibcite{loshchilov2017adamw}{{6}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training progression showing loss values across different experimental configurations. The curves highlight the effectiveness of the warmup period (12,000 steps) and the stability challenges when compression ratio drops below 95\%.}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:training_curves}{{2}{7}{Training progression showing loss values across different experimental configurations. The curves highlight the effectiveness of the warmup period (12,000 steps) and the stability challenges when compression ratio drops below 95\%}{figure.caption.2}{}}
\newlabel{fig:training_curves@cref}{{[figure][2][]2}{[1][6][]7}{}{}{}}
\bibcite{Michel2019AreSH}{{7}{2019}{{Michel et~al.}}{{Michel, Levy, and Neubig}}}
\bibcite{gpt4}{{8}{2024}{{OpenAI}}{{}}}
\bibcite{paszke2019pytorch}{{9}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.}}}
\bibcite{Sarti2023InseqAI}{{10}{2023}{{Sarti et~al.}}{{Sarti, Feldhus, Sickert, van~der Wal, Nissim, and Bisazza}}}
\bibcite{Tang2021ManifoldRD}{{11}{2021}{{Tang et~al.}}{{Tang, Wang, Xu, Deng, Xu, Tao, and Xu}}}
\bibcite{vaswani2017attention}{{12}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{Voita2019AnalyzingMS}{{13}{2019}{{Voita et~al.}}{{Voita, Talbot, Moiseev, Sennrich, and Titov}}}
\ttl@finishall
\gdef \@abspage@last{8}
