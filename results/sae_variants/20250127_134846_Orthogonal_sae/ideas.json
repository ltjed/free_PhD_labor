[
    {
        "Name": "Orthogonal_sae",
        "Title": "Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders",
        "Experiment": "1. Implement adaptive orthogonality loss with controlled feature sharing\n2. Add batch-wise feature grouping with periodic updates\n3. Train on Pythia-70M using WMDP-bio and WikiText datasets\n4. Compare unlearning performance against baseline and fixed orthogonal SAE\n5. Analyze condition numbers of feature subspaces\n6. Evaluate impact of different \u03b1 values for controlled sharing",
        "Technical_Details": "The method uses an adaptive objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_ortho where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_ortho = ||W_1^T W_2 - \u03b1I||_F allows controlled feature sharing through parameter \u03b1. Feature groups are updated every n=100 steps using efficient batch statistics. Implementation includes early stopping based on condition number thresholds. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality.",
        "Research_Impact": "A key challenge in selective unlearning is finding the optimal balance between knowledge separation and model performance. Current approaches either sacrifice reconstruction quality for separation or vice versa. This research addresses the challenge through adaptive training and controlled feature sharing, achieving better separation while maintaining performance. The theoretical framework provides precise control over the separation-performance trade-off.",
        "Implementation_Plan": "1. Add AdaptiveOrthogonalityLoss with scheduling\n2. Implement efficient batch-wise feature grouping\n3. Modify CustomSAE to include group-specific biases\n4. Add condition number calculation utilities\n5. Update CustomTrainer with adaptive loss weight\n6. Add evaluation metrics for controlled sharing",
        "Interestingness_Evaluation": "The combination of theoretical insights about feature sharing with practical adaptive training creates a more nuanced and effective approach to knowledge separation.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient with batch-wise updates and simple matrix operations; adaptive weighting adds minimal overhead; controlled sharing through \u03b1 provides easy tuning; all computations well within 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive approach with controlled feature sharing provides a novel theoretical framework for knowledge separation that bridges the gap between strict orthogonality and practical performance.",
        "Novelty": 9,
        "Overall_Score": 9.0,
        "novel": true
    }
]