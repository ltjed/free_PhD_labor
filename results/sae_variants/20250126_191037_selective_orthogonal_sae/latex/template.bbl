\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bengio(2007)]{Bengio2007LearningDA}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock \emph{Found. Trends Mach. Learn.}, 2:\penalty0 1--127, 2007.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{Burgess2018UnderstandingDI}
Christopher~P. Burgess, I.~Higgins, Arka Pal, L.~Matthey, Nicholas Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in β-vae.
\newblock \emph{ArXiv}, abs/1804.03599, 2018.

\bibitem[Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and
  Garriga-Alonso]{Conmy2023TowardsAC}
Arthur Conmy, Augustine~N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and
  Adrià Garriga-Alonso.
\newblock Towards automated circuit discovery for mechanistic interpretability.
\newblock \emph{ArXiv}, abs/2304.14997, 2023.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever,
  Leike, and Wu]{Gao2024ScalingAE}
Leo Gao, Tom~Dupr'e la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec
  Radford, I.~Sutskever, J.~Leike, and Jeffrey Wu.
\newblock Scaling and evaluating sparse autoencoders.
\newblock \emph{ArXiv}, abs/2406.04093, 2024.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{Higgins2017betaVAELB}
Irina Higgins, L.~Matthey, Arka Pal, Christopher~P. Burgess, Xavier Glorot,
  Matthew~M. Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock 2017.

\bibitem[Hu et~al.(2020)Hu, Xiao, and Pennington]{Hu2020ProvableBO}
Wei Hu, Lechao Xiao, and Jeffrey Pennington.
\newblock Provable benefit of orthogonal initialization in optimizing deep
  linear networks.
\newblock \emph{ArXiv}, abs/2001.05992, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kissane et~al.(2024)Kissane, Krzyzanowski, Bloom, Conmy, and
  Nanda]{Kissane2024InterpretingAL}
Connor Kissane, Robert Krzyzanowski, J.~Bloom, Arthur Conmy, and Neel Nanda.
\newblock Interpreting attention layer outputs with sparse autoencoders.
\newblock \emph{ArXiv}, abs/2406.17759, 2024.

\bibitem[Li et~al.(2019)Li, Jia, Wen, Liu, and Tao]{Li2019OrthogonalDN}
Shuai Li, K.~Jia, Yuxin Wen, Tongliang Liu, and D.~Tao.
\newblock Orthogonal deep neural networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 43:\penalty0 1352--1368, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\end{thebibliography}
