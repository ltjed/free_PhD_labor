\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bau et~al.(2017)Bau, Zhou, Khosla, Oliva, and
  Torralba]{Bau2017NetworkDQ}
David Bau, Bolei Zhou, A.~Khosla, A.~Oliva, and A.~Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual
  representations.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  3319--3327, 2017.

\bibitem[Bell \& Sejnowski(1996)Bell and Sejnowski]{Bell1996EdgesAT}
A.~J. Bell and T.~Sejnowski.
\newblock Edges are the independent components of natural scenes.
\newblock pp.\  831--837, 1996.

\bibitem[Bhaskar et~al.(2024)Bhaskar, Wettig, Friedman, and
  Chen]{Bhaskar2024FindingTC}
Adithya Bhaskar, Alexander Wettig, Dan Friedman, and Danqi Chen.
\newblock Finding transformer circuits with edge pruning.
\newblock \emph{ArXiv}, abs/2406.16778, 2024.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Geiger et~al.(2023)Geiger, Ibeling, Zur, Chaudhary, Chauhan, Huang,
  Arora, Wu, Goodman, Potts, and Icard]{Geiger2023CausalAA}
Atticus Geiger, D.~Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing
  Huang, Aryaman Arora, Zhengxuan Wu, Noah~D. Goodman, Christopher Potts, and
  Thomas~F. Icard.
\newblock Causal abstraction: A theoretical foundation for mechanistic
  interpretability.
\newblock 2023.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee et~al.(2011)Lee, Grosse, Ranganath, and Ng]{Lee2011UnsupervisedLO}
Honglak Lee, R.~Grosse, R.~Ranganath, and A.~Ng.
\newblock Unsupervised learning of hierarchical representations with
  convolutional deep belief networks.
\newblock \emph{Communications of the ACM}, 54:\penalty0 95 -- 103, 2011.

\bibitem[Mairal et~al.(2009{\natexlab{a}})Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineDL}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online dictionary learning for sparse coding.
\newblock pp.\  689--696, 2009{\natexlab{a}}.

\bibitem[Mairal et~al.(2009{\natexlab{b}})Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineLF}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009{\natexlab{b}}.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rai et~al.(2024)Rai, Zhou, Feng, Saparov, and Yao]{Rai2024APR}
Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao.
\newblock A practical review of mechanistic interpretability for
  transformer-based language models.
\newblock \emph{ArXiv}, abs/2407.02646, 2024.

\bibitem[Ruslim et~al.(2024)Ruslim, Spencer, Hogendoorn, Meffin, Lian, and
  Burkitt]{Ruslim2024EmergenceOS}
Marko~A. Ruslim, Martin~J. Spencer, Hinze Hogendoorn, H.~Meffin, Yanbo Lian,
  and A.~Burkitt.
\newblock Emergence of sparse coding, balance and decorrelation from a
  biologically-grounded spiking neural network model of learning in the primary
  visual cortex.
\newblock \emph{bioRxiv}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Tiňo, Leonardis, and Tang]{Zhang2020ASO}
Yu~Zhang, P.~Tiňo, A.~Leonardis, and K.~Tang.
\newblock A survey on neural network interpretability.
\newblock \emph{IEEE Transactions on Emerging Topics in Computational
  Intelligence}, 5:\penalty0 726--742, 2020.

\end{thebibliography}
