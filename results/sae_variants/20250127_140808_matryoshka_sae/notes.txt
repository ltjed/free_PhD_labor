# Title: Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability

# Visualization Analysis

## Figure 1: Metrics Comparison (metrics_comparison.png)
This figure presents a comprehensive comparison of key metrics across different runs using bar charts. The visualization includes:

1. KL Divergence: Shows how well each model preserves the original behavior of the network. Notably, the baseline (α=0.0) achieves the highest score of 0.795, with α=0.2 following closely at 0.787. This suggests that the nested architecture maintains good behavior preservation even with reduced shared capacity.

2. Explained Variance: Measures reconstruction quality. Surprisingly, the α=0.2 configuration matches the baseline's performance (0.299), indicating that a smaller shared dictionary can maintain reconstruction fidelity. This challenges the intuition that larger shared dictionaries are necessary for good reconstruction.

3. Cosine Similarity: Reflects feature alignment. The consistent scores across configurations (0.762-0.766) suggest that the nested architecture maintains good feature alignment regardless of the sharing ratio. This is an unexpected finding that supports the robustness of the approach.

4. L0 Sparsity: Indicates feature selectivity. The remarkably consistent values (85.07-86.18) across all configurations suggest that the nested architecture maintains selective feature activation regardless of the sharing ratio. This is a key finding supporting the effectiveness of the approach.

5. Training Loss: Shows final optimization performance. The higher losses in nested configurations (405-417) compared to baseline (200) suggest a trade-off between architectural flexibility and optimization ease. However, the α=0.2 configuration shows the best balance among nested variants.

## Figure 2: SCR Performance (scr_metrics.png)
This plot tracks Sparse Coding Rate (SCR) performance across different thresholds, revealing several key insights:

1. Low Thresholds (n=2,5):
   - Baseline shows strongest performance
   - α=0.2 configuration approaches baseline performance
   - Larger α values show degraded performance
   - This suggests that smaller shared dictionaries better preserve fine-grained feature selectivity

2. Mid-range Thresholds (n=10,20):
   - All nested configurations show improved performance
   - α=0.2 achieves best performance in this range
   - This indicates that nested architectures excel at capturing mid-level feature abstractions

3. High Thresholds (n=50,100):
   - Performance convergence across configurations
   - Gradual decline in all models
   - Suggests that high-level feature capture is similarly challenging for all architectures

The unexpected strength of α=0.2 in mid-range thresholds suggests an optimal balance between shared and specialized feature learning.

## Figure 3: Training Progression (training_progression.png)
This visualization tracks the training loss over time, revealing several critical patterns:

1. Initial Phase (Steps 0-1000):
   - Baseline shows rapid convergence
   - Nested configurations show higher initial losses
   - α=0.2 configuration shows fastest convergence among nested variants
   - This suggests that smaller shared dictionaries enable faster initial learning

2. Middle Phase (Steps 1000-3000):
   - All configurations show steady improvement
   - Nested variants maintain consistent relative ordering
   - α=0.2 maintains lowest loss among nested configurations
   - This indicates stable optimization despite architectural constraints

3. Final Phase (Steps 3000-4882):
   - Loss curves flatten, indicating convergence
   - Final ordering: Baseline < α=0.2 < α=0.3 < α=0.5 < α=0.1
   - This suggests that α=0.2 achieves optimal balance between shared capacity and optimization stability

Unexpected Findings:
1. The superior performance of α=0.2 across multiple metrics challenges the intuition that larger shared dictionaries are better
2. The consistency of sparsity metrics across configurations suggests architectural robustness
3. The clear sweet spot at α=0.2 indicates a non-linear relationship between shared capacity and performance

These visualizations collectively support the hypothesis that nested architectures with moderate sharing (α=0.2) can achieve comparable or superior performance to traditional SAEs while enabling more flexible feature organization.
