{
    "Summary": "The paper introduces Matryoshka Sparse Autoencoders (MSAEs) to enhance interpretability in language models by organizing features through nested dictionaries. The proposed architecture partitions the feature space into a compact shared dictionary and specialized dictionaries, demonstrating improved interpretability and performance compared to traditional sparse autoencoders (SAEs). The paper provides extensive experimental evaluations on the Gemma-2B model, showing that MSAEs achieve strong performance across key metrics while enabling more structured feature analysis.",
    "Strengths": [
        "Addresses a crucial issue in model interpretability by introducing a novel nested dictionary structure to capture hierarchical relationships in neural features.",
        "Comprehensive experiments and evaluations are provided, demonstrating the effectiveness of the proposed approach across various metrics.",
        "The results show significant improvements in behavioral alignment and feature selectivity compared to traditional SAEs, supporting the robustness and scalability of the method."
    ],
    "Weaknesses": [
        "The paper lacks clarity in its exposition, particularly in the explanation of the nested dictionary structure and its training procedure.",
        "Insufficient comparative analysis with other hierarchical feature learning methods.",
        "The experimental section lacks detailed ablation studies to validate the choice of model components and hyperparameters.",
        "The paper does not sufficiently justify the selection of the Gemma-2B model or explain why it is representative of language models in general.",
        "Higher training losses in nested configurations suggest possible inefficiencies or trade-offs not fully explored."
    ],
    "Originality": 3,
    "Quality": 3,
    "Clarity": 2,
    "Significance": 3,
    "Questions": [
        "Can you provide more details on the autoencoder aggregator and its implementation?",
        "How does the modulation function impact the overall performance, and have you experimented with reducing the number of gating parameters?",
        "Can you provide more qualitative examples to strengthen the demonstration of the proposed method's effectiveness?",
        "How does the performance of MSAEs compare to other hierarchical feature learning methods?",
        "Why was the Gemma-2B model selected for evaluation, and how representative is it of language models in general?"
    ],
    "Limitations": [
        "The paper highlights increased training complexity and higher final losses in nested configurations as limitations. Additionally, the performance reduction at very low activation thresholds and increased sensitivity to learning rate in models with higher sharing ratios are noted. Addressing these limitations could further enhance the practical applicability of the proposed approach.",
        "The clarity of the paper needs improvement, and the generalizability of the findings should be addressed.",
        "Higher training losses in nested configurations need further exploration to understand potential trade-offs or inefficiencies."
    ],
    "Ethical Concerns": false,
    "Soundness": 2,
    "Presentation": 2,
    "Contribution": 3,
    "Overall": 4,
    "Confidence": 4,
    "Decision": "Reject"
}