\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alemi et~al.(2017)Alemi, Fischer, and Dillon]{Alemi2017DeepVI}
Alexander~A. Alemi, Ian Fischer, and Joshua~V. Dillon.
\newblock Deep variational information bottleneck.
\newblock \emph{ArXiv}, abs/1612.00410, 2017.

\bibitem[Bell \& Sejnowski(1995)Bell and Sejnowski]{Bell1995AnIA}
A.~J. Bell and T.~Sejnowski.
\newblock An information-maximization approach to blind separation and blind
  deconvolution.
\newblock \emph{Neural Computation}, 7:\penalty0 1129--1159, 1995.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{Chen2016InfoGANIR}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, I.~Sutskever, and P.~Abbeel.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock pp.\  2172--2180, 2016.

\bibitem[F.R.S.(1901)]{F.R.S.1901LIIIOL}
Karl~Pearson F.R.S.
\newblock Liii. on lines and planes of closest fit to systems of points in
  space.
\newblock \emph{Philosophical Magazine Series 1}, 2:\penalty0 559--572, 1901.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{Higgins2016betaVAELB}
I.~Higgins, L.~Matthey, Arka Pal, Christopher~P. Burgess, Xavier Glorot,
  M.~Botvinick, S.~Mohamed, and Alexander Lerchner.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Locatello et~al.(2018)Locatello, Bauer, Lucic, Gelly, Scholkopf, and
  Bachem]{Locatello2018ChallengingCA}
Francesco Locatello, Stefan Bauer, Mario Lucic, S.~Gelly, B.~Scholkopf, and
  Olivier Bachem.
\newblock Challenging common assumptions in the unsupervised learning of
  disentangled representations.
\newblock pp.\  4114--4124, 2018.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and
  Sapiro]{Mairal2009OnlineLF}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{Tishby2015DeepLA}
Naftali Tishby and Noga Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock \emph{2015 IEEE Information Theory Workshop (ITW)}, pp.\  1--5, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol]{Vincent2010StackedDA}
Pascal Vincent, H.~Larochelle, Isabelle Lajoie, Yoshua Bengio, and
  Pierre-Antoine Manzagol.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 3371--3408, 2010.

\end{thebibliography}
