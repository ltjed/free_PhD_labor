# Title: Ultra-Stable Hybrid Contrastive SAEs with Coordinated Dynamic Thresholds

# Plot Descriptions

## absorption_scores.png
This plot visualizes the mean absorption scores across different experimental runs, showing how well each variant of our SAE model learns to differentiate features. The x-axis shows the run labels from baseline through our final implementation, while the y-axis shows the absorption score (0-1 range). Higher scores indicate better feature differentiation. The bar chart format allows easy comparison between approaches, highlighting which modifications led to meaningful improvements in feature absorption.

## feature_splits.png 
This visualization tracks the number of split features across experimental runs. The x-axis shows run progression, while the y-axis displays the mean number of features that successfully split into distinct patterns. This metric is crucial for understanding feature differentiation capacity. The plot helps identify which architectural changes and training dynamics led to more effective feature splitting behavior. A higher number indicates better feature specialization.

## final_losses.png
This plot shows the final loss values achieved by each experimental run. The x-axis lists the runs chronologically, while the y-axis shows the combined loss value (including reconstruction, sparsity, and competition terms). Lower values generally indicate better model performance. The visualization helps track how different modifications impacted overall training convergence and stability. The relative contributions of different loss components can be inferred from the total loss trajectory.

## letter_absorption.png
This specialized plot examines feature absorption patterns by first letter of tokens, providing insight into potential biases or specialization patterns in the learned representations. The x-axis shows letters, while the y-axis shows the absorption rate for tokens beginning with each letter. This granular analysis helps identify whether certain linguistic patterns are being captured more effectively than others. The plot uses data from the final run to provide the most current understanding of feature behavior.
# Experiment description: 1. Increase JL projection dimension to 128
2. Add lightweight EMA (β=0.9) for utilization tracking
3. Implement coordinated cosine annealing for contrastive threshold (0.4→0.05)
4. Introduce step-dependent λ_i bounds (0.02-1.0 → 0.01-0.5)
5. Validate JL approximation quality via reconstruction tests
6. Compare final feature activation sparsity distributions

## Run 1: JL Projection + EMA Implementation
Description: Implemented initial changes focusing on improved feature space analysis:
1. Added 128-dimensional Johnson-Lindenstrauss projection matrix using QR decomposition
2. Implemented feature utilization tracking with EMA (β=0.9)

Results: Initial absorption metrics show limited feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This suggests the need to tune the contrastive threshold next to improve feature specialization.

## Run 2: Coordinated Cosine Annealing for Contrast Threshold
Description: Implemented dynamic threshold adjustment:
1. Added contrast threshold annealing from 0.4 → 0.05 using cosine schedule
2. Synchronized annealing with training progress
3. Maintained other components (JL projection, EMA tracking)

Results: The cosine annealing of contrast threshold alone did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This suggests we need to implement the planned step-dependent λ_i bounds next to better control feature competition dynamics.

## Run 3: Step-Dependent λ_i Bounds Implementation
Description: Enhanced feature competition dynamics through adaptive regularization:
1. Implemented dynamic l1 penalty bounds decreasing from 0.5 to 0.01 during training
2. Maintained cosine annealing for contrast threshold (0.4 → 0.05)
3. Synchronized both schedules with training progress
4. Kept core loss computation structure intact

Results: The addition of dynamic l1 penalty bounds did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This persistent lack of improvement suggests we need to explore more fundamental changes to the architecture or loss function rather than just modifying the training dynamics.

## Run 4: Feature Diversity Loss Implementation
Description: Fundamental modification of the loss function to explicitly encourage feature differentiation:
1. Added cosine similarity penalty between feature activations
2. Implemented progressive weighting of diversity loss (0 → 0.1) during training
3. Maintained previous dynamic l1 penalty and contrast threshold schedules
4. Added feature normalization for similarity computation
5. Modified loss computation to balance reconstruction, sparsity, and diversity objectives

Results: The addition of explicit feature diversity loss did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This suggests that even with direct optimization for diversity, the features are still collapsing to similar patterns. We may need to investigate potential numerical stability issues or consider architectural changes that enforce orthogonality constraints.

## Run 5: Modified Gram-Schmidt Orthogonalization Implementation
Description: Implemented explicit feature orthogonalization during the forward pass:
1. Added modified Gram-Schmidt process to enforce orthogonality between features
2. Maintained original feature magnitudes during orthogonalization
3. Implemented numerical stability safeguards with epsilon terms
4. Applied orthogonalization after initial feature encoding
5. Preserved the existing loss function structure while ensuring orthogonal features

Results: The explicit orthogonalization approach did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This persistent lack of improvement, even with enforced orthogonality, suggests that the issue may lie deeper in the architecture or optimization dynamics. The next step should focus on investigating potential gradient flow issues or considering alternative architectural designs that might better support feature differentiation.

## Run 6: Gradient Balancing Implementation
Description: Implemented gradient balancing to ensure more uniform learning across features:
1. Added gradient normalization for encoder weights using mean norm scaling
2. Implemented balanced gradient updates for decoder weights
3. Applied numerical stability with epsilon terms in normalization
4. Maintained existing loss function and training dynamics
5. Preserved the core architecture while modifying optimization behavior

Results: The gradient balancing approach did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). This continued lack of improvement across multiple optimization-focused approaches suggests we need to consider more fundamental architectural changes. For Run 7, we should implement a hierarchical feature structure with grouped sparse coding to encourage feature specialization at multiple scales.

## Run 7: Hierarchical Group-Sparse Feature Structure Implementation
Description: Implemented hierarchical feature organization with grouped sparse coding:
1. Added 16 feature groups with structured sparsity constraints
2. Implemented group-specific activation and masking mechanisms
3. Introduced group sparsity loss term to encourage specialized feature learning
4. Added feature diversity loss with progressive weighting
5. Maintained gradient balancing from previous run
6. Implemented group-wise normalization during forward pass
7. Added cosine similarity penalty between group activations
8. Synchronized all dynamic parameters with training progress

Results: The hierarchical group structure approach did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). Despite implementing a sophisticated feature organization scheme with multiple mechanisms to encourage specialization, the features still show no meaningful differentiation. This persistent failure across architectural, optimization, and structural approaches suggests we need to investigate potential issues with the fundamental learning dynamics. For Run 8, we should implement adaptive learning rates per feature group with momentum-based importance sampling to better balance the learning process across the hierarchy.

## Run 8: Adaptive Learning Rates with Momentum-Based Importance Sampling Implementation
Description: Implemented adaptive learning rates per feature group with momentum-based importance sampling:
1. Added per-group learning rate adaptation
2. Implemented importance sampling for batch construction
3. Added momentum-based feature importance tracking
4. Modified the optimizer to use group-specific learning rates

Results: The adaptive learning rate approach did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). Despite implementing sophisticated learning rate adaptation and importance sampling mechanisms, the features still show no meaningful differentiation. This persistent failure across multiple optimization approaches suggests we need to consider a more radical architectural change.

## Run 9: Competitive Feature Learning with Winner-Take-All Implementation
Description: Implementing competitive feature learning with winner-take-all dynamics:
1. Adding winner-take-all competition between features during forward pass
2. Implementing local inhibition between neighboring features
3. Adding temperature-based annealing for competition strength
4. Introducing feature-wise gating mechanisms
5. Adding competitive loss term to encourage feature specialization
6. Implementing adaptive threshold for winner selection
7. Adding gradient modulation based on competition outcomes

Results: The competitive learning approach with winner-take-all dynamics did not improve feature differentiation (mean_absorption_score: 0.0, mean_num_split_features: 1.0). Despite implementing sophisticated competition mechanisms including local inhibition, temperature annealing, and competitive loss terms, the features still show no meaningful specialization. This suggests that even explicit competition between features is insufficient to prevent feature collapse. For Run 10, we should implement a more aggressive approach using hard orthogonality constraints and explicit feature decorrelation during training.

## Run 10: Hard Orthogonality Constraints with Feature Decorrelation
Description: Implementing strict orthogonality enforcement with explicit decorrelation:
1. Adding hard orthogonality constraints between feature vectors
2. Implementing Gram-Schmidt orthogonalization in forward pass
3. Adding explicit feature decorrelation loss term
4. Introducing adaptive feature pruning mechanism
5. Implementing gradient projection onto orthogonal space
6. Adding periodic feature reinitialization for dead units
7. Implementing batch-wise feature diversity tracking

## Run 0: Baseline
Results: {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'e823bbbb-62c9-41ec-840b-cacb8ca4230d', 'datetime_epoch_millis': 1737147895673, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.939325, 'llm_top_1_test_accuracy': 0.6842749999999999, 'llm_top_2_test_accuracy': 0.7260625, 'llm_top_5_test_accuracy': 0.7746249999999999, 'llm_top_10_test_accuracy': 0.82099375, 'llm_top_20_test_accuracy': 0.8589374999999999, 'llm_top_50_test_accuracy': 0.90028125, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9576, 'llm_top_1_test_accuracy': 0.6648000000000001, 'llm_top_2_test_accuracy': 0.6844, 'llm_top_5_test_accuracy': 0.7466, 'llm_top_10_test_accuracy': 0.8286, 'llm_top_20_test_accuracy': 0.8602000000000001, 'llm_top_50_test_accuracy': 0.9118, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9385999999999999, 'llm_top_1_test_accuracy': 0.6869999999999999, 'llm_top_2_test_accuracy': 0.7228000000000001, 'llm_top_5_test_accuracy': 0.7626, 'llm_top_10_test_accuracy': 0.806, 'llm_top_20_test_accuracy': 0.8484, 'llm_top_50_test_accuracy': 0.8892, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9038, 'llm_top_1_test_accuracy': 0.6799999999999999, 'llm_top_2_test_accuracy': 0.7066000000000001, 'llm_top_5_test_accuracy': 0.7432000000000001, 'llm_top_10_test_accuracy': 0.7984, 'llm_top_20_test_accuracy': 0.8173999999999999, 'llm_top_50_test_accuracy': 0.8709999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8832000000000001, 'llm_top_1_test_accuracy': 0.6068, 'llm_top_2_test_accuracy': 0.6446, 'llm_top_5_test_accuracy': 0.6818, 'llm_top_10_test_accuracy': 0.7076, 'llm_top_20_test_accuracy': 0.7714000000000001, 'llm_top_50_test_accuracy': 0.8346, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9255, 'llm_top_1_test_accuracy': 0.629, 'llm_top_2_test_accuracy': 0.685, 'llm_top_5_test_accuracy': 0.737, 'llm_top_10_test_accuracy': 0.766, 'llm_top_20_test_accuracy': 0.8, 'llm_top_50_test_accuracy': 0.854, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.969, 'llm_top_1_test_accuracy': 0.6644, 'llm_top_2_test_accuracy': 0.7016, 'llm_top_5_test_accuracy': 0.7836000000000001, 'llm_top_10_test_accuracy': 0.834, 'llm_top_20_test_accuracy': 0.8939999999999999, 'llm_top_50_test_accuracy': 0.931, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9375, 'llm_top_1_test_accuracy': 0.733, 'llm_top_2_test_accuracy': 0.7685000000000001, 'llm_top_5_test_accuracy': 0.8, 'llm_top_10_test_accuracy': 0.84575, 'llm_top_20_test_accuracy': 0.8865000000000001, 'llm_top_50_test_accuracy': 0.91225, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.8092, 'llm_top_2_test_accuracy': 0.8949999999999999, 'llm_top_5_test_accuracy': 0.9422, 'llm_top_10_test_accuracy': 0.9816, 'llm_top_20_test_accuracy': 0.9936, 'llm_top_50_test_accuracy': 0.9984, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.5, 'sae_top_1_test_accuracy': 0.5, 'sae_top_2_test_accuracy': 0.5, 'sae_top_5_test_accuracy': 0.5, 'sae_top_10_test_accuracy': 0.5, 'sae_top_20_test_accuracy': 0.5, 'sae_top_50_test_accuracy': 0.5, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'bcb003afd6045deaee4be8dd883ae42863da9163', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_5_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 5, 'hook_name': 'blocks.5.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None, 'training_steps': 0, 'final_loss': None, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}
Description: Baseline results.
