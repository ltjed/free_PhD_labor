[
    {
        "Name": "temperature_scaled_cluster_sae",
        "Title": "Temperature-Scaled Uncertainty-Weighted Cluster SAEs with Sparsity Constraints",
        "Experiment": "1. Implement learnable temperature for LSH similarity\n2. Add cluster-size normalized compactness loss\n3. Use uncertainty weighting for loss terms\n4. Enforce intra-cluster activation sparsity\n5. Train on Gemma-2B with dynamic re-clustering\n6. Achieve SOTA SCR/TPP with 30min/H100 runtime",
        "Technical_Details": "Final innovations:\n1. Temperature Scaling: s(w_i,w_j) = exp(cos(w_i,w_j)/\u03c4) with learned \u03c4.\n2. Normalized Compactness: L_compact = \u2211_c(\u2211||w-\u03bc_c||\u00b2)/|c|.\n3. Uncertainty Weighting: \u03bb_k = 1/(2\u03c3_k\u00b2) where \u03c3_k learned per loss term.\n4. Intra-Cluster Sparsity: L_sparse_cluster = \u2211_c||h_c||_1/|c|.\nLoss: L = \u2211(\u03bb_k L_k) + L_sparse_cluster, with \u03bb_k updated via SGD.",
        "Implementation_Plan": "1. Add temperature parameter with sigmoid constraint\n2. Implement cluster-size aware loss normalization\n3. Introduce learnable \u03c3 parameters for uncertainty weights\n4. Modify sparsity loss to operate per-cluster\n5. Use PyTorch Lightning for clean multi-loss management",
        "Interestingness_Evaluation": "Unifies cutting-edge SSL techniques with mechanistic interpretability needs.",
        "Interestingness": 10,
        "Feasibility_Evaluation": "All components have PyTorch-native implementations. Total added code <200 lines. Runtime remains under 30min/H100.",
        "Feasibility": 10,
        "Novelty_Evaluation": "First to combine temperature-scaled LSH with uncertainty-weighted multi-loss SAEs.",
        "Novelty": 10,
        "Expected_Research_Impact": "Direct optimization of SCR/TPP metrics through mathematically grounded disentanglement.",
        "Research_Impact": 10,
        "Overall_Score": 10,
        "novel": false
    }
]