{
    "system": "You are an expert AI Researcher deeply immersed in the field of mechanistic interpretability, specifically focusing on improving the interpretability of sparse autoencoders (SAEs). Your goal is to publish a paper to advance progress on a specific benchmark. Reason step-by-step how your proposed variants of SAE could improve on the target benchmark",
    "task_description": "Your current research focuses on addressing the challenge of limited interpretability in the latent space of sparse autoencoders. The core problem is polysemanticity, where individual latent features appear to represent multiple, semantically distinct concepts. This hinders our ability to understand what neural networks are truly learning.\n\nYour task is to propose novel and feasible variants of sparse autoencoders that result in more interpretable latents compared to the internal activations they are trained to reconstruct. Consider the insights from the following abstract of a key paper in this area:\n\n\"One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task wang2022interpretability to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\"\n\nSpecifically, you should focus on the following benchmarks: core, sparse_probing.\n\nCore contains a suite of 6 metrics detailed below. 1. L0 Sparsity Measures the average number of non-zero feature activations. A lower number indicates higher sparsity. Measures the change in model performance when replacing activations with the corresponding SAE reconstruction during a forward pass. 2. Cross Entropy Loss Score. This metric is quantified as (H^* - H0) / (H_{orig} - H0), where H_{orig} is the cross-entropy loss of the original model for next-token prediction, H^* is the cross-entropy loss after substituting the model activation x with its SAE reconstruction during the forward pass, and H_0 is the cross-entropy loss when zero-ablating x. Higher Cross Entropy Loss Score is better. The range of the metric is 0-1. 3. Feature Density Statistics. We track the activation frequency for every SAE latent. This can be informative for both the proportion of dead features that never activate (indicating wasted capacity) and dense features that activate with a very high frequency (which tend to be less interpretable). These can also be visualized as a log-scale histogram. 4. L2 Ratio Compares the L2 norm (Euclidean distance) of different components, typically between the original and reconstructed representations. Helps assess how well the model preserves the magnitude of the input signals. 5. Explained Variance Quantifies how much of the variability in the data is captured by the model. Higher values of Explained Variance (closer to 1) indicate the model better explains the patterns in the input data. 6. KL Divergence Kullback-Leibler divergence measures the difference between two probability distributions, often used to compare the model's output distribution to the target distribution. Lower values of KL divergence indicate better alignment.We evaluate our sparse autoencoders' ability to learn specified features through a series of targeted probing tasks across diverse domains, including language identification, profession classification, and sentiment analysis. For each task, we encode inputs through the SAE, apply mean pooling over non-padding tokens, and select the top-K latents using maximum mean difference. We train a logistic regression probe on the resulting representations and evaluate classification performance on held-out test data. Our evaluation spans 35 distinct binary classification tasks derived from five datasets. Our probing evaluation encompasses five datasets spanning different domains and tasks: bias_in_bios for Profession Classification (predicting professional roles from biographical text), Amazon Reviews for Product Classification & Sentiment (dual tasks: category prediction and sentiment analysis), Europarl for Language Identification (detecting document language), GitHub for Programming Language Classification (identifying coding language from source code), and AG News for Topic Categorization (classifying news articles by subject). To ensure consistent computational requirements across tasks, we sample 4,000 training and 1,000 test examples per binary classification task and truncate all inputs to 128 tokens. For GitHub data, we follow Gurnee et al. by excluding the first 150 characters (approximately 50 tokens) as a crude attempt to avoid license headers. We evaluated both mean pooling and max pooling across non-padding tokens, and used mean pooling as it obtained slightly higher accuracy. From each dataset, we select subsets containing up to five classes. Multiple subsets may be drawn from the same dataset to maintain positive ratios \u22650.2."
}