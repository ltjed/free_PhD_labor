\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio(2007)]{Bengio2007LearningDA}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock \emph{Found. Trends Mach. Learn.}, 2:\penalty0 1--127, 2007.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{Burgess2018UnderstandingDI}
Christopher~P. Burgess, I.~Higgins, Arka Pal, L.~Matthey, Nicholas Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in Î²-vae.
\newblock \emph{ArXiv}, abs/1804.03599, 2018.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kissane et~al.(2024)Kissane, Krzyzanowski, Bloom, Conmy, and
  Nanda]{Kissane2024InterpretingAL}
Connor Kissane, Robert Krzyzanowski, J.~Bloom, Arthur Conmy, and Neel Nanda.
\newblock Interpreting attention layer outputs with sparse autoencoders.
\newblock \emph{ArXiv}, abs/2406.17759, 2024.

\bibitem[Makelov et~al.(2024)Makelov, Lange, and Nanda]{Makelov2024TowardsPE}
Aleksandar Makelov, Georg Lange, and Neel Nanda.
\newblock Towards principled evaluations of sparse autoencoders for
  interpretability and control.
\newblock \emph{ArXiv}, abs/2405.08366, 2024.

\bibitem[Mu \& Andreas(2020)Mu and Andreas]{Mu2020CompositionalEO}
Jesse Mu and Jacob Andreas.
\newblock Compositional explanations of neurons.
\newblock \emph{ArXiv}, abs/2006.14032, 2020.

\bibitem[Narayanaswamy et~al.(2017)Narayanaswamy, Paige, van~de Meent,
  Desmaison, Goodman, Kohli, Wood, and Torr]{Narayanaswamy2017LearningDR}
Siddharth Narayanaswamy, Brooks Paige, Jan-Willem van~de Meent, Alban
  Desmaison, Noah~D. Goodman, Pushmeet Kohli, Frank~D. Wood, and Philip H.~S.
  Torr.
\newblock Learning disentangled representations with semi-supervised deep
  generative models.
\newblock \emph{ArXiv}, abs/1706.00400, 2017.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Park et~al.(2024)Park, Ahn, Kim, and Kang]{Park2024MonetMO}
Jungwoo Park, Y.~Ahn, Kee-Eung Kim, and Jaewoo Kang.
\newblock Monet: Mixture of monosemantic experts for transformers.
\newblock \emph{ArXiv}, abs/2412.04139, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\end{thebibliography}
