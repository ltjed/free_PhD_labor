{
  "training results for layer 19": {
    "config": {
      "trainer_class": "AdaptiveOrthogonalTrainer",
      "activation_dim": 2304,
      "dict_size": 2304,
      "lr": 0.0009,
      "l1_penalty": 0.04,
      "warmup_steps": 1000,
      "resample_steps": null,
      "device": "cuda",
      "layer": 19,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "CustomTrainer",
      "submodule_name": "resid_post_layer_19"
    },
    "final_info": {
      "training_steps": 4882,
      "final_loss": 270.982666015625,
      "layer": 19,
      "dict_size": 2304,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "57b5dc99-f617-43b3-887f-1f80a1a6bfad",
    "datetime_epoch_millis": 1737930178442,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.010124895517470242,
        "mean_num_split_features": 1.2
      }
    },
    "eval_result_details": [
      {
        "first_letter": "a",
        "absorption_rate": 0.00986971969996052,
        "num_absorption": 25,
        "num_probe_true_positives": 2533,
        "num_split_features": 1
      },
      {
        "first_letter": "b",
        "absorption_rate": 0.005050505050505051,
        "num_absorption": 8,
        "num_probe_true_positives": 1584,
        "num_split_features": 1
      },
      {
        "first_letter": "c",
        "absorption_rate": 0.02792890823358723,
        "num_absorption": 77,
        "num_probe_true_positives": 2757,
        "num_split_features": 1
      },
      {
        "first_letter": "d",
        "absorption_rate": 0.008631319358816275,
        "num_absorption": 14,
        "num_probe_true_positives": 1622,
        "num_split_features": 1
      },
      {
        "first_letter": "e",
        "absorption_rate": 0.002523659305993691,
        "num_absorption": 4,
        "num_probe_true_positives": 1585,
        "num_split_features": 1
      },
      {
        "first_letter": "f",
        "absorption_rate": 0.004942339373970346,
        "num_absorption": 6,
        "num_probe_true_positives": 1214,
        "num_split_features": 1
      },
      {
        "first_letter": "g",
        "absorption_rate": 0.0026362038664323375,
        "num_absorption": 3,
        "num_probe_true_positives": 1138,
        "num_split_features": 1
      },
      {
        "first_letter": "h",
        "absorption_rate": 0.08004158004158005,
        "num_absorption": 77,
        "num_probe_true_positives": 962,
        "num_split_features": 1
      },
      {
        "first_letter": "i",
        "absorption_rate": 0.007402837754472548,
        "num_absorption": 12,
        "num_probe_true_positives": 1621,
        "num_split_features": 2
      },
      {
        "first_letter": "j",
        "absorption_rate": 0.03546099290780142,
        "num_absorption": 15,
        "num_probe_true_positives": 423,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.001557632398753894,
        "num_absorption": 1,
        "num_probe_true_positives": 642,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 1196,
        "num_split_features": 1
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.0060406370126304225,
        "num_absorption": 11,
        "num_probe_true_positives": 1821,
        "num_split_features": 1
      },
      {
        "first_letter": "n",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 845,
        "num_split_features": 1
      },
      {
        "first_letter": "o",
        "absorption_rate": 0.007070707070707071,
        "num_absorption": 7,
        "num_probe_true_positives": 990,
        "num_split_features": 1
      },
      {
        "first_letter": "p",
        "absorption_rate": 0.0004332755632582322,
        "num_absorption": 1,
        "num_probe_true_positives": 2308,
        "num_split_features": 2
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.011428571428571429,
        "num_absorption": 2,
        "num_probe_true_positives": 175,
        "num_split_features": 1
      },
      {
        "first_letter": "r",
        "absorption_rate": 0.006053268765133172,
        "num_absorption": 10,
        "num_probe_true_positives": 1652,
        "num_split_features": 2
      },
      {
        "first_letter": "s",
        "absorption_rate": 0.005263157894736842,
        "num_absorption": 15,
        "num_probe_true_positives": 2850,
        "num_split_features": 1
      },
      {
        "first_letter": "t",
        "absorption_rate": 0.00357355568790947,
        "num_absorption": 6,
        "num_probe_true_positives": 1679,
        "num_split_features": 2
      },
      {
        "first_letter": "u",
        "absorption_rate": 0.0013297872340425532,
        "num_absorption": 1,
        "num_probe_true_positives": 752,
        "num_split_features": 2
      },
      {
        "first_letter": "v",
        "absorption_rate": 0.0012738853503184713,
        "num_absorption": 1,
        "num_probe_true_positives": 785,
        "num_split_features": 1
      },
      {
        "first_letter": "w",
        "absorption_rate": 0.004201680672268907,
        "num_absorption": 3,
        "num_probe_true_positives": 714,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.02040816326530612,
        "num_absorption": 3,
        "num_probe_true_positives": 147,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 230,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "968c10e714d56a267c6e55fd44e4abe047973ccb",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_19_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 19,
      "hook_name": "blocks.19.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_19_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_19_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.7639751552795031,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 2.375
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.7664473684210527,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 5.15625,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": -0.1259765625,
        "mse": 30.125,
        "cossim": 0.6484375
      },
      "shrinkage": {
        "l2_norm_in": 308.0,
        "l2_norm_out": 126.0,
        "l2_ratio": 0.4140625,
        "relative_reconstruction_bias": 0.62890625
      },
      "sparsity": {
        "l0": 49.78644561767578,
        "l1": 160.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr and tpp evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "3891200c-4dbe-4225-b291-2f80b3e540cb",
    "datetime_epoch_millis": 1737930414483,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": 0.05454857535490252,
        "scr_metric_threshold_2": 0.05222507146597071,
        "scr_dir2_threshold_2": 0.05222507146597071,
        "scr_dir1_threshold_5": 0.11602918469468353,
        "scr_metric_threshold_5": 0.06579763565867107,
        "scr_dir2_threshold_5": 0.06579763565867107,
        "scr_dir1_threshold_10": 0.10596609562906438,
        "scr_metric_threshold_10": 0.08929603334534648,
        "scr_dir2_threshold_10": 0.08929603334534648,
        "scr_dir1_threshold_20": 0.04838161848814389,
        "scr_metric_threshold_20": 0.08542467985784395,
        "scr_dir2_threshold_20": 0.08542467985784395,
        "scr_dir1_threshold_50": -0.045587064121252206,
        "scr_metric_threshold_50": 0.08732123523169502,
        "scr_dir2_threshold_50": 0.08732123523169502,
        "scr_dir1_threshold_100": -0.16504029211664906,
        "scr_metric_threshold_100": 0.07640850395124162,
        "scr_dir2_threshold_100": 0.07640850395124162,
        "scr_dir1_threshold_500": -0.2536083522087841,
        "scr_metric_threshold_500": 0.05514398000664202,
        "scr_dir2_threshold_500": 0.05514398000664202
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": -0.08955290270293834,
        "scr_metric_threshold_2": 0.0479797789751554,
        "scr_dir2_threshold_2": 0.0479797789751554,
        "scr_dir1_threshold_5": -0.0298512641082919,
        "scr_metric_threshold_5": 0.055555589003726494,
        "scr_dir2_threshold_5": 0.055555589003726494,
        "scr_dir1_threshold_10": -0.014926076865114633,
        "scr_metric_threshold_10": 0.058080808840993786,
        "scr_dir2_threshold_10": 0.058080808840993786,
        "scr_dir1_threshold_20": -0.0298512641082919,
        "scr_metric_threshold_20": 0.06060602867826107,
        "scr_dir2_threshold_20": 0.06060602867826107,
        "scr_dir1_threshold_50": -0.3582098315678786,
        "scr_metric_threshold_50": 0.07828286857267056,
        "scr_dir2_threshold_50": 0.07828286857267056,
        "scr_dir1_threshold_100": -0.4776122191352341,
        "scr_metric_threshold_100": 0.08585867860124165,
        "scr_dir2_threshold_100": 0.08585867860124165,
        "scr_dir1_threshold_500": -0.5671651218381725,
        "scr_metric_threshold_500": 0.09090911827577622,
        "scr_dir2_threshold_500": 0.09090911827577622
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": -0.027272372600506963,
        "scr_metric_threshold_2": 0.017595255120973825,
        "scr_dir2_threshold_2": 0.017595255120973825,
        "scr_dir1_threshold_5": 0.09090899238902972,
        "scr_metric_threshold_5": 0.04692073858715326,
        "scr_dir2_threshold_5": 0.04692073858715326,
        "scr_dir1_threshold_10": 0.0727274106553584,
        "scr_metric_threshold_10": 0.04692073858715326,
        "scr_dir2_threshold_10": 0.04692073858715326,
        "scr_dir1_threshold_20": 0.08181820152219406,
        "scr_metric_threshold_20": 0.041055711811385044,
        "scr_dir2_threshold_20": 0.041055711811385044,
        "scr_dir1_threshold_50": -0.0727274106553584,
        "scr_metric_threshold_50": 0.04692073858715326,
        "scr_dir2_threshold_50": 0.04692073858715326,
        "scr_dir1_threshold_100": -0.14545427945038022,
        "scr_metric_threshold_100": 0.005865026775768214,
        "scr_dir2_threshold_100": 0.005865026775768214,
        "scr_dir1_threshold_500": -0.24545460456658216,
        "scr_metric_threshold_500": 0.055718453544474765,
        "scr_dir2_threshold_500": 0.055718453544474765
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.01818158173367131,
        "scr_metric_threshold_2": 0.03194105964755349,
        "scr_dir2_threshold_2": 0.03194105964755349,
        "scr_dir1_threshold_5": 0.01818158173367131,
        "scr_metric_threshold_5": 0.051596972435561446,
        "scr_dir2_threshold_5": 0.051596972435561446,
        "scr_dir1_threshold_10": 0.0,
        "scr_metric_threshold_10": 0.06879606087991978,
        "scr_dir2_threshold_10": 0.06879606087991978,
        "scr_dir1_threshold_20": -0.34545547154312073,
        "scr_metric_threshold_20": 0.058968031261537414,
        "scr_dir2_threshold_20": 0.058968031261537414,
        "scr_dir1_threshold_50": -0.4363644639321504,
        "scr_metric_threshold_50": 0.058968031261537414,
        "scr_dir2_threshold_50": 0.058968031261537414,
        "scr_dir1_threshold_100": -0.5272734563211802,
        "scr_metric_threshold_100": 0.058968031261537414,
        "scr_dir2_threshold_100": 0.058968031261537414,
        "scr_dir1_threshold_500": -0.41818179847780595,
        "scr_metric_threshold_500": 0.04914000164315505,
        "scr_dir2_threshold_500": 0.04914000164315505
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": -0.17054273953095306,
        "scr_metric_threshold_2": 0.02686567854631583,
        "scr_dir2_threshold_2": 0.02686567854631583,
        "scr_dir1_threshold_5": -0.11627895156822583,
        "scr_metric_threshold_5": 0.059701428803238504,
        "scr_dir2_threshold_5": 0.059701428803238504,
        "scr_dir1_threshold_10": -0.0852711387397361,
        "scr_metric_threshold_10": 0.08358207149425091,
        "scr_dir2_threshold_10": 0.08358207149425091,
        "scr_dir1_threshold_20": -0.10852711387397361,
        "scr_metric_threshold_20": 0.06865671429344128,
        "scr_dir2_threshold_20": 0.06865671429344128,
        "scr_dir1_threshold_50": -0.11627895156822583,
        "scr_metric_threshold_50": 0.09850742869506053,
        "scr_dir2_threshold_50": 0.09850742869506053,
        "scr_dir1_threshold_100": 0.03875965052274194,
        "scr_metric_threshold_100": 0.09552239283975711,
        "scr_dir2_threshold_100": 0.09552239283975711,
        "scr_dir1_threshold_500": -0.20155055235944278,
        "scr_metric_threshold_500": 0.06567167843813786,
        "scr_dir2_threshold_500": 0.06567167843813786
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": 0.2108431615163708,
        "scr_metric_threshold_2": 0.16058383839570395,
        "scr_dir2_threshold_2": 0.16058383839570395,
        "scr_dir1_threshold_5": 0.2349395946453488,
        "scr_metric_threshold_5": 0.17153282051583255,
        "scr_dir2_threshold_5": 0.17153282051583255,
        "scr_dir1_threshold_10": 0.29518049793574047,
        "scr_metric_threshold_10": 0.17518240871080587,
        "scr_dir2_threshold_10": 0.17518240871080587,
        "scr_dir1_threshold_20": 0.2349395946453488,
        "scr_metric_threshold_20": 0.16423342659067727,
        "scr_dir2_threshold_20": 0.16423342659067727,
        "scr_dir1_threshold_50": 0.2228915576129131,
        "scr_metric_threshold_50": 0.021897746705048578,
        "scr_dir2_threshold_50": 0.021897746705048578,
        "scr_dir1_threshold_100": 0.1445782397097613,
        "scr_metric_threshold_100": 0.047445299140279104,
        "scr_dir2_threshold_100": 0.047445299140279104,
        "scr_dir1_threshold_500": -0.10843376954834762,
        "scr_metric_threshold_500": 0.06569345765035435,
        "scr_dir2_threshold_500": 0.06569345765035435
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.21176476363079202,
        "scr_metric_threshold_2": 0.0902257357587491,
        "scr_dir2_threshold_2": 0.0902257357587491,
        "scr_dir1_threshold_5": 0.30588259218470987,
        "scr_metric_threshold_5": 0.10150378467040429,
        "scr_dir2_threshold_5": 0.10150378467040429,
        "scr_dir1_threshold_10": 0.18235302986157348,
        "scr_metric_threshold_10": 0.1766918978026952,
        "scr_dir2_threshold_10": 0.1766918978026952,
        "scr_dir1_threshold_20": 0.16470584935383312,
        "scr_metric_threshold_20": 0.18796994671435038,
        "scr_dir2_threshold_20": 0.18796994671435038,
        "scr_dir1_threshold_50": 0.005882627246262175,
        "scr_metric_threshold_50": 0.184210597077132,
        "scr_dir2_threshold_50": 0.184210597077132,
        "scr_dir1_threshold_100": 0.005882627246262175,
        "scr_metric_threshold_100": 0.157894701461434,
        "scr_dir2_threshold_100": 0.157894701461434,
        "scr_dir1_threshold_500": -0.058823467538437105,
        "scr_metric_threshold_500": 0.03383459489013476,
        "scr_dir2_threshold_500": 0.03383459489013476
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.19642861894500888,
        "scr_metric_threshold_2": 0.033434757835790134,
        "scr_dir2_threshold_2": 0.033434757835790134,
        "scr_dir1_threshold_5": 0.29464319450956306,
        "scr_metric_threshold_5": 0.030395283805928035,
        "scr_dir2_threshold_5": 0.030395283805928035,
        "scr_dir1_threshold_10": 0.2678573044130302,
        "scr_metric_threshold_10": 0.06382986047261584,
        "scr_dir2_threshold_10": 0.06382986047261584,
        "scr_dir1_threshold_20": 0.24999986695397514,
        "scr_metric_threshold_20": 0.08814601504971734,
        "scr_dir2_threshold_20": 0.08814601504971734,
        "scr_dir1_threshold_50": 0.24107141431649734,
        "scr_metric_threshold_50": 0.20060800350743382,
        "scr_dir2_threshold_50": 0.20060800350743382,
        "scr_dir1_threshold_100": -0.4553569385364618,
        "scr_metric_threshold_100": 0.13677814303481797,
        "scr_dir2_threshold_100": 0.13677814303481797,
        "scr_dir1_threshold_500": -0.48214229644889517,
        "scr_metric_threshold_500": 0.1398176170646801,
        "scr_dir2_threshold_500": 0.1398176170646801
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": 0.08653849184777547,
        "scr_metric_threshold_2": 0.009174467447523987,
        "scr_dir2_threshold_2": 0.009174467447523987,
        "scr_dir1_threshold_5": 0.1298077377716632,
        "scr_metric_threshold_5": 0.009174467447523987,
        "scr_dir2_threshold_5": 0.009174467447523987,
        "scr_dir1_threshold_10": 0.1298077377716632,
        "scr_metric_threshold_10": 0.04128441997433719,
        "scr_dir2_threshold_10": 0.04128441997433719,
        "scr_dir1_threshold_20": 0.1394232849551862,
        "scr_metric_threshold_20": 0.013761564463381832,
        "scr_dir2_threshold_20": 0.013761564463381832,
        "scr_dir1_threshold_50": 0.1490385455779229,
        "scr_metric_threshold_50": 0.009174467447523987,
        "scr_dir2_threshold_50": 0.009174467447523987,
        "scr_dir1_threshold_100": 0.09615403903129846,
        "scr_metric_threshold_100": 0.02293575849509752,
        "scr_dir2_threshold_100": 0.02293575849509752,
        "scr_dir1_threshold_500": 0.05288479310741072,
        "scr_metric_threshold_500": -0.05963308145357687,
        "scr_dir2_threshold_500": -0.05963308145357687
      }
    ],
    "sae_bench_commit_hash": "968c10e714d56a267c6e55fd44e4abe047973ccb",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_19_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 19,
      "hook_name": "blocks.19.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "71fcb4cf-5f68-4eba-922f-06ae5979e2a7",
    "datetime_epoch_millis": 1737930470578,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.9518812499999999,
        "llm_top_1_test_accuracy": 0.69914375,
        "llm_top_2_test_accuracy": 0.75795625,
        "llm_top_5_test_accuracy": 0.81723125,
        "llm_top_10_test_accuracy": 0.8663375000000001,
        "llm_top_20_test_accuracy": 0.90273125,
        "llm_top_50_test_accuracy": 0.9340875,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9399375487118959,
        "sae_top_1_test_accuracy": 0.7716625,
        "sae_top_2_test_accuracy": 0.81795625,
        "sae_top_5_test_accuracy": 0.86661875,
        "sae_top_10_test_accuracy": 0.8920750000000001,
        "sae_top_20_test_accuracy": 0.9113375000000001,
        "sae_top_50_test_accuracy": 0.92443125,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.9606,
        "llm_top_1_test_accuracy": 0.6564,
        "llm_top_2_test_accuracy": 0.7246,
        "llm_top_5_test_accuracy": 0.8052000000000001,
        "llm_top_10_test_accuracy": 0.8672000000000001,
        "llm_top_20_test_accuracy": 0.9133999999999999,
        "llm_top_50_test_accuracy": 0.9513999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9524000525474549,
        "sae_top_1_test_accuracy": 0.7744000000000001,
        "sae_top_2_test_accuracy": 0.8324,
        "sae_top_5_test_accuracy": 0.8795999999999999,
        "sae_top_10_test_accuracy": 0.9103999999999999,
        "sae_top_20_test_accuracy": 0.9246000000000001,
        "sae_top_50_test_accuracy": 0.9432,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9485999999999999,
        "llm_top_1_test_accuracy": 0.667,
        "llm_top_2_test_accuracy": 0.709,
        "llm_top_5_test_accuracy": 0.7674000000000001,
        "llm_top_10_test_accuracy": 0.8308000000000002,
        "llm_top_20_test_accuracy": 0.8798,
        "llm_top_50_test_accuracy": 0.9238,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9290000557899475,
        "sae_top_1_test_accuracy": 0.7524,
        "sae_top_2_test_accuracy": 0.7918,
        "sae_top_5_test_accuracy": 0.8318,
        "sae_top_10_test_accuracy": 0.8603999999999999,
        "sae_top_20_test_accuracy": 0.8886,
        "sae_top_50_test_accuracy": 0.9126,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.9139999999999999,
        "llm_top_1_test_accuracy": 0.6856,
        "llm_top_2_test_accuracy": 0.7252,
        "llm_top_5_test_accuracy": 0.7811999999999999,
        "llm_top_10_test_accuracy": 0.8384,
        "llm_top_20_test_accuracy": 0.8782,
        "llm_top_50_test_accuracy": 0.9012,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9082000494003296,
        "sae_top_1_test_accuracy": 0.746,
        "sae_top_2_test_accuracy": 0.8211999999999999,
        "sae_top_5_test_accuracy": 0.8404,
        "sae_top_10_test_accuracy": 0.8777999999999999,
        "sae_top_20_test_accuracy": 0.8865999999999999,
        "sae_top_50_test_accuracy": 0.8944000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.9155999999999999,
        "llm_top_1_test_accuracy": 0.6298,
        "llm_top_2_test_accuracy": 0.7021999999999999,
        "llm_top_5_test_accuracy": 0.7524000000000001,
        "llm_top_10_test_accuracy": 0.8013999999999999,
        "llm_top_20_test_accuracy": 0.8586,
        "llm_top_50_test_accuracy": 0.8854,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.884000051021576,
        "sae_top_1_test_accuracy": 0.6953999999999999,
        "sae_top_2_test_accuracy": 0.7667999999999999,
        "sae_top_5_test_accuracy": 0.8211999999999999,
        "sae_top_10_test_accuracy": 0.8353999999999999,
        "sae_top_20_test_accuracy": 0.8503999999999999,
        "sae_top_50_test_accuracy": 0.8640000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9704999999999999,
        "llm_top_1_test_accuracy": 0.697,
        "llm_top_2_test_accuracy": 0.743,
        "llm_top_5_test_accuracy": 0.79,
        "llm_top_10_test_accuracy": 0.86,
        "llm_top_20_test_accuracy": 0.874,
        "llm_top_50_test_accuracy": 0.942,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9520000219345093,
        "sae_top_1_test_accuracy": 0.836,
        "sae_top_2_test_accuracy": 0.864,
        "sae_top_5_test_accuracy": 0.931,
        "sae_top_10_test_accuracy": 0.934,
        "sae_top_20_test_accuracy": 0.935,
        "sae_top_50_test_accuracy": 0.932,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.9645999999999999,
        "llm_top_1_test_accuracy": 0.6319999999999999,
        "llm_top_2_test_accuracy": 0.6843999999999999,
        "llm_top_5_test_accuracy": 0.8046,
        "llm_top_10_test_accuracy": 0.8638,
        "llm_top_20_test_accuracy": 0.9204000000000001,
        "llm_top_50_test_accuracy": 0.9420000000000002,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9616000533103943,
        "sae_top_1_test_accuracy": 0.897,
        "sae_top_2_test_accuracy": 0.9086000000000001,
        "sae_top_5_test_accuracy": 0.9235999999999999,
        "sae_top_10_test_accuracy": 0.9463999999999999,
        "sae_top_20_test_accuracy": 0.9566000000000001,
        "sae_top_50_test_accuracy": 0.9593999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.9417500000000001,
        "llm_top_1_test_accuracy": 0.68775,
        "llm_top_2_test_accuracy": 0.79025,
        "llm_top_5_test_accuracy": 0.84425,
        "llm_top_10_test_accuracy": 0.8704999999999999,
        "llm_top_20_test_accuracy": 0.89825,
        "llm_top_50_test_accuracy": 0.9275,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9395000487565994,
        "sae_top_1_test_accuracy": 0.7595000000000001,
        "sae_top_2_test_accuracy": 0.80925,
        "sae_top_5_test_accuracy": 0.87375,
        "sae_top_10_test_accuracy": 0.8839999999999999,
        "sae_top_20_test_accuracy": 0.9145,
        "sae_top_50_test_accuracy": 0.92725,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 0.9994,
        "llm_top_1_test_accuracy": 0.9376,
        "llm_top_2_test_accuracy": 0.9850000000000001,
        "llm_top_5_test_accuracy": 0.9927999999999999,
        "llm_top_10_test_accuracy": 0.9986,
        "llm_top_20_test_accuracy": 0.9992000000000001,
        "llm_top_50_test_accuracy": 0.9994,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9928000569343567,
        "sae_top_1_test_accuracy": 0.7125999999999999,
        "sae_top_2_test_accuracy": 0.7496,
        "sae_top_5_test_accuracy": 0.8316000000000001,
        "sae_top_10_test_accuracy": 0.8882000000000001,
        "sae_top_20_test_accuracy": 0.9343999999999999,
        "sae_top_50_test_accuracy": 0.9625999999999999,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "968c10e714d56a267c6e55fd44e4abe047973ccb",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_19_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 2304,
      "hook_layer": 19,
      "hook_name": "blocks.19.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "AdaptiveOrthogonal",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "relu",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  }
}