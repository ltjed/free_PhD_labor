\begin{thebibliography}{10}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{Frankle2018TheLT}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv: Learning}, 2018.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Hitzler et~al.(2020)Hitzler, Bianchi, Ebrahimi, and
  Sarker]{Hitzler2020NeuralsymbolicIA}
P.~Hitzler, Federico Bianchi, Monireh Ebrahimi, and Md~Kamruzzaman Sarker.
\newblock Neural-symbolic integration and the semantic web.
\newblock \emph{Semantic Web}, 11:\penalty0 3--11, 2020.

\bibitem[Kreutz-Delgado et~al.(2003)Kreutz-Delgado, Murray, Rao, Engan, Lee,
  and Sejnowski]{Kreutz-Delgado2003DictionaryLA}
K.~Kreutz-Delgado, Joseph~F. Murray, B.~Rao, K.~Engan, Te-Won Lee, and
  T.~Sejnowski.
\newblock Dictionary learning algorithms for sparse representation.
\newblock \emph{Neural Computation}, 15:\penalty0 349--396, 2003.

\bibitem[Maass(2000)]{Maass2000OnTC}
W.~Maass.
\newblock On the computational power of winner-take-all.
\newblock \emph{Neural Computation}, 12:\penalty0 2519--2535, 2000.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and
  Belinkov]{Meng2022LocatingAE}
Kevin Meng, David Bau, A.~Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock 2022.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{Vincent2008ExtractingAC}
Pascal Vincent, H.~Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock pp.\  1096--1103, 2008.

\end{thebibliography}
