\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bowren(2021)]{Bowren2021ASC}
Joshua Bowren.
\newblock A sparse coding interpretation of neural networks and theoretical
  implications.
\newblock \emph{ArXiv}, abs/2108.06622, 2021.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{Burgess2018UnderstandingDI}
Christopher~P. Burgess, I.~Higgins, Arka Pal, L.~Matthey, Nicholas Watters,
  Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in Î²-vae.
\newblock \emph{ArXiv}, abs/1804.03599, 2018.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{Chen2016InfoGANIR}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, I.~Sutskever, and P.~Abbeel.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock pp.\  2172--2180, 2016.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{Cunningham2023SparseAF}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, R.~Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{ArXiv}, abs/2309.08600, 2023.

\bibitem[Eryilmaz \& Dundar(2022)Eryilmaz and
  Dundar]{Eryilmaz2022UnderstandingHO}
S.~Eryilmaz and A.~Dundar.
\newblock Understanding how orthogonality of parameters improves quantization
  of neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  34:\penalty0 10737--10746, 2022.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{Kingma2013AutoEncodingVB}
Diederik~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{CoRR}, abs/1312.6114, 2013.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{Olshausen1996EmergenceOS}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Pascanu et~al.(2012)Pascanu, Mikolov, and Bengio]{Pascanu2012OnTD}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock pp.\  1310--1318, 2012.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Suteu \& Guo(2019)Suteu and Guo]{Suteu2019RegularizingDM}
Mihai Suteu and Yike Guo.
\newblock Regularizing deep multi-task networks using orthogonal gradients.
\newblock \emph{ArXiv}, abs/1912.06844, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhang et~al.(2022)Zhang, Zheng, Zhou, and Lu]{Zhang2022BortTE}
Borui Zhang, Wenzhao Zheng, Jie Zhou, and Jiwen Lu.
\newblock Bort: Towards explainable neural networks with bounded orthogonal
  constraint.
\newblock \emph{ArXiv}, abs/2212.09062, 2022.

\end{thebibliography}
