[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "HierSAE: Hierarchical Sparse Autoencoders for Interpretable Feature Extraction in Language Models",
        "Experiment": "1. Implement two-level hierarchical SAE with coarse and fine features\n2. Add soft tree structure constraints using mutual information\n3. Implement progressive training curriculum (coarse->fine)\n4. Create hierarchical feature visualization dashboard\n5. Compare against baseline SAE using:\n   - Feature recovery metrics\n   - Parent-child mutual information\n   - Downstream task performance\n   - Human interpretability studies",
        "Technical_Details": "Two-level architecture with dimensions d_coarse=2d_in and d_fine=4d_in. Soft tree constraints implemented via mutual information regularizer: L_MI = -I(f_coarse; f_fine). Total loss: L = ||x - x'||^2 + \u03bb_1||f_coarse||_1 + \u03bb_2||f_fine||_1 + \u03bb_MI*L_MI. Training uses curriculum learning: first optimize coarse features, then jointly train both levels with increasing \u03bb_MI. Feature visualization uses t-SNE plots of activation patterns with hierarchical clustering overlay. Parent-child relationships measured using normalized pointwise mutual information (NPMI) between feature activations.",
        "Research_Impact": "Recent work (Elhage et al. 2022, Anthropic) highlights the challenge of feature entanglement in transformer interpretability. Current SAEs often fail to separate features that are semantically distinct but statistically correlated. HierSAE addresses this by explicitly modeling feature relationships through a two-level hierarchy, allowing more natural decomposition of complex features. The soft tree constraints and mutual information metrics provide quantitative ways to assess feature organization quality, while the simplified architecture maintains feasibility.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]