{
  "training results for layer 12": {
    "config": {
      "trainer_class": "TrainerTopK",
      "dict_class": "AutoEncoderTopK",
      "lr": 0.0001885618083164127,
      "steps": 4882,
      "seed": 42,
      "activation_dim": 2304,
      "dict_size": 18432,
      "k": 160,
      "device": "cuda",
      "layer": 12,
      "lm_name": "google/gemma-2-2b",
      "wandb_name": "AutoEncoderTopK",
      "submodule_name": "resid_post_layer_12",
      "ortho_weight": 0.1
    },
    "final_info": {
      "training_steps": 4882,
      "final_loss": 3044.309326171875,
      "layer": 12,
      "dict_size": 18432,
      "learning_rate": 0.0003,
      "sparsity_penalty": 0.04
    }
  },
  "absorption evaluation results": {
    "eval_type_id": "absorption_first_letter",
    "eval_config": {
      "model_name": "google/gemma-2-2b",
      "random_seed": 42,
      "f1_jump_threshold": 0.03,
      "max_k_value": 10,
      "prompt_template": "{word} has the first letter:",
      "prompt_token_pos": -6,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "k_sparse_probe_l1_decay": 0.01,
      "k_sparse_probe_batch_size": 4096,
      "k_sparse_probe_num_epochs": 50
    },
    "eval_id": "c7664078-a7b6-4cbf-a601-a647e422810d",
    "datetime_epoch_millis": 1738280177955,
    "eval_result_metrics": {
      "mean": {
        "mean_absorption_score": 0.0217884822419512,
        "mean_num_split_features": 1.16
      }
    },
    "eval_result_details": [
      {
        "first_letter": "a",
        "absorption_rate": 0.005066250974279034,
        "num_absorption": 13,
        "num_probe_true_positives": 2566,
        "num_split_features": 2
      },
      {
        "first_letter": "b",
        "absorption_rate": 0.001924310455420141,
        "num_absorption": 3,
        "num_probe_true_positives": 1559,
        "num_split_features": 1
      },
      {
        "first_letter": "c",
        "absorption_rate": 0.0007057163020465773,
        "num_absorption": 2,
        "num_probe_true_positives": 2834,
        "num_split_features": 2
      },
      {
        "first_letter": "d",
        "absorption_rate": 0.00424757281553398,
        "num_absorption": 7,
        "num_probe_true_positives": 1648,
        "num_split_features": 1
      },
      {
        "first_letter": "e",
        "absorption_rate": 0.0050031269543464665,
        "num_absorption": 8,
        "num_probe_true_positives": 1599,
        "num_split_features": 1
      },
      {
        "first_letter": "g",
        "absorption_rate": 0.0045662100456621,
        "num_absorption": 5,
        "num_probe_true_positives": 1095,
        "num_split_features": 1
      },
      {
        "first_letter": "h",
        "absorption_rate": 0.00303951367781155,
        "num_absorption": 3,
        "num_probe_true_positives": 987,
        "num_split_features": 1
      },
      {
        "first_letter": "i",
        "absorption_rate": 0.0017964071856287425,
        "num_absorption": 3,
        "num_probe_true_positives": 1670,
        "num_split_features": 1
      },
      {
        "first_letter": "j",
        "absorption_rate": 0.00437636761487965,
        "num_absorption": 2,
        "num_probe_true_positives": 457,
        "num_split_features": 1
      },
      {
        "first_letter": "k",
        "absorption_rate": 0.001392757660167131,
        "num_absorption": 1,
        "num_probe_true_positives": 718,
        "num_split_features": 1
      },
      {
        "first_letter": "l",
        "absorption_rate": 0.0008795074758135445,
        "num_absorption": 1,
        "num_probe_true_positives": 1137,
        "num_split_features": 1
      },
      {
        "first_letter": "m",
        "absorption_rate": 0.006420545746388443,
        "num_absorption": 12,
        "num_probe_true_positives": 1869,
        "num_split_features": 1
      },
      {
        "first_letter": "n",
        "absorption_rate": 0.001128668171557562,
        "num_absorption": 1,
        "num_probe_true_positives": 886,
        "num_split_features": 2
      },
      {
        "first_letter": "o",
        "absorption_rate": 0.22784810126582278,
        "num_absorption": 234,
        "num_probe_true_positives": 1027,
        "num_split_features": 2
      },
      {
        "first_letter": "p",
        "absorption_rate": 0.0035087719298245615,
        "num_absorption": 8,
        "num_probe_true_positives": 2280,
        "num_split_features": 1
      },
      {
        "first_letter": "q",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 178,
        "num_split_features": 1
      },
      {
        "first_letter": "r",
        "absorption_rate": 0.13799283154121864,
        "num_absorption": 231,
        "num_probe_true_positives": 1674,
        "num_split_features": 1
      },
      {
        "first_letter": "s",
        "absorption_rate": 0.0010252904989747095,
        "num_absorption": 3,
        "num_probe_true_positives": 2926,
        "num_split_features": 1
      },
      {
        "first_letter": "t",
        "absorption_rate": 0.0024024024024024023,
        "num_absorption": 4,
        "num_probe_true_positives": 1665,
        "num_split_features": 1
      },
      {
        "first_letter": "u",
        "absorption_rate": 0.07015130674002751,
        "num_absorption": 51,
        "num_probe_true_positives": 727,
        "num_split_features": 1
      },
      {
        "first_letter": "v",
        "absorption_rate": 0.0024154589371980675,
        "num_absorption": 2,
        "num_probe_true_positives": 828,
        "num_split_features": 1
      },
      {
        "first_letter": "w",
        "absorption_rate": 0.005934718100890208,
        "num_absorption": 4,
        "num_probe_true_positives": 674,
        "num_split_features": 1
      },
      {
        "first_letter": "x",
        "absorption_rate": 0.012345679012345678,
        "num_absorption": 1,
        "num_probe_true_positives": 81,
        "num_split_features": 1
      },
      {
        "first_letter": "y",
        "absorption_rate": 0.0,
        "num_absorption": 0,
        "num_probe_true_positives": 149,
        "num_split_features": 1
      },
      {
        "first_letter": "z",
        "absorption_rate": 0.04054054054054054,
        "num_absorption": 9,
        "num_probe_true_positives": 222,
        "num_split_features": 1
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 18432,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "TopK",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "TopK",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "core evaluation results": {
    "unique_id": "google/gemma-2-2b_layer_12_sae_custom_sae",
    "sae_set": "google/gemma-2-2b_layer_12_sae",
    "sae_id": "custom_sae",
    "eval_cfg": {
      "model_name": "google/gemma-2-2b",
      "llm_dtype": "bfloat16",
      "batch_size_prompts": 16,
      "n_eval_reconstruction_batches": 200,
      "n_eval_sparsity_variance_batches": 2000,
      "dataset": "Skylion007/openwebtext",
      "context_size": 128,
      "compute_kl": true,
      "compute_ce_loss": true,
      "compute_l2_norms": true,
      "compute_sparsity_metrics": true,
      "compute_variance_metrics": true,
      "compute_featurewise_density_statistics": false,
      "compute_featurewise_weight_based_metrics": false,
      "exclude_special_tokens_from_reconstruction": true,
      "verbose": false
    },
    "metrics": {
      "model_behavior_preservation": {
        "kl_div_score": 0.9864130434782609,
        "kl_div_with_ablation": 10.0625,
        "kl_div_with_sae": 0.13671875
      },
      "model_performance_preservation": {
        "ce_loss_score": 0.9868421052631579,
        "ce_loss_with_ablation": 12.4375,
        "ce_loss_with_sae": 3.0625,
        "ce_loss_without_sae": 2.9375
      },
      "reconstruction_quality": {
        "explained_variance": 0.7265625,
        "mse": 1.71875,
        "cossim": 0.91796875
      },
      "shrinkage": {
        "l2_norm_in": 149.0,
        "l2_norm_out": 137.0,
        "l2_ratio": 0.9140625,
        "relative_reconstruction_bias": 1.0
      },
      "sparsity": {
        "l0": 160.0,
        "l1": 676.0
      },
      "token_stats": {
        "total_tokens_eval_reconstruction": 409600,
        "total_tokens_eval_sparsity_variance": 4096000
      }
    }
  },
  "scr evaluations results": {
    "eval_type_id": "scr",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": true,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "39907fd9-cb66-4cdf-8033-fdaf5da7bd6d",
    "datetime_epoch_millis": 1738282392612,
    "eval_result_metrics": {
      "scr_metrics": {
        "scr_dir1_threshold_2": 0.23056695649332098,
        "scr_metric_threshold_2": 0.11365828019170202,
        "scr_dir2_threshold_2": 0.11489987599653104,
        "scr_dir1_threshold_5": 0.25611521549262817,
        "scr_metric_threshold_5": 0.17500322279550776,
        "scr_dir2_threshold_5": 0.1732609070898092,
        "scr_dir1_threshold_10": 0.26267066577523407,
        "scr_metric_threshold_10": 0.2265787989645937,
        "scr_dir2_threshold_10": 0.22250666164282235,
        "scr_dir1_threshold_20": 0.24526542979007093,
        "scr_metric_threshold_20": 0.2837691195194172,
        "scr_dir2_threshold_20": 0.28040972579363094,
        "scr_dir1_threshold_50": 0.23689533496298865,
        "scr_metric_threshold_50": 0.32044169439589076,
        "scr_dir2_threshold_50": 0.3223806879047685,
        "scr_dir1_threshold_100": 0.11628465274619314,
        "scr_metric_threshold_100": 0.36726687953066145,
        "scr_dir2_threshold_100": 0.36104115852457136,
        "scr_dir1_threshold_500": -0.05245434317824951,
        "scr_metric_threshold_500": 0.2572990938321963,
        "scr_dir2_threshold_500": 0.266902117434121
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_professor_nurse_results",
        "scr_dir1_threshold_2": 0.4843747235133985,
        "scr_metric_threshold_2": 0.01474211549298745,
        "scr_dir2_threshold_2": 0.01474211549298745,
        "scr_dir1_threshold_5": 0.45312510186348476,
        "scr_metric_threshold_5": 0.03439802540241183,
        "scr_dir2_threshold_5": 0.03439802540241183,
        "scr_dir1_threshold_10": 0.46874991268844163,
        "scr_metric_threshold_10": 0.039312112716319424,
        "scr_dir2_threshold_10": 0.039312112716319424,
        "scr_dir1_threshold_20": 0.37500011641541114,
        "scr_metric_threshold_20": 0.05159711132798542,
        "scr_dir2_threshold_20": 0.05159711132798542,
        "scr_dir1_threshold_50": 0.23437495634422081,
        "scr_metric_threshold_50": 0.11793619170022296,
        "scr_dir2_threshold_50": 0.11793619170022296,
        "scr_dir1_threshold_100": 0.2812493888190914,
        "scr_metric_threshold_100": 0.1277642198793028,
        "scr_dir2_threshold_100": 0.1277642198793028,
        "scr_dir1_threshold_500": 0.17187478172110407,
        "scr_metric_threshold_500": 0.18673224250504664,
        "scr_dir2_threshold_500": 0.18673224250504664
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_architect_journalist_results",
        "scr_dir1_threshold_2": 0.2524268642223358,
        "scr_metric_threshold_2": 0.12607442075894992,
        "scr_dir2_threshold_2": 0.12607442075894992,
        "scr_dir1_threshold_5": 0.29126189995330004,
        "scr_metric_threshold_5": 0.16905449606916736,
        "scr_dir2_threshold_5": 0.16905449606916736,
        "scr_dir1_threshold_10": 0.29126189995330004,
        "scr_metric_threshold_10": 0.25501430511568424,
        "scr_dir2_threshold_10": 0.25501430511568424,
        "scr_dir1_threshold_20": 0.2038832142301191,
        "scr_metric_threshold_20": 0.3151863080778133,
        "scr_dir2_threshold_20": 0.3151863080778133,
        "scr_dir1_threshold_50": 0.16504817849915482,
        "scr_metric_threshold_50": 0.39541553150267933,
        "scr_dir2_threshold_50": 0.39541553150267933,
        "scr_dir1_threshold_100": 0.14563094997665002,
        "scr_metric_threshold_100": 0.4412607288367632,
        "scr_dir2_threshold_100": 0.4412607288367632,
        "scr_dir1_threshold_500": -0.5922335715397617,
        "scr_metric_threshold_500": 0.002865292810825434,
        "scr_dir2_threshold_500": 0.002865292810825434
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results",
        "scr_dir1_threshold_2": 0.5714285714285714,
        "scr_metric_threshold_2": 0.022784816238899015,
        "scr_dir2_threshold_2": 0.022784816238899015,
        "scr_dir1_threshold_5": 0.6190479344159929,
        "scr_metric_threshold_5": 0.058227846955206435,
        "scr_dir2_threshold_5": 0.058227846955206435,
        "scr_dir1_threshold_10": 0.6031751287885597,
        "scr_metric_threshold_10": 0.09113926495559606,
        "scr_dir2_threshold_10": 0.09113926495559606,
        "scr_dir1_threshold_20": 0.6190479344159929,
        "scr_metric_threshold_20": 0.12151907024006786,
        "scr_dir2_threshold_20": 0.12151907024006786,
        "scr_dir1_threshold_50": 0.49206359718628334,
        "scr_metric_threshold_50": 0.1620253263882109,
        "scr_dir2_threshold_50": 0.1620253263882109,
        "scr_dir1_threshold_100": 0.0,
        "scr_metric_threshold_100": 0.23544315143456293,
        "scr_dir2_threshold_100": 0.23544315143456293,
        "scr_dir1_threshold_500": -0.7301585199131475,
        "scr_metric_threshold_500": 0.04556963247779803,
        "scr_dir2_threshold_500": 0.04556963247779803
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results",
        "scr_dir1_threshold_2": 0.2656252619344312,
        "scr_metric_threshold_2": 0.12759656040532572,
        "scr_dir2_threshold_2": 0.12759656040532572,
        "scr_dir1_threshold_5": 0.28125005820765137,
        "scr_metric_threshold_5": 0.16617226060069454,
        "scr_dir2_threshold_5": 0.16617226060069454,
        "scr_dir1_threshold_10": 0.1250002328306055,
        "scr_metric_threshold_10": 0.20474778392768772,
        "scr_dir2_threshold_10": 0.20474778392768772,
        "scr_dir1_threshold_20": 0.06250011641530274,
        "scr_metric_threshold_20": 0.27002972449920604,
        "scr_dir2_threshold_20": 0.27002972449920604,
        "scr_dir1_threshold_50": 0.1171878346939954,
        "scr_metric_threshold_50": 0.3649852261845848,
        "scr_dir2_threshold_50": 0.3649852261845848,
        "scr_dir1_threshold_100": -0.07812491268852294,
        "scr_metric_threshold_100": 0.40949556785537544,
        "scr_dir2_threshold_100": 0.40949556785537544,
        "scr_dir1_threshold_500": 0.3671878346939954,
        "scr_metric_threshold_500": -0.026706240376149513,
        "scr_dir2_threshold_500": -0.026706240376149513
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results",
        "scr_dir1_threshold_2": 0.04918033854781611,
        "scr_metric_threshold_2": 0.2549018966193889,
        "scr_dir2_threshold_2": 0.2549018966193889,
        "scr_dir1_threshold_5": 0.04371592886659017,
        "scr_metric_threshold_5": 0.4392157806886135,
        "scr_dir2_threshold_5": 0.4392157806886135,
        "scr_dir1_threshold_10": 0.016393554752069144,
        "scr_metric_threshold_10": 0.5725490049415746,
        "scr_dir2_threshold_10": 0.5725490049415746,
        "scr_dir1_threshold_20": -0.03825151918536423,
        "scr_metric_threshold_20": 0.674509857086792,
        "scr_dir2_threshold_20": 0.674509857086792,
        "scr_dir1_threshold_50": -0.016393554752069144,
        "scr_metric_threshold_50": 0.7019607118990246,
        "scr_dir2_threshold_50": 0.7019607118990246,
        "scr_dir1_threshold_100": -0.22404372830578545,
        "scr_metric_threshold_100": 0.7411763523414453,
        "scr_dir2_threshold_100": 0.7411763523414453,
        "scr_dir1_threshold_500": -0.26775965717237565,
        "scr_metric_threshold_500": 0.7490196206761223,
        "scr_dir2_threshold_500": 0.7490196206761223
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results",
        "scr_dir1_threshold_2": 0.06666642213478169,
        "scr_metric_threshold_2": 0.0927419335456348,
        "scr_dir2_threshold_2": 0.0927419335456348,
        "scr_dir1_threshold_5": 0.08205114567773082,
        "scr_metric_threshold_5": 0.11693564862104187,
        "scr_dir2_threshold_5": 0.11693564862104187,
        "scr_dir1_threshold_10": 0.19487173844135988,
        "scr_metric_threshold_10": 0.13306447154960044,
        "scr_dir2_threshold_10": 0.13306447154960044,
        "scr_dir1_threshold_20": 0.2666666055336954,
        "scr_metric_threshold_20": 0.2137097878988163,
        "scr_dir2_threshold_20": 0.2137097878988163,
        "scr_dir1_threshold_50": 0.3128204704976866,
        "scr_metric_threshold_50": 0.1572581866250075,
        "scr_dir2_threshold_50": 0.1572581866250075,
        "scr_dir1_threshold_100": 0.30769233120498896,
        "scr_metric_threshold_100": 0.20967734182539205,
        "scr_dir2_threshold_100": 0.20967734182539205,
        "scr_dir1_threshold_500": 0.32307674908308187,
        "scr_metric_threshold_500": 0.2661291834404855,
        "scr_dir2_threshold_500": 0.2661291834404855
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results",
        "scr_dir1_threshold_2": 0.10762327403784601,
        "scr_metric_threshold_2": 0.22321430234504325,
        "scr_dir2_threshold_2": 0.22321430234504325,
        "scr_dir1_threshold_5": 0.18834079638758416,
        "scr_metric_threshold_5": 0.3258928654582359,
        "scr_dir2_threshold_5": 0.3258928654582359,
        "scr_dir1_threshold_10": 0.26457380405145886,
        "scr_metric_threshold_10": 0.3794643938142097,
        "scr_dir2_threshold_10": 0.3794643938142097,
        "scr_dir1_threshold_20": 0.3273543367994015,
        "scr_metric_threshold_20": 0.47767846332864744,
        "scr_dir2_threshold_20": 0.47767846332864744,
        "scr_dir1_threshold_50": 0.44843035303859413,
        "scr_metric_threshold_50": 0.5223215366713525,
        "scr_dir2_threshold_50": 0.5223215366713525,
        "scr_dir1_threshold_100": 0.30044831782587905,
        "scr_metric_threshold_100": 0.5758927989352057,
        "scr_dir2_threshold_100": 0.5758927989352057,
        "scr_dir1_threshold_500": 0.08520176975018703,
        "scr_metric_threshold_500": 0.6116071511725216,
        "scr_dir2_threshold_500": 0.6116071511725216
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results",
        "scr_dir1_threshold_2": 0.047210196127387125,
        "scr_metric_threshold_2": 0.047210196127387125,
        "scr_dir2_threshold_2": 0.05714296256601923,
        "scr_dir1_threshold_5": 0.09012885856869063,
        "scr_metric_threshold_5": 0.09012885856869063,
        "scr_dir2_threshold_5": 0.07619033292310208,
        "scr_dir1_threshold_10": 0.13733905469607777,
        "scr_metric_threshold_10": 0.13733905469607777,
        "scr_dir2_threshold_10": 0.10476195612190681,
        "scr_dir1_threshold_20": 0.1459226336960092,
        "scr_metric_threshold_20": 0.1459226336960092,
        "scr_dir2_threshold_20": 0.11904748388971893,
        "scr_dir1_threshold_50": 0.1416308441960435,
        "scr_metric_threshold_50": 0.1416308441960435,
        "scr_dir2_threshold_50": 0.1571427922670651,
        "scr_dir1_threshold_100": 0.19742487513724416,
        "scr_metric_threshold_100": 0.19742487513724416,
        "scr_dir2_threshold_100": 0.14761910708852366,
        "scr_dir1_threshold_500": 0.22317586795092056,
        "scr_metric_threshold_500": 0.22317586795092056,
        "scr_dir2_threshold_500": 0.300000056766318
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 18432,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "TopK",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "TopK",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "tpp evaluations results": {
    "eval_type_id": "tpp",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "canrager/amazon_reviews_mcauley_1and5"
      ],
      "perform_scr": false,
      "early_stopping_patience": 20,
      "train_set_size": 4000,
      "test_set_size": 1000,
      "context_length": 128,
      "probe_train_batch_size": 16,
      "probe_test_batch_size": 500,
      "probe_epochs": 20,
      "probe_lr": 0.001,
      "probe_l1_penalty": 0.001,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "lower_vram_usage": false,
      "model_name": "google/gemma-2-2b",
      "n_values": [
        2,
        5,
        10,
        20,
        50,
        100,
        500
      ],
      "column1_vals_lookup": {
        "LabHC/bias_in_bios_class_set1": [
          [
            "professor",
            "nurse"
          ],
          [
            "architect",
            "journalist"
          ],
          [
            "surgeon",
            "psychologist"
          ],
          [
            "attorney",
            "teacher"
          ]
        ],
        "canrager/amazon_reviews_mcauley_1and5": [
          [
            "Books",
            "CDs_and_Vinyl"
          ],
          [
            "Software",
            "Electronics"
          ],
          [
            "Pet_Supplies",
            "Office_Products"
          ],
          [
            "Industrial_and_Scientific",
            "Toys_and_Games"
          ]
        ]
      }
    },
    "eval_id": "8a5975bc-a72b-47e9-8386-7d9f863ae68f",
    "datetime_epoch_millis": 1738282863038,
    "eval_result_metrics": {
      "tpp_metrics": {
        "tpp_threshold_2_total_metric": 0.004700000584125518,
        "tpp_threshold_2_intended_diff_only": 0.007700002193450928,
        "tpp_threshold_2_unintended_diff_only": 0.003000001609325409,
        "tpp_threshold_5_total_metric": 0.009024998545646668,
        "tpp_threshold_5_intended_diff_only": 0.012400001287460327,
        "tpp_threshold_5_unintended_diff_only": 0.0033750027418136598,
        "tpp_threshold_10_total_metric": 0.012399999797344206,
        "tpp_threshold_10_intended_diff_only": 0.018199998140335082,
        "tpp_threshold_10_unintended_diff_only": 0.005799998342990875,
        "tpp_threshold_20_total_metric": 0.02667499929666519,
        "tpp_threshold_20_intended_diff_only": 0.03390000462532043,
        "tpp_threshold_20_unintended_diff_only": 0.007225005328655242,
        "tpp_threshold_50_total_metric": 0.0714500054717064,
        "tpp_threshold_50_intended_diff_only": 0.07880000472068788,
        "tpp_threshold_50_unintended_diff_only": 0.007349999248981476,
        "tpp_threshold_100_total_metric": 0.13479999899864198,
        "tpp_threshold_100_intended_diff_only": 0.14449999928474427,
        "tpp_threshold_100_unintended_diff_only": 0.009700000286102295,
        "tpp_threshold_500_total_metric": 0.38912501782178877,
        "tpp_threshold_500_intended_diff_only": 0.4045000195503235,
        "tpp_threshold_500_unintended_diff_only": 0.015375001728534697
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_tpp_results",
        "tpp_threshold_2_total_metric": 0.004600000381469726,
        "tpp_threshold_2_intended_diff_only": 0.007600009441375732,
        "tpp_threshold_2_unintended_diff_only": 0.003000009059906006,
        "tpp_threshold_5_total_metric": 0.008649986982345582,
        "tpp_threshold_5_intended_diff_only": 0.011399996280670167,
        "tpp_threshold_5_unintended_diff_only": 0.002750009298324585,
        "tpp_threshold_10_total_metric": 0.015000012516975402,
        "tpp_threshold_10_intended_diff_only": 0.018800008296966552,
        "tpp_threshold_10_unintended_diff_only": 0.00379999577999115,
        "tpp_threshold_20_total_metric": 0.028250008821487427,
        "tpp_threshold_20_intended_diff_only": 0.03300001621246338,
        "tpp_threshold_20_unintended_diff_only": 0.004750007390975952,
        "tpp_threshold_50_total_metric": 0.07850000262260437,
        "tpp_threshold_50_intended_diff_only": 0.08280000686645508,
        "tpp_threshold_50_unintended_diff_only": 0.004300004243850708,
        "tpp_threshold_100_total_metric": 0.15100001394748688,
        "tpp_threshold_100_intended_diff_only": 0.1570000171661377,
        "tpp_threshold_100_unintended_diff_only": 0.006000003218650818,
        "tpp_threshold_500_total_metric": 0.4311500072479248,
        "tpp_threshold_500_intended_diff_only": 0.4404000163078308,
        "tpp_threshold_500_unintended_diff_only": 0.009250009059906006
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_tpp_results",
        "tpp_threshold_2_total_metric": 0.004800000786781311,
        "tpp_threshold_2_intended_diff_only": 0.007799994945526123,
        "tpp_threshold_2_unintended_diff_only": 0.002999994158744812,
        "tpp_threshold_5_total_metric": 0.009400010108947754,
        "tpp_threshold_5_intended_diff_only": 0.013400006294250488,
        "tpp_threshold_5_unintended_diff_only": 0.0039999961853027345,
        "tpp_threshold_10_total_metric": 0.009799987077713013,
        "tpp_threshold_10_intended_diff_only": 0.017599987983703613,
        "tpp_threshold_10_unintended_diff_only": 0.007800000905990601,
        "tpp_threshold_20_total_metric": 0.025099989771842957,
        "tpp_threshold_20_intended_diff_only": 0.03479999303817749,
        "tpp_threshold_20_unintended_diff_only": 0.009700003266334533,
        "tpp_threshold_50_total_metric": 0.06440000832080842,
        "tpp_threshold_50_intended_diff_only": 0.07480000257492066,
        "tpp_threshold_50_unintended_diff_only": 0.010399994254112244,
        "tpp_threshold_100_total_metric": 0.11859998404979707,
        "tpp_threshold_100_intended_diff_only": 0.13199998140335084,
        "tpp_threshold_100_unintended_diff_only": 0.013399997353553772,
        "tpp_threshold_500_total_metric": 0.3471000283956528,
        "tpp_threshold_500_intended_diff_only": 0.3686000227928162,
        "tpp_threshold_500_unintended_diff_only": 0.02149999439716339
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 18432,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "TopK",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "TopK",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "sparse probing evaluation results": {
    "eval_type_id": "sparse_probing",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "LabHC/bias_in_bios_class_set1",
        "LabHC/bias_in_bios_class_set2",
        "LabHC/bias_in_bios_class_set3",
        "canrager/amazon_reviews_mcauley_1and5",
        "canrager/amazon_reviews_mcauley_1and5_sentiment",
        "codeparrot/github-code",
        "fancyzhx/ag_news",
        "Helsinki-NLP/europarl"
      ],
      "probe_train_set_size": 4000,
      "probe_test_set_size": 1000,
      "context_length": 128,
      "sae_batch_size": 125,
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16",
      "model_name": "google/gemma-2-2b",
      "k_values": [
        1,
        2,
        5,
        10,
        20,
        50
      ],
      "lower_vram_usage": false
    },
    "eval_id": "2c545414-32ae-4138-8981-95c15eb15a52",
    "datetime_epoch_millis": 1738283943745,
    "eval_result_metrics": {
      "llm": {
        "llm_test_accuracy": 0.95098125,
        "llm_top_1_test_accuracy": 0.6566687499999999,
        "llm_top_2_test_accuracy": 0.719425,
        "llm_top_5_test_accuracy": 0.77980625,
        "llm_top_10_test_accuracy": 0.82968125,
        "llm_top_20_test_accuracy": 0.8788375,
        "llm_top_50_test_accuracy": 0.92123125,
        "llm_top_100_test_accuracy": null
      },
      "sae": {
        "sae_test_accuracy": 0.9585187990218401,
        "sae_top_1_test_accuracy": 0.7962374999999999,
        "sae_top_2_test_accuracy": 0.8497999999999999,
        "sae_top_5_test_accuracy": 0.8918875,
        "sae_top_10_test_accuracy": 0.90980625,
        "sae_top_20_test_accuracy": 0.92666875,
        "sae_top_50_test_accuracy": 0.94339375,
        "sae_top_100_test_accuracy": null
      }
    },
    "eval_result_details": [
      {
        "dataset_name": "LabHC/bias_in_bios_class_set1_results",
        "llm_test_accuracy": 0.9596,
        "llm_top_1_test_accuracy": 0.6432,
        "llm_top_2_test_accuracy": 0.6916,
        "llm_top_5_test_accuracy": 0.7912,
        "llm_top_10_test_accuracy": 0.8336,
        "llm_top_20_test_accuracy": 0.8962,
        "llm_top_50_test_accuracy": 0.9384,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.969800066947937,
        "sae_top_1_test_accuracy": 0.829,
        "sae_top_2_test_accuracy": 0.86,
        "sae_top_5_test_accuracy": 0.8754,
        "sae_top_10_test_accuracy": 0.9179999999999999,
        "sae_top_20_test_accuracy": 0.9432,
        "sae_top_50_test_accuracy": 0.9544,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set2_results",
        "llm_test_accuracy": 0.9434000000000001,
        "llm_top_1_test_accuracy": 0.6712,
        "llm_top_2_test_accuracy": 0.7158,
        "llm_top_5_test_accuracy": 0.7654,
        "llm_top_10_test_accuracy": 0.8102,
        "llm_top_20_test_accuracy": 0.8645999999999999,
        "llm_top_50_test_accuracy": 0.9067999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9544000387191772,
        "sae_top_1_test_accuracy": 0.7958000000000001,
        "sae_top_2_test_accuracy": 0.8234,
        "sae_top_5_test_accuracy": 0.8894,
        "sae_top_10_test_accuracy": 0.901,
        "sae_top_20_test_accuracy": 0.9254,
        "sae_top_50_test_accuracy": 0.9458,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "LabHC/bias_in_bios_class_set3_results",
        "llm_test_accuracy": 0.9132,
        "llm_top_1_test_accuracy": 0.6925999999999999,
        "llm_top_2_test_accuracy": 0.7328,
        "llm_top_5_test_accuracy": 0.7602,
        "llm_top_10_test_accuracy": 0.8005999999999999,
        "llm_top_20_test_accuracy": 0.8542,
        "llm_top_50_test_accuracy": 0.885,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9326000571250915,
        "sae_top_1_test_accuracy": 0.7724,
        "sae_top_2_test_accuracy": 0.8030000000000002,
        "sae_top_5_test_accuracy": 0.8774000000000001,
        "sae_top_10_test_accuracy": 0.8868,
        "sae_top_20_test_accuracy": 0.8996000000000001,
        "sae_top_50_test_accuracy": 0.9092,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_results",
        "llm_test_accuracy": 0.9034000000000001,
        "llm_top_1_test_accuracy": 0.6052,
        "llm_top_2_test_accuracy": 0.643,
        "llm_top_5_test_accuracy": 0.6682,
        "llm_top_10_test_accuracy": 0.7338,
        "llm_top_20_test_accuracy": 0.8078,
        "llm_top_50_test_accuracy": 0.8598000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9180000305175782,
        "sae_top_1_test_accuracy": 0.746,
        "sae_top_2_test_accuracy": 0.7909999999999999,
        "sae_top_5_test_accuracy": 0.8256,
        "sae_top_10_test_accuracy": 0.8392,
        "sae_top_20_test_accuracy": 0.8630000000000001,
        "sae_top_50_test_accuracy": 0.8926000000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "canrager/amazon_reviews_mcauley_1and5_sentiment_results",
        "llm_test_accuracy": 0.9815,
        "llm_top_1_test_accuracy": 0.671,
        "llm_top_2_test_accuracy": 0.724,
        "llm_top_5_test_accuracy": 0.766,
        "llm_top_10_test_accuracy": 0.826,
        "llm_top_20_test_accuracy": 0.847,
        "llm_top_50_test_accuracy": 0.933,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9760000705718994,
        "sae_top_1_test_accuracy": 0.878,
        "sae_top_2_test_accuracy": 0.923,
        "sae_top_5_test_accuracy": 0.93,
        "sae_top_10_test_accuracy": 0.948,
        "sae_top_20_test_accuracy": 0.961,
        "sae_top_50_test_accuracy": 0.969,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "codeparrot/github-code_results",
        "llm_test_accuracy": 0.967,
        "llm_top_1_test_accuracy": 0.6614,
        "llm_top_2_test_accuracy": 0.6950000000000001,
        "llm_top_5_test_accuracy": 0.7649999999999999,
        "llm_top_10_test_accuracy": 0.8051999999999999,
        "llm_top_20_test_accuracy": 0.8684,
        "llm_top_50_test_accuracy": 0.9263999999999999,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9702000498771668,
        "sae_top_1_test_accuracy": 0.6526000000000001,
        "sae_top_2_test_accuracy": 0.8168,
        "sae_top_5_test_accuracy": 0.8646,
        "sae_top_10_test_accuracy": 0.8817999999999999,
        "sae_top_20_test_accuracy": 0.908,
        "sae_top_50_test_accuracy": 0.9507999999999999,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "fancyzhx/ag_news_results",
        "llm_test_accuracy": 0.9397500000000001,
        "llm_top_1_test_accuracy": 0.66875,
        "llm_top_2_test_accuracy": 0.773,
        "llm_top_5_test_accuracy": 0.82525,
        "llm_top_10_test_accuracy": 0.8672499999999999,
        "llm_top_20_test_accuracy": 0.9025000000000001,
        "llm_top_50_test_accuracy": 0.92225,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9487500339746475,
        "sae_top_1_test_accuracy": 0.7935000000000001,
        "sae_top_2_test_accuracy": 0.842,
        "sae_top_5_test_accuracy": 0.8885000000000001,
        "sae_top_10_test_accuracy": 0.90925,
        "sae_top_20_test_accuracy": 0.91675,
        "sae_top_50_test_accuracy": 0.9277500000000001,
        "sae_top_100_test_accuracy": null
      },
      {
        "dataset_name": "Helsinki-NLP/europarl_results",
        "llm_test_accuracy": 1.0,
        "llm_top_1_test_accuracy": 0.64,
        "llm_top_2_test_accuracy": 0.7802,
        "llm_top_5_test_accuracy": 0.8972,
        "llm_top_10_test_accuracy": 0.9608000000000001,
        "llm_top_20_test_accuracy": 0.99,
        "llm_top_50_test_accuracy": 0.9982000000000001,
        "llm_top_100_test_accuracy": null,
        "sae_test_accuracy": 0.9984000444412231,
        "sae_top_1_test_accuracy": 0.9026,
        "sae_top_2_test_accuracy": 0.9391999999999999,
        "sae_top_5_test_accuracy": 0.9842000000000001,
        "sae_top_10_test_accuracy": 0.9944,
        "sae_top_20_test_accuracy": 0.9964000000000001,
        "sae_top_50_test_accuracy": 0.9975999999999999,
        "sae_top_100_test_accuracy": null
      }
    ],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 18432,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "TopK",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "TopK",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  },
  "unlearning evaluation results": {
    "eval_type_id": "unlearning",
    "eval_config": {
      "random_seed": 42,
      "dataset_names": [
        "wmdp-bio",
        "high_school_us_history",
        "college_computer_science",
        "high_school_geography",
        "human_aging"
      ],
      "intervention_method": "clamp_feature_activation",
      "retain_thresholds": [
        0.001,
        0.005,
        0.01,
        0.02
      ],
      "n_features_list": [
        10,
        100,
        1000
      ],
      "multipliers": [
        10,
        25,
        50,
        100,
        200
      ],
      "dataset_size": 1024,
      "seq_len": 1024,
      "n_batch_loss_added": 50,
      "target_metric": "correct",
      "save_metrics": true,
      "model_name": "google/gemma-2-2b-it",
      "llm_batch_size": 32,
      "llm_dtype": "bfloat16"
    },
    "eval_id": "05e63d49-faa4-4ee4-8a2b-65b9a26c87b7",
    "datetime_epoch_millis": 1738284602129,
    "eval_result_metrics": {
      "unlearning": {
        "unlearning_score": 0.03745317459106445
      }
    },
    "eval_result_details": [],
    "sae_bench_commit_hash": "1c64f65e68c0e3cdaf116a205182859fd51f77f7",
    "sae_lens_id": "custom_sae",
    "sae_lens_release_id": "google/gemma-2-2b_layer_12_sae",
    "sae_lens_version": "5.3.0",
    "sae_cfg_dict": {
      "model_name": "google/gemma-2-2b",
      "d_in": 2304,
      "d_sae": 18432,
      "hook_layer": 12,
      "hook_name": "blocks.12.hook_resid_post",
      "context_size": null,
      "hook_head_index": null,
      "architecture": "TopK",
      "apply_b_dec_to_input": true,
      "finetuning_scaling_factor": null,
      "activation_fn_str": "TopK",
      "prepend_bos": true,
      "normalize_activations": "none",
      "dtype": "bfloat16",
      "device": "",
      "dataset_path": "",
      "dataset_trust_remote_code": true,
      "seqpos_slice": [
        null
      ],
      "training_tokens": -100000,
      "sae_lens_training_version": null,
      "neuronpedia_id": null
    },
    "eval_result_unstructured": null
  }
}