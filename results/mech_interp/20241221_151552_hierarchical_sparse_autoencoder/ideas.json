[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Two-Level Hierarchical Sparse Autoencoders for Interpretable Feature Discovery in Language Models",
        "Experiment": "1. Implement two-layer sparse autoencoder:\n   - First layer: dense feature extraction\n   - Second layer: sparse abstract features\n2. Three-phase training curriculum:\n   - Lower layer pretraining\n   - Upper layer training (frozen lower)\n   - Full model fine-tuning\n3. Loss components:\n   - Progressive L1 penalties (\u03b2\u2081 < \u03b2\u2082)\n   - Multi-level reconstruction loss\n   - Cosine similarity grouping\n   - L2,1 group sparsity\n4. Evaluation metrics:\n   - Mutual information between layers\n   - Feature clustering analysis\n   - Human interpretability studies\n   - Causal tracing experiments\n5. Comparative analysis:\n   - Against flat autoencoders\n   - Against traditional clustering\n   - Ablation studies per component\n6. Case studies with specific examples",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": true
    }
]