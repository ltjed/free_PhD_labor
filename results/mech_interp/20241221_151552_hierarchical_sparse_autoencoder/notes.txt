# Title: Two-Level Hierarchical Sparse Autoencoders for Interpretable Feature Discovery in Language Models
# Experiment description: 1. Implement two-layer sparse autoencoder:
   - First layer: dense feature extraction
   - Second layer: sparse abstract features
2. Three-phase training curriculum:
   - Lower layer pretraining
   - Upper layer training (frozen lower)
   - Full model fine-tuning
3. Loss components:
   - Progressive L1 penalties (β₁ < β₂)
   - Multi-level reconstruction loss
   - Cosine similarity grouping
   - L2,1 group sparsity
4. Evaluation metrics:
   - Mutual information between layers
   - Feature clustering analysis
   - Human interpretability studies
   - Causal tracing experiments
5. Comparative analysis:
   - Against flat autoencoders
   - Against traditional clustering
   - Ablation studies per component
6. Case studies with specific examples
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
