# Title: Two-Level Hierarchical Sparse Autoencoders with Inter-Layer Consistency for Interpretable Language Model Analysis
# Experiment description: 1. Implement two-layer sparse autoencoder with:
   - Different sparsity levels for each layer
   - Gated connections between layers
   - Inter-layer consistency loss
2. Training procedure:
   - Joint training with carefully scaled loss components
   - Dynamic sparsity adjustment
   - Gradient balance between layers
3. Evaluation metrics:
   - Feature clustering coherence scores
   - Layer-wise activation sparsity patterns
   - Cross-layer feature composition analysis
   - Semantic similarity between connected features
4. Ablation studies on:
   - Inter-layer consistency strength
   - Gating mechanisms
   - Sparsity ratios between layers
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
