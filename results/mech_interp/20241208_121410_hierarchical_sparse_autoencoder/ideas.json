[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Two-Level Hierarchical Sparse Autoencoders with Inter-Layer Consistency for Interpretable Language Model Analysis",
        "Experiment": "1. Implement two-layer sparse autoencoder with:\n   - Different sparsity levels for each layer\n   - Gated connections between layers\n   - Inter-layer consistency loss\n2. Training procedure:\n   - Joint training with carefully scaled loss components\n   - Dynamic sparsity adjustment\n   - Gradient balance between layers\n3. Evaluation metrics:\n   - Feature clustering coherence scores\n   - Layer-wise activation sparsity patterns\n   - Cross-layer feature composition analysis\n   - Semantic similarity between connected features\n4. Ablation studies on:\n   - Inter-layer consistency strength\n   - Gating mechanisms\n   - Sparsity ratios between layers",
        "Interestingness": 8,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]