# Title: Temporal-Aware Sparse Autoencoders for Sequence-Level Feature Disentanglement in Language Models
# Experiment description: 1. Modify autoencoder architecture:
   - Add lightweight temporal attention layer (single-head)
   - Implement sliding window context (k=5 positions)
   - Add temporal consistency loss using cosine similarity
2. Training curriculum:
   - Stage 1: Train standard sparse AE (10 epochs)
   - Stage 2: Introduce temporal attention (5 epochs)
   - Stage 3: Add temporal consistency loss (5 epochs)
3. Implementation details:
   - Modify SparseAutoencoder class to include TemporalAttention module
   - Add position embeddings to input features
   - Implement efficient batch processing for temporal windows
4. Evaluation metrics:
   - Feature temporal stability score (auto-correlation across positions)
   - Cross-position feature similarity matrix
   - Temporal coherence measure (% features maintaining semantic role)
   - Ablation studies comparing with/without temporal components
5. Analysis tools:
   - Visualization of feature evolution across positions
   - Statistical tests for feature consistency
   - Comparison with baseline polysemantic measures
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
