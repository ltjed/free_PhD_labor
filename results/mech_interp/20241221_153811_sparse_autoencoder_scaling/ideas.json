[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": true
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Hierarchical Sparse Autoencoders for Multi-Level Feature Disentanglement in Language Models",
        "Experiment": "1. Implement two-layer hierarchical autoencoder:\n   - Layer 1: 2x input dim, 1% sparsity\n   - Layer 2: 1x input dim, 5% sparsity\n2. Training curriculum:\n   - Phase 1: Train Layer 1 only\n   - Phase 2: Gradually incorporate Layer 2\n   - Phase 3: End-to-end fine-tuning\n3. Loss components:\n   - Layer-specific L1 penalties\n   - InfoNCE-based abstraction loss\n   - Reconstruction loss with auxiliary terms\n4. Evaluation metrics:\n   - Feature hierarchy measures (coverage ratio, clustering coefficients)\n   - Causal intervention tests\n   - Conditional activation analysis\n   - Comparison with single-layer baseline\n5. Theoretical analysis:\n   - Connection to transformer attention hierarchies\n   - Information bottleneck principles\n6. Ablation studies on architecture and training choices",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]