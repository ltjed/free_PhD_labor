[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "contrastive_sparse_autoencoder",
        "Title": "Improving Sparse Autoencoder Interpretability through Contrastive Learning of Semantic Features",
        "Experiment": "1. Implement enhanced sparse autoencoder:\n   - Momentum-based encoder architecture\n   - Large batch training with gradient accumulation\n   - Mutual information based diversity regularization\n2. Context similarity computation:\n   - Weighted combination of attention, hidden states, PMI\n   - PBT for automatic weight optimization\n3. Training procedure:\n   - Curriculum learning with adaptive scheduling\n   - Population-based training for hyperparameters\n   - Distributed training on multiple GPUs\n4. Evaluation framework:\n   - Automated metrics (HSIC, MI, probing)\n   - Human expert evaluation of features\n   - Semantic concept alignment\n   - Polysemanticity case studies",
        "Technical_Details": "Loss function L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_contrast + \u03bb_3 * L_mi_div where L_mi_div penalizes mutual information between latent dimensions. Momentum encoder with momentum m=0.999 and queue size 65536. PBT optimization of temperature \u03c4 \u2208 [0.1, 0.8], curriculum rate \u03b1 \u2208 [0.001, 0.1], similarity weights w_i \u2208 [0, 1]. Batch size 1024 with gradient accumulation over 8 steps. Similarity score s = w_1*s_attn + w_2*s_hidden + w_3*s_pmi with PBT-tuned weights.",
        "Research_Impact": "Provides a practical, scalable approach to learning interpretable features through four key innovations: (1) mutual information based diversity instead of strict orthogonality, (2) automatic hyperparameter optimization via PBT, (3) training stability improvements through momentum encoders, and (4) comprehensive human evaluation of semantic interpretability. The approach bridges the gap between theoretical interpretability objectives and practical implementation challenges, enabling reliable extraction of human-understandable features from language models.",
        "Interestingness": 8,
        "Feasibility": 8,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "causal_sparse_autoencoder",
        "Title": "Causally-Guided Sparse Autoencoders for Interpretable Feature Discovery in Language Models",
        "Experiment": "1. Implement selective causal intervention framework:\n   - Top-k active latent dimension selection\n   - Structured intervention patterns\n   - Gradient-based importance sampling\n2. Enhanced training procedure:\n   - Three-phase training: reconstruction, intervention, refinement\n   - Explicit probing tasks for syntax and semantics\n   - Targeted feature discovery using activation patterns\n3. Evaluation methodology:\n   - Quantitative metrics: intervention effect size, feature stability\n   - Controlled experiments on synthetic data\n   - Comparison with existing interpretation methods\n4. Analysis:\n   - Detailed ablation studies on intervention strategies\n   - Feature interaction analysis\n   - Computational efficiency studies",
        "Technical_Details": "Loss function L = L_recon + \u03bb_1*L_sparse + \u03bb_2*L_causal + \u03bb_3*L_probe where L_causal focuses on top-k (k=100) active dimensions per batch. Interventions use structured patterns: \u03b4z_i = \u03b1*v where v is drawn from a set of predefined perturbation vectors. Probing tasks include dependency parsing (syntax) and word sense disambiguation (semantics). Feature importance measured through counterfactual token prediction differences. Intervention sampling uses importance reweighting based on activation magnitude and gradient sensitivity. Three-phase training schedule: 10k steps reconstruction, 5k steps intervention, 5k steps refinement.",
        "Research_Impact": "Addresses key limitations in current interpretability methods by: (1) providing a computationally tractable approach to causal feature discovery, (2) establishing concrete links between learned features and linguistic structures, and (3) enabling quantitative evaluation of feature quality. The selective intervention approach makes the method practical for large models while maintaining interpretability benefits. The explicit probing tasks provide validation of feature semantics, while the three-phase training ensures stable feature discovery.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Hierarchical Sparse Autoencoders: Learning Interpretable Feature Trees in Language Models",
        "Experiment": "1. Implement hierarchical sparse autoencoder:\n   - DAG-structured latent space with learned edge weights\n   - Gated residual connections for feature inheritance\n   - Gumbel-Softmax for structure learning\n   - Progressive structure refinement\n2. Training procedure:\n   - Curriculum learning starting with dense connections\n   - Gradual sparsification of connections\n   - Edge pruning based on attention scores\n3. Evaluation framework:\n   - Hierarchy consistency score (HCS)\n   - Feature utilization ratio\n   - Path importance analysis\n   - Abstraction level validation\n4. Analysis:\n   - Feature subgraph extraction\n   - Quantitative abstraction metrics\n   - Comparison with baseline methods",
        "Technical_Details": "Latent space organized as DAG with maximum path length L. Feature inheritance implemented as h_i = \u03c3(Wx_i + \u03a3_j g_ij\u2299h_j) where g_ij = sigmoid(\u03b1_ij) is learned gate for edge (i,j). Structure learning uses Gumbel-Softmax temperature annealing from 1.0 to 0.1. Loss function L = L_recon + \u03bb_1*L_sparse + \u03bb_2*L_dag + \u03bb_3*L_util where L_dag enforces DAG constraints via trace exponential penalty and L_util encourages balanced feature utilization. HCS computed as average KL divergence between feature activation distributions at adjacent levels. Edge pruning threshold determined adaptively using validation performance. Features organized into maximum of 5 abstraction levels with skip connections allowed.",
        "Research_Impact": "Addresses previous limitations in hierarchical feature learning by: (1) allowing flexible DAG structure instead of rigid trees, (2) providing stable training through continuous relaxation, (3) introducing quantitative metrics for hierarchy quality, and (4) enabling interpretable feature navigation through learned paths. The gated inheritance mechanism allows for selective feature composition while maintaining interpretability. The progressive structure refinement allows the model to discover natural feature hierarchies rather than imposing them. This approach provides a middle ground between completely flat autoencoders and strict hierarchical models, potentially offering better scalability to large language models.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "compositional_sparse_autoencoder",
        "Title": "Compositional Sparse Autoencoders: Learning Interpretable Feature Combinations in Language Models",
        "Experiment": "1. Architecture modifications:\n   - Two-stream architecture: feature extractor (d_f=256) and composition network (d_c=128)\n   - Multi-head attention (8 heads) for feature composition\n   - Residual connections with gating mechanism\n   - Bottleneck layer enforcing sparsity (k=32)\n2. Training procedure:\n   - End-to-end training with graduated sparsity\n   - Batch size 512, learning rate 3e-4\n   - Composition complexity curriculum (2->4 feature combinations)\n3. Evaluation framework:\n   - Synthetic tests: arithmetic, boolean logic, simple grammar\n   - Feature interaction analysis via attention patterns\n   - Human evaluation (n=50) of feature interpretability\n   - Comparison with PCA, NMF, and standard sparse AE\n4. Analysis:\n   - Ablation studies on architecture components\n   - Composition pattern visualization\n   - Feature reuse statistics",
        "Technical_Details": "Loss function L = L_recon + \u03bb_1*L_sparse + \u03bb_2*L_comp where L_sparse uses continuous top-k approximation and L_comp = -log(p(valid_composition)). Attention-based composition: h = MultiHead(Q=f_i, K=f_j, V=f_j) where f_i,f_j are features. Sparsity enforced via continuous top-k activation (k=32) with temperature annealing. Composition validity scored by learned scorer network s(f_i, f_j) trained on synthetic data. Feature interaction measured via attention entropy H(A) where A is the attention matrix. Graduated sparsity schedule: k_t = k_max - (k_max-k_min)(1-t/T)^2.",
        "Research_Impact": "Addresses limitations in current approaches by: (1) providing a more practical and trainable architecture for learning compositional features, (2) introducing concrete evaluation metrics and synthetic tasks, (3) enabling systematic analysis of feature interactions, and (4) incorporating human evaluation to validate interpretability claims. The simplified architecture and single-phase training make the method more feasible while maintaining the benefits of compositional feature learning.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "temporal_sparse_autoencoder",
        "Title": "Temporal Sparse Autoencoders: Learning Interpretable Feature Dynamics in Language Models",
        "Experiment": "1. Architecture implementation:\n   - LSTM-based encoder/decoder with sparse gates\n   - Sliding window feature memory (window size w=16)\n   - Feature lifecycle tracking module\n   - Static-temporal bridge connections\n2. Training procedure:\n   - Initialize from pretrained static autoencoder\n   - Three-phase training:\n     a) Static feature distillation\n     b) Temporal feature introduction\n     c) End-to-end refinement\n   - Progressive sparsity annealing\n3. Evaluation methods:\n   - Temporal stability score (TSS)\n   - Feature lifetime analysis\n   - Causal intervention tests\n   - Temporal probing tasks:\n     * Subject-verb agreement\n     * Coreference resolution\n     * Context-dependent disambiguation\n4. Analysis:\n   - Feature evolution visualization\n   - Transition pattern mining\n   - Ablation studies on window size\n   - Comparison with baseline methods",
        "Technical_Details": "Modified LSTM architecture with sparse gates: i_t,f_t,o_t \u2208 \u211d^d have sparsity constraint ||g_t||_0 \u2264 k for g \u2208 {i,f,o}. Temporal stability score TSS(f) = 1/T\u2211_t exp(-||h_t - h_{t-1}||\u2082/\u03c4) when feature f is active. Feature lifecycle tracked via state machine with states {dormant, emerging, active, declining}. Loss function L = L_recon + \u03bb_1*L_sparse + \u03bb_2*L_temporal + \u03bb_3*L_bridge where L_bridge = ||h_static - h_temporal||\u2082 encourages consistency with static features. Window size w=16 with stride 8 for memory efficiency. Sparsity annealing schedule: k_t = k_min + (k_max-k_min)(1-exp(-\u03b1t)) with \u03b1=0.001.",
        "Research_Impact": "Addresses limitations of previous approach through: (1) simplified architecture improving trainability while maintaining expressivity, (2) concrete quantitative metrics for temporal feature quality, (3) principled evaluation through probing tasks and interventions, and (4) bridge connections to leverage existing static feature knowledge. The three-phase training procedure provides a practical path to learning temporal features while the feature lifecycle tracking enables systematic analysis of feature dynamics. The simplified memory mechanism and sparse gates make the method more computationally feasible while retaining interpretability benefits.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": true
    }
]