[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Two-Level Hierarchical Sparse Autoencoders with Attention for Interpretable Feature Learning",
        "Experiment": "1. Implement two-level autoencoder architecture:\n   - Primary layer for high-level concepts\n   - Secondary layer for detailed features\n   - Attention mechanism connecting levels\n2. Modified loss function incorporating:\n   - Level-specific sparsity constraints\n   - Attention-based regularization\n   - Feature diversity penalties\n3. Evaluation framework:\n   - Measure feature coherence within/between levels\n   - Analyze attention patterns\n   - Compare interpretability with baseline\n4. Visualization tools for:\n   - Feature relationships via attention maps\n   - Concept clustering analysis\n5. Ablation studies on:\n   - Attention mechanism\n   - Sparsity levels\n   - Architecture choices",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]