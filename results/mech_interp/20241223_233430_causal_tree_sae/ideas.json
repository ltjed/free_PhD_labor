[
    {
        "Name": "sparse_autoencoder_scaling",
        "Title": "Scaling Laws and Evaluation Methods for Sparse Autoencoders in Language Models",
        "Experiment": "The paper presents several key implementations and experiments:\n1. A k-sparse autoencoder architecture using TopK activation function replacing traditional L1 penalty\n2. Auxiliary loss (AuxK) and initialization techniques to prevent dead latents\n3. Training methodology scaled up to 16M latent dimensions on GPT-4\n4. Multiple evaluation metrics including:\n   - Downstream loss\n   - Feature probe recovery\n   - N2G explanations for interpretability\n   - Ablation sparsity measures\n5. Systematic study of scaling relationships between:\n   - Number of latents (n)\n   - Sparsity level (k)\n   - Model size\n   - Compute budget\n6. Analysis of progressive feature recovery and activation patterns",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "hierarchical_disentangled_sae",
        "Title": "Two-Level Hierarchical Sparse Autoencoders with Adaptive Group Assignment",
        "Experiment": "1. Implement simplified hierarchical autoencoder:\n   - Two-level encoder (group assignment \u2192 feature extraction)\n   - Adaptive group assignment based on activation patterns\n   - Group-wise contrastive loss\n   - Group sparsity regularization\n2. Implementation changes:\n   - Add TwoLevelEncoder class with adaptive grouping\n   - Add group_contrastive_loss function\n   - Add group_sparsity_regularizer\n3. Evaluation:\n   - Measure group coherence and specialization\n   - Compare feature interpretability within groups\n   - Analyze group adaptation during training\n4. Ablation studies on:\n   - Number of groups\n   - Group size\n   - Impact of different losses",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "causal_tree_sae",
        "Title": "Tree-Structured Sparse Autoencoders with Curriculum Learning",
        "Experiment": "1. Implement tree-enhanced autoencoder:\n   - Add tree adjacency matrix from WordNet\n   - Hierarchical encoder with parent-child relationships\n   - Tree-based sparsity regularization\n   - Curriculum learning scheduler\n2. Code changes:\n   - New TreeSparseAutoencoder class\n   - tree_structure_loss function\n   - curriculum_scheduler class\n   - WordNet integration utils\n3. Evaluation:\n   - Feature coherence with WordNet\n   - Parent-child activation correlation\n   - Compare with flat SAE baseline\n   - Measure hierarchical interpretability\n4. Curriculum phases:\n   - Phase 1: Learn leaf concepts\n   - Phase 2: Add parent relationships\n   - Phase 3: Full tree structure\n5. Specific metrics:\n   - Tree consistency score\n   - Path activation analysis\n   - Hierarchical intervention tests",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "adversarial_feature_purification",
        "Title": "Feature Purification in Sparse Autoencoders via Targeted Perturbation Training",
        "Experiment": "1. Implementation of feature perturbation framework:\n   - Add FeaturePerturbationLayer class\n   - Implement targeted noise injection\n   - Add stability loss function\n2. Specific modifications:\n   - Feature stability regularizer: L2 norm of feature changes under perturbation\n   - Cross-validation of feature robustness\n   - Efficient batch-wise perturbation generation\n3. Concrete evaluation metrics:\n   - Feature stability score (FSS): % change in feature activation under perturbation\n   - Semantic consistency index (SCI): correlation of feature patterns\n   - Computational overhead measurement\n4. Technical validation:\n   - Feature activation distributions\n   - Robustness curves across perturbation magnitudes\n   - Ablation studies on noise types\n5. Efficiency improvements:\n   - Parallel perturbation generation\n   - Adaptive perturbation magnitude\n   - Early stopping based on stability metrics",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "mutual_information_gating",
        "Title": "Interpretable Feature Learning via Mutual Information Gated Sparse Autoencoders",
        "Experiment": "1. Implementation additions:\n   - Add KernelMIEstimator class (more efficient)\n   - Implement MI-aware sparse activation function\n   - Add interpretability scoring module\n2. Architecture changes:\n   - Single integrated training phase\n   - MI-based feature pruning mechanism\n   - Sparse activation with MI penalties\n3. Training procedure:\n   - Joint optimization of reconstruction and MI objectives\n   - Adaptive sparsity based on MI scores\n   - Progressive feature refinement\n4. Evaluation metrics:\n   - Feature independence score\n   - Interpretability correlation\n   - Clustering coefficient\n   - Human evaluation protocol\n5. Analysis tools:\n   - Feature similarity graphs\n   - Interpretability dashboards\n   - Automated feature labeling\n6. Efficiency improvements:\n   - Batch-wise MI estimation\n   - Sparse computation optimizations\n   - Memory-efficient implementation",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "temporal_coherent_sae",
        "Title": "Temporally Coherent Sparse Autoencoders with Sequential Attention for Interpretable Language Model Features",
        "Experiment": "1. Architecture modifications:\n   - Add TemporalAttentionLayer with sliding window\n   - Implement WindowedFeatureAggregation\n   - Temporal sparsity regularizer with local coherence\n2. Implementation details:\n   - Efficient sliding window attention (window size k)\n   - Local temporal consistency loss\n   - Sparse activation with temporal smoothing\n   - Memory-efficient implementation using chunking\n3. Evaluation methods:\n   - Feature consistency score across related tokens\n   - Temporal stability index\n   - Comparative analysis with standard SAE\n   - Memory and compute benchmarks\n4. Analysis components:\n   - Feature activation trajectories\n   - Linguistic pattern correlation\n   - Attention pattern visualization\n5. Specific metrics:\n   - Token-to-token feature stability\n   - Syntactic relationship preservation\n   - Computational overhead measurement\n6. Efficiency optimizations:\n   - Sparse attention computation\n   - Gradient checkpointing\n   - Batch sequence processing",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 9,
        "novel": true
    }
]