# Title: Hierarchical Sparse Autoencoders for Multi-Level Interpretability of Language Model Representations
# Experiment description: 1. Implement 2-layer hierarchical autoencoder:
   - Bottom layer: 90% sparsity, focus on basic features
   - Top layer: 70% sparsity, focus on abstract features
   - Skip connections with gating mechanism
2. Training protocol:
   - Phase 1: Train bottom layer with L1 penalty
   - Phase 2: Fix bottom layer, train top layer
   - Phase 3: End-to-end fine-tuning
3. Loss components:
   - Layer-specific L1 penalties
   - Orthogonality constraint between layers
   - Mutual information minimization
4. Evaluation metrics:
   - Feature activation correlation analysis
   - Semantic level assessment via NLP probing
   - Compression efficiency
   - Ablation studies per layer
5. Comparison with single-layer baseline
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
