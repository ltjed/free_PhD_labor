# Title: Feature Purification in Sparse Autoencoders via Targeted Perturbation Training
# Experiment description: 1. Implementation of feature perturbation framework:
   - Add FeaturePerturbationLayer class
   - Implement targeted noise injection
   - Add stability loss function
2. Specific modifications:
   - Feature stability regularizer: L2 norm of feature changes under perturbation
   - Cross-validation of feature robustness
   - Efficient batch-wise perturbation generation
3. Concrete evaluation metrics:
   - Feature stability score (FSS): % change in feature activation under perturbation
   - Semantic consistency index (SCI): correlation of feature patterns
   - Computational overhead measurement
4. Technical validation:
   - Feature activation distributions
   - Robustness curves across perturbation magnitudes
   - Ablation studies on noise types
5. Efficiency improvements:
   - Parallel perturbation generation
   - Adaptive perturbation magnitude
   - Early stopping based on stability metrics
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'enwik8': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'text8': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}}
Description: Baseline results.
