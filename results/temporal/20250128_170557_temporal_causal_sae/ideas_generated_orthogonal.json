[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "adaptive_encoder_separation_sae",
        "Title": "Adaptive Encoder Weight Separation for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement adaptive threshold computation\n2. Add random pair sampling\n3. Compare against baseline SAE on interpretability metrics\n4. Analyze adaptation patterns\n5. Evaluate computational efficiency",
        "Technical_Details": "The method uses an adaptive encoder separation loss: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_enc_sep where L_enc_sep = \u03a3_{i,j\u2208S} max(0, cos_sim(w_i, w_j) - \u03c4_t) for randomly sampled co-active pairs S. The threshold \u03c4_t = \u03bc_t + \u03c3_t where \u03bc_t is the mean similarity between random weight pairs and \u03c3_t is their standard deviation, computed efficiently using matrix operations. Implementation samples min(100, n_coactive) pairs per batch for computational efficiency. No additional parameters needed beyond existing ones.",
        "Implementation_Plan": "1. Add adaptive threshold computation\n2. Implement efficient random pair sampling\n3. Modify separation loss computation\n4. Update training loop\n5. Add adaptation analysis utilities",
        "Interestingness_Evaluation": "The adaptive approach automatically handles different scales and patterns of weights while maintaining computational efficiency.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only basic matrix operations; random sampling reduces computation; adaptive threshold adds minimal overhead; fits well within 30-minute limit; implementation remains simple with clear modifications.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of adaptive thresholds and efficient sampling provides a novel and practical approach to weight separation.",
        "Novelty": 8,
        "Expected_Research_Impact": "The adaptive nature should improve sparse_probing by creating appropriately separated features across different scales, while maintaining good core metrics through efficient implementation.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We present an adaptive approach to encoder weight separation in sparse autoencoders. Our method dynamically adjusts the separation threshold based on the natural similarity patterns in the model's weights, while using efficient random sampling of co-active feature pairs. This creates a robust and computationally efficient approach to reducing polysemanticity that automatically scales with different layers and weight patterns. The method combines standard reconstruction and sparsity losses with a novel adaptive separation term that operates on encoder weights. We evaluate our approach on standard interpretability benchmarks and analyze how the adaptive threshold responds to different weight patterns.",
        "novel": true
    },
    {
        "Name": "persample_ortho_sae",
        "Title": "Per-Sample Top-k Orthogonalization for Feature Separation in Sparse Autoencoders",
        "Experiment": "1. Implement per-sample top-k selection\n2. Add sample-level orthogonality loss\n3. Train with different k values\n4. Compare against baseline SAE\n5. Analyze per-sample feature interactions",
        "Technical_Details": "The method enforces orthogonality between most active features per sample: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_ortho where L_ortho = (1/B) * \u03a3_i ||W_i^T W_i - I||_F for W_i containing k most active features in sample i (B is batch size). Fixed k (e.g. k=8) ensures consistent computation per sample. This targets orthogonalization specifically at features with strongest activations within each individual context.",
        "Implementation_Plan": "1. Add PerSampleFeatureSelector class\n2. Modify CustomTrainer for per-sample processing\n3. Add per-sample activation logging\n4. Implement context-specific visualization\n5. Add sample-level evaluation metrics",
        "Interestingness_Evaluation": "The per-sample approach provides an elegant and focused solution to polysemanticity by targeting feature interactions within individual contexts.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only simple top-k selection per sample; training very stable with consistent per-sample constraint; highly efficient with fixed per-sample computation.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While orthogonality in SAEs is known, applying it at the individual sample level with top-k selection is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The context-focused approach should significantly improve interpretability by targeting feature interactions where they matter most - within individual samples.",
        "Research_Impact": 8,
        "Overall_Score": 8.2,
        "Abstract": "Polysemanticity in neural networks manifests at the level of individual contexts, where multiple features may encode overlapping concepts within the same input. While sparse autoencoders help extract interpretable features, they may still suffer from feature entanglement within specific contexts. We propose a per-sample orthogonalization approach that enforces feature separation between the most active features for each individual input in the training batch. Our method maintains the simplicity of standard sparse autoencoders while focusing orthogonality constraints at the most fine-grained level where feature interactions occur. By targeting only the top-k most active features per sample, the approach is highly efficient and scalable. We analyze how features interact within individual contexts and evaluate our method on standard interpretability benchmarks.",
        "novel": true
    },
    {
        "Name": "window_adaptive_sae",
        "Title": "Window-Based Adaptive Sparsity for Feature Specialization in Sparse Autoencoders",
        "Experiment": "1. Implement sliding window activation tracking\n2. Add window-based sparsity adjustment\n3. Train with adaptive feature-specific sparsity\n4. Compare feature distinctness using window statistics\n5. Evaluate impact on interpretability",
        "Technical_Details": "We modify the SAE loss to use feature-specific sparsity penalties: L = L_recon + \u03a3_i \u03bb_i(t) * |f_i|, where \u03bb_i(t) is adjusted based on recent activation history. We maintain a fixed-size buffer (last 1000 batches) of binary activation indicators for each feature. The sparsity penalty is directly proportional to the activation rate: \u03bb_i(t) = \u03bb_base * (1 + window_freq_i), where \u03bb_base = 0.04 and window_freq_i is the fraction of batches in the window where feature i was active. This creates immediate feedback between feature usage and specialization pressure.",
        "Implementation_Plan": "1. Add sliding window buffer to CustomTrainer\n2. Implement simple window-based sparsity in loss()\n3. Add window updates in update()\n4. Add logging for window statistics\n5. Add evaluation of feature dynamics",
        "Interestingness_Evaluation": "The use of direct, window-based activation tracking provides an elegant and responsive approach to feature specialization.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation requires only a simple fixed-size buffer; extremely efficient updates; can be implemented in a few days; training time identical to baseline SAE.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While sliding windows are common in other contexts, their use for direct sparsity adjustment in SAEs provides a novel approach to feature specialization.",
        "Novelty": 6,
        "Expected_Research_Impact": "The simple, responsive approach to feature specialization should lead to more interpretable features while maintaining perfect computational efficiency.",
        "Research_Impact": 9,
        "Overall_Score": 8.7,
        "Abstract": "We present a simple and efficient approach to training sparse autoencoders that addresses the challenge of polysemanticity through window-based adaptive sparsity. Our method modifies the standard sparse autoencoder by introducing feature-specific sparsity penalties that adapt based on recent activation history, maintained in a fixed-size sliding window. This approach provides a direct and responsive way to reduce feature entanglement while maintaining the ability to learn effective representations. Unlike previous methods that use complex statistical calculations or delayed feedback mechanisms, our window-based approach allows for immediate adaptation of feature behavior based on recent usage patterns, while maintaining the simplicity and efficiency of the base architecture.",
        "novel": true
    },
    {
        "Name": "simple_threshold_sae",
        "Title": "Simple Threshold-based Feature Separation for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement fixed threshold similarity penalty\n2. Train with different threshold values (0.05, 0.1, 0.2)\n3. Evaluate impact on sparse_probing metrics\n4. Analyze feature separation vs reconstruction trade-off\n5. Compare with baseline SAE",
        "Technical_Details": "The method uses loss L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_thresh, where L_thresh = ReLU(cos_sim - threshold) summed over feature pairs. Cosine similarity is computed between normalized columns of W_enc. Fixed thresholds eliminate need for scheduling. \u03bb_2 is selected based on preliminary sparse_probing results. Implementation focuses on efficient computation of pairwise similarities using standard PyTorch operations.",
        "Implementation_Plan": "1. Add SimpleSimilarityLoss class\n2. Add threshold hyperparameter to CustomSAE\n3. Modify CustomTrainer loss computation\n4. Add similarity statistics logging\n5. Create threshold sweep experiment",
        "Interestingness_Evaluation": "The extreme simplicity combined with direct connection to interpretability metrics makes this a compelling approach.",
        "Interestingness": 6,
        "Feasibility_Evaluation": "Implementation requires only basic matrix operations; fixed threshold eliminates scheduling complexity; hyperparameter sweep is straightforward; all components are standard PyTorch; training time well within 30-minute limit.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While simple, the direct connection between thresholds and interpretability metrics provides a useful tool for analysis.",
        "Novelty": 5,
        "Expected_Research_Impact": "The simplicity and clear connection to metrics should provide reliable improvements in feature interpretability.",
        "Research_Impact": 7,
        "Overall_Score": 7.5,
        "Abstract": "We present a minimalist approach to improving feature interpretability in sparse autoencoders through fixed threshold-based feature separation. Current methods often introduce unnecessary complexity that obscures the relationship between feature separation and interpretability metrics. Our method uses a simple threshold on feature similarities, providing direct control over feature relationships while maintaining reconstruction quality. This straightforward approach offers a clear framework for analyzing the trade-off between feature separation and model performance. We discuss the theoretical motivations for threshold-based separation and its implications for mechanistic interpretability research.",
        "novel": false
    },
    {
        "Name": "entropy_guided_sae",
        "Title": "Entropy-Guided Early-Phase Curriculum for Sparse Autoencoders",
        "Experiment": "1. Implement linear early-phase sparsity scheduling\n2. Train with 10% warmup period\n3. Track feature activation entropy\n4. Compare early feature formation patterns\n5. Evaluate interpretability metrics",
        "Technical_Details": "The method uses a two-phase sparsity schedule: (1) Early phase (0-10% steps) with linear ramp-up \u03bb(t) = \u03bb_final * (t/t_0) where t_0 is 10% of total steps, chosen based on typical entropy stabilization points, (2) Standard phase (10-100% steps) with constant \u03bb = \u03bb_final = 0.04. Feature activation entropy H = -\u03a3p_i*log(p_i) is tracked during early phase using 500-step windows. The linear schedule provides transparent regularization growth during the critical period of feature formation.",
        "Implementation_Plan": "1. Add LinearEarlyScheduler class with 10% warmup\n2. Modify CustomTrainer.update() for linear schedule\n3. Add entropy calculation in loss function\n4. Add early_phase_stats to logging parameters\n5. Create entropy visualization function",
        "Interestingness_Evaluation": "The entropy-guided approach provides a principled way to study and control early feature formation dynamics.",
        "Interestingness": 6,
        "Feasibility_Evaluation": "Implementation requires only basic modifications to training loop; linear scheduling and entropy tracking are straightforward; completable within 1 week; runtime well within 30-minute limit.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While the approach is simple, the specific focus on entropy-guided scheduling length offers a modest contribution to SAE training.",
        "Novelty": 4,
        "Expected_Research_Impact": "The controlled early feature formation process should provide modest improvements to interpretability metrics through cleaner initial feature separation.",
        "Research_Impact": 5,
        "Overall_Score": 6.5,
        "Abstract": "We propose an entropy-guided early-phase curriculum for training sparse autoencoders in mechanistic interpretability. Our method focuses on the critical initial period of training using a linear ramp-up of the sparsity penalty during the first 10% of training steps, with this duration chosen based on typical feature entropy stabilization patterns. This intervention aims to provide controlled conditions for initial feature formation, using a transparent and predictable regularization schedule. The approach maintains the simplicity of standard sparse autoencoder training while introducing a principled modification to early learning dynamics. We implement this through a two-phase training schedule with careful monitoring of feature activation entropy during the crucial early phase.",
        "novel": true
    },
    {
        "Name": "activation_ordered_sae",
        "Title": "Activation-Based Feature Ordering for Interpretable Sparse Autoencoders",
        "Experiment": "1. Train SAE with standard L1 penalty\n2. Track feature activation frequencies\n3. Reorder features after warmup period\n4. Train on Pythia-70M using standard datasets\n5. Analyze activation patterns\n6. Evaluate interpretability impact",
        "Technical_Details": "The method uses standard SAE training with L1 sparsity penalty L = L_recon + \u03bb * L_sparse. Feature importance is measured simply by activation frequency: S_i = E[|h_i|] where h_i is the i-th hidden activation. Features are reordered once after the warmup period (default 1000 steps), swapping encoder and decoder weights to maintain model equivalence. This reveals natural feature hierarchies based on usage patterns while ensuring stable training through a single, well-timed reordering.",
        "Implementation_Plan": "1. Add activation tracking to CustomSAE.forward() using running mean\n2. Add reorder_features() method to CustomSAE\n3. Modify CustomTrainer.update() to call reordering after warmup\n4. Add activation frequency logging to training loop\n5. Add simple plotting function for activation distribution\n6. Add evaluation code comparing pre/post reordering interpretability",
        "Interestingness_Evaluation": "The simplified approach focuses on the most fundamental aspect of feature importance - how often they are actually used.",
        "Interestingness": 6,
        "Feasibility_Evaluation": "Implementation is extremely simple with minimal code changes; activation tracking adds negligible overhead; single reordering step has no impact on training time; very stable and reproducible; analysis requires only basic statistics.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While feature activation analysis is common, using it for one-time reordering to reveal natural hierarchies is a novel twist.",
        "Novelty": 5,
        "Expected_Research_Impact": "The clear ordering of features by activation should improve interpretability and sparse probing while maintaining core metrics since the model is unchanged.",
        "Research_Impact": 6,
        "Overall_Score": 7.3,
        "Abstract": "We propose an activation-based feature ordering approach for sparse autoencoders (SAEs) that aims to improve interpretability through a simple yet effective reorganization of learned features. Our method tracks feature activation frequencies during training and performs a one-time reordering of features based on their usage patterns. This reveals natural hierarchies in the learned representations, where frequently activated features represent common patterns while rarely activated features capture more specific details. The approach is motivated by the observation that feature importance in SAEs is fundamentally reflected in how often they are used, and making this structure explicit through reordering can provide valuable insights into the model's learned representations. By maintaining the exact same model with a more interpretable ordering, our method offers a minimalist approach to analyzing learned features.",
        "novel": true
    },
    {
        "Name": "hierarchical_sparse_autoencoder",
        "Title": "Simple Hierarchical Learning for Feature Absorption Prevention in Sparse Autoencoders",
        "Experiment": "1. Implement linear sparsity scheduling\n2. Add proportional window tracking\n3. Implement simple resampling\n4. Train with scaled parameters\n5. Analyze feature stability\n6. Measure absorption prevention",
        "Technical_Details": "The method uses a linear sparsity schedule \u03bb(t) = \u03bb_init * (1 - t/T) where \u03bb_init = 1/sqrt(d_sae) for dictionary size d_sae. Feature activation frequency is tracked using a running average over the last N = max(100, T/100) batches, where T is total training steps. Features are resampled if inactive for N/10 consecutive batches. Resampling uses standard normal initialization scaled by the mean norm of active features. The decoder weights maintain unit L2-norm through gradient projection.",
        "Implementation_Plan": "1. Add LinearSparsityScheduler class\n2. Implement ProportionalWindowTracker\n3. Simplify resample_neurons\n4. Update CustomTrainer\n5. Add basic statistics logging\n6. Update evaluation code",
        "Interestingness_Evaluation": "The approach achieves feature absorption prevention through minimal, principled modifications to standard sparse autoencoders.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires moderate effort for window tracking; parameter choices are simple and scaled; memory usage is bounded; standard initialization methods; well within 30-minute limit on H100.",
        "Feasibility": 8,
        "Novelty_Evaluation": "The use of proportional windows and simplified resampling for hierarchical feature learning is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The robust, parameter-efficient approach should improve sparse probing and core metrics through reliable feature separation.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "We propose a minimalist approach to preventing feature absorption in sparse autoencoders through hierarchical learning. Our method introduces a simple linear sparsity scheduling scheme combined with proportional window-based feature tracking. The approach automatically scales with model size and training duration, requiring minimal hyperparameter tuning. By tracking feature activations over proportionally-sized windows and using straightforward resampling rules, we aim to maintain clear feature separation while keeping implementation complexity at a minimum. The method integrates easily with existing frameworks and provides simple ways to monitor feature stability and hierarchy development.",
        "novel": true
    }
]