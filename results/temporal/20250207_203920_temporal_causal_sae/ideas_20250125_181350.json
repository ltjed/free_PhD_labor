[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "controlled_feature_sharing_sae",
        "Title": "Controlled Feature Sharing for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement soft orthogonality loss with tunable sharing parameter \u03b1\n2. Add adaptive feature grouping with periodic updates based on cosine similarity thresholds (\u03c4=0.8)\n3. Train on Pythia-70M using WikiText and WMDP-bio datasets\n4. Compare performance on sparse_probing and core benchmarks against baseline and fully orthogonal SAEs\n5. Analyze feature subspaces using condition numbers, cosine similarity metrics, and feature activation patterns\n6. Evaluate impact of different \u03b1 values on interpretability and reconstruction quality using grid search with cross-validation",
        "Technical_Details": "The method uses a modified objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_soft_ortho, where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_soft_ortho = ||W_1^T W_2 - \u03b1I||_F allows controlled feature sharing through parameter \u03b1, which determines the degree of feature overlap. Feature groups are updated every n=100 steps using cosine similarity thresholds (\u03c4=0.8) to dynamically adjust feature subspaces. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality. Early stopping is based on condition number thresholds and reconstruction error. Feature activation patterns are visualized using heatmaps and correlation analysis with ground truth labels.",
        "Implementation_Plan": "1. Add SoftOrthogonalityLoss with scheduling\n2. Implement efficient batch-wise feature grouping using cosine similarity thresholds\n3. Modify CustomSAE to include group-specific biases and soft orthogonality loss\n4. Add condition number, cosine similarity, and feature activation pattern calculation utilities\n5. Update CustomTrainer with adaptive loss weight and feature grouping\n6. Add evaluation metrics for controlled sharing, interpretability, and feature activation patterns\n7. Implement grid search for \u03b1 values with cross-validation to optimize the trade-off between feature separation and reconstruction quality\n8. Visualize feature activation patterns using heatmaps and correlation analysis",
        "Interestingness_Evaluation": "The idea of controlled feature sharing provides a nuanced approach to balancing feature separation and reconstruction quality, making it highly relevant for mechanistic interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation involves straightforward modifications to the loss function and feature grouping, with minimal computational overhead. All computations are well within the 30-minute limit on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The introduction of a soft orthogonality constraint with controlled feature sharing is a novel approach to addressing polysemanticity in sparse autoencoders.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed model is expected to improve interpretability on the sparse_probing benchmark while maintaining or improving reconstruction quality on the core benchmark, making it a promising approach for mechanistic interpretability.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but they often suffer from polysemanticity, where individual latent features represent multiple, semantically distinct concepts. This paper proposes a Controlled Feature Sharing Sparse Autoencoder (CFS-SAE), which introduces a soft orthogonality constraint to balance feature separation and reconstruction quality. By allowing controlled sharing of feature directions, the CFS-SAE achieves better interpretability without sacrificing performance. We evaluate the model on the sparse_probing and core benchmarks, demonstrating its effectiveness in improving interpretability while maintaining high reconstruction quality. The results suggest that controlled feature sharing is a promising approach for addressing polysemanticity in sparse autoencoders.",
        "novel": true
    },
    {
        "Name": "dynamic_hierarchical_orthogonal_sae",
        "Title": "Dynamic Hierarchical Orthogonal Feature Learning for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "1. Implement hierarchical orthogonality loss with global and group-level constraints\n2. Define initial feature groups based on semantic priors\n3. Add lightweight group refinement using feature activation similarity\n4. Introduce adaptive weighting for global and group-level orthogonality\n5. Train on Pythia-70M using WMDP-bio and WikiText datasets\n6. Compare performance on sparse_probing and core benchmarks against baseline and fixed orthogonal SAE\n7. Analyze feature separation within and across refined groups\n8. Evaluate impact of different refinement intervals and similarity thresholds",
        "Technical_Details": "The method uses a hierarchical objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_global_ortho + \u03bb_3(t) * L_group_ortho, where \u03bb_2(t) and \u03bb_3(t) are adaptive weights that increase linearly until step t_0. L_global_ortho = ||W^T W - \u03b1I||_F enforces global orthogonality, while L_group_ortho = \u03a3_{g=1}^G ||W_g^T W_g - \u03b2I||_F enforces orthogonality within each group g. Initial groups are predefined based on semantic categories, but are refined every n=100 steps using cosine similarity between feature activations. The adaptive weighting mechanism dynamically adjusts the relative importance of global and group-level orthogonality based on the reconstruction error and sparsity of the latents. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality.",
        "Implementation_Plan": "1. Add DynamicHierarchicalOrthogonalityLoss with global and group-level terms\n2. Define initial semantic groups based on prior knowledge\n3. Implement lightweight group refinement using cosine similarity\n4. Introduce adaptive weighting for global and group-level orthogonality\n5. Modify CustomSAE to include group-specific biases\n6. Add utilities for group analysis and visualization\n7. Update CustomTrainer with adaptive loss weights and refinement intervals\n8. Add evaluation metrics for hierarchical feature separation",
        "Interestingness_Evaluation": "The adaptive weighting mechanism adds a dynamic element to the hierarchy, making the approach more robust to variations in the data while retaining simplicity.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient with lightweight group refinement and adaptive weighting; adaptive weights and group-specific biases are easy to implement; all computations remain within the 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The adaptive weighting of global and group-level orthogonality provides a novel balance between fixed structure and dynamic adaptability, addressing polysemanticity more effectively.",
        "Novelty": 9,
        "Expected_Research_Impact": "The dynamic hierarchy is expected to improve performance on the sparse_probing benchmark by reducing feature absorption and improving the monosemanticity of latents.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "Polysemanticity, where individual latent features represent multiple, semantically distinct concepts, remains a key challenge in mechanistic interpretability. This paper introduces Dynamic Hierarchical Orthogonal Sparse Autoencoders (DHOSAEs), a novel approach that enforces orthogonality at both global and group levels using a predefined hierarchy of semantic categories. The hierarchy is periodically refined during training based on feature activation similarity, and the relative importance of global and group-level orthogonality is dynamically adjusted based on the reconstruction error and sparsity of the latents. By applying local orthogonality constraints within each refined group, DHOSAEs achieve a more interpretable latent space while maintaining reconstruction quality. We evaluate our approach on the sparse_probing benchmark, demonstrating its potential to improve feature separation and interpretability compared to baseline methods. Our results suggest that dynamic hierarchical orthogonality is a promising direction for future work in mechanistic interpretability.",
        "novel": false
    },
    {
        "Name": "soft_orthogonal_sae",
        "Title": "Soft Orthogonal Sparse Autoencoders for Interpretable Feature Learning",
        "Experiment": "1. Implement a soft orthogonality loss with a fixed schedule for \u03b1(t) and \u03bb_2(t). 2. Train on Pythia-70M using WikiText and WMDP-bio datasets. 3. Compare interpretability and reconstruction quality against baseline SAE and strict orthogonal SAE. 4. Evaluate on sparse_probing benchmark to measure improvements in feature interpretability. 5. Analyze feature activation patterns to quantify polysemanticity reduction.",
        "Technical_Details": "The soft orthogonality loss is defined as L_soft_ortho = ||W_1^T W_2 - \u03b1(t)I||_F, where \u03b1(t) follows a fixed schedule: \u03b1(t) = \u03b1_max * min(1, t/t_0), with \u03b1_max = 0.5 and t_0 = 10,000 steps. The total loss is L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_soft_ortho, where \u03bb_2(t) = \u03bb_2_max * (1 - min(1, t/t_0)), with \u03bb_2_max = 0.1. This fixed schedule simplifies the dynamic weighting scheme while maintaining the balance between feature separation and reconstruction quality. Feature activations are monitored using batch-wise statistics to ensure interpretability. Polysemanticity reduction is measured by the fraction of latents that activate on multiple distinct concepts in the sparse_probing benchmark, specifically focusing on tasks such as language identification, profession classification, and sentiment analysis. The fraction of latents activating on multiple concepts is calculated by counting the number of latents that activate on more than one distinct concept in the sparse_probing tasks and dividing by the total number of latents.",
        "Implementation_Plan": "1. Add SoftOrthogonalityLoss class with fixed schedules for \u03b1(t) and \u03bb_2(t). 2. Modify CustomSAE to include the soft orthogonality loss by adding a new method `compute_soft_ortho_loss` that calculates L_soft_ortho. 3. Update CustomTrainer to handle the fixed loss weighting by modifying the `loss` method to include L_soft_ortho. 4. Add evaluation metrics for polysemanticity reduction, specifically the fraction of latents activating on multiple concepts in sparse_probing tasks. 5. Integrate sparse_probing evaluation into the training pipeline by adding a new method `evaluate_sparse_probing` that runs the benchmark tasks and logs the results.",
        "Interestingness_Evaluation": "The idea is interesting because it introduces a novel, flexible approach to feature separation that balances interpretability and reconstruction quality, addressing a key challenge in mechanistic interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is feasible because it builds on existing SAE frameworks, introduces only simple matrix operations, and uses efficient batch-wise updates. The fixed weighting scheme reduces implementation complexity, and all experiments can be completed within 30 minutes on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The soft orthogonality approach is novel because it provides a principled way to relax orthogonality constraints, enabling better interpretability without sacrificing reconstruction quality.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed model is expected to improve interpretability on the sparse_probing benchmark by reducing polysemanticity and enabling better feature separation, while maintaining strong reconstruction performance.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is often limited by polysemanticity in latent representations. We propose a Soft Orthogonal Sparse Autoencoder (Soft Orthogonal SAE) that introduces a controlled relaxation of orthogonality constraints, enabling more interpretable and monosemantic feature learning. By using a fixed schedule to balance feature separation and reconstruction quality, our approach addresses the trade-off between interpretability and performance. We evaluate the model on the sparse_probing benchmark, focusing on tasks such as language identification, profession classification, and sentiment analysis, to demonstrate improvements in feature interpretability and reduction of polysemanticity. This work provides a scalable and efficient method for improving the interpretability of neural network representations.",
        "novel": true
    },
    {
        "Name": "hierarchical_sae",
        "Title": "Hierarchical Feature Learning for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement correlation-based feature grouping using activation pattern correlations. 2. Add simplified adaptive sparsity constraints for rare features. 3. Train on Pythia-70M using WikiText and WMDP-bio datasets. 4. Compare performance on sparse_probing tasks against baseline and orthogonal SAE. 5. Analyze feature hierarchy and sparsity patterns. 6. Evaluate impact of different correlation thresholds and sparsity schedules.",
        "Technical_Details": "The method extends the standard SAE loss function: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_hier, where L_hier = \u03a3_g (w_g * ||W_g^T W_g - I||_F) penalizes deviations from orthogonality within feature groups. Feature groups are determined using correlation thresholds on activation patterns, updated every n=100 steps. Simplified adaptive sparsity constraints are implemented as \u03bb_1_g = \u03bb_1_base * (1 + \u03b1 * (1 - f_g)), where f_g is the activation frequency of group g and \u03b1 controls the strength of adaptation. The decoder uses group-specific biases to maintain reconstruction quality.",
        "Implementation_Plan": "1. Add CorrelationBasedFeatureGrouping class with correlation thresholding. 2. Modify CustomSAE to include group-specific sparsity penalties. 3. Implement simplified adaptive sparsity scheduling in CustomTrainer. 4. Add utilities for analyzing feature hierarchies. 5. Update evaluation metrics to measure hierarchical interpretability. 6. Integrate with sparse_probing benchmark.",
        "Interestingness_Evaluation": "The combination of correlation-based feature grouping and simplified adaptive sparsity constraints provides a novel approach to reducing polysemanticity while maintaining interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is efficient, relying on correlation-based grouping and simple modifications to the loss function. The adaptive sparsity constraints add minimal overhead, and all computations can be completed within the 30-minute limit on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The correlation-based feature grouping approach is novel and addresses a key limitation of existing methods by explicitly modeling feature relationships.",
        "Novelty": 9,
        "Expected_Research_Impact": "The proposed method is expected to improve performance on the sparse_probing benchmark, particularly for tasks involving hierarchical relationships, by reducing polysemanticity and preserving rare features.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is often limited by polysemanticity in the latent space. We propose a hierarchical feature learning approach that addresses this challenge by grouping features based on activation pattern correlations and applying simplified adaptive sparsity constraints. Our method extends the standard SAE loss function to include a hierarchical orthogonality penalty and dynamically adjusts sparsity penalties based on feature activation frequency. By explicitly modeling hierarchical relationships between features, our approach reduces polysemanticity and improves interpretability. We evaluate our method on the sparse_probing benchmark, demonstrating its effectiveness in tasks involving hierarchical relationships. This work provides a new framework for learning interpretable representations in SAEs, with potential applications in model transparency and steerability.",
        "novel": false
    },
    {
        "Name": "controlled_feature_sharing_sae",
        "Title": "Controlled Feature Sharing for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement controlled feature sharing through a tunable overlap parameter. 2. Add dynamic sparsity penalty based on feature overlap. 3. Train on Pythia-70M using WikiText and WMDP-bio datasets. 4. Evaluate on sparse_probing benchmark across diverse tasks. 5. Compare performance against baseline SAE and orthogonal SAE. 6. Analyze feature overlap and interpretability using automated methods and activation patterns.",
        "Technical_Details": "The method introduces a controlled feature sharing mechanism: L = L_recon + \u03bb_1(t) * (L_sparse + \u03bb_2 * L_overlap), where L_overlap = ||W_1^T W_2 - \u03b1I||_F. Here, \u03b1 controls the degree of feature sharing, and I is the identity matrix. The sparsity penalty \u03bb_1(t) is dynamically adjusted based on the overlap: \u03bb_1(t) = \u03bb_1_max * (1 - \u03b2 * overlap(t)), where \u03b2 controls the sensitivity to overlap and overlap(t) is computed as the mean cosine similarity between feature pairs. Feature overlap is computed batch-wise and updated every n=100 steps. The decoder uses shared weights with feature-specific biases to balance reconstruction and interpretability.",
        "Implementation_Plan": "1. Add ControlledFeatureSharingLoss with dynamic sparsity adjustment. 2. Implement batch-wise overlap computation and update mechanism. 3. Modify CustomSAE to include feature-specific biases. 4. Add evaluation metrics for feature overlap, interpretability, and activation patterns. 5. Update CustomTrainer with dynamic loss weighting. 6. Integrate controlled sharing into the training loop.",
        "Interestingness_Evaluation": "The idea is interesting because it provides a principled way to balance feature separation and sharing, addressing a key challenge in mechanistic interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is feasible because it builds on existing SAE frameworks with minimal additional complexity. The dynamic sparsity penalty and overlap computation are computationally efficient and can be implemented within the 30-minute runtime constraint on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The controlled feature sharing mechanism is novel and provides a more flexible alternative to strict orthogonality, addressing a key limitation in current approaches.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed model is expected to improve performance on the sparse_probing benchmark by learning more interpretable, monosemantic features while maintaining reconstruction quality.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but they often struggle with polysemanticity in latent representations. We propose a Controlled Feature Sharing SAE (CFS-SAE) that addresses this challenge by allowing controlled overlap between features while maintaining interpretability. Our approach introduces a tunable parameter \u03b1 that governs the degree of feature sharing, enabling the model to balance the trade-off between strict orthogonality and practical performance. By dynamically adjusting the sparsity penalty based on feature overlap, the model reduces polysemanticity while preserving reconstruction quality. We evaluate our method on the sparse_probing benchmark, demonstrating its ability to learn interpretable, monosemantic features across diverse tasks. This work provides a principled framework for improving the interpretability of SAEs, with potential applications in model transparency and steerability.",
        "novel": false
    },
    {
        "Name": "adaptive_feature_sharing_sae_v5",
        "Title": "Robust Hierarchical Adaptive Feature Sharing for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement adaptive feature sharing using a piecewise linear schedule for \u03b1(t) with a single predefined reconstruction error threshold. 2. Add simplified adaptive orthogonality constraints that adjust based on reconstruction error. 3. Train on Pythia-70M using WikiText and WMDP-bio datasets. 4. Compare performance on sparse_probing benchmark, including hierarchical probing tasks (e.g., profession classification, sentiment analysis) with cross-validation and feature absorption analysis, against baseline and strict orthogonal SAE. 5. Analyze the impact of adaptive feature sharing on interpretability and reconstruction quality.",
        "Technical_Details": "The method uses a modified loss function: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_ortho, where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_ortho = ||W_1^T W_2 - \u03b1(t)I||_F, where \u03b1(t) follows a piecewise linear schedule updated at a single predefined reconstruction error threshold. Feature groups are updated every n=100 steps using efficient batch statistics. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality. The adaptive orthogonality constraints are simplified to reduce computational overhead.",
        "Implementation_Plan": "1. Add AdaptiveFeatureSharingLoss with a piecewise linear schedule for \u03b1(t) and a single predefined reconstruction error threshold. 2. Implement efficient batch-wise feature grouping. 3. Modify CustomSAE to include group-specific biases and adaptive orthogonality constraints. 4. Add condition number calculation utilities. 5. Update CustomTrainer with adaptive loss weight and controlled feature sharing. 6. Add evaluation metrics for hierarchical probing with cross-validation, feature absorption, and adaptive orthogonality.",
        "Interestingness_Evaluation": "The idea of adaptive feature sharing with a single predefined reconstruction error threshold provides a more robust and practical approach to balancing feature separation and reconstruction quality, which is crucial for interpretability.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation is simplified with a single predefined reconstruction error threshold for \u03b1(t), reducing computational overhead; batch-wise updates and simple matrix operations ensure efficiency; all computations are well within the 30-minute limit on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The introduction of a single predefined reconstruction error threshold for adaptive feature sharing is a novel and practical approach to addressing the trade-off between feature separation and reconstruction quality.",
        "Novelty": 9,
        "Expected_Research_Impact": "The proposed model is expected to improve interpretability (as measured by hierarchical probing with cross-validation and feature absorption analysis) while maintaining or improving reconstruction quality, making it a promising approach for mechanistic interpretability.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but they often suffer from polysemanticity, where individual latent features represent multiple, semantically distinct concepts. This paper introduces Robust Hierarchical Adaptive Feature Sharing Sparse Autoencoders (AFS-SAE), a novel approach that balances feature separation and reconstruction quality through adaptive feature sharing. By using a piecewise linear schedule for the degree of feature overlap with a single predefined reconstruction error threshold and incorporating simplified adaptive orthogonality constraints, AFS-SAE achieves better interpretability while maintaining reconstruction fidelity. We evaluate AFS-SAE on the sparse_probing benchmark, including hierarchical probing tasks with cross-validation and feature absorption analysis, demonstrating its effectiveness in improving mechanistic interpretability without sacrificing model performance.",
        "novel": true
    },
    {
        "Name": "hierarchical_sae_v7",
        "Title": "Task-Aligned Hierarchical Feature Learning with Validation-Based Scheduling for Interpretable Sparse Autoencoders",
        "Experiment": "1. Initialize hierarchical feature groups using task-specific embeddings derived from the target benchmark tasks. 2. Implement adaptive group updates with momentum-based updates and validation-based learning rate scheduling. 3. Add approximate layer-wise relevance propagation (LRP) for efficient feature importance metrics. 4. Train on Pythia-70M using WikiText and WMDP-bio datasets. 5. Compare performance on sparse_probing and core benchmarks against baseline and prototype SAE. 6. Analyze feature activation patterns to evaluate interpretability. 7. Evaluate impact of different validation-based scheduling strategies and task-specific initialization methods.",
        "Technical_Details": "The method extends the prototype's adaptive objective: L = L_recon + \u03bb_1(t) * L_sparse + \u03bb_2(t) * L_ortho, where \u03bb_1(t) and \u03bb_2(t) are dynamically adjusted based on hierarchical feature levels, reconstruction feedback, and approximate layer-wise relevance scores. L_sparse is computed as a weighted sum of L1 norms, with weights determined by the hierarchical level, reconstruction contribution, and approximate LRP of each feature. L_ortho is computed within hierarchical groups, ensuring orthogonality only within semantically related features. Feature groups are initialized using task-specific embeddings derived from the target benchmark tasks and updated adaptively with momentum-based updates and validation-based learning rate scheduling. The decoder uses group-specific biases and shared weights within hierarchical clusters to balance separation and reconstruction quality.",
        "Implementation_Plan": "1. Add TaskSpecificInitialization module for hierarchical grouping. 2. Implement ValidationBasedScheduling for adaptive learning rate updates. 3. Modify CustomSAE to include task-specific initialization, adaptive updates, and approximate LRP-aware group-specific biases. 4. Add utilities for task-specific initialization analysis. 5. Update CustomTrainer with dynamic loss weights, adaptive updates, and approximate LRP-aware grouping. 6. Add evaluation metrics for task-specific initialization and interpretability.",
        "Interestingness_Evaluation": "The integration of task-specific initialization and validation-based scheduling provides a more aligned and efficient approach to reducing polysemanticity in sparse autoencoders.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient, with task-specific initialization and validation-based scheduling adding minimal overhead. All computations are well within the 30-minute limit on an H100 GPU, and the coding work is manageable for a junior CS PhD student within 1 month.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of task-specific initialization and validation-based scheduling introduces a novel way to improve the alignment and efficiency of hierarchical feature learning in sparse autoencoders.",
        "Novelty": 9,
        "Expected_Research_Impact": "The proposed model is expected to improve performance on the sparse_probing benchmark by better capturing semantic relationships and reducing polysemanticity. The task-specific initialization and validation-based scheduling should also improve reconstruction quality, making the model more useful for mechanistic interpretability tasks.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is often limited by polysemanticity in the latent space. We propose a task-aligned hierarchical feature learning approach that addresses this challenge by explicitly modeling semantic relationships between features. Our method introduces hierarchical feature grouping using task-specific initialization, adaptive momentum-based updates with validation-based learning rate scheduling, and approximate layer-wise relevance propagation. By reducing feature absorption and improving interpretability, our approach aims to enhance the utility of SAEs for tasks such as sparse probing and core evaluation. The proposed method is efficient, scalable, and grounded in practical insights about feature hierarchies, making it a promising direction for future research in mechanistic interpretability.",
        "novel": true
    },
    {
        "Name": "soft_orthogonal_sae",
        "Title": "Soft Orthogonal Feature Learning for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement a soft orthogonality loss with controlled feature sharing. 2. Add adaptive feature grouping based on cosine similarity. 3. Train on Pythia-70M using WikiText and WMDP-bio datasets. 4. Compare interpretability against baseline and strict orthogonal SAE using the sparse_probing benchmark. 5. Analyze feature overlap, reconstruction quality, and condition number of the feature matrix. 6. Evaluate impact of different sharing coefficients on interpretability.",
        "Technical_Details": "The method uses a modified objective: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2(t) * L_soft_ortho, where \u03bb_2(t) = \u03bb_2_max * min(1, t/t_0) increases linearly until step t_0. L_soft_ortho = ||W_1^T W_2 - \u03b1S||_F, where S is a diagonal matrix initialized to the identity matrix and updated every n=100 steps based on cosine similarity between feature vectors. Feature groups are dynamically updated using efficient batch statistics. The decoder uses shared weights with group-specific biases to balance separation and reconstruction quality.",
        "Implementation_Plan": "1. Add SoftOrthogonalityLoss with scheduling. 2. Implement efficient batch-wise feature grouping based on cosine similarity. 3. Modify CustomSAE to include group-specific biases. 4. Add feature overlap analysis utilities. 5. Update CustomTrainer with adaptive loss weight. 6. Add evaluation metrics for controlled sharing, interpretability, and condition number.",
        "Interestingness_Evaluation": "The combination of soft orthogonality and adaptive feature grouping provides a more flexible and effective approach to reducing polysemanticity in sparse autoencoders.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains efficient with batch-wise updates and simple matrix operations; adaptive weighting adds minimal overhead; controlled sharing through \u03b1 provides easy tuning; all computations well within 30-minute limit on H100.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The soft orthogonality constraint with controlled feature sharing provides a novel theoretical framework for balancing feature separation and interpretability.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed model is expected to improve interpretability on the sparse_probing benchmark by reducing polysemanticity while maintaining reconstruction quality.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is often limited by polysemanticity in latent representations. We propose a soft orthogonal feature learning approach that reduces polysemanticity by introducing a controlled feature sharing mechanism. Our method uses a soft orthogonality loss that penalizes excessive feature overlap while allowing for natural feature sharing, combined with adaptive feature grouping based on cosine similarity to dynamically group related features during training. We evaluate our approach on the sparse_probing benchmark, demonstrating improved interpretability of latent features without sacrificing reconstruction quality. This work provides a more flexible and effective framework for understanding the internal representations of neural networks.",
        "novel": true
    },
    {
        "Name": "hierarchical_orthogonal_sae",
        "Title": "Hierarchical Orthogonal Feature Learning with Adaptive Group-Wise Sparsity and Group-Wise Feature Importance Decay for Enhanced Interpretability in Sparse Autoencoders",
        "Experiment": "1. Implement fixed hierarchical feature grouping with two levels using cosine similarity between decoder weights. 2. Add sliding window-based adaptive sparsity constraints to estimate feature importance efficiently. 3. Introduce adaptive group-wise sparsity thresholds based on group contribution to reconstruction loss. 4. Add group-wise feature importance decay to gradually reduce the influence of less important features within each hierarchical group over time. 5. Train on Pythia-70M using WMDP-bio and WikiText datasets. 6. Compare performance on sparse_probing and core benchmarks against baseline and fixed orthogonal SAE. 7. Analyze feature separation using hierarchical clustering metrics. 8. Evaluate impact of different hierarchical grouping thresholds, adaptive group-wise sparsity strengths, and group-wise decay rates.",
        "Technical_Details": "The method extends the prototype's adaptive objective: L = L_recon + \u03bb_1(t) * L_sparse + \u03bb_2(t) * L_ortho + \u03bb_3(t) * L_group_sparse, where \u03bb_1(t), \u03bb_2(t), and \u03bb_3(t) are dynamically adjusted based on feature importance and hierarchical grouping. L_ortho = ||W_1^T W_2 - \u03b1I||_F is applied within two fixed hierarchical levels, allowing for controlled feature sharing at multiple levels of abstraction. L_group_sparse enforces sparsity within each hierarchical group, with the sparsity penalty for each group dynamically adjusted based on its contribution to the reconstruction loss. Feature importance is estimated using a sliding window of reconstruction loss contributions, with a group-wise decay mechanism applied to gradually reduce the influence of less important features within each hierarchical group over time. Features with importance below a dynamically adjusted threshold are pruned to reduce redundancy. Hierarchical grouping is fixed after initialization and updated only if significant drift in feature similarity is detected.",
        "Implementation_Plan": "1. Add FixedHierarchicalFeatureGrouping module with two levels of cosine similarity-based clustering. 2. Implement SlidingWindowSparsityLoss with dynamic weighting based on feature importance. 3. Add AdaptiveGroupWiseSparsityLoss to enforce sparsity within hierarchical groups, with penalties adjusted based on group contribution to reconstruction loss. 4. Add GroupWiseFeatureImportanceDecay module to gradually reduce the influence of less important features within each hierarchical group over time. 5. Modify CustomSAE to include hierarchical group-specific biases, adaptive sparsity, adaptive group-wise sparsity, and group-wise feature importance decay. 6. Add utilities for hierarchical clustering analysis. 7. Update CustomTrainer with adaptive loss weights, fixed hierarchical grouping, adaptive group-wise sparsity, and group-wise feature importance decay. 8. Add evaluation metrics for hierarchical feature separation, adaptive group-wise sparsity effectiveness, and group-wise decay impact.",
        "Interestingness_Evaluation": "The combination of fixed hierarchical feature grouping, sliding window-based adaptive sparsity constraints, adaptive group-wise sparsity, and group-wise feature importance decay provides a novel and practical approach to reducing polysemanticity in SAEs.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation leverages efficient batch-wise updates and simple matrix operations; fixed hierarchical grouping reduces overhead; sliding window-based sparsity constraints, adaptive group-wise sparsity, and group-wise feature importance decay are easy to tune; all computations remain within the 30-minute limit on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The fixed hierarchical grouping mechanism, sliding window-based adaptive sparsity constraints, adaptive group-wise sparsity, and group-wise feature importance decay introduce a novel and theoretically grounded approach to feature separation in SAEs.",
        "Novelty": 9,
        "Expected_Research_Impact": "The proposed method is expected to improve performance on the sparse_probing and core benchmarks by enabling better separation of semantically distinct features and reducing redundancy, leading to more interpretable latent representations.",
        "Research_Impact": 9,
        "Overall_Score": 9.0,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is often limited by polysemanticity in the latent space. We propose a novel approach to address this challenge by introducing fixed hierarchical feature grouping, sliding window-based adaptive sparsity constraints, adaptive group-wise sparsity, and group-wise feature importance decay. Our method extends the adaptive orthogonality loss from prior work, applying it within two fixed hierarchical levels to enable more nuanced separation of semantically distinct concepts. By dynamically adjusting sparsity constraints, enforcing adaptive group-wise sparsity based on group contribution to reconstruction loss, and gradually reducing the influence of less important features within each hierarchical group over time, we ensure that critical features are preserved while reducing redundancy. We evaluate our approach on the sparse_probing and core benchmarks, demonstrating its potential to improve interpretability and performance in SAEs.",
        "novel": false
    },
    {
        "Name": "hierarchical_sae_v7",
        "Title": "Group-Specific Reconstruction Weights with Hierarchical Clustering for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement group-specific reconstruction weights based on feature importance and activation frequency. 2. Use hierarchical clustering for adaptive group merging. 3. Train on Pythia-70M using WikiText and WMDP-bio datasets. 4. Evaluate on the sparse_probing benchmark, focusing on hierarchical classification tasks. 5. Compare against baseline SAE and fixed orthogonal SAE. 6. Analyze feature activation patterns and group coherence over training.",
        "Technical_Details": "The method extends the standard SAE loss function: L = \u03a3_g w(g) * L_recon(g) + \u03a3_g \u03bb_1(g, t) * L_sparse(g) + \u03bb_2 * L_group, where L_group enforces hierarchical structure. Feature groups are identified using dynamic correlation thresholds, adjusted every n=100 steps using exponential moving averages of activation statistics. Group-specific reconstruction weights are applied: w(g) = w_base(tier(g)) * (1 + \u03b2 * activation_frequency(g)), where \u03b2 controls the strength of adaptation. Adaptive group merging is applied using hierarchical clustering, ensuring coherence. The decoder uses shared weights within groups but separate biases to maintain reconstruction quality. Hierarchical clustering is implemented efficiently using batch statistics and incremental updates.",
        "Implementation_Plan": "1. Add GroupSpecificReconstructionLoss with group-specific weight computation. 2. Modify CustomSAE to include hierarchical clustering for group merging. 3. Update CustomTrainer to handle group-specific reconstruction weights and periodic updates. 4. Add utilities for hierarchical clustering analysis. 5. Integrate evaluation metrics for group coherence and interpretability. 6. Update the sparse_probing benchmark to include hierarchical tasks.",
        "Interestingness_Evaluation": "The use of group-specific reconstruction weights and hierarchical clustering provides a novel and adaptive approach to improving interpretability in SAEs.",
        "Interestingness": 9,
        "Feasibility_Evaluation": "The implementation remains computationally efficient, with group-specific reconstruction weights and hierarchical clustering adding minimal overhead. All operations are well within the 30-minute limit on an H100 GPU.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The group-specific reconstruction weights approach introduces a new way to adaptively model feature relationships in SAEs, addressing a key limitation of existing methods.",
        "Novelty": 8,
        "Expected_Research_Impact": "The proposed method is expected to significantly improve performance on the sparse_probing benchmark by reducing polysemanticity and improving feature interpretability.",
        "Research_Impact": 9,
        "Overall_Score": 8.8,
        "Abstract": "Sparse autoencoders (SAEs) are a powerful tool for mechanistic interpretability, but their effectiveness is limited by polysemanticity, where individual latent features represent multiple, semantically distinct concepts. We propose a novel approach to address this challenge by introducing group-specific reconstruction weights and hierarchical clustering for adaptive group merging. Our method groups features based on dynamically adjusted correlation thresholds and applies reconstruction weights according to group-specific schedules. This approach improves the interpretability of latent features while maintaining reconstruction quality. We evaluate our method on the sparse_probing benchmark, demonstrating its ability to better separate hierarchical concepts and reduce feature absorption. Our results suggest that group-specific reconstruction weights are a promising direction for improving the interpretability of SAEs in mechanistic interpretability tasks.",
        "novel": false
    }
]