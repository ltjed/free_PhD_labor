# Title: Batch Size Grokking: Assessing the impact of the training batchsize on the grokking phenomenon
# Experiment description: Modify the experiments to dynamically adjust the batch size during training, starting with a small batch size and gradually increasing it. This could potentially lead to faster generalization on the validation set.
## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.0071290188158551855, 'final_val_loss_mean': 0.008281217887997627, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4353.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.006014337918410699, 'final_val_loss_mean': 0.007335097063332796, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4403.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.0029467708275963864, 'final_val_loss_mean': 0.003054410684853792, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2640.0}, 'permutation': {'final_train_loss_mean': 0.12446165302147467, 'final_val_loss_mean': 8.616557439168295, 'final_train_acc_mean': 0.9800130526224772, 'final_val_acc_mean': 0.009114583333333334, 'step_val_acc_99_mean': 7500.0}}
Description: Baseline results with fixed batch size of 512.

## Run 1: Linear Batch Size Increase
Results: {'x_div_y': {'final_train_loss_mean': 2.266246477762858, 'final_val_loss_mean': 2.2404908339182534, 'final_train_acc_mean': 0.22557078177730241, 'final_val_acc_mean': 0.22672526041666666, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 2.595820109049479, 'final_val_loss_mean': 2.601343035697937, 'final_train_acc_mean': 0.2403783512612184, 'final_val_acc_mean': 0.2520345052083333, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 1.0273989240328472, 'final_val_loss_mean': 1.0622934301694233, 'final_train_acc_mean': 0.5804957747459412, 'final_val_acc_mean': 0.5758463541666666, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.347664674123128, 'final_val_loss_mean': 4.353273868560791, 'final_train_acc_mean': 0.01917808239037792, 'final_val_acc_mean': 0.019612630208333332, 'step_val_acc_99_mean': 7500.0}}
Description: Linear batch size increase from 32 to 512 over training steps. The results show significantly worse performance compared to the baseline across all operations. None of the models achieved successful grokking (99% validation accuracy). The linear schedule may be too slow, keeping batch sizes too small for too long, preventing effective learning. The best performance was on x_plus_y (~58% accuracy) while other operations showed very poor learning (1-25% accuracy). This suggests that the linear batch size increase strategy significantly impairs the model's ability to learn the underlying patterns.

## Run 2: Exponential Batch Size Increase
Results: {'x_div_y': {'final_train_loss_mean': 2.3000516494115195, 'final_val_loss_mean': 2.2981305519739785, 'final_train_acc_mean': 0.1594771221280098, 'final_val_acc_mean': 0.15616861979166666, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 4.574722448984782, 'final_val_loss_mean': 4.57502285639445, 'final_train_acc_mean': 0.0110457514723142, 'final_val_acc_mean': 0.008951822916666666, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 2.6451091368993125, 'final_val_loss_mean': 2.6454552014668784, 'final_train_acc_mean': 0.22705881421764693, 'final_val_acc_mean': 0.222412109375, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.787588119506836, 'final_val_loss_mean': 4.788022359212239, 'final_train_acc_mean': 0.008366013100991646, 'final_val_acc_mean': 0.00830078125, 'step_val_acc_99_mean': 7500.0}}
Description: Exponential increase in batch size from 32 to 512 over training steps. The results show that this approach also failed to achieve successful learning or grokking. Performance was even worse than the linear schedule for most operations. x_div_y achieved ~16% accuracy, x_plus_y ~22%, while x_minus_y and permutation barely learned anything (~1% accuracy). The exponential schedule appears to increase batch sizes too aggressively, potentially preventing the model from establishing good initial learning trajectories. This suggests that dynamic batch sizing, whether linear or exponential, may be fundamentally incompatible with the grokking phenomenon in this setting.

## Run 3: Delayed Start Large Batch
Results: {'x_div_y': {'final_train_loss_mean': 1.685325026512146, 'final_val_loss_mean': 1.5671646197636921, 'final_train_acc_mean': 0.440690112610658, 'final_val_acc_mean': 0.4593098958333333, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 1.1696660816669464, 'final_val_loss_mean': 1.2242175141970317, 'final_train_acc_mean': 0.5555338660875956, 'final_val_acc_mean': 0.515380859375, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 1.270975112915039, 'final_val_loss_mean': 1.2511341373125713, 'final_train_acc_mean': 0.5414713621139526, 'final_val_acc_mean': 0.5577799479166666, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.787484963734944, 'final_val_loss_mean': 4.787497679392497, 'final_train_acc_mean': 0.007617187686264515, 'final_val_acc_mean': 0.007975260416666666, 'step_val_acc_99_mean': 7500.0}}
Description: The two-phase approach showed better performance than both linear and exponential batch size schedules, but still failed to achieve grokking. While no operation reached 99% validation accuracy, the model achieved moderate success on x_minus_y (~52%), x_plus_y (~56%), and x_div_y (~46%). This is a significant improvement over previous dynamic batch size approaches, suggesting that a clean separation between small-batch initial learning and large-batch later training is more effective than gradual transitions. However, the permutation operation still showed almost no learning (~0.8% accuracy), indicating that the fundamental challenge of grokking this operation remains unsolved by batch size modifications.

## Run 4: Extended Small Batch Phase
Results: {'x_div_y': {'final_train_loss_mean': 1.6747297445933025, 'final_val_loss_mean': 1.7545265356699626, 'final_train_acc_mean': 0.34524739782015484, 'final_val_acc_mean': 0.3085123697916667, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 2.0797253052393594, 'final_val_loss_mean': 2.0885650714238486, 'final_train_acc_mean': 0.2822265724341075, 'final_val_acc_mean': 0.2670084635416667, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 2.59316615263621, 'final_val_loss_mean': 2.570964495340983, 'final_train_acc_mean': 0.23899740570535263, 'final_val_acc_mean': 0.251708984375, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.787609577178955, 'final_val_loss_mean': 4.787416299184163, 'final_train_acc_mean': 0.008658854446063438, 'final_val_acc_mean': 0.009114583333333334, 'step_val_acc_99_mean': 7500.0}}
Description: The extended small-batch phase (2500 steps vs 1000 steps) actually led to worse performance compared to Run 3 across all operations. While Run 3 achieved moderate success (46-56% accuracy), Run 4's accuracies dropped significantly: x_div_y (~31%), x_minus_y (~27%), x_plus_y (~25%), and permutation (~0.9%). This suggests that extending the small-batch phase was counterproductive, possibly because it delayed the transition to the more stable large-batch training for too long. The results indicate that the original 1000-step small-batch phase in Run 3 struck a better balance between initial learning and stability.

## Run 5: Shorter Small Batch Phase
Description: Given that extending the small-batch phase in Run 4 led to worse performance, and Run 3's results were the best so far, we'll try reducing the small-batch phase to 500 steps (half of Run 3's duration). This will test whether we can maintain the benefits of initial small-batch learning while transitioning to stable large-batch training earlier. The hypothesis is that a shorter small-batch phase might be sufficient for establishing good initial learning trajectories while allowing more time for the model to benefit from large-batch training.

# Plot Analysis

For each dataset (x_div_y, x_minus_y, x_plus_y, permutation), we generated four types of plots:

## Training Loss Plots (train_loss_{dataset}.png)
These plots show how the training loss evolves over update steps for each experimental configuration. The baseline (fixed batch size=512) consistently shows the most stable and effective training trajectory, reaching the lowest training loss values. The dynamic batch size approaches (linear and exponential) show higher volatility and generally worse performance. The two-phase approaches (Runs 3-5) show intermediate performance, with the 500-step small batch phase (Run 3) performing best among the dynamic approaches.

## Validation Loss Plots (val_loss_{dataset}.png)
The validation loss plots reveal the generalization capabilities of each approach. The baseline again shows superior performance, with smooth convergence to low validation loss values. The dynamic batch size approaches show high validation loss values, indicating poor generalization. Notably, the validation loss curves closely mirror the training loss patterns, suggesting that the models are not overfitting but rather struggling to learn the underlying patterns effectively.

## Training Accuracy Plots (train_acc_{dataset}.png)
Training accuracy plots demonstrate the model's ability to fit the training data. The baseline achieves near-perfect training accuracy on most operations except permutation. The dynamic batch size approaches show significantly lower training accuracy, with the linear and exponential schedules performing particularly poorly. The two-phase approaches achieve moderate training accuracy, but still fall short of the baseline's performance.

## Validation Accuracy Plots (val_acc_{dataset}.png)
These plots are perhaps the most important for understanding grokking behavior. The baseline shows clear evidence of grokking on x_div_y, x_minus_y, and x_plus_y operations, with sudden jumps in validation accuracy after extended training. None of the dynamic batch size approaches achieve comparable grokking behavior. The permutation operation remains challenging across all approaches, suggesting that the task's complexity may require different architectural or training modifications beyond batch size adjustments.

Key Observations:
1. The baseline's fixed batch size of 512 consistently outperforms all dynamic batch size approaches
2. Dynamic batch size strategies (both gradual and two-phase) appear to interfere with the grokking phenomenon
3. Shorter small-batch phases (500 steps) perform better than longer ones (2500 steps)
4. The permutation operation remains resistant to grokking across all experimental conditions
5. Error bands (shown as shaded regions) indicate that results are consistent across multiple seeds

These visualizations strongly suggest that the grokking phenomenon may be dependent on maintaining a consistent, relatively large batch size throughout training. The attempt to accelerate grokking through dynamic batch sizes appears to be counterproductive, possibly because it disrupts the stable optimization dynamics needed for the phenomenon to emerge.
