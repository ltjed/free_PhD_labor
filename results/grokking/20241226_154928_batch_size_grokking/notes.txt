# Title: Batch Size Grokking: Assessing the impact of the training batchsize on the grokking phenomenon
# Experiment description: Modify the experiments to dynamically adjust the batch size during training, starting with a small batch size and gradually increasing it. This could potentially lead to faster generalization on the validation set.
## Run 0: Baseline
## Run 1: Linear Batch Size Growth
Description: Start with batch_size=32, linearly increase to 512 over training steps. This provides a smooth, gradual increase in batch size which may help with early learning while still achieving good final convergence.
Results: {'x_div_y': {'final_train_loss_mean': 0.025387086905539036, 'final_val_loss_mean': 0.02870293737699588, 'final_train_acc_mean': 0.9997395873069763, 'final_val_acc_mean': 0.999267578125, 'step_val_acc_99_mean': 3866.6666666666665}, 'x_minus_y': {'final_train_loss_mean': 0.029672636029620964, 'final_val_loss_mean': 0.02961081111182769, 'final_train_acc_mean': 0.9998698035875956, 'final_val_acc_mean': 0.9994303385416666, 'step_val_acc_99_mean': 2753.3333333333335}, 'x_plus_y': {'final_train_loss_mean': 0.0042654947222520905, 'final_val_loss_mean': 0.004420199353868763, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2060.0}, 'permutation': {'final_train_loss_mean': 0.07891359661395352, 'final_val_loss_mean': 4.712180217728019, 'final_train_acc_mean': 0.986328125, 'final_val_acc_mean': 0.3477376302083333, 'step_val_acc_99_mean': 7233.333333333333}}
Analysis: Linear batch size growth showed promising results for arithmetic operations, particularly for x_minus_y and x_plus_y where we saw 37% and 22% improvements in time to reach 99% validation accuracy respectively. The approach maintained high final accuracies while achieving faster generalization. However, it did not help with the more complex permutation task, suggesting that dynamic batch sizing may be most effective for simpler arithmetic operations.

## Run 2: Exponential Batch Size Growth
Description: Start with batch_size=32, exponentially increase to 512 over training steps. This provides faster batch size growth early in training while still maintaining the benefits of small initial batches. The exponential growth may help bridge the gap between early learning and final convergence more aggressively than linear growth.
Results: {'x_div_y': {'final_train_loss_mean': 0.010787689747909704, 'final_val_loss_mean': 0.024116545915603638, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.997314453125, 'step_val_acc_99_mean': 3896.6666666666665}, 'x_minus_y': {'final_train_loss_mean': 0.017275613422195118, 'final_val_loss_mean': 0.04886412930985292, 'final_train_acc_mean': 0.9998043179512024, 'final_val_acc_mean': 0.9931640625, 'step_val_acc_99_mean': 4770.0}, 'x_plus_y': {'final_train_loss_mean': 0.006537169683724642, 'final_val_loss_mean': 0.0068177216065426665, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2423.3333333333335}, 'permutation': {'final_train_loss_mean': 0.0918336349229018, 'final_val_loss_mean': 5.4179532925287885, 'final_train_acc_mean': 0.9902152816454569, 'final_val_acc_mean': 0.17399088541666666, 'step_val_acc_99_mean': 7500.0}}
Analysis: The exponential batch size growth strategy showed mixed results compared to linear growth. While it maintained good final accuracies for arithmetic operations, it generally took longer to reach 99% validation accuracy. For x_minus_y, performance was notably worse than linear growth (4770 vs 2750 steps), and x_plus_y also showed slightly degraded performance (2423 vs 2060 steps). The approach struggled significantly with the permutation task, failing to reach 99% validation accuracy. This suggests that more aggressive batch size growth may interfere with the model's ability to learn complex patterns, particularly for more challenging tasks like permutations. The results indicate that linear batch size growth may be more suitable for balancing early learning and final convergence.

## Run 3: Curriculum Learning with Fixed Batch Size
Description: Instead of varying batch size, implement curriculum learning where we start with simpler arithmetic operations (x_plus_y) and gradually introduce more complex ones (x_minus_y, x_div_y, permutation). Use a fixed batch size of 512 but modify the training data distribution to focus on mastering simpler operations first.

Results: {'x_div_y': {'final_train_loss_mean': 0.03908689490829905, 'final_val_loss_mean': 0.7522831012805303, 'final_train_acc_mean': 0.996289074420929, 'final_val_acc_mean': 0.7864583333333334, 'step_val_acc_99_mean': 7183.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.28617105229447287, 'final_val_loss_mean': 0.1240572224681576, 'final_train_acc_mean': 0.942578136920929, 'final_val_acc_mean': 0.9816080729166666, 'step_val_acc_99_mean': 5666.666666666667}, 'x_plus_y': {'final_train_loss_mean': 0.0055594964263339835, 'final_val_loss_mean': 0.005967737020303805, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2333.3333333333335}, 'permutation': {'final_train_loss_mean': 0.1223279641320308, 'final_val_loss_mean': 9.488898595174154, 'final_train_acc_mean': 0.983789066473643, 'final_val_acc_mean': 0.003173828125, 'step_val_acc_99_mean': 7500.0}}

Analysis: The curriculum learning approach showed mixed results. For x_plus_y, it performed well with fast convergence (2333 steps to 99% validation accuracy) and perfect final accuracy. However, performance degraded for more complex operations. While x_minus_y achieved good final validation accuracy (98.2%), it took longer to converge (5666 steps). x_div_y struggled more, reaching only 78.6% validation accuracy, and the permutation task performed extremely poorly with 0.3% validation accuracy. This suggests that while curriculum learning helped with simpler operations, it may have interfered with learning more complex tasks, possibly due to catastrophic forgetting when transitioning between curriculum stages. The fixed batch size of 512 may have also been suboptimal for learning the more complex operations.

## Run 4: Hybrid Approach - Curriculum Learning with Dynamic Batch Sizes
Description: Combine the insights from previous runs by implementing curriculum learning with task-specific batch sizes. Use smaller batch sizes (32) for complex operations (permutation, x_div_y) and larger ones (512) for simpler tasks (x_plus_y, x_minus_y). This may help balance the learning dynamics across different operation complexities while maintaining the benefits of curriculum learning.

Results: {'x_div_y': {'final_train_loss_mean': 0.45525435109933216, 'final_val_loss_mean': 0.6134743392467499, 'final_train_acc_mean': 0.8854166865348816, 'final_val_acc_mean': 0.8046875, 'step_val_acc_99_mean': 7370.0}, 'x_minus_y': {'final_train_loss_mean': 0.00508837114709119, 'final_val_loss_mean': 0.006377934788664182, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 5130.0}, 'x_plus_y': {'final_train_loss_mean': 0.7489134383698305, 'final_val_loss_mean': 0.46188898601879674, 'final_train_acc_mean': 0.84375, 'final_val_acc_mean': 0.9119466145833334, 'step_val_acc_99_mean': 2210.0}, 'permutation': {'final_train_loss_mean': 4.319496313730876, 'final_val_loss_mean': 4.926805814107259, 'final_train_acc_mean': 0.055208334078391395, 'final_val_acc_mean': 0.005208333333333333, 'step_val_acc_99_mean': 7500.0}}

Analysis: The hybrid approach combining curriculum learning with task-specific batch sizes showed mixed results. While x_minus_y achieved perfect validation accuracy and x_plus_y showed reasonable performance (91.2% validation accuracy), both x_div_y and permutation tasks struggled. The permutation task performed particularly poorly with only 0.5% validation accuracy, suggesting that neither curriculum learning nor smaller batch sizes were sufficient to learn this complex operation. The x_div_y task showed moderate performance (80.5% validation accuracy) but failed to reach the 99% threshold. These results indicate that while task-specific batch sizes may help with certain operations, the fundamental challenge of learning complex operations like permutations may require more sophisticated approaches beyond batch size tuning and curriculum learning.

## Run 5: Extended Training with Increased Model Capacity
Description: Given the consistent difficulty with the permutation task across all previous approaches, we'll try increasing the model's capacity by doubling the number of transformer layers (from 2 to 4) and the embedding dimension (from 128 to 256). This may help the model learn more complex patterns, particularly for the challenging permutation task, while maintaining good performance on arithmetic operations.

# Plot Analysis

For each dataset (x_div_y, x_minus_y, x_plus_y, permutation), we generated four types of plots to visualize the learning dynamics across different experimental runs:

## Training Loss Plots (train_loss_{dataset}.png)
These plots show how the training loss evolves over update steps for each experimental run. The y-axis represents the training loss value, while the x-axis shows the number of update steps. Lower values indicate better performance during training. Key observations:
- Shaded areas represent the standard error across multiple seeds
- Smoother curves generally indicate more stable training
- Sharp drops may indicate significant learning breakthroughs
- Plateaus suggest periods where the model's learning has stabilized

## Validation Loss Plots (val_loss_{dataset}.png)
These plots track the validation loss over time, providing insights into the model's generalization capabilities. The relationship between training and validation loss helps identify potential overfitting:
- Divergence between training and validation loss suggests overfitting
- Parallel trends between training and validation indicate good generalization
- Sudden spikes in validation loss while training loss remains low are red flags
- The final validation loss values help compare the ultimate effectiveness of different approaches

## Training Accuracy Plots (train_acc_{dataset}.png)
These visualizations show the percentage of correct predictions on the training set over time:
- Steeper initial slopes indicate faster learning
- Perfect accuracy (1.0) suggests the model has memorized the training data
- Oscillations might indicate unstable learning or optimization issues
- The speed of accuracy improvement helps compare learning efficiency across approaches

## Validation Accuracy Plots (val_acc_{dataset}.png)
These are perhaps the most important plots for assessing real-world performance:
- Higher final validation accuracy indicates better generalization
- The point where validation accuracy plateaus suggests when learning saturates
- Comparing validation accuracy curves across runs reveals which approaches generalize better
- The gap between training and validation accuracy helps identify overfitting

Key Findings from Plots:
1. Simpler operations (x_plus_y, x_minus_y) generally show faster convergence and higher final accuracies across all approaches
2. The permutation task shows consistently challenging behavior, with high variance and poor generalization
3. Curriculum learning approaches (Runs 3 and 4) show interesting step-wise improvements corresponding to curriculum stages
4. The extended model capacity (Run 5) shows improved learning stability but doesn't fully solve the permutation task challenges
5. Batch size variations (Runs 1 and 2) demonstrate different convergence patterns, with linear growth often performing better than exponential

These visualizations provide crucial insights for understanding the effectiveness of different approaches and guiding future experimental designs. The plots should be examined together to form a complete picture of model behavior and performance.
