# Title: Batch Size Grokking: Assessing the impact of the training batchsize on the grokking phenomenon
# Experiment description: Modify the experiments to dynamically adjust the batch size during training, starting with a small batch size and gradually increasing it. This could potentially lead to faster generalization on the validation set.
## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.0071290188158551855, 'final_val_loss_mean': 0.008281217887997627, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4353.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.006014337918410699, 'final_val_loss_mean': 0.007335097063332796, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4403.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.0029467708275963864, 'final_val_loss_mean': 0.003054410684853792, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2640.0}, 'permutation': {'final_train_loss_mean': 0.12446165302147467, 'final_val_loss_mean': 8.616557439168295, 'final_train_acc_mean': 0.9800130526224772, 'final_val_acc_mean': 0.009114583333333334, 'step_val_acc_99_mean': 7500.0}}
Description: Baseline results.

## Run 1: Linear Batch Size Schedule
Description: Linear increase in batch size from 32 to 512 over training steps
Results: {'x_div_y': {'final_train_loss_mean': 1.5077176888783772, 'final_val_loss_mean': 1.510762373606364, 'final_train_acc_mean': 0.31985678275426227, 'final_val_acc_mean': 0.31591796875, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 1.5455852349599202, 'final_val_loss_mean': 1.461051066716512, 'final_train_acc_mean': 0.3548828065395355, 'final_val_acc_mean': 0.4324544270833333, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 0.36443069266776246, 'final_val_loss_mean': 0.3441591343532006, 'final_train_acc_mean': 0.856054683526357, 'final_val_acc_mean': 0.8670247395833334, 'step_val_acc_99_mean': 5400.0}, 'permutation': {'final_train_loss_mean': 4.618711789449056, 'final_val_loss_mean': 4.625306924184163, 'final_train_acc_mean': 0.01302083395421505, 'final_val_acc_mean': 0.012044270833333334, 'step_val_acc_99_mean': 7500.0}}
Analysis: The linear batch size scaling performed significantly worse than the baseline across all operations. Only x_plus_y showed reasonable performance with ~86% accuracy, while other operations failed to learn effectively. This suggests the linear scaling was too aggressive, not giving the model enough time with smaller batch sizes for initial learning.

## Run 2: Exponential Batch Size Schedule
Description: Exponential increase in batch size from 32 to 512 over training steps, providing more time with smaller batches early in training
Results: {'x_div_y': {'final_train_loss_mean': 2.1066681146621704, 'final_val_loss_mean': 2.0280085802078247, 'final_train_acc_mean': 0.26152344048023224, 'final_val_acc_mean': 0.263427734375, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 3.507563829421997, 'final_val_loss_mean': 3.5120228131612143, 'final_train_acc_mean': 0.16425781852255264, 'final_val_acc_mean': 0.160400390625, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 3.568186561266581, 'final_val_loss_mean': 3.542318105697632, 'final_train_acc_mean': 0.12630208767950535, 'final_val_acc_mean': 0.1376953125, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.787406285603841, 'final_val_loss_mean': 4.787602583567302, 'final_train_acc_mean': 0.008138020988553762, 'final_val_acc_mean': 0.007975260416666666, 'step_val_acc_99_mean': 7500.0}}
Analysis: The exponential batch size scaling performed even worse than linear scaling. Despite providing more training time with smaller batches early on, the model failed to learn effectively across all operations. This suggests that any aggressive batch size increase schedule (whether linear or exponential) is detrimental to learning in this setting.

## Run 3: Step-wise Batch Size Schedule
Description: Step-wise increase in batch size from 32 to 512, with longer periods at each batch size (32->64->128->256->512) to allow for better learning consolidation at each stage
Results: {'x_div_y': {'final_train_loss_mean': 2.698624769846598, 'final_val_loss_mean': 2.6567535400390625, 'final_train_acc_mean': 0.2148437493791183, 'final_val_acc_mean': 0.237060546875, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 4.574860731760661, 'final_val_loss_mean': 4.5746660232543945, 'final_train_acc_mean': 0.01158854179084301, 'final_val_acc_mean': 0.010416666666666666, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 2.6381684939066568, 'final_val_loss_mean': 2.6314213275909424, 'final_train_acc_mean': 0.2759114544217785, 'final_val_acc_mean': 0.2788899739583333, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.7876877784729, 'final_val_loss_mean': 4.787555535634358, 'final_train_acc_mean': 0.00865885429084301, 'final_val_acc_mean': 0.008544921875, 'step_val_acc_99_mean': 7500.0}}
Analysis: The step-wise batch size schedule also failed to achieve good performance. Despite providing longer periods at each batch size to allow for better learning consolidation, the model still struggled to learn the tasks effectively. This suggests that any form of batch size increase during training, whether linear, exponential, or step-wise, is detrimental to learning these mathematical operations.

## Run 4: Constant Small Batch Size
Description: Using a constant small batch size of 32 throughout training to test the hypothesis that maintaining small batches is crucial for learning these operations effectively
Results: {'x_div_y': {'final_train_loss_mean': 4.47291882832845, 'final_val_loss_mean': 4.449561436971028, 'final_train_acc_mean': 0.02083333395421505, 'final_val_acc_mean': 0.021647135416666668, 'step_val_acc_99_mean': 7500.0}, 'x_minus_y': {'final_train_loss_mean': 4.574707984924316, 'final_val_loss_mean': 4.575027783711751, 'final_train_acc_mean': 0.012500000341484943, 'final_val_acc_mean': 0.009521484375, 'step_val_acc_99_mean': 7500.0}, 'x_plus_y': {'final_train_loss_mean': 4.576561450958252, 'final_val_loss_mean': 4.575206597646077, 'final_train_acc_mean': 0.006250000170742472, 'final_val_acc_mean': 0.010335286458333334, 'step_val_acc_99_mean': 7500.0}, 'permutation': {'final_train_loss_mean': 4.78788693745931, 'final_val_loss_mean': 4.788247426350911, 'final_train_acc_mean': 0.006250000170742472, 'final_val_acc_mean': 0.009195963541666666, 'step_val_acc_99_mean': 7500.0}}
Analysis: Surprisingly, using a constant small batch size of 32 performed worse than all previous approaches. The model failed to learn any of the operations, with accuracies hovering around 1-2% across all tasks. This suggests that the batch size alone is not the critical factor in the learning process, and the baseline configuration (Run 0) with its default batch size settings remains the most effective approach. This result contradicts our initial hypothesis that smaller batch sizes would be beneficial for learning these mathematical operations.

## Run 5: Larger Model Capacity
Description: Testing if the poor performance in previous runs was due to insufficient model capacity by doubling the model size (increasing layers from 2 to 4 and hidden dimension from 128 to 256)

# Generated Plots Description

The experiment generated multiple plots for each dataset (x_div_y, x_minus_y, x_plus_y, permutation), showing the learning dynamics across all runs:

## Training Loss Plots (train_loss_*.png)
These plots show how the training loss evolves over update steps for each run. Lower values indicate better performance on the training set. The plots include error bands showing the standard error across seeds, helping visualize the consistency of results. Key observations:
- Baseline (Run 0) achieves the lowest training loss across all operations
- Dynamic batch size approaches (Runs 1-3) show intermediate performance
- Constant small batch size (Run 4) and larger model (Run 5) struggle to minimize training loss

## Validation Loss Plots (val_loss_*.png)
These plots track the validation loss over time, indicating how well the models generalize to unseen examples. The error bands show consistency across different random seeds. Notable patterns:
- Baseline maintains the best generalization performance
- Other approaches show significantly higher validation loss, suggesting poor generalization
- The gap between training and validation loss indicates potential overfitting in some cases

## Training Accuracy Plots (train_acc_*.png)
These plots show the percentage of correct predictions on the training set over time. The error bands indicate variation across seeds. Key findings:
- Baseline achieves perfect training accuracy on most operations
- Other approaches show varying degrees of success in fitting the training data
- Addition (x_plus_y) appears easier to learn than other operations across all approaches

## Validation Accuracy Plots (val_acc_*.png)
These plots demonstrate generalization performance in terms of prediction accuracy. Error bands show consistency across seeds. Important observations:
- Baseline reaches near-perfect validation accuracy on arithmetic operations
- Dynamic batch size approaches show partial success on simpler operations
- Permutation task proves challenging across all approaches
- The larger model (Run 5) fails to show improved generalization despite increased capacity

The plots collectively demonstrate that the baseline configuration significantly outperforms all attempted modifications, suggesting that the original hyperparameters were well-tuned for these tasks. The failure of larger models and various batch size schedules indicates that the learning dynamics are sensitive to more than just model capacity or batch size variations.
