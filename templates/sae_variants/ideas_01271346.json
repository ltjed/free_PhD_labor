[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "utilization_progressive_sae",
        "Title": "Utilization-Based Progressive Feature Activation for Interpretable Sparse Autoencoders",
        "Experiment": "1. Modify SAE to include feature utilization tracking\n2. Implement L1-based activation criterion\n3. Train on Pythia-70M using standard text datasets\n4. Compare interpretability metrics against baseline SAE\n5. Analyze feature utilization patterns",
        "Technical_Details": "Uses single dictionary with feature activation based on L1 utilization. Features start masked except for initial k=64. Every n=100 steps, compute U_i = ||f_i||_1 for each masked feature i over current batch. Activate features with highest utilization when max(U_i) > \u03c4 where \u03c4=0.1. Features activated in groups of g=32. Loss remains L = ||x - x_hat||^2 + \u03bb||f||_1 with masked features f_effective = f * M(t). Single hyperparameter \u03c4 controls activation threshold.",
        "Implementation_Plan": "1. Add feature activation mask to CustomSAE\n2. Add feature utilization tracking\n3. Implement utilization-based activation trigger\n4. Add logging for utilization patterns\n5. Update evaluation metrics",
        "Interestingness_Evaluation": "The utilization-based approach provides an elegant and direct way to discover feature hierarchies based on actual feature importance.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires minimal code changes - just adding L1 norm tracking and threshold logic; training time identical to baseline SAE; computations simpler than previous version; can be implemented in 1 day.",
        "Feasibility": 10,
        "Novelty_Evaluation": "Using direct feature utilization metrics for progressive activation in SAEs is a novel and elegant approach.",
        "Novelty": 8,
        "Expected_Research_Impact": "The utilization-based progression should improve sparse_probing by ensuring features are activated based on their actual importance, while maintaining strong core metrics through careful feature management.",
        "Research_Impact": 9,
        "Overall_Score": 9.2,
        "Abstract": "We propose Utilization-Based Progressive Sparse Autoencoders (UPSAEs), a simplified approach to improving the interpretability of neural network representations. UPSAEs introduce a feature activation mechanism that progressively unmasks features based on their utilization strength during training, measured by their L1 activation norms. This approach encourages the discovery of natural feature hierarchies by activating new features only when there is strong evidence for their utility in the current feature set. Unlike previous approaches that use complex scheduling or error tracking, UPSAEs achieve this through a simple utilization threshold mechanism that integrates seamlessly with standard sparse autoencoder training. This method maintains the computational efficiency of traditional sparse autoencoders while providing a more direct and intuitive approach to feature discovery."
    },
    {
        "Name": "gradient_scaled_sae",
        "Title": "Gradient-Scaled Training for Adaptive Feature Separation in Sparse Autoencoders",
        "Experiment": "1. Modify SAE to compute primary reconstruction error\n2. Implement gradient scaling for specialized features\n3. Train with standard loss and single learning rate\n4. Evaluate feature separation patterns\n5. Analyze adaptation behavior",
        "Technical_Details": "The method uses a single learning rate but scales gradients for specialized features based on primary reconstruction error. First compute e_p = ||x - Dec(Enc(x)[:d/2])||_2. Then scale gradients for features [d/2:] by min(1, e_p/e_0) where e_0 is a running average of e_p. Standard loss L = ||x - x\u0302||_2 + \u03bb * ||Enc(x)||_1 and unit-norm decoder constraints are maintained. When primary features reconstruct well (low e_p), specialized features learn slowly, encouraging natural separation.",
        "Implementation_Plan": "1. Add primary reconstruction error tracking\n2. Implement gradient scaling in backward pass\n3. Add running average computation\n4. Create feature analysis tools\n5. Add visualization for adaptation process",
        "Interestingness_Evaluation": "The automatic adaptation of specialized feature learning based on primary feature performance provides an elegant self-regulating mechanism.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only basic gradient scaling and error tracking. Training time similar to baseline with minimal overhead for error computation. All operations standard and well within H100 30-minute limit.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While gradient scaling exists in other contexts, this specific application for automatic feature separation in SAEs is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The self-regulating nature of feature separation should improve interpretability metrics while maintaining reconstruction quality.",
        "Research_Impact": 8,
        "Overall_Score": 8.2,
        "Abstract": "We propose a gradient-scaled training approach for improving feature interpretability in sparse autoencoders. Our method automatically regulates the learning of specialized features based on the reconstruction performance of primary features, creating natural pressure for feature separation without requiring hyperparameter tuning. Primary features maintain standard gradients while specialized feature gradients are scaled by the primary reconstruction error, encouraging specialization only when fundamental patterns are well-captured. This approach addresses feature absorption and polysemanticity through a simple yet principled modification to the training process."
    },
    {
        "Name": "binary_sparsity_sae",
        "Title": "Binary Sparsity-Guided Orthogonality for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement binary activation pattern tracking\n2. Modify orthogonality loss to use binary patterns\n3. Train on standard datasets\n4. Evaluate feature interpretability\n5. Compare against continuous sparsity baseline\n6. Analyze binary activation patterns",
        "Technical_Details": "The method uses a binary sparsity-based orthogonality loss: L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_binary_ortho, where L_binary_ortho = \u03a3_ij (b_i * b_j) * |W_i^T W_j|. The binary factors b_i = (activation_i > threshold).float() indicate feature presence. This creates stronger orthogonality constraints between frequently active features while allowing overlap between rarely active ones. The binary metrics provide a clearer signal for feature independence than continuous values.",
        "Implementation_Plan": "1. Add binary activation computation\n2. Implement BinaryOrthogonalityLoss class\n3. Modify CustomTrainer for binary scaling\n4. Add activation pattern utilities\n5. Implement evaluation metrics\n6. Add visualization for binary patterns",
        "Interestingness_Evaluation": "The binary approach provides the simplest and most interpretable way to connect feature activation patterns with orthogonality constraints.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The implementation is maximally feasible as it only requires simple thresholding operations and binary masks. The approach reduces computational complexity compared to continuous values and ensures training remains well within the 30-minute limit.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While building on sparsity principles, using binary activation patterns to guide orthogonality represents a novel and elegantly simple approach to feature disentanglement.",
        "Novelty": 8,
        "Expected_Research_Impact": "The binary approach should excel at sparse probing tasks by creating clearer feature separation based on interpretable activation patterns.",
        "Research_Impact": 8,
        "Overall_Score": 8.5,
        "Abstract": "We present a binary sparsity-guided approach to orthogonality constraints in sparse autoencoders that aims to improve feature interpretability. Our method uses binary activation patterns to determine the strength of orthogonality constraints between features, enforcing stronger separation between frequently active features that likely represent distinct concepts. This creates a simple yet effective mechanism for balancing feature independence and reconstruction quality. The approach requires minimal modifications to standard architectures and reduces computational complexity through binary operations. We propose a modified loss function that scales orthogonality constraints based on binary activation patterns computed during training."
    },
    {
        "Name": "matryoshka_sae",
        "Title": "Matryoshka Training for Hierarchical Feature Organization in Sparse Autoencoders",
        "Experiment": "1. Implement continuous feature activation schedule\n2. Train with linearly increasing active feature count\n3. Compare feature absorption metrics against baseline\n4. Analyze feature hierarchy through activation patterns\n5. Evaluate different warmup periods",
        "Technical_Details": "The method uses a single dictionary with a continuously expanding active feature set. After a warmup period w, the number of active features n(t) increases linearly: n(t) = n_0 for t \u2264 w, n(t) = n_0 + (n_full - n_0) * ((t-w)/(T-w)) for t > w, where T is total steps. Features use standard Kaiming initialization. The feature mask is applied after ReLU activation in the encoder to maintain clean gradients. L1 sparsity penalty is computed only on unmasked features. Decoder weights are L2-normalized after each update. This creates natural pressure for general features to emerge early while specific features develop later.",
        "Implementation_Plan": "1. Add feature count scheduler with warmup to CustomSAE\n2. Implement post-activation feature masking in forward pass\n3. Modify L1 penalty computation for masked features\n4. Update training loop with scheduler\n5. Add utilities to track feature emergence order\n6. Add evaluation metrics",
        "Interestingness_Evaluation": "The continuous Matryoshka training approach with warmup provides a robust and practical solution to feature absorption through controlled capacity expansion.",
        "Interestingness": 6,
        "Feasibility_Evaluation": "The implementation remains very straightforward with only feature masking and basic scheduling; training time identical to baseline; all operations use standard PyTorch functionality.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While progressive training is not new, this specific application with careful handling of masking and sparsity interactions is novel.",
        "Novelty": 6,
        "Expected_Research_Impact": "The controlled feature emergence with proper gradient handling should improve sparse probing while maintaining strong core metrics.",
        "Research_Impact": 7,
        "Overall_Score": 7.5,
        "Abstract": "We propose Matryoshka training, a simple yet effective training approach for sparse autoencoders that continuously expands the active feature set during training to encourage hierarchical feature organization. This method addresses the challenge of feature absorption by creating natural pressure for general features to emerge early in training when capacity is limited. Our approach requires no additional loss terms or architectural changes, instead relying only on a linear feature activation schedule with initial warmup period. The method carefully handles the interaction between feature masking, activation functions, and sparsity penalties to maintain stable training dynamics. The implementation is lightweight, requiring only minor modifications to standard sparse autoencoder training while providing explicit control over feature emergence order."
    },
    {
        "Name": "matryoshka_sae",
        "Title": "Matryoshka Sparse Autoencoders: Separating General and Specific Features through Two-Scale Reconstruction",
        "Experiment": "1. Implement SAE with two-scale reconstruction (full and half dict)\n2. Train with balanced reconstruction losses\n3. Compare activation patterns between scales\n4. Evaluate feature absorption\n5. Assess interpretability against baseline",
        "Technical_Details": "The method uses two nested reconstruction objectives: L = L_recon_full + 0.5*L_recon_half where L_recon_half uses only the first half of the dictionary. This creates two distinct feature populations: general features in the first half that participate in both reconstructions, and specific features in the second half for fine-tuning. The 0.5 weight on L_recon_half maintains consistent per-feature reconstruction pressure. This naturally separates general and specific features through the sparsity objective without requiring explicit constraints.",
        "Implementation_Plan": "1. Add use_half parameter to CustomSAE\n2. Modify forward() to return full and half-dictionary reconstructions\n3. Update loss() to compute weighted reconstruction loss\n4. Add metrics tracking feature usage across scales\n5. Implement basic feature analysis tools",
        "Interestingness_Evaluation": "The simple two-scale approach provides a clear test of whether hierarchical organization improves interpretability while maintaining simplicity.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation requires minimal changes to existing code; two-scale approach keeps computation well within limits; straightforward to analyze with clear comparison to baseline.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While the approach is simple, using two-scale reconstruction specifically for separating general and specific features in SAEs is novel.",
        "Novelty": 6,
        "Expected_Research_Impact": "The clear separation between general and specific features should improve interpretability metrics while maintaining reconstruction quality.",
        "Research_Impact": 7,
        "Overall_Score": 7.5,
        "Abstract": "We present Matryoshka Sparse Autoencoders (M-SAE), a simple approach to improving feature interpretability in neural networks through two-scale reconstruction. M-SAE learns hierarchically organized features by training the first half of the dictionary to participate in both full and half-scale reconstructions, while the second half specializes in fine-tuning. This creates a natural separation between general features that capture broad patterns and specific features that handle details. Our method requires minimal changes to standard sparse autoencoders, using only a modified training objective with two reconstruction scales. This approach provides a systematic way to separate general and specific features, potentially offering clearer insights into how neural networks organize information at different levels of abstraction."
    },
    {
        "Name": "simple_hierarchical_sae",
        "Title": "Simple Hierarchical Feature Organization for Interpretable Sparse Autoencoders",
        "Experiment": "1. Add activation-based feature scaling\n2. Implement scaled reconstruction loss\n3. Compare feature absorption metrics\n4. Evaluate interpretability benchmarks\n5. Analyze feature hierarchies",
        "Technical_Details": "The SAE uses maximum activation values to organize features hierarchically. For each feature i in batch B, we use its maximum activation a_i = max(|f_i(B)|) to create scaling factors s_i = a_i/max(a). The loss function L = ||x - D(E(x))||^2 + 0.5||x - D(s\u2299E(x))||^2 where s is the vector of scaling factors. This naturally emphasizes strongly-activated features in reconstruction while maintaining standard L1 sparsity globally.",
        "Implementation_Plan": "1. Add max activation tracking to CustomSAE.forward()\n2. Implement scaled reconstruction in loss calculation\n3. Modify CustomTrainer.loss() to use activation scaling\n4. Add basic feature hierarchy metrics\n5. Create activation pattern visualizations",
        "Interestingness_Evaluation": "The approach provides an elegant way to organize features by importance while requiring minimal modifications to standard SAE training.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation requires only tracking max activations (already needed for L1) and simple scaling operations; training time identical to baseline; easily fits within 30-min limit.",
        "Feasibility": 10,
        "Novelty_Evaluation": "Using maximum activation values for feature organization is a novel yet remarkably simple approach to improving SAE interpretability.",
        "Novelty": 7,
        "Expected_Research_Impact": "The simplified approach should effectively reduce feature absorption and improve sparse_probing interpretability while maintaining strong core metrics.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We propose a simple hierarchical approach to feature learning in sparse autoencoders (SAEs) that aims to improve the interpretability of learned representations. Our method organizes features based on their maximum activation values within each batch, creating a natural importance hierarchy without additional parameters or complex mechanisms. The implementation requires minimal modifications to standard SAEs, leveraging existing activation tracking to scale feature contributions during reconstruction. This approach provides a lightweight framework for improving feature interpretability through natural activation patterns."
    },
    {
        "Name": "ranked_sparsity_orthogonal_sae",
        "Title": "Rank-Based Sparsity-Guided Orthogonal Feature Learning for Interpretable Sparse Autoencoders",
        "Experiment": "1. Implement sparsity ranking computation\n2. Modify orthogonality constraints to use rank-based weights\n3. Train on standard datasets with varying constraint scales\n4. Compare interpretability metrics against baseline SAE\n5. Analyze stability of feature hierarchies across datasets\n6. Evaluate impact on sparse probing tasks",
        "Technical_Details": "The method modifies orthogonal SAE by using relative sparsity rankings to guide feature separation. For each batch, features are ranked by their L0 sparsity, and the rank is normalized to [0,1]. The orthogonality constraint for each feature is weighted by its normalized rank r_i. The loss function becomes L = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * \u03a3_i r_i * ||w_i^T W||_2, where w_i is the i-th feature vector and W is the full feature matrix. This creates consistent feature hierarchies regardless of absolute sparsity levels, improving stability and generalization.",
        "Implementation_Plan": "1. Add sparsity ranking function to CustomSAE\n2. Implement rank-based orthogonality loss\n3. Add rank computation to training loop\n4. Add visualization tools for rank stability\n5. Update training parameters for consistent scaling",
        "Interestingness_Evaluation": "The approach provides a robust and principled way to create consistent feature hierarchies across different training conditions.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation requires only simple ranking operation; extremely efficient computationally; all operations easily fit within 30-minute limit; minimal code changes needed.",
        "Feasibility": 10,
        "Novelty_Evaluation": "While building on previous sparsity-based approaches, the use of relative rankings provides a novel and more robust way to guide feature separation.",
        "Novelty": 7,
        "Expected_Research_Impact": "The rank-based approach should provide more stable and consistent improvements in both sparse probing and core metrics through better-regulated feature hierarchies.",
        "Research_Impact": 9,
        "Overall_Score": 8.9,
        "Abstract": "We present a rank-based sparsity-guided orthogonal sparse autoencoder that improves feature interpretability by creating consistent feature hierarchies. Our approach extends standard sparse autoencoders by using relative sparsity rankings to modulate orthogonality constraints between features. This creates stable feature separation patterns that are invariant to absolute sparsity levels and consistent across different datasets. The method requires minimal computational overhead, using only simple ranking operations on existing sparsity statistics."
    }
]