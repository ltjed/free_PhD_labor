[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "feature_decorrelation_regularization",
        "Title": "Feature Decorrelation Regularization for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Implement feature decorrelation regularization in SAE training\n2. Add covariance penalty to discourage redundant features\n3. Train SAEs with varying decorrelation strengths\n4. Evaluate unlearning performance using standard methodology\n5. Compare feature independence metrics between baseline and decorrelated SAEs\n6. Analyze feature selectivity on knowledge-specific inputs\n7. Measure unlearning precision with different feature selection thresholds",
        "Technical_Details": "Feature Decorrelation Regularization (FDR) enhances SAE training by encouraging learned features to be statistically independent, reducing redundancy in the feature space. The modified loss function is: L_total = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_decorr, where L_recon is the standard reconstruction loss, L_sparse is the L1 sparsity penalty, and L_decorr is the new decorrelation term. For a batch of encoded feature activations F (size batch_size \u00d7 feature_dim), we compute the feature correlation matrix C = F^T F / batch_size - \u03bc\u03bc^T, where \u03bc is the mean activation of each feature across the batch. The decorrelation loss is defined as: L_decorr = ||C - diag(C)||_F^2, which is the squared Frobenius norm of the off-diagonal elements of C. This penalizes correlated activations between different features, encouraging each feature to capture unique information. To improve computational efficiency, we can compute this loss on a subset of features or use a running estimate of the correlation matrix. The decorrelation weight \u03bb_2 is a hyperparameter typically set between 0.01 and 0.1 times the reconstruction loss weight.",
        "Rationale": "The unlearning benchmark evaluates how effectively we can identify and suppress features encoding specific knowledge (dangerous biology) while preserving general capabilities. A key challenge in this process is that knowledge is often distributed across multiple correlated features, making it difficult to identify all relevant features for suppression. Feature Decorrelation Regularization directly addresses this challenge by encouraging features to be statistically independent, leading to several benefits for unlearning: 1) When features are decorrelated, specific knowledge tends to be encoded in fewer, more distinct features rather than being distributed across many redundant features. This makes it easier to identify and target all features related to the knowledge we want to unlearn; 2) The feature selection process in unlearning relies on measuring sparsity differences between 'forget' and 'retain' datasets. With decorrelated features, these differences become more pronounced as each feature specializes in representing distinct concepts; 3) When suppressing selected features during inference, decorrelated features cause fewer side effects because each feature is more likely to represent a single concept rather than a mixture of concepts. This approach doesn't require prior knowledge of what content needs to be unlearned, making it broadly applicable. It works with the existing feature selection methodology while improving the quality of the features being selected. The decorrelation penalty complements the L1 sparsity penalty: while L1 encourages each sample to use few features, decorrelation encourages different features to capture different information, together promoting a more disentangled representation.",
        "Implementation_Plan": "1. Implement a feature_correlation_matrix function to compute correlations between feature activations\n2. Add a decorrelation_loss function that computes the Frobenius norm of off-diagonal elements\n3. Modify the loss function in CustomTrainer to include the decorrelation term\n4. Add decorr_weight hyperparameter to control regularization strength\n5. Add batch-wise tracking of average feature correlations during training\n6. Update the training loop to compute and log correlation metrics\n7. Implement analysis functions to visualize feature correlation matrices\n8. Keep the core SAE architecture unchanged to maintain compatibility with evaluation pipeline\n9. Add a sweep over decorr_weight values to find optimal settings",
        "novel": true
    },
    {
        "Name": "content_selective_sparse_autoencoder",
        "Title": "Content-Selective Sparse Autoencoders for Enhanced Knowledge Unlearning",
        "Experiment": "1. Implement split-batch selective regularization for SAE training\n2. Modify loss function to encourage different activation patterns within batches\n3. Train SAEs with varying selective regularization strengths\n4. Evaluate unlearning performance using standard methodology\n5. Compare feature selectivity between baseline and content-selective SAEs\n6. Analyze feature activation patterns across different content types\n7. Measure unlearning precision with different feature selection thresholds",
        "Technical_Details": "The Content-Selective Sparse Autoencoder (CSSAE) extends standard SAEs by adding a regularization term that encourages features to have different activation patterns for different inputs within the same batch. The modified loss function is: L_total = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_selective, where L_recon is the standard reconstruction loss, L_sparse is the L1 sparsity penalty, and L_selective is the new selectivity term. To compute L_selective, we split each batch of size B into two halves of size B/2. For each feature j, we compute its average activation across each half: a_j1 = mean(f_j[:B/2]) and a_j2 = mean(f_j[B/2:]). The selectivity loss is then: L_selective = -mean(|a_j1 - a_j2|), where the negative sign indicates we want to maximize the difference. This encourages features to activate differently for different subsets of the batch, making them more selective in their activation patterns. To prevent features from simply increasing their magnitude to artificially inflate differences, we use binary activation indicators: a_j1 = mean(f_j[:B/2] > 0) and a_j2 = mean(f_j[B/2:] > 0), which measure the proportion of samples where the feature is active. The selective weight \u03bb_2 is typically set between 0.01 and 0.1 times the reconstruction loss weight to balance reconstruction quality and feature selectivity. During training, we track the average activation differences to monitor the effectiveness of the regularization.",
        "Rationale": "The unlearning benchmark evaluates how effectively we can identify and suppress features encoding specific knowledge while preserving general capabilities. The key to successful unlearning is identifying features that are predominantly active for content we want to forget but not for content we want to retain. The Content-Selective Sparse Autoencoder directly optimizes for this property by encouraging features to activate differently for different subsets of data. This approach addresses several challenges in knowledge unlearning: 1) By encouraging features to have different activation patterns within each batch, we create pressure for features to become more selective in what content they respond to; 2) Features that are selective in their activations are easier to identify during the feature selection process, as they will show clearer differences between 'forget' and 'retain' datasets; 3) When features are more selective, suppressing those associated with unwanted knowledge is less likely to affect general capabilities; 4) The split-batch approach simulates the distinction between different content types without requiring explicit dataset labels, making it broadly applicable. The selectivity regularization works by creating a natural pressure for features to specialize: since each batch is randomly split, features that consistently show different activation patterns across the splits are more likely to be responding to specific content characteristics rather than general patterns. This is different from standard sparsity regularization, which encourages each feature to be inactive most of the time but doesn't address how activation patterns vary across inputs. The approach doesn't make assumptions about what specific content might need to be unlearned, but rather creates a general tendency for features to become more selective, which benefits any subsequent unlearning process.",
        "Implementation_Plan": "1. Modify the training loop to split each batch into two halves\n2. Implement a feature_activation_difference function that computes the difference in activation rates between the two halves\n3. Add a selectivity_loss function that computes the negative mean of these differences\n4. Modify the loss function in CustomTrainer to include the selectivity regularization term\n5. Add selectivity_weight hyperparameter to control the strength of regularization\n6. Update the training loop to compute and track average activation differences\n7. Implement visualization functions to analyze feature activation patterns within batches\n8. Keep the core SAE architecture unchanged to maintain compatibility with evaluation pipeline\n9. Add a sweep over selectivity_weight values to find optimal settings",
        "novel": true
    },
    {
        "Name": "feature_specialization_loss",
        "Title": "Feature Specialization Loss for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Implement feature specialization loss in SAE training\n2. Add penalty for features that activate similarly across different inputs\n3. Train SAEs with varying specialization regularization strengths\n4. Evaluate unlearning performance using standard methodology\n5. Compare feature specialization metrics between baseline and FSL SAEs\n6. Analyze how specialized features impact unlearning precision\n7. Measure unlearning-retention tradeoffs with different regularization settings",
        "Technical_Details": "Feature Specialization Loss (FSL) encourages each feature to activate selectively across different inputs rather than responding similarly to many inputs. This promotes a more disentangled representation where individual features correspond to specific concepts or knowledge domains.\n\nThe implementation works as follows:\n\n1. During standard SAE training with reconstruction and sparsity losses, we add a specialization term to the loss function.\n\n2. For each batch of activations, we compute the feature specialization score using coefficient of variation:\n   - For each feature j, we calculate its activation variance across the batch:\n     var_j = var(f_j) where f_j is the vector of activations for feature j across all samples\n   - We normalize this by the feature's mean activation to get a coefficient of variation:\n     cv_j = sqrt(var_j) / (mean(f_j) + \u03b5)\n   where \u03b5 is a small constant (e.g., 1e-5) to prevent division by zero.\n\n3. The feature specialization loss is defined as:\n   - L_spec = mean(exp(-\u03b1 * cv_j))\n   where \u03b1 is a scaling factor (typically set to 5). This formulation creates a smooth penalty that approaches zero for highly specialized features and one for non-specialized features.\n\n4. We update the model using a modified loss function:\n   - L_total = L_recon + \u03bb_1 * L_sparse + \u03bb_2 * L_spec\n   where \u03bb_2 controls the strength of specialization regularization.\n\n5. To balance specialization with reconstruction quality, we apply a gradual curriculum where \u03bb_2 starts small and increases during training.\n\n6. We only apply the specialization penalty to features with sufficient activation (mean(f_j) > threshold * max_j(mean(f_j))) to avoid constraining features that are still developing or rarely active.",
        "Rationale": "The unlearning benchmark evaluates how effectively we can identify and suppress features encoding specific knowledge (dangerous biology) while preserving general capabilities. The key challenge is identifying features that cleanly separate knowledge domains, as the feature selection process relies on finding features with high activation on the forget dataset (WMDP-bio) but low activation on the retain dataset (WikiText).\n\nStandard SAE training optimizes for reconstruction quality and sparsity but doesn't encourage features to specialize in specific types of knowledge. This leads to two fundamental issues for unlearning:\n\n1. Feature entanglement: Features often encode mixtures of concepts spanning both domains we want to forget and retain. When features respond similarly to inputs from different domains, they become difficult to identify through sparsity differences, which is the primary selection criterion in the unlearning benchmark.\n\n2. Imprecise suppression: When suppressing entangled features during inference, we inadvertently affect knowledge we want to retain, leading to degraded performance on unrelated tasks.\n\nFeature Specialization Loss addresses these issues by encouraging each feature to respond selectively to specific types of inputs. Here's why this approach should improve unlearning performance:\n\n1. Statistical foundation: Information theory and statistical learning theory suggest that features that vary significantly in their activation patterns across different inputs are more likely to capture distinct, meaningful concepts. The coefficient of variation (CV) effectively measures this property by normalizing variance by mean, making it suitable for comparing specialization across features with different activation magnitudes.\n\n2. Connection to domain separation: When features are encouraged to specialize, they naturally tend to align with inherent distinctions in the data. Since there are fundamental differences between biology knowledge and general knowledge, specialized features are more likely to align with these domain boundaries than non-specialized features.\n\n3. Improved feature selection: Specialized features will show greater activation differences between forget and retain datasets, making the sparsity-based selection process more effective at identifying domain-specific features. This leads to more precise targeting of knowledge to be unlearned.\n\n4. Reduced side effects: When features are more specialized, suppressing those associated with dangerous biology knowledge is less likely to affect unrelated knowledge domains, resulting in better preservation of performance on retained tasks.\n\nImportantly, FSL doesn't require access to domain-specific data during training or changes to the existing feature selection and suppression methodology. It works by enhancing the underlying representation to be more amenable to the existing unlearning process.\n\nRegarding potential trade-offs with reconstruction quality, our approach includes three mechanisms to maintain balance: (1) a curriculum approach that gradually increases the specialization weight, (2) a threshold that only applies the penalty to sufficiently active features, and (3) an exponential formulation that creates a smooth penalty rather than a hard constraint. These ensure that specialization doesn't come at the expense of reconstruction ability.",
        "Implementation_Plan": "1. Modify the CustomTrainer class to include feature specialization functionality:\n   - Add a calculate_coefficient_of_variation function that computes sqrt(variance)/mean for each feature\n   - Implement feature_specialization_loss function using the exponential formulation\n\n2. Add hyperparameters to control the method:\n   - spec_weight: Controls regularization strength (\u03bb_2)\n   - spec_alpha: Scaling factor for the exponential penalty (\u03b1)\n   - activation_threshold: Relative threshold for applying specialization loss\n   - use_curriculum: Whether to use curriculum learning for spec_weight\n\n3. Modify the loss method in CustomTrainer:\n   - Calculate coefficient of variation for each feature's activations across the batch\n   - Apply the activation threshold based on the maximum mean activation\n   - Compute the specialization loss using the exponential formulation\n   - Add the weighted specialization loss to the total loss\n\n4. Update the training loop to support curriculum learning:\n   - If use_curriculum is enabled, gradually increase spec_weight during training\n   - Track and log the effective specialization weight at each step\n\n5. Add monitoring code to track specialization metrics:\n   - Average coefficient of variation across features\n   - Distribution of CV values across features\n\n6. Integrate with the existing update method in experiment.py:\n   - Keep the core SAE architecture unchanged\n   - Add the new loss term and monitoring to the update method\n   - Return specialization metrics as part of the loss dictionary\n\n7. Implement analysis functions to evaluate the impact on unlearning:\n   - Calculate average CV for features selected by the unlearning process\n   - Measure correlation between CV and domain specificity\n   - Compare unlearning performance between baseline and FSL models\n\n8. Add hyperparameter sweep to find optimal settings:\n   - Test different specialization weights and scaling factors\n   - Evaluate impact on the unlearning benchmark metrics",
        "novel": true
    },
    {
        "Name": "adaptive_hierarchical_regularization",
        "Title": "Adaptive Hierarchical Regularization for Improved Knowledge Separation in Sparse Autoencoders",
        "Experiment": "1. Modify SAE training to use feature-specific regularization strengths\n2. Implement adaptive regularization based on feature activation frequency\n3. Train SAEs with varying hierarchical regularization parameters\n4. Evaluate unlearning performance using standard methodology\n5. Compare feature specialization between baseline and hierarchical SAEs\n6. Analyze correlation between regularization strength and domain specificity\n7. Measure unlearning precision with different feature selection thresholds",
        "Technical_Details": "Adaptive Hierarchical Regularization (AHR) modifies the standard SAE training process by applying different L1 regularization strengths to different features based on their activation patterns. The modified loss function is: L_total = L_recon + sum_j(\u03bb_j * |f_j|), where L_recon is the standard reconstruction loss, f_j is the activation of feature j, and \u03bb_j is a feature-specific regularization strength. The key innovation is computing \u03bb_j adaptively during training based on the feature's activation frequency. We define the activation frequency of feature j as: freq_j = mean(f_j > 0), which measures how often the feature is active across samples. The regularization strength is then computed as: \u03bb_j = \u03bb_base * (freq_j / (mean_freq + \u03b5))^\u03b1, where \u03bb_base is the base regularization strength, mean_freq is the average activation frequency across all features, \u03b5 is a small constant (1e-5) for numerical stability, and \u03b1 is a hyperparameter controlling the strength of the hierarchical effect (typically between 0.5 and 2.0). To maintain the same overall regularization level as the baseline, we normalize the \u03bb_j values by multiplying them by a factor \u03b2 such that mean(\u03bb_j) = \u03bb_base. This ensures that we're redistributing regularization rather than changing its overall strength. To prevent instability during early training, we use a moving average of freq_j with a momentum of 0.9, and we delay applying hierarchical regularization until after a stabilization period (default 10% of training steps or when the change in average feature frequency falls below a threshold). We update feature frequencies and regularization strengths every 10 batches to balance computational efficiency with responsiveness to changing activation patterns. We also clip the regularization strength to be within [0.25 * \u03bb_base, 4.0 * \u03bb_base] to prevent extreme values while allowing for meaningful differentiation.",
        "Rationale": "The unlearning benchmark evaluates how effectively we can identify and suppress features encoding specific knowledge (dangerous biology) while preserving general capabilities. The benchmark's feature selection process identifies features that are active on the forget dataset (WMDP-bio) but inactive on the retain dataset (WikiText). For this selection to be effective, features need to exhibit clear activation differences between these datasets. Standard SAE training with uniform L1 regularization doesn't optimize for this property. Studies on interpretability of sparse autoencoders (such as Cunningham et al., 2023 \"Sparse Autoencoders Find Coherent Features in Neural Networks\") have shown that SAE features naturally vary in their activation frequency, with some features activating broadly and others activating only in specific contexts. This frequency distribution often correlates with semantic generality - a pattern observed across different domains of representation learning. Adaptive Hierarchical Regularization leverages this observation to improve knowledge separation through three mechanisms: 1) Resource allocation: By applying stronger regularization to frequently activated features, we limit their capacity, effectively allocating more representational resources to rarely activated features. This encourages rarely activated features to capture more specialized information that appears in specific contexts. 2) Specialization incentive: Frequently activated features face higher regularization costs, which incentivizes them to specialize in capturing broadly useful patterns with high reconstruction value relative to their regularization cost. This naturally pushes domain-specific knowledge toward less frequently activated features, where the regularization cost is lower. 3) Activation sharpening: Differentiated regularization encourages features to have more distinct activation patterns - either activating frequently with high utility (general knowledge) or activating rarely but strongly when needed (specialized knowledge). This creates clearer separation between different types of knowledge in the feature space. For the unlearning benchmark specifically, AHR complements the existing feature selection process in two ways: a) Enhanced feature separation: By encouraging domain-specific knowledge to concentrate in rarely activated features, AHR creates a stronger correlation between activation frequency and domain specificity. This means features that are highly active on WMDP-bio but not on WikiText are more likely to specifically encode biology knowledge rather than general knowledge that happens to appear in biology contexts. b) Reduced side effects: When features are better separated by knowledge domain, suppressing features identified as encoding dangerous biology knowledge is less likely to affect general capabilities. This should lead to better maintenance of MMLU accuracy on unrelated tasks while effectively reducing WMDP-bio accuracy. The approach is computationally efficient, adding minimal overhead to the training process (approximately 5% increase in computation time). By normalizing regularization strengths to maintain the same average L1 penalty as the baseline, we ensure that improvements come from better regularization distribution rather than simply changing overall regularization strength. While we expect AHR to improve unlearning performance, the exact magnitude of improvement will depend on how strongly knowledge domains are naturally separated in the data and how effectively the hierarchical regularization enhances this separation. We validate this through empirical evaluation on the unlearning benchmark.",
        "Implementation_Plan": "1. Modify the CustomTrainer class to support feature-specific regularization:\n   - Add tracking variables for feature activation frequencies (self.feature_freqs)\n   - Initialize feature_freqs with zeros at the start of training\n   - Add hyperparameters for controlling hierarchical regularization (hier_alpha, freq_momentum, stabilization_steps, update_interval)\n\n2. Implement feature frequency tracking and updating:\n   - In the update method, calculate binary activation indicators (f > 0) for each feature\n   - Update moving average of activation frequencies using freq_momentum\n   - Only update frequencies every update_interval steps to reduce computational overhead\n\n3. Implement adaptive regularization calculation:\n   - Compute mean activation frequency across all features (with small epsilon for stability)\n   - Calculate unnormalized feature-specific regularization strengths: \u03bb_j = \u03bb_base * (freq_j / (mean_freq + \u03b5))^\u03b1\n   - Normalize regularization strengths to maintain same average: \u03bb_j = \u03bb_j * (\u03bb_base / mean(\u03bb_j))\n   - Apply clipping to keep regularization strengths within reasonable bounds\n\n4. Implement stabilization period logic:\n   - Track change in average feature frequency between updates\n   - Apply hierarchical regularization only after stabilization_steps or when frequency change falls below threshold\n   - During stabilization, use uniform regularization to allow features to develop naturally\n\n5. Modify the loss method to use feature-specific regularization:\n   - Replace uniform L1 penalty with sum of feature-specific penalties\n   - Ensure backward compatibility by using uniform penalty when hier_alpha is 0\n\n6. Add monitoring code to track regularization effectiveness:\n   - Log distribution of regularization strengths (min, max, mean, std)\n   - Track correlation between activation frequency and regularization strength\n   - Log histogram of feature frequencies at regular intervals\n   - Calculate and log the percentage of features at min/max regularization\n\n7. Implement analysis functions to evaluate impact on unlearning:\n   - After training, calculate correlation between feature frequency and activation on WMDP-bio vs. WikiText\n   - Analyze whether features selected by unlearning process have different frequency characteristics than non-selected features\n   - Measure how regularization strength correlates with feature effectiveness for unlearning\n\n8. Add hyperparameter sweep in experiment.py:\n   - Test different hier_alpha values (0.5, 1.0, 1.5, 2.0)\n   - Try different stabilization approaches (fixed steps vs. convergence-based)\n   - Evaluate impact on unlearning benchmark metrics (WMDP-bio accuracy and MMLU accuracy)",
        "novel": true
    }
]