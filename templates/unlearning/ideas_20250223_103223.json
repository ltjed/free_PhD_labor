[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "contrastive_sae",
        "Title": "Contrastive Learning for Knowledge-Specialized Features in Sparse Autoencoders",
        "Experiment": "1. Implement simple contrastive training\n2. Create balanced WMDP-bio and WikiText samples\n3. Train SAE with feature specialization tracking\n4. Evaluate unlearning performance\n5. Analyze feature characteristics",
        "Technical_Details": "Training alternates between WMDP-bio and WikiText batches. Feature Specialization Score (FSS) for feature i uses exponential moving averages: FSS_i = |EMA(f_i_spec) - EMA(f_i_gen)| / (EMA_std(f_i_spec) + EMA_std(f_i_gen)) where EMA decay=0.99. Contrastive loss: L_contrast = mean(max(0, cos_sim(f_top_k, f_other) - margin)) where f_top_k are the k most active features per batch. Total loss: L_total = L_recon + lambda * L_contrast + gamma * L_sparse with lambda=0.1. Features with FSS > 0.5 are candidates for unlearning.",
        "Rationale": "1. Alternating batches provides clean separation between domains while maintaining simplicity:\n   a. No complex sampling needed\n   b. Clear feature exposure to both domains\n   c. Minimal memory overhead\n2. FSS with EMA provides stable feature measurement:\n   a. Computationally efficient\n   b. Captures long-term feature behavior\n   c. Resistant to temporary fluctuations\n3. Top-k feature selection for contrastive loss:\n   a. Focuses on most relevant features\n   b. Consistent training signal\n   c. Efficient computation\n4. Simple FSS threshold for feature selection:\n   a. Clear decision boundary\n   b. Easy to validate\n   c. Directly tied to unlearning goals",
        "Implementation_Plan": "1. Add alternating batch support to ActivationBuffer\n2. Implement FSS tracking with EMA\n3. Add contrastive_loss with top-k selection\n4. Modify CustomTrainer for alternating training\n5. Add FSS threshold parameter\n6. Implement basic monitoring\n7. Add FSS-based feature selection",
        "novel": true
    },
    {
        "Name": "gradient_guided_sae",
        "Title": "Gradient-Guided Feature Selection for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Add activation gradient tracking to CustomTrainer\n2. Implement importance score computation\n3. Train SAE with standard reconstruction objective\n4. Evaluate using standard unlearning protocol\n5. Compare computational overhead vs baseline\n6. Analyze selected feature characteristics",
        "Technical_Details": "Track gradients of reconstruction loss with respect to SAE activations (not model weights) every 1000 steps using batch size 256. For each feature i:\nEMA_grad_bio_i = 0.99 * EMA_grad_bio_i + 0.01 * |grad_bio_i|  # 0.99 chosen to match buffer refresh rate\nEMA_var_bio_i = 0.99 * EMA_var_bio_i + 0.01 * (grad_bio_i - EMA_grad_bio_i)^2\nImportance score: I_i = (EMA_grad_bio_i / sqrt(EMA_var_bio_i + 1e-6)) / (EMA_grad_wiki_i / sqrt(EMA_var_wiki_i + 1e-6) + 1e-6)\nInitialize EMA values to 0. Skip importance score updates for first 10k steps to allow EMA warmup. Success metrics: (1) WMDP-bio accuracy at least 5% lower than baseline while maintaining MMLU accuracy above 0.99, (2) Less than 10% training time increase",
        "Rationale": "1. Using activation gradients ensures compatibility:\n   a. Always available during training\n   b. Directly measures feature importance\n   c. Computationally efficient\n2. Parameter choices justified:\n   a. 0.99 EMA decay matches data buffer updates\n   b. 256 batch size balances stability and compute\n   c. 10k step warmup ensures stable statistics\n3. Realistic success criteria:\n   a. 5% improvement threshold is meaningful\n   b. 10% overhead allows for gradient computation\n   c. MMLU constraint ensures general knowledge\n4. Clear evaluation protocol using existing benchmark",
        "Implementation_Plan": "1. Add activation gradient computation to forward pass\n2. Implement EMA tracking in CustomTrainer\n3. Add importance score calculation\n4. Modify feature selection logic\n5. Add warmup period handling\n6. Implement overhead tracking\n7. Add detailed logging",
        "novel": true
    },
    {
        "Name": "layerwise_importance_sae",
        "Title": "Layer-Weighted Feature Selection for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Implement layer importance tracking\n2. Add normalized feature sampling\n3. Train SAE with standard objective\n4. Compare against baselines\n5. Measure computational impact\n6. Analyze score stability",
        "Technical_Details": "Layer importance computation:\n1. Every 5000 steps, for each layer l:\n   - Sample min(64, batch_size) examples\n   - Sample sqrt(n_features_l) features where n_features_l is layer size\n   - Compute base accuracy acc_base on WMDP-bio\n   - Clamp sampled features to -2.0\n   - Compute new accuracy acc_clamped\n   - Score I_l = (acc_base - acc_clamped) / acc_base\n2. Update average: I_l = 0.95 * I_l + 0.05 * new_I_l\n3. Normalize scores:\n   w_l = clip(I_l / mean(I_all), 0.5, 2.0)\n4. Apply to thresholds: t_l = base_threshold / w_l\nFeature selection:\n- Allocate features proportionally: n_l = floor(n_total * w_l / sum(w))\n- Select by sparsity within each layer budget",
        "Rationale": "1. Sampling improvements:\n   a. sqrt(n_features) balances layer sizes\n   b. Minimum batch size ensures stability\n   c. Fixed -2.0 clamp matches unlearning\n2. Success criteria:\n   a. WMDP-bio accuracy reduction > uniform + 1 std dev\n   b. Score stability: max deviation < 20% after warmup\n   c. Runtime overhead < 2% vs baseline\n3. Implementation benefits:\n   a. Proportional allocation respects layer capacity\n   b. Fixed evaluation schedule simplifies tracking\n   c. Conservative bounds prevent instability\n4. Validation metrics:\n   a. Track importance score variance across 3 runs\n   b. Measure per-layer feature activation distributions\n   c. Compare training time in GPU-hours",
        "Implementation_Plan": "1. Add importance tracking to CustomSAE\n2. Implement sqrt(n) feature sampling\n3. Add score normalization\n4. Update feature selection\n5. Add variance tracking\n6. Implement timing metrics\n7. Add distribution analysis",
        "novel": false
    },
    {
        "Name": "selective_sparsity_sae",
        "Title": "Selective Sparsity Relaxation for Domain-Specific Feature Learning",
        "Experiment": "1. Add activation tracking\n2. Implement selective penalty\n3. Train SAE with modified sparsity\n4. Compare feature distributions\n5. Evaluate unlearning performance\n6. Analyze training stability",
        "Technical_Details": "Track feature statistics with EMA (decay=0.99):\nact_bio_i = EMA(mean activation of feature i on bio data)\nact_gen_i = EMA(mean activation of feature i on general data)\nCompute robust scaling:\niqr_act = interquartile range of all activations\nwarmup_factor = min(1.0, step/warmup_steps)\nCompute per-feature penalties:\nl1_penalty_i = base_penalty * max(0.5, 1 - beta * warmup_factor * (act_bio_i - act_gen_i)/iqr_act)\nwhere:\n- base_penalty = 0.04 (baseline optimal)\n- beta = 0.5 (moderate reduction)\n- warmup_steps = 5000\nFeature selection:\n1. Compute specificity s_i = (act_bio_i - act_gen_i)/iqr_act\n2. Select features where:\n   - s_i > 0.6745 (matches 1-sigma for normal dist)\n   - act_bio_i > median(act_bio)\n3. If selected < min_features, add highest s_i features\nwhere min_features = 0.1 * total_features\nSuccess metrics:\n1. WMDP-bio accuracy reduction >5% vs baseline\n2. MMLU accuracy >0.99\n3. Training time increase <5%",
        "Rationale": "1. Robust statistics benefits:\n   a. IQR resistant to outliers\n   b. 0.6745 threshold has statistical basis\n   c. Median activation ensures relevance\n2. Parameter choices justified:\n   a. 5000 warmup from convergence analysis\n   b. 0.5 beta maintains reasonable bounds\n   c. 10% minimum features from ablation studies\n3. Implementation advantages:\n   a. Simple statistical computations\n   b. Smooth penalty transitions\n   c. Guaranteed feature pool size\n4. Expected improvements based on:\n   a. More stable training dynamics\n   b. Robust feature selection\n   c. Reliable unlearning candidates",
        "Implementation_Plan": "1. Add EMA tracking to CustomTrainer\n2. Implement IQR computation\n3. Add smooth warmup\n4. Modify penalty calculation\n5. Add distribution tracking\n6. Implement selection logic\n7. Add stability metrics",
        "novel": false
    },
    {
        "Name": "momentum_reset_sae",
        "Title": "Selective Momentum Reset for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Add activation ratio tracking between forget/retain datasets\n2. Implement selective momentum reset in optimizer\n3. Train SAE with standard reconstruction objective\n4. Evaluate using standard unlearning benchmark",
        "Technical_Details": "Track feature activation ratios:\nFor each feature i:\nratio_i = EMA(mean_activation_bio_i) / (EMA(mean_activation_wiki_i) + epsilon)\nwhere:\n- EMA decay = 0.99\n- epsilon = 1e-6\n- mean_activation_x_i = mean activation of feature i on dataset x\n\nCompute conservative threshold:\nthreshold = percentile_90(EMA(ratio_i))\n\nEvery 1000 steps after initial training:\n1. Identify candidate features where:\n   - ratio_i > threshold\n   - retain_set_sparsity < 0.5 * max_retain_sparsity\n   - Steps since last reset > 200\n2. Reset Adam momentum:\n   - Set exp_avg to 0 for corresponding weights\n   - Set exp_avg_sq to initial value (1e-6)\n\nHyperparameters:\n- Reset frequency: 1000 steps\n- Initial training steps: 10000\n- Minimum steps between resets: 200\n- EMA decay: 0.99\n\nEvaluation:\n1. WMDP-bio accuracy (target: lower)\n2. MMLU accuracy (must stay >0.99)",
        "Rationale": "1. Momentum reset mechanism:\n   a. Removes optimization inertia only for most extreme bio-specific features\n   b. Allows features to more quickly reduce harmful activations\n   c. Conservative threshold minimizes disruption to general knowledge\n2. Parameter choices justified:\n   a. 1000 step frequency matches data buffer updates\n   b. 10000 initial steps ensures stable feature development\n   c. 200 step minimum prevents excessive resets\n3. Integration with unlearning:\n   a. Uses same retain sparsity criteria\n   b. Affects only features already candidates for unlearning\n   c. Complements rather than replaces existing method\n4. Expected benchmark improvement:\n   a. More effective suppression of bio knowledge\n   b. Maintained MMLU performance through selective application\n   c. Clean evaluation through standard metrics",
        "Implementation_Plan": "1. Add EMA tracking to CustomTrainer\n2. Implement ratio computation\n3. Add percentile threshold calculation\n4. Add initial training period check\n5. Implement momentum reset\n6. Modify optimizer step\n7. Run standard evaluation",
        "novel": true
    },
    {
        "Name": "probe_guided_sae",
        "Title": "Probe-Guided Feature Selection for Improved Knowledge Unlearning",
        "Experiment": "1. Add periodic feature probing during training\n2. Implement simple impact scoring\n3. Modify feature selection logic\n4. Train SAE with standard reconstruction objective\n5. Compare unlearning performance against baseline\n6. Analyze feature characteristics",
        "Technical_Details": "During training, every 1000 steps:\n1. Sample 32 examples each from WMDP-bio and general dataset\n2. For each batch:\n   - Get base model outputs y_base\n   - For features with activation > mean + std:\n     * Clamp feature to -1.0\n     * Get new outputs y_new\n     * bio_impact = mean(|y_base_bio - y_new_bio|)\n     * gen_impact = mean(|y_base_gen - y_new_gen|)\n3. Update exponential moving averages:\n   bio_impact_i = 0.9 * bio_impact_i + 0.1 * bio_impact\n   gen_impact_i = 0.9 * gen_impact_i + 0.1 * gen_impact\n   active_freq_i = 0.9 * active_freq_i + 0.1 * (activation > threshold)\n\nFeature selection for unlearning:\n1. Filter features where:\n   - retain_sparsity < retain_threshold\n   - active_freq_i > 0.01\n   - bio_impact_i > gen_impact_i\n2. Select top n_features by (bio_impact_i - gen_impact_i) * active_freq_i\n\nHyperparameters:\n- Probe frequency: 1000 steps\n- Sample size: 32\n- EMA decay: 0.9\n- Activation threshold: mean + std",
        "Rationale": "1. Direct output measurement benefits:\n   a. Captures actual knowledge changes\n   b. Natural comparison between bio and general impact\n   c. No arbitrary thresholds or normalizations\n2. Simplified tracking advantages:\n   a. EMA provides stable estimates\n   b. Active frequency captures consistent features\n   c. Dynamic threshold adapts to activation patterns\n3. Feature selection logic:\n   a. Basic retain_sparsity maintains original constraints\n   b. Frequency weighting prefers reliable features\n   c. Direct bio vs general impact comparison\n4. Expected improvements:\n   a. Better feature selection from measuring actual output changes\n   b. More stable unlearning from consistent feature identification\n   c. Computational overhead <0.5% from reduced sample size",
        "Implementation_Plan": "1. Add probing to CustomTrainer\n2. Implement impact calculation\n3. Add EMA tracking\n4. Modify feature selection\n5. Add output logging\n6. Implement threshold computation\n7. Add timing checks",
        "novel": true
    },
    {
        "Name": "gradient_variance_sae",
        "Title": "Gradient-Based Feature Selection for Knowledge Unlearning",
        "Experiment": "1. Add gradient tracking for reconstruction loss\n2. Implement simplified importance scoring\n3. Modify feature selection criteria\n4. Train SAE with standard objective\n5. Compare unlearning performance\n6. Analyze computational overhead",
        "Technical_Details": "Track reconstruction gradients:\n1. Every 500 steps, compute reconstruction loss gradients g_i for feature i:\n   - Sample batch from bio dataset (2048 tokens)\n   - Sample batch from wiki dataset (2048 tokens)\n   - Compute reconstruction loss gradients for each\n2. Update mean gradients (EMA decay = 0.95):\n   mean_grad_bio_i = EMA(|g_bio_i|)\n   mean_grad_wiki_i = EMA(|g_wiki_i|)\n3. Compute importance score:\n   score_i = mean_grad_bio_i / (mean_grad_wiki_i + 1e-6)\n\nFeature selection:\n1. Filter features where:\n   - retain_sparsity < retain_threshold\n   - mean_grad_wiki_i > 0.1 * mean(mean_grad_wiki)\n2. Select top n_features by score_i\n\nHyperparameters:\n- EMA decay: 0.95\n- Gradient computation frequency: 500 steps\n- Minimum steps before scoring: 2000\n- Relative gradient threshold: 0.1",
        "Rationale": "1. Simplified gradient tracking:\n   a. Mean absolute gradients directly measure feature importance\n   b. Larger bio gradients indicate features used more for bio content\n   c. Ratio naturally captures relative importance between domains\n2. Efficient computation:\n   a. 500-step interval reduces overhead while maintaining stability\n   b. Single EMA per feature per domain minimizes memory usage\n   c. Simple ratio avoids numerical instability\n3. Practical improvements:\n   a. Relative gradient threshold adapts to model scale\n   b. Retain sparsity constraint preserves general knowledge\n   c. Clear relationship between gradients and feature importance\n4. Expected benefits:\n   a. More efficient feature selection process\n   b. Stable importance scores from simplified computation\n   c. <1% training overhead from reduced frequency",
        "Implementation_Plan": "1. Add gradient computation to CustomTrainer\n2. Implement EMA tracking\n3. Add score calculation\n4. Modify feature selection\n5. Add relative threshold check\n6. Implement periodic updates\n7. Add overhead tracking",
        "novel": true
    },
    {
        "Name": "sequential_activation_sae",
        "Title": "Sequential Activation Analysis for Improved Knowledge Unlearning in Sparse Autoencoders",
        "Experiment": "1. Add sequential activation tracking\n2. Implement simple co-activation scoring\n3. Modify feature selection criteria\n4. Train SAE with standard objective\n5. Compare unlearning performance\n6. Analyze temporal patterns",
        "Technical_Details": "Track sequential activations:\n1. For each batch:\n   prev_acts = feature activations at t-1\n   curr_acts = feature activations at t\n   threshold = median(activations) + 0.5 * iqr(activations)  # More robust threshold\n   co_active = (prev_acts > threshold) & (curr_acts > threshold)\n\n2. Update statistics (EMA decay = 0.99):\n   bio_seq_i = EMA(mean co_active of feature i on bio data)\n   gen_seq_i = EMA(mean co_active of feature i on general data)\n\n3. Compute sequence score:\n   seq_score_i = bio_seq_i / (gen_seq_i + 1e-6)\n\n4. Compute selection ratio:\n   seq_ratio = min(0.5, mean(seq_score_i > 2 * median(seq_score_i)))\n   # Adapts based on strength of sequential signal\n\nFeature selection:\n1. Filter candidates where:\n   - retain_sparsity < retain_threshold\n   - gen_seq_i > 0.1 * median(gen_seq_i)  # Relative threshold\n2. Select features:\n   n_sparsity = round((1 - seq_ratio) * n_features)\n   n_sequential = n_features - n_sparsity\n   features = top_k_by_sparsity(n_sparsity) + \\\n              top_k_by_sequence_score(n_sequential)\n\nComputational cost:\n- Additional memory: O(n_features) for EMA tracking\n- Additional compute per batch: O(batch_size * n_features)\n- Storage overhead: ~8 bytes * n_features * 2",
        "Rationale": "1. Robust statistical choices:\n   a. Median + IQR threshold resistant to outliers\n   b. Relative minimum activation adapts to data\n   c. Adaptive selection ratio based on signal strength\n2. Stability measures:\n   a. Upper bound of 0.5 on sequential selection\n   b. Minimum 50% features selected by proven sparsity method\n   c. EMA smoothing prevents sudden changes\n3. Computational efficiency:\n   a. Linear memory growth with features\n   b. Constant factor overhead per batch\n   c. Negligible storage requirements\n4. Expected improvements:\n   a. 2-5% better unlearning on strongly sequential content\n   b. No degradation on other content types\n   c. <1% training time increase\n5. Failure safeguards:\n   a. Defaults to sparsity-based selection if sequential signal weak\n   b. Conservative thresholds prevent over-aggressive selection\n   c. Bounded impact on training dynamics",
        "Implementation_Plan": "1. Add sequence tracking to CustomTrainer\n2. Implement robust threshold computation\n3. Add EMA updates\n4. Add adaptive ratio calculation\n5. Modify feature selection\n6. Add overhead tracking\n7. Update evaluation metrics",
        "novel": true
    },
    {
        "Name": "feature_coupling_sae",
        "Title": "Adaptive Feature Coupling Analysis for Knowledge Unlearning",
        "Experiment": "1. Add coupling tracking with adaptive storage\n2. Implement stability-based consistency checks\n3. Train SAE with standard objective\n4. Compare unlearning performance\n5. Analyze coupling patterns\n6. Measure implementation overhead",
        "Technical_Details": "During training, every 2000 steps:\n1. Sample batches (256 tokens each):\n   - bio_batch from WMDP-bio\n   - gen_batch from WikiText\n2. For top 10% bio-active features f:\n   For each batch type:\n     - Get base activations a_base\n     - Clamp feature f to -1.0\n     - Get new activations a_new\n     - c_i = median(|a_new_i - a_base_i|)/iqr(a_base_i)\n     If c_i > 75th_percentile(previous_c):\n       Update coupling matrices (EMA 0.95):\n       C_bio[f,i] = 0.95 * C_bio[f,i] + 0.05 * c_i_bio\n       C_gen[f,i] = 0.95 * C_gen[f,i] + 0.05 * c_i_gen\n\nCompute coupling scores:\nscore_i = median(C_bio[f,i] for f in sparse_features) / \n         (median(C_gen[f,i] for f in sparse_features) + 1e-6)\n\nAdaptive stability check:\n- window_size = max(3, round(1000/steps_per_loss_plateau))\n- stable_i = std(last_window_size_scores_i) < 0.1 * mean(last_window_size_scores_i)\n\nFeature selection:\n1. Get sparsity-selected features\n2. Compute coupling threshold:\n   t = 95th_percentile(score)\n3. For remaining features, select where:\n   - retain_sparsity < retain_threshold\n   - score_i > t\n   - stable_i is True\n   - Up to min(0.2, mean(stable)) * n_features\n\nStorage optimization:\nIf n_features > 10000:\n  Use sparse matrices, store top 25% values\nElse:\n  Use dense matrices with regular pruning",
        "Rationale": "1. Expected improvement sources:\n   a. Coupling detection finds ~5-10% more bio-related features\n   b. These features have 0.2-0.3x impact of direct features\n   c. Therefore expect ~0.5-1% additional WMDP-bio reduction\n2. Stability check benefits:\n   a. Window size adapts to training dynamics\n   b. Relative threshold handles different scales\n   c. Prevents selecting unstable couplings\n3. Storage efficiency:\n   a. Dense matrices optimal for typical sizes\n   b. Sparse only needed for very large models\n   c. Regular pruning maintains efficiency\n4. Feature limit justification:\n   a. Proportion matches stability statistics\n   b. Maximum 20% preserves main sparsity signals\n   c. Adapts down if few stable couplings found\n5. Failure prevention:\n   a. Defaults to sparsity-only selection if unstable\n   b. Maintains all original constraints\n   c. Bounded computational overhead",
        "Implementation_Plan": "1. Add coupling matrices\n2. Implement adaptive stability\n3. Add conditional sparse storage\n4. Modify feature selection\n5. Add stability tracking\n6. Update evaluation metrics\n7. Add efficiency checks",
        "novel": true
    },
    {
        "Name": "temporal_clustering_sae",
        "Title": "Temporal Clustering Analysis for Improved Feature Selection in Knowledge Unlearning",
        "Experiment": "1. Add sliding window activation tracking\n2. Implement burst ratio computation\n3. Modify feature selection criteria\n4. Train SAE with standard objective\n5. Compare unlearning performance\n6. Analyze temporal patterns",
        "Technical_Details": "Track temporal patterns:\n1. Maintain sliding windows (size=16) of activation indicators:\n   active_i = activation > retain_threshold  # Same as unlearning benchmark\n2. Update feature statistics (EMA decay=0.99):\n   bio_burst_i = EMA(sum(active_window_bio_i)/window_size)\n   gen_burst_i = EMA(sum(active_window_gen_i)/window_size)\n3. Compute burst ratio:\n   ratio_i = bio_burst_i / (gen_burst_i + 1e-6)\n4. Normalize ratio:\n   norm_ratio_i = (ratio_i - median(ratio)) / (iqr(ratio) + 1e-6)\n\nFeature selection:\n1. Get candidate features where:\n   - retain_sparsity < retain_threshold\n   - gen_burst_i > 0.1 * median(gen_burst)\n2. Compute combined score:\n   score_i = 0.8 * sparsity_score_i + 0.2 * norm_ratio_i\n3. Select top n_features by score_i\n4. Validation check:\n   Every 5000 steps, on validation set:\n   If temporal_selection_score > 0.95 * sparsity_selection_score:\n     Continue using temporal selection\n   Else:\n     Revert to sparsity selection\n\nMemory overhead:\n- Per feature: 2 EMAs (8 bytes) + window (16 bits) = 10 bytes\n- Total: n_features * 10 bytes (~160KB for 16K features)\n\nHyperparameters:\n- Window size: 16 tokens\n- EMA decay: 0.99\n- Burst weight: 0.2\n- Validation frequency: 5000 steps",
        "Rationale": "1. Design choices justified by:\n   a. Using retain_threshold aligns with existing benchmark\n   b. 16-token window keeps memory overhead bounded\n   c. 0.2 weight preserves sparsity as primary signal\n2. Memory analysis:\n   a. 10 bytes per feature is negligible overhead\n   b. 160KB total for 16K features (typical size)\n   c. Window bits can be packed for efficiency\n3. Validation safeguards:\n   a. 5000-step interval limits compute overhead\n   b. 0.95 threshold ensures no significant degradation\n   c. Clean fallback to proven baseline\n4. Expected improvements based on:\n   a. ~0.5% better unlearning from temporal consistency\n   b. Improvement comes from better feature selection precision\n   c. Overhead < 0.05% (160KB memory, simple operations)",
        "Implementation_Plan": "1. Add window tracking to CustomTrainer\n2. Implement burst computation\n3. Add ratio normalization\n4. Add validation checks\n5. Modify feature selection\n6. Add memory tracking\n7. Update evaluation metrics",
        "novel": true
    }
]