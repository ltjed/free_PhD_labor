[
    {
        "Name": "adaptive_sparse_autoencoders",
        "Title": "Adaptive Computation in Sparse Autoencoders",
        "Experiment": "1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice",
        "Technical_Details": "The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.",
        "Research_Impact": "A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "gated_sparse_autoencoder",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024",
        "Experiment": "1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study",
        "Technical_Details": "The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.",
        "Research_Impact": "A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "batchtopk_sae",
        "Title": "Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024",
        "Experiment": "1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.",
        "Technical_Details": "BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).",
        "Research_Impact": "A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "jumprelu_sae",
        "Title": "\u00a92024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024",
        "Experiment": "Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.",
        "Technical_Details": "JumpReLU SAE introduces a threshold parameter \u03c4 for each feature. The activation function zeroes out pre-activations below \u03c4. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.",
        "Research_Impact": "Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "clustered_sparse_autoencoders",
        "Title": "Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models",
        "Experiment": "1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach",
        "Technical_Details": "The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.",
        "Research_Impact": "This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "mutual_feature_regularization",
        "Title": "1",
        "Experiment": "1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity",
        "Technical_Details": "Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics",
        "Research_Impact": "MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.",
        "Interestingness": 7,
        "Feasibility": 8,
        "Novelty": 6,
        "novel": false
    },
    {
        "Name": "switch_sparse_autoencoder",
        "Title": "Switch Sparse Autoencoders",
        "Experiment": "1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection",
        "Technical_Details": "The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.",
        "Research_Impact": "A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 8,
        "novel": false
    },
    {
        "Name": "sparse_autoencoder_improvements",
        "Title": "Sparse Autoencoder Viewer",
        "Experiment": "1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.",
        "Technical_Details": "The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.",
        "Research_Impact": "This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "interaction_aware_contrast_sae",
        "Title": "Interaction-Aware Contrast-Guided Feature Selection for Knowledge Unlearning",
        "Experiment": "1. Implement batch-normalized contrast loss computation\n2. Add layer-dependent contrast weight scaling\n3. Track per-feature contrast scores during training\n4. Compute pairwise feature interaction scores\n5. Implement interaction-aware feature selection\n6. Compare unlearning performance across selection methods",
        "Technical_Details": "Primary contrast score: C_i = |mean(f_i_forget) - mean(f_i_retain)|. Interaction score: I_ij = corr(f_i, f_j) * min(C_i, C_j) for top-k contrasting features. Adjusted contrast score: C_i_adj = C_i + gamma * max(I_ij) where gamma is interaction weight. Combined selection score: S_i = beta*sparsity_score_i + (1-beta)*normalized_C_i_adj. Features selected when S_i > selection_threshold. Layer-adaptive contrast weight: lambda_contrast(l) = base_lambda * (l/L)^alpha. Total loss: L_total = L_recon + lambda_l1*L_sparse + lambda_contrast(l)*L_contrast - lambda_entropy*H.",
        "Motivation_Rationale": "Knowledge in language models is often represented through combinations of features rather than individual features alone. By considering feature interactions in our selection process, we can better identify and remove knowledge-specific feature patterns while preserving general capabilities.",
        "Implementation_Plan": "1. Add interaction score computation function\n2. Modify contrast score tracking to include interactions\n3. Update feature selection logic with adjusted scores\n4. Add parameters for interaction computation (k, gamma)\n5. Implement efficient top-k feature filtering",
        "Interestingness_Evaluation": "Considering feature interactions provides a more nuanced approach to identifying knowledge-specific representations while maintaining simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "The addition of pairwise interaction computation for top-k features adds minimal overhead, keeping implementation and runtime within constraints.",
        "Feasibility": 8,
        "Novelty_Evaluation": "The incorporation of feature interactions into contrast-guided selection represents a novel refinement for knowledge unlearning.",
        "Novelty": 7,
        "Expected_Research_Impact": "The interaction-aware approach should better capture and remove knowledge-specific feature patterns while preserving general language capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "We present Interaction-Aware Contrast-Guided Feature Selection, an enhanced approach to knowledge unlearning in sparse autoencoders that considers feature interactions when identifying knowledge-specific representations. Our method extends contrast-guided selection by incorporating pairwise feature interaction scores, helping identify features that work together to encode specific knowledge. The approach maintains computational efficiency by considering only top-k contrasting features for interaction analysis. This refinement requires minimal modifications to existing architectures while potentially enabling more precise identification of knowledge representations. We evaluate this method on the challenging task of selectively removing specific knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "momentum_knowledge_gradient_sae",
        "Title": "Momentum-Based Knowledge Gradient Selection with Exponential Averaging",
        "Experiment": "1. Implement EMA-based correlation tracking\n2. Add momentum updates for adaptive thresholds\n3. Train SAEs with momentum-based selection on WMDP-bio\n4. Compare stability and memory usage against window-based approach\n5. Analyze convergence of gradient scores and thresholds\n6. Evaluate unlearning performance across different momentum values",
        "Technical_Details": "Knowledge gradient uses EMA of Spearman correlations: K_i(t) = beta*K_i(t-1) + (1-beta)*spearman(f_i, y) where beta=0.9 is EMA decay. Threshold updated with momentum: t(t) = gamma*t(t-1) + (1-gamma)*(median(K) + alpha*mad(K)) where gamma=0.95 is momentum coefficient. Selection score S_i = w*sparsity_i + (1-w)*norm(K_i) with w=0.7. Features selected when S_i > t. EMA statistics initialized to zero, threshold initialized to 80th percentile of first batch scores.",
        "Motivation_Rationale": "Rolling windows for correlation tracking are memory-intensive and can lag behind distribution shifts. EMA provides efficient, adaptive correlation estimates while momentum stabilizes threshold updates. This maintains the benefits of robust statistics while improving computational efficiency and stability.",
        "Implementation_Plan": "1. Replace window tracking with EMA updates\n2. Add momentum-based threshold computation\n3. Modify initialization for EMA statistics\n4. Update feature selection with momentum thresholds\n5. Add beta/gamma hyperparameters",
        "Interestingness_Evaluation": "The combination of EMA and momentum provides an elegant, computationally efficient solution to stability issues in knowledge-guided selection.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation simplifies previous approach by removing window tracking, requires only simple EMA/momentum updates with minimal state.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While EMA and momentum are standard techniques, their application to stabilize knowledge gradient selection is novel.",
        "Novelty": 7,
        "Expected_Research_Impact": "The improved efficiency and stability should enable more reliable knowledge removal in practical applications.",
        "Research_Impact": 8,
        "Overall_Score": 8.2,
        "Abstract": "We present a momentum-based approach to knowledge gradient selection for sparse autoencoders, designed to improve the efficiency and stability of selective knowledge removal. Our method replaces window-based correlation tracking with exponential moving averages and introduces momentum-based threshold updates. This addresses practical limitations of previous approaches while maintaining computational simplicity. The method uses EMA to track feature-knowledge correlations and momentum to stabilize selection thresholds, enabling efficient adaptation to distribution shifts during training. We evaluate this approach on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "rank_uniqueness_threshold_sae",
        "Title": "Rank-Based Uniqueness Thresholding for Robust Knowledge Unlearning",
        "Experiment": "1. Implement rank-based uniqueness computation\n2. Add mini-batch averaging for uniqueness scores\n3. Modify threshold computation for rank statistics\n4. Train SAEs with rank-based thresholds on WMDP-bio\n5. Compare stability and efficiency against previous approaches\n6. Analyze uniqueness score distributions across layers",
        "Technical_Details": "Rank-based uniqueness: u_i = spearman_corr(rank(f_i_forget)) / spearman_corr(rank(f_i_retain)) computed over k=3 mini-batches. Combined threshold: t_i,l = base_threshold * (r_i * u_i^beta)^alpha_l where r_i is median activation ratio, beta=0.3. Layer-specific alpha_l = base_alpha * (l/L)^0.5. Mini-batch averaging: u_i_final = mean(u_i) over last k mini-batches. Rankings computed using torch.argsort for efficiency.",
        "Motivation_Rationale": "Rank-based correlations provide more robust uniqueness estimates than raw similarities, while being computationally efficient. Mini-batch averaging stabilizes estimates without requiring large batches. This approach better handles varying activation scales and outliers.",
        "Implementation_Plan": "1. Add compute_rank_uniqueness function\n2. Implement mini-batch averaging logic\n3. Modify threshold updates for rank statistics\n4. Update feature selection with stabilized scores\n5. Add rank computation utilities",
        "Interestingness_Evaluation": "The use of rank statistics provides a more robust and efficient approach to measuring feature uniqueness.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation is simpler and more efficient than previous version, with rank computations replacing matrix multiplications and better numerical stability.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The application of rank statistics to feature uniqueness measurement represents a novel improvement in SAE feature selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The improved robustness and efficiency should enable more reliable knowledge removal across different models and datasets.",
        "Research_Impact": 8,
        "Overall_Score": 8.2,
        "Abstract": "We present Rank-Based Uniqueness Thresholding, a refined approach to feature selection in sparse autoencoders for knowledge unlearning. Our method uses rank correlations and mini-batch averaging to measure feature uniqueness, replacing less robust similarity computations. The approach combines rank-based uniqueness scores with activation ratios to identify knowledge-specific features, while maintaining computational efficiency through rank statistics. This method offers a more robust way to distinguish between general and knowledge-specific features, potentially enabling more reliable feature selection for knowledge removal.",
        "novel": true
    },
    {
        "Name": "rank_neighborhood_sae",
        "Title": "Rank-Based Neighborhood Consistency for Robust Knowledge Unlearning",
        "Experiment": "1. Implement rank-based consistency metrics\n2. Add density and alignment computations\n3. Modify selection criteria for dual checks\n4. Train SAEs with rank consistency on WMDP-bio\n5. Compare stability across different thresholds\n6. Analyze feature cluster patterns",
        "Technical_Details": "Local density d_i = |{j in N_i : rank(sparsity_j) > n*0.8}| / |N_i| where n is feature count. Alignment score a_i = mean(spearman_corr(rank(f_i), rank(f_j)) for j in N_i). Layer-adaptive thresholds: t_d = 0.6 * (l/L)^0.5, t_a = 0.4 * (l/L)^0.5. Feature selected if rank(sparsity_i) > n*0.8 AND d_i > t_d AND a_i > t_a. Neighborhoods k_l = max(5, width_l/100) computed every 1000 steps.",
        "Motivation_Rationale": "Rank statistics provide better robustness to outliers and scaling differences. Using separate density and alignment checks helps confirm features are truly part of knowledge-encoding clusters rather than statistical artifacts.",
        "Implementation_Plan": "1. Add rank computation utilities\n2. Implement density and alignment scoring\n3. Modify threshold logic for dual criteria\n4. Update feature selection pipeline\n5. Add rank correlation function",
        "Interestingness_Evaluation": "The use of rank statistics provides a more robust way to identify knowledge-encoding feature clusters.",
        "Interestingness": 7,
        "Feasibility_Evaluation": "Implementation remains simple with rank operations adding minimal overhead, and dual criteria computation uses existing neighborhood structure.",
        "Feasibility": 8,
        "Novelty_Evaluation": "The combination of rank statistics with dual consistency criteria represents a novel improvement to neighborhood-based selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more robust selection criteria should enable more reliable identification of knowledge-specific features.",
        "Research_Impact": 8,
        "Overall_Score": 7.8,
        "Abstract": "We present a rank-based neighborhood consistency approach for knowledge unlearning in sparse autoencoders. Our method uses rank statistics to compute two complementary consistency metrics: local density and activation alignment. This provides a more robust way to identify features that are part of knowledge-encoding clusters while being less sensitive to outliers and scaling differences. The approach maintains computational efficiency by leveraging existing neighborhood computations while adding rank-based robustness. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "warmed_temporal_stability_sae",
        "Title": "Warm-Started Temporal Stability Selection for Robust Knowledge Unlearning",
        "Experiment": "1. Implement warm-up period tracking\n2. Add quantile-based thresholds\n3. Modify EMA computation for layers\n4. Train SAEs with warm-started stability\n5. Compare convergence speed\n6. Analyze warm-up effects",
        "Technical_Details": "Warm-up period: First 1000 steps use running average instead of EMA. Layer-dependent decay: beta_l = 0.9 * (1 - 0.1*l/L) for faster adaptation in higher layers. Stability score: s_i(t) = beta_l*s_i(t-1) + (1-beta_l)*spearman_corr(rank(f_i_p), rank(f_i_q)). Quantile threshold: q_t = torch.quantile(mean(f_i), 0.6) per layer. Early stopping when |s_i(t) - s_i(t-1)| < 0.01 for 100 steps. Selection score: C_i = w*s_i + (1-w)*sparsity_i where w=0.7. Feature selected if C_i > 0.8 AND mean(f_i) > q_t.",
        "Motivation_Rationale": "Warm-up period ensures more reliable stability measures early in training, while quantile-based thresholds simplify computation and improve robustness. Layer-dependent decay rates better handle varying feature dynamics across network depth.",
        "Implementation_Plan": "1. Add warm-up tracking logic\n2. Implement quantile thresholds\n3. Modify EMA update rules\n4. Add convergence checking\n5. Update selection criteria",
        "Interestingness_Evaluation": "The warm-up and layer-adaptive components provide practical improvements while maintaining theoretical soundness.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation becomes even simpler by replacing median/MAD with quantiles and adding straightforward warm-up logic.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of warm-up period and layer-adaptive stability tracking represents a novel practical improvement to temporal stability methods.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more robust initialization and simplified thresholds should lead to more reliable feature selection and knowledge removal.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a warm-started temporal stability approach to feature selection in sparse autoencoders for knowledge unlearning. Our method introduces a warm-up period for stability tracking, layer-dependent decay rates, and simplified quantile-based thresholds to improve robustness and efficiency. The approach uses running averages during initial training before transitioning to exponential moving averages, with decay rates adapted to layer depth. By combining these practical improvements with rank-based stability measures, we aim to provide more reliable identification of knowledge-encoding features while actually reducing computational complexity. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "adaptive_length_sequence_sae",
        "Title": "Adaptive-Length Sequence Selection for Efficient Knowledge Unlearning",
        "Experiment": "1. Implement adaptive sequence lengths\n2. Add EMA-based threshold transition\n3. Modify ratio consistency checking\n4. Train SAEs with adaptive sequences\n5. Compare computational efficiency\n6. Analyze sequence utilization",
        "Technical_Details": "Layer-dependent decay: gamma_l = 0.7 * (1 + 0.2*l/L). Adaptive sequence length: k_max_l = min(5, ceil(3 * gamma_l)). Sequence score s_i(t) = sum(gamma_l^k * activation(t-k)) for k=0 to k_max_l. Ratio r_i = s_i_forget / (s_i_retain + 1e-6). Threshold EMA: t_ema = 0.9*t_ema + 0.1*(mean(r) + alpha*std(r)) where alpha decays exponentially from 1.5 to 1.0. Ratio consistency: std(r_i) over last 3 updates < 0.1*mean(r_i). Selection score C_i = 0.6*sparsity_i + 0.4*normalized(r_i). Features selected when C_i > t_ema AND ratio consistent.",
        "Motivation_Rationale": "Adaptive sequence lengths better match actual temporal dependencies while reducing computation. Smoother threshold transitions and ratio consistency checks help ensure stable feature selection. These refinements maintain simplicity while improving efficiency and robustness, particularly important for reliable knowledge removal.",
        "Implementation_Plan": "1. Add adaptive length computation\n2. Implement EMA threshold updates\n3. Add ratio consistency tracking\n4. Update selection criteria\n5. Add efficiency metrics",
        "Interestingness_Evaluation": "The adaptive-length approach provides an elegant way to balance computational efficiency with temporal pattern detection.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with only basic parameter adjustments and statistical tracking, actually reducing computation through adaptive lengths.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of adaptive sequence lengths and consistency checking represents a novel refinement to temporal feature selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more efficient and stable sequence-based selection should enable more reliable knowledge removal while reducing computational overhead.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present an adaptive-length sequence-based approach to feature selection in sparse autoencoders for knowledge unlearning. Our method automatically determines appropriate sequence lengths for different network layers while using exponential moving averages and consistency checks to ensure stable feature selection. The approach reduces computational overhead by avoiding unnecessarily long sequences while maintaining the ability to capture relevant temporal patterns. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "task_weighted_curriculum_sae",
        "Title": "Task-Specific Feature Weighting for Curriculum Complementary Sparsity",
        "Experiment": "1. Implement per-task importance computation\n2. Add minimum-based weight aggregation\n3. Modify separation loss weighting\n4. Train SAEs with task-specific weights\n5. Compare task-specific performance\n6. Analyze feature specialization",
        "Technical_Details": "Per-task importance w_i_k = mean(activation_i) computed on MMLU subset k (history, geography, computer science, aging). Final importance w_i = min(w_i_k) across tasks k. Danger score d = max(n-gram overlap with WMDP-bio) for n in [1,2,3]. Weighted loss: L_comp = mean((1 - w_i) * spearman_corr(f_i, d)) where f_i are activations, w_i normalized to [0,1]. Curriculum weight: lambda_comp(t) = min(0.1, 0.001 * (1 + t/1000)). Total loss: L = L_recon + lambda_l1*L_sparse + lambda_comp(t)*L_comp. Weights updated every 1000 steps using EMA with beta=0.9.",
        "Motivation_Rationale": "Task-specific importance weighting provides more precise protection of features that are consistently important across different general tasks. Using the minimum weight ensures we only reduce separation pressure when a feature is valuable for multiple capabilities. This maintains simplicity while better preserving task-specific performance.",
        "Implementation_Plan": "1. Add per-task importance tracking\n2. Implement minimum aggregation\n3. Modify weight computation\n4. Update EMA logic\n5. Adjust training loop",
        "Interestingness_Evaluation": "The task-specific weighting provides a more nuanced approach to preserving general capabilities while maintaining algorithmic simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with basic statistics computed on different data subsets. Training time similar with minimal overhead from additional importance computations.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of task-specific importance weighting represents a novel refinement to curriculum complementary sparsity.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more precise preservation of task-specific capabilities should enable better unlearning while maintaining performance across different tasks.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a task-specific approach to feature weighting in curriculum complementary sparsity training for sparse autoencoders. Our method computes importance weights separately for different categories of general tasks and uses their minimum to modulate the separation penalty for each feature. This is combined with continuous danger scoring and curriculum scheduling to provide more precise protection of consistently important features. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities across multiple domains.",
        "novel": true
    },
    {
        "Name": "targeted_stability_sae",
        "Title": "Task-Targeted Stability Guidance for Knowledge Unlearning",
        "Experiment": "1. Implement correlation-based stability thresholds\n2. Add task-specific EMA tracking\n3. Modify stability checks for targeting\n4. Train SAEs with targeted stability\n5. Compare feature preservation patterns\n6. Analyze threshold adaptation",
        "Technical_Details": "Feature EMA: s_i(t) = beta*s_i(t-1) + (1-beta)*f_i where beta=0.9. Task correlation: c_i = corr(s_i, forget_data) computed every 100 steps. Adaptive threshold: t_i = base_threshold * (1 + (1-c_i)) where base_threshold=0.1. Relative change: r_i(t) = |s_i(t) - s_i(t-1)|/|s_i(t-1)|. Layer scaling: lambda_l1(t,l) = min(target_l1, base_l1 * (1 + t/(1000*(1 + 0.2*l/L)))) if r_i < t_i else current_l1. Similar for lambda_u. Base penalties: l1_base=0.01, u_base=0.001. Target penalties: l1_target=0.1, u_target=0.01. Updates every 100 steps.",
        "Motivation_Rationale": "Task-targeted stability thresholds allow more aggressive changes to knowledge-specific features while being more conservative with general features. This helps achieve better unlearning while carefully preserving general capabilities. The approach makes efficient use of existing EMA tracking while maintaining computational simplicity.",
        "Implementation_Plan": "1. Add correlation computation\n2. Implement threshold adjustment\n3. Modify stability checking\n4. Update parameter updates\n5. Adjust training loop",
        "Interestingness_Evaluation": "The targeted stability approach provides a natural way to balance aggressive unlearning with careful preservation of general capabilities.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple, just adding basic correlation computation and threshold adjustment to existing EMA tracking. Training time nearly unchanged.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of task-specific correlation to guide stability thresholds represents a novel but straightforward improvement to stability-guided training.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more targeted stability approach should enable more effective knowledge removal while better preserving general capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a task-targeted approach to stability-guided regularization for knowledge unlearning in sparse autoencoders. Our method adjusts feature stability thresholds based on their correlation with the target unlearning task, allowing more aggressive changes to knowledge-specific features while being more conservative with general features. This is combined with layer-adaptive scaling and EMA-based stability tracking to provide smooth but effective feature separation. The approach maintains computational simplicity while potentially enabling more precise knowledge removal. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "domain_aware_uncertainty_sae",
        "Title": "Domain-Aware Uncertainty Weighting for Knowledge Unlearning",
        "Experiment": "1. Add biology/non-biology logit separation\n2. Modify uncertainty computation\n3. Update weighting scheme\n4. Train SAEs with domain-aware weighting\n5. Compare knowledge separation\n6. Analyze domain-specific patterns",
        "Technical_Details": "Bio vs non-bio logits: b_max = max(bio_logits), nb_max = max(non_bio_logits). Domain gap: g = b_max - nb_max. Domain uncertainty: d = sigmoid(g) * H where H is prediction entropy. Confidence score: conf = (max(logits) - second_max(logits)) * (1 + d). Weighted averages: c_i = sum(conf * activation_i) / sum(conf) for correct predictions, w_i similar for wrong. Activation ratio r_i = (c_i + epsilon)/(w_i + epsilon) where epsilon=1e-6. Selection score s_i = 0.7*sparsity_i + 0.3*normalized(r_i). Quantile threshold: t_q = torch.quantile(s, 0.7). Warm-up: Use only sparsity for first 1000 steps. Features selected if s_i > t_q AND training_step > warmup_steps. Update every 100 steps using batch of 32 questions.",
        "Motivation_Rationale": "For unlearning biology knowledge, the difference between model confidence in biology-related versus unrelated answers provides a more specific signal about which features encode biology knowledge. This domain-aware uncertainty helps better identify features that are truly specific to biology rather than general language understanding. The approach maintains simplicity while making the uncertainty weighting more targeted for knowledge removal.",
        "Implementation_Plan": "1. Add logit separation logic\n2. Implement domain gap computation\n3. Update uncertainty weighting\n4. Modify training loop\n5. Add domain metrics",
        "Interestingness_Evaluation": "Using domain-specific uncertainty provides an elegant way to make feature selection more targeted for knowledge removal while maintaining simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with only basic logit statistics added. Training time unchanged as computations are lightweight and done periodically.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using domain-specific answer separation to refine uncertainty weighting represents a novel but straightforward improvement for knowledge-specific feature selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more targeted identification of biology-specific features should enable more precise knowledge removal while better preserving general capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a domain-aware approach to uncertainty-weighted feature selection in sparse autoencoders for knowledge unlearning. Our method separates model predictions on biology-related versus unrelated answer choices to compute a domain-specific uncertainty measure, which is then used to adjust feature importance weights. This helps focus feature selection specifically on biology knowledge rather than general language understanding. The approach maintains computational simplicity while introducing basic domain awareness for more targeted feature identification. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "stable_interaction_consistency",
        "Title": "Stability-Weighted Interaction Consistency for Knowledge Unlearning",
        "Experiment": "1. Add interaction stability tracking\n2. Implement adaptive weighting\n3. Modify selection criteria\n4. Train SAEs with stability weighting\n5. Compare selection reliability\n6. Analyze stability patterns",
        "Technical_Details": "Base consistency: c_i = spearman_corr(rank(f_i_bio), rank(f_i_general)). Top-k selection: k = min(50, width/10) features by sparsity. Interaction: i_ij(t) = corr(f_i, f_j) for top-k. Stability score: s_ij = mean(|i_ij(t) - i_ij(t-1)|) over last 3 updates. Interaction weight: w_ij = 0.2 * exp(-2*s_ij). Adjusted score: a_i = c_i + max(w_ij * i_ij) for stable pairs (s_ij < 0.1). Layer decay: beta_l = 0.9 * (1 - 0.1*l/L). Running average: r_i(t) = beta_l*r_i(t-1) + (1-beta_l)*a_i. Layer threshold: t_l = 0.1 * (1 + 0.2*l/L). Selection score: s_i = 0.7*sparsity_i + 0.3*r_i. Feature selected if s_i > t_l AND training_step > 1000. Updates every 100 steps using 32 samples per domain.",
        "Motivation_Rationale": "Not all feature interactions are equally reliable indicators of knowledge encoding. By weighting interactions based on their stability over time, we can focus on more reliable patterns while filtering out noise. This refinement helps ensure we only consider consistent interaction patterns when identifying knowledge-specific features, leading to more reliable selection.",
        "Implementation_Plan": "1. Add stability tracking\n2. Implement adaptive weights\n3. Update interaction logic\n4. Modify selection criteria\n5. Add stability metrics",
        "Interestingness_Evaluation": "Using interaction stability to guide feature selection provides an elegant way to focus on reliable patterns while maintaining simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with basic stability tracking added to existing interaction computation. Training time nearly unchanged as stability updates use existing periodic schedule.",
        "Feasibility": 8,
        "Novelty_Evaluation": "The addition of stability-based interaction weighting represents a novel but straightforward improvement to interaction-aware selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more reliable interaction-based selection should enable more precise identification and removal of knowledge-encoding features.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "We present a stability-weighted approach to interaction-aware consistency selection in sparse autoencoders for knowledge unlearning. Our method extends interaction-based feature selection by incorporating temporal stability measures to identify reliable feature interaction patterns. The approach adaptively weights interactions based on their consistency over time, helping filter out spurious correlations while maintaining computational efficiency. This is combined with layer-adaptive parameters and rank-based consistency measures to provide robust feature selection. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "adaptive_contrast_discrimination_sae",
        "Title": "Adaptive Domain-Contrasted Discrimination for Safe Feature Selection",
        "Experiment": "1. Add stability-based EMA scaling\n2. Implement recovery detection\n3. Add safety margin computation\n4. Train SAEs with adaptive contrast\n5. Compare selection stability\n6. Analyze recovery patterns",
        "Technical_Details": "Stability score: v_i = std(d_i) over last 5 updates where d_i = r_i_bio - r_i_gen. Adaptive decay: beta_i = 0.9 * (1 - 0.2*exp(-2*v_i)). EMA update: d_i(t) = beta_i*d_i(t-1) + (1-beta_i)*d_i(t). Score variance: sigma_i = std(r_i) over batch. Safety margins: m_bio = 0.1*median(sigma_i), m_gen similar. Thresholds: t_bio = q_75(r_i_bio) + m_bio, t_gen = q_25(r_i_gen) - m_gen. Recovery check: alert if mean(r_i_gen) increases by >10% after removal. Selection score s_i = 0.6*sparsity_i + 0.4*d_i. Features selected when s_i > 0.7 AND r_i_bio > t_bio AND r_i_gen < t_gen AND no recovery alert. Updates every 100 steps using 32 questions per domain.",
        "Motivation_Rationale": "Adaptive smoothing helps balance stability and responsiveness based on feature behavior. Recovery detection and safety margins help prevent over-aggressive feature removal. These refinements make the selection process more careful while maintaining computational simplicity.",
        "Implementation_Plan": "1. Add stability computation\n2. Implement adaptive decay\n3. Add safety margins\n4. Update selection logic\n5. Add recovery metrics",
        "Interestingness_Evaluation": "The adaptive smoothing and safety checks provide a more careful way to identify knowledge-specific features while maintaining efficiency.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with only basic statistical checks added. Training time nearly unchanged as computations use existing update schedule.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using stability-based adaptation and recovery detection represents a novel but straightforward improvement to contrast-based selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more careful and adaptive feature selection should enable safer knowledge removal while maintaining general capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present an adaptive domain-contrasted approach to feature selection in sparse autoencoders for knowledge unlearning. Our method adjusts feature smoothing rates based on stability measures and incorporates safety margins and recovery detection to prevent over-aggressive feature removal. This is combined with domain-specific discrimination contrast to identify knowledge-specific features more carefully. The approach maintains computational simplicity while introducing basic adaptivity for safer feature selection. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": false
    },
    {
        "Name": "robust_entropy_ratio",
        "Title": "Robust Layer-Adaptive Entropy-Ratio Selection for Knowledge Unlearning",
        "Experiment": "1. Implement Mann-Whitney U test\n2. Add layer-dependent scaling\n3. Modify parameter computation\n4. Train SAEs with robust ratios\n5. Compare selection stability\n6. Analyze layer patterns",
        "Technical_Details": "Mann-Whitney U statistic computed on H_i_bio vs H_i_gen over k=5 mini-batches. P-value from U using normal approximation. Confidence: c_i = 1 - 2*p_value. Layer scaling: alpha_l = 0.8 * (1 + 0.2*l/L) for confidence threshold, w_l = 0.7 * (1 - 0.2*l/L) for sparsity weight. Update trigger: trigger if max(|H_i(t) - H_i(t-1)|) > 0.1 OR steps_since_update > 100. Adaptive bins: n_bins = min(20, max(5, sqrt(n_samples))). Feature epsilon: epsilon_i = 0.1 * std(f_i). Entropy ratio: r_i = (H_i_bio + epsilon_i)/(H_i_gen + epsilon_i). Selection score: s_i = w_l*sparsity_i + (1-w_l)*c_i*normalized(r_i). Base threshold: t_base = mean(s) + std(s). Features selected when s_i > t_base AND c_i > alpha_l AND training_step > 1000. Default batch size 32 per domain.",
        "Motivation_Rationale": "Using non-parametric statistics and layer-adaptive parameters makes the method more robust while maintaining efficiency. The Mann-Whitney test provides more reliable confidence estimates without distribution assumptions, while layer scaling helps handle varying feature behaviors.",
        "Implementation_Plan": "1. Add Mann-Whitney function\n2. Implement layer scaling\n3. Update parameter logic\n4. Modify selection criteria\n5. Add layer metrics",
        "Interestingness_Evaluation": "Using non-parametric statistics and layer adaptation provides a more robust way to identify knowledge-specific features while maintaining efficiency.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with only basic statistical changes and layer scaling. Training time similar as computations are still lightweight.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Incorporating non-parametric statistics and layer adaptation represents a novel but straightforward improvement to entropy-ratio selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more robust statistical approach should enable more reliable knowledge removal across different network layers.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a robust layer-adaptive approach to entropy-ratio feature selection in sparse autoencoders for knowledge unlearning. Our method extends entropy-ratio selection by incorporating non-parametric statistical testing and layer-dependent parameter scaling. This provides a more robust way to identify reliable differences in feature activation patterns while adapting to varying behaviors across network layers. The approach combines information-theoretic principles with distribution-free statistics for more reliable feature selection. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "local_adaptive_emergence_sae",
        "Title": "Local-Global Adaptive Phase Detection for Feature Selection",
        "Experiment": "1. Add feature clustering\n2. Implement local phase detection\n3. Modify selection criteria\n4. Train SAEs with local phases\n5. Compare feature identification\n6. Analyze cluster patterns",
        "Technical_Details": "Feature clustering: assign features to k=3 clusters using k-means on activation patterns every 500 steps. Global change: d_g(t) = mean(|f_i(t) - f_i(t-100)|) across all features. Local change: d_c(t) = mean(|f_i(t) - f_i(t-100)|) for cluster c. Phase transition when d_g(t) > mean(d_g) + std(d_g) OR d_c(t) > mean(d_c) + std(d_c) for any cluster c, AND steps_since_last_transition > 500. Maximum 3 phases. Phase emergence: e_i_p(t) = mean(f_i[phase_start:t]) for phase p. Emergence ratio: r_i_p = e_i_p_bio / (e_i_p_gen + epsilon) where epsilon=1e-6. Cluster consistency: cc_i = mean(r_i_p) for features in same cluster. Combined score: s_i = 0.6*sparsity_i + 0.2*normalized(r_i_p) + 0.2*normalized(cc_i). Threshold: t = mean(s) + 0.5*std(s). Features selected when s_i > t AND cc_i > 1.5 AND at least 2 phases detected.",
        "Motivation_Rationale": "Knowledge-specific features likely develop in coordinated patterns distinct from general features. By detecting phases at both global and local levels, we can better identify groups of features that consistently encode specific knowledge. This maintains computational simplicity while capturing more nuanced development patterns.",
        "Implementation_Plan": "1. Add clustering function\n2. Implement local detection\n3. Update phase tracking\n4. Modify selection logic\n5. Add cluster metrics",
        "Interestingness_Evaluation": "Using both local and global phase detection provides a more nuanced way to identify coordinated feature development while maintaining simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple, just adding basic clustering and local detection using existing metrics. Training time nearly unchanged as clustering is infrequent.",
        "Feasibility": 8,
        "Novelty_Evaluation": "Local-global phase detection for coordinated feature development represents a novel but straightforward improvement to adaptive phase analysis.",
        "Novelty": 7,
        "Expected_Research_Impact": "The local-global approach should enable better identification of coordinated knowledge-specific features, improving unlearning precision.",
        "Research_Impact": 8,
        "Overall_Score": 7.9,
        "Abstract": "We present a local-global approach to phase-aware feature selection in sparse autoencoders for knowledge unlearning. Our method combines global phase detection with local analysis of feature clusters to identify coordinated development patterns characteristic of knowledge-specific features. This multi-level temporal analysis helps distinguish between features that consistently encode specific knowledge and those serving more general purposes. The approach maintains computational efficiency while adding basic clustering for more nuanced pattern detection. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "comparative_emergence_sae",
        "Title": "Comparative Emergence Pattern Detection for Domain-Specific Feature Selection",
        "Experiment": "1. Add domain-specific tracking\n2. Implement pattern comparison\n3. Modify selection criteria\n4. Train SAEs with comparative detection\n5. Compare selection accuracy\n6. Analyze pattern differences",
        "Technical_Details": "Track activations separately for bio/general inputs. For each domain d: Initial level i_d = median(f_i_d[0:100]). Rise time t_rise_d = argmax(diff(median(f_i_d[t:t+100]))) for t in first 500 steps. Stable level s_d = median(f_i_d[t_rise_d+100:t_rise_d+200]). Rise magnitude m_d = s_d - i_d. Sharpness ratio sr_d = m_d / (t_rise_d + 10). Stability ratio st_d = std(f_i_d[t_rise_d+100:t_rise_d+200]) / s_d. Shape score h_d = sr_d * exp(-2*st_d). Pattern difference: p_i = (h_bio - h_gen) / (h_bio + h_gen + 0.1). Confidence c_i = min(st_bio, st_gen). Layer weight w_l = min(0.4, 0.2 + 0.2*exp(-median(c_i))). Selection score: score_i = (1-w_l)*sparsity_i + w_l*normalized(p_i). Adaptive threshold t = median(score) + 0.5*IQR(score). Features selected when score_i > t AND p_i > 0.2 AND training_step > 1000. Update every 100 steps using batch size 16 per domain.",
        "Motivation_Rationale": "Knowledge-specific features should show characteristic emergence patterns primarily on biology inputs, while general features show similar patterns across all inputs. By directly comparing emergence patterns between domains, we get a stronger signal for identifying truly knowledge-specific features. This maintains computational simplicity while providing more targeted feature selection.",
        "Implementation_Plan": "1. Add domain tracking function\n2. Implement pattern comparison\n3. Update selection criteria\n4. Modify training loop\n5. Add comparison metrics",
        "Interestingness_Evaluation": "Using domain-comparative emergence patterns provides a more precise way to identify knowledge-specific features while maintaining simplicity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple, just tracking existing statistics separately for each domain with no architectural changes. Training time nearly unchanged with small batches per domain.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using domain-comparative emergence patterns represents a novel but straightforward improvement to emergence-based selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The domain-comparative approach should enable more precise identification of truly knowledge-specific features.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a comparative approach to emergence pattern detection in sparse autoencoders for knowledge unlearning. Our method tracks how features emerge differently during early training when processing biology-related versus general inputs. By comparing emergence patterns between domains, we can better identify features that are truly specific to biology knowledge rather than serving general language functions. The approach maintains computational simplicity while providing more targeted feature selection. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "adaptive_cached_clamping",
        "Title": "Adaptive Cache Updates for Gradient-Guided Knowledge Unlearning",
        "Experiment": "1. Implement entropy-based confidence\n2. Add adaptive cache updates\n3. Modify normalization scheme\n4. Compare update frequency patterns\n5. Analyze confidence stability\n6. Evaluate layer-wise behavior",
        "Technical_Details": "Entropy-based confidence: c = 1 - H/log(n_classes) where H is prediction entropy. Prediction stability: s = mean(|p_t - p_t-1|) where p_t are class probabilities. Cache update triggered when s > 0.1 * exp(-n_updates/1000) OR steps_since_update > 2000. Layer-wise normalization: g_norm_i = g_i / (mean_l(|g|) + 1e-6) where mean_l is mean for layer l. Warmup period: First 100 steps use fixed base_multiplier=1.0. Sparsity scaling: v_i = base_multiplier * sign(g_norm_i) * (1 + tanh(2*|g_norm_i|)) * c * sqrt(sparsity_i) where sparsity_i is feature sparsity. Update exponential backoff starts after 3 updates. Implementation uses torch.autograd.grad(create_graph=False) with batch_size=32.",
        "Motivation_Rationale": "Fixed cache update schedules and simple confidence metrics can lead to suboptimal or unnecessary updates. By making updates adaptive and improving confidence estimation, we can achieve more reliable knowledge removal with fewer updates. The layer-wise normalization and sparsity scaling help better handle varying feature behaviors across the network.",
        "Implementation_Plan": "1. Add entropy computation\n2. Implement adaptive updates\n3. Add layer normalization\n4. Update scaling function\n5. Add stability tracking",
        "Interestingness_Evaluation": "Using adaptive updates and improved confidence metrics provides a more robust way to guide feature suppression while maintaining efficiency.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with basic statistical changes and adaptive triggering. The improved update scheme actually reduces overall computation by avoiding unnecessary updates.",
        "Feasibility": 9,
        "Novelty_Evaluation": "While adaptive caching and entropy-based confidence are known techniques, their combination for efficient gradient-guided clamping represents a novel but straightforward improvement.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more robust and adaptive approach should enable more reliable knowledge removal while preserving general capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present an adaptive cached gradient-guided approach to feature clamping in sparse autoencoders for knowledge unlearning. Our method improves upon basic gradient caching by introducing entropy-based confidence estimation and adaptive update scheduling with exponential backoff. The approach incorporates layer-wise normalization and sparsity scaling while maintaining computational efficiency through intelligent cache management. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": false
    },
    {
        "Name": "adaptive_early_shaping_sae",
        "Title": "Adaptive Early Feature Shaping with Layer-Aware Statistics",
        "Experiment": "1. Implement adaptive threshold computation\n2. Add rank-based emergence tracking\n3. Modify layer-aware decay\n4. Train SAEs with adaptive shaping\n5. Compare feature stability\n6. Analyze layer patterns",
        "Technical_Details": "Adaptive threshold q_l = (60 + 20*l/L)th percentile for layer l. Emergence rate e_i(t) = (rank(activation_i) / batch_size) averaged over 3 mini-batches. Task scores: t_bio = rank(bio_loss) / n_samples, t_gen similar. Rank correlation r_i = spearman(rank(e_i), t_bio - t_gen). Layer-aware decay w_l(t) = 1/(1 + t/(1000*(1 + 0.2*l/L))). Running statistics: mean_l, std_l updated with momentum 0.95 every 25 steps. Relative bounds: clip(r_i, -2*std_l, 2*std_l). Shaping loss L_shape = w_l(t) * mean(r_i * activation_i). Total loss L = L_recon + L_sparse + L_shape. Batch size 32, increased update frequency to every 25 steps in first 1000 steps.",
        "Motivation_Rationale": "The refined approach makes feature shaping more adaptive and theoretically grounded while actually reducing complexity. Using rank-based statistics and layer-aware parameters provides more reliable guidance across different network depths, while unifying the decay schedules simplifies implementation. This maintains or improves robustness while reducing the number of components that need tuning.",
        "Implementation_Plan": "1. Add adaptive threshold function\n2. Implement rank tracking\n3. Add layer-aware decay\n4. Update loss computation\n5. Add layer metrics",
        "Interestingness_Evaluation": "Using layer-adaptive rank statistics provides a more principled way to guide feature development while reducing complexity.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation becomes even simpler by unifying decay schedules and using rank-based measures. Training time slightly improved as update frequency adjusts automatically.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of layer-adaptive statistics and rank-based measures represents a novel simplification of emergence-based guidance.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more principled and unified approach should enable more reliable feature separation across different network layers.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present an adaptive early feature shaping approach for sparse autoencoders in selective unlearning. Our method uses layer-aware rank statistics and adaptive thresholds to guide feature development during initial training. The approach unifies previously separate mechanisms through a layer-adaptive framework while maintaining computational efficiency. By using rank-based measures and relative constraints, the method aims to provide more reliable guidance across different network depths. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "adaptive_coactivation_sae",
        "Title": "Adaptive Co-activation Selection with Efficient Pair Filtering",
        "Experiment": "1. Add magnitude-based pre-filtering\n2. Implement adaptive threshold\n3. Add frequency tracking\n4. Train SAEs with adaptive selection\n5. Compare computational efficiency\n6. Analyze threshold behavior",
        "Technical_Details": "Pre-filter features with mean activation > 0.1 * max_activation per batch. For filtered subset, track co-activation frequency f_ij = mean(activation_i > 0 AND activation_j > 0) with momentum 0.9. Compute approximate rank correlation only for pairs with f_ij > 0.05. Adaptive threshold t = median(|R_bio|) + 0.5*IQR(|R_bio|) where R_bio is correlation matrix. Keep pairs where |R_bio| > t. Domain difference D = |R_bio - R_gen| * f_ij. Penalty L_coact = mean(D * |R_bio|) for selected pairs. Warmup schedule: lambda_coact = 0.1 * sigmoid((t-500)/200). Total loss L = L_recon + lambda_l1*L_sparse + lambda_coact*L_coact. Update frequencies and threshold every 100 steps using batch size 32 per domain.",
        "Motivation_Rationale": "Pre-filtering and adaptive thresholds make co-activation guidance more computationally efficient while better focusing on consistently interacting features. Using activation frequencies provides a simpler and more direct measure of feature interactions than raw correlations. These refinements maintain the core simplicity while improving practical effectiveness.",
        "Implementation_Plan": "1. Add pre-filtering function\n2. Implement frequency tracking\n3. Add adaptive threshold\n4. Update pair selection\n5. Add efficiency metrics",
        "Interestingness_Evaluation": "Using activation frequencies and adaptive thresholds provides a more efficient and focused way to identify relevant feature interactions.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation becomes even simpler by avoiding full correlation computation. Training time improved through pre-filtering and focused correlation computation.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of frequency-based filtering and adaptive thresholds represents a novel but straightforward improvement to co-activation guidance.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more efficient and focused approach should enable better scaling to larger feature sets while maintaining effectiveness.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present an adaptive approach to co-activation guided training in sparse autoencoders for knowledge unlearning. Our method combines magnitude-based pre-filtering with activation frequency tracking to efficiently identify relevant feature interactions. The approach uses adaptive thresholds based on correlation distributions while maintaining computational efficiency through focused computation on consistently co-activating features. This requires no architectural changes or complex statistical measures, providing a practical way to guide knowledge separation during training. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "shared_activation_init_sae",
        "Title": "Shared Activation Detection for Knowledge-Aware Initialization",
        "Experiment": "1. Implement shared activation tracking\n2. Add convergence detection\n3. Modify initialization scheme\n4. Train SAEs with activation-aware init\n5. Compare feature relationships\n6. Analyze convergence patterns",
        "Technical_Details": "Pre-training samples: 1200 (200 each domain) in batches of 32. For each feature i, find top-k activated samples (k=5) per batch. Shared activation s_ij = count of samples where both i,j in top-k, normalized by k. Domain score d_i = mean(s_ij) computed separately for bio vs general inputs. Specificity spec_i = d_i_bio / (d_i_gen + 1e-6). Stability check: stop pre-training when max(|spec_i(t) - spec_i(t-5)|) < 0.05 for 3 consecutive updates. Minimum 100 steps, maximum 300. Adaptive scale a_i = 0.1 * spec_i * sqrt(d_i_bio). Initial encoder weights W_enc = W_base + a_i * sign(mean_bio - mean_gen) * |W_base|. Pre-training learning rate 1e-3, main training unchanged.",
        "Motivation_Rationale": "Shared activation patterns directly capture how features work together to encode knowledge, while adaptive pre-training length ensures reliable initialization signals. This simplifies the implementation while making it more targeted for knowledge separation.",
        "Implementation_Plan": "1. Add activation tracking\n2. Implement convergence check\n3. Update score computation\n4. Modify initialization logic\n5. Add stability metrics",
        "Interestingness_Evaluation": "Using direct shared activation measures provides a simpler and more targeted way to identify knowledge-encoding feature groups.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation becomes even simpler by replacing correlations with basic counting operations. Training time varies but is bounded, with early stopping often reducing computation.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The use of shared activation patterns and adaptive pre-training represents a novel simplification of knowledge-aware initialization.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more direct and targeted initialization approach should enable more effective separation of knowledge-specific features.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a shared activation approach to initialization in sparse autoencoders for knowledge unlearning. Our method tracks which features tend to activate strongly on the same inputs during a brief pre-training phase, directly identifying groups of features that work together to encode specific knowledge. The pre-training length adapts automatically based on the stability of these activation patterns. These patterns then inform initialization perturbations that help align features with knowledge boundaries. The approach simplifies previous methods while maintaining a focus on identifying coordinated feature activity. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "weighted_mad_consistency_sae",
        "Title": "Confidence-Weighted MAD Consistency for Knowledge-Specific Feature Detection",
        "Experiment": "1. Add confidence-based weight computation\n2. Implement weighted MAD tracking\n3. Modify statistic updates\n4. Train SAEs with weighted consistency\n5. Compare detection precision\n6. Analyze confidence patterns",
        "Technical_Details": "Sample weights w = (max_logit - second_max_logit) / max_logit computed from model predictions. For each feature i: Compute weighted median m_i = weighted_median(x, w) and weighted MAD mad_i = weighted_median(|x - m_i|, w) separately for biology and general inputs over mini-batches of 32. Robust CV rcv_i = mad_i/(m_i + epsilon) where epsilon=1e-6. Consistency ratio r_i = rcv_i_gen / (rcv_i_bio + epsilon). Momentum warmup: beta(t) = 0.5 + 0.4*(1 - exp(-t/500)) for first 1000 steps, then fixed at 0.9. Running statistics: s(t) = beta(t)*s(t-1) + (1-beta(t))*s(t). Stability check: max(|r_i(t) - r_i(t-1)|) < 0.1 over last 3 updates. Layer threshold t_l = weighted_median(r) + 0.5*(1 + 0.2*l/L)*weighted_MAD(r) for stable scores in layer l. Selection score s_i = 0.7*sparsity_i + 0.3*normalized(r_i). Features selected when s_i > t_l AND r_i > 1.2 AND score is stable AND training_step > 1000. Update every 100 steps.",
        "Motivation_Rationale": "High-confidence predictions likely provide clearer signals about which features are truly knowledge-specific. Weighting samples by prediction confidence helps focus the statistical measures on these more informative examples. This refinement improves signal quality while maintaining the robustness of MAD and adding minimal computational overhead.",
        "Implementation_Plan": "1. Add weight computation function\n2. Implement weighted statistics\n3. Update tracking logic\n4. Modify selection criteria\n5. Add confidence metrics",
        "Interestingness_Evaluation": "Using prediction confidence to weight statistics provides a more focused approach while maintaining statistical robustness.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with only basic weighted statistics added. Training time nearly unchanged as weight computation is lightweight and uses existing model outputs.",
        "Feasibility": 9,
        "Novelty_Evaluation": "Using prediction confidence to weight MAD-based consistency measures represents a novel but straightforward improvement to the robust approach.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more focused statistical approach should enable more precise identification of truly knowledge-specific features.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a confidence-weighted approach to consistency detection in sparse autoencoders for knowledge unlearning. Our method extends MAD-based consistency measures by weighting samples according to model prediction confidence, helping focus the statistics on more informative examples. This combines robust statistics with targeted weighting while maintaining computational simplicity and requiring no architectural changes. The approach uses basic weighted statistics to better distinguish between knowledge-specific and general-purpose features. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    },
    {
        "Name": "targeted_efficient_refinement_sae",
        "Title": "Task-Targeted Efficient Feature Refinement for Knowledge Unlearning",
        "Experiment": "1. Implement rank-based pattern tracking\n2. Add layer-adaptive suppression\n3. Modify stability metrics\n4. Update selection criteria\n5. Compare task impact\n6. Analyze preservation patterns",
        "Technical_Details": "Initial score s_i = 0.7*sparsity_i + 0.3*normalized(activation_i) on biology inputs. Sort scores descending, find largest gap in top 50% as initial threshold. Layer-specific suppression: v_l = -1.0 * (1 + 0.2*l/L) * median(std(activation)) for layer l. For each unselected feature j: Track rank of top-10 biology-related tokens using argsort. Rank change r_k = |rank_normal[k] - rank_suppressed[k]|/n_tokens for biology tokens k. Pattern score p_j = mean(r_k * token_relevance[k]) where relevance is normalized n-gram overlap with biology corpus. Response magnitude m_j = |mean(activation_j_suppressed) - mean(activation_j_normal)|/mad(activation_j_normal). Combined score c_j = m_j * p_j updated with momentum 0.9. Selection threshold t_l = mean(c) * (1 + 0.3*l/L). Add features where c_j > t_l AND max(r_k) > 0.3 to selection set. Check stability every 10 steps using proxy_mmlu_loss on small cached validation set. Stop if proxy loss increases by >1% OR relative score change < 0.05 for 3 checks OR 5 passes completed.",
        "Motivation_Rationale": "Basic overlap metrics and fixed suppression values may not capture knowledge-specific pattern changes well enough. Using rank changes and domain-relevant tokens provides more targeted selection while maintaining efficiency. Layer-adaptive parameters and focused stability checks help better preserve general capabilities.",
        "Implementation_Plan": "1. Add rank tracking function\n2. Implement layer scaling\n3. Update pattern scoring\n4. Modify stability checks\n5. Add proxy validation\n6. Update selection logic",
        "Interestingness_Evaluation": "Using domain-specific rank changes provides a more targeted way to identify knowledge-related features while maintaining efficiency.",
        "Interestingness": 8,
        "Feasibility_Evaluation": "Implementation remains simple with basic rank operations and cached validation, while layer adaptation requires only scalar multipliers.",
        "Feasibility": 9,
        "Novelty_Evaluation": "The combination of rank-based patterns and domain-focused stability represents a novel refinement to efficient feature selection.",
        "Novelty": 7,
        "Expected_Research_Impact": "The more targeted process should enable more precise knowledge removal while better preserving general capabilities.",
        "Research_Impact": 8,
        "Overall_Score": 8.3,
        "Abstract": "We present a task-targeted approach to efficient feature selection in sparse autoencoders for knowledge unlearning. Our method extends efficient iterative refinement by incorporating rank-based pattern analysis focused on domain-relevant tokens and layer-adaptive suppression values. The approach maintains computational efficiency while providing more targeted feature selection through careful tracking of knowledge-specific activation patterns. This requires no architectural changes or complex statistics, using only basic ranking operations and cached validation data. We evaluate this method on the task of removing dangerous biology knowledge while preserving general language capabilities.",
        "novel": true
    }
]