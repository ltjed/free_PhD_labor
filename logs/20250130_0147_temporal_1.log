Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.
Using OpenAI API with deepseek-reasoner.
ideas
Loaded existing ideas:
Processing idea: sparse_orthogonal_sae
2025-01-30 01:47:05
*Starting idea: 20250130_014704_sparse_orthogonal_sae*
2025-01-30 01:47:05
*Starting Experiments*
Starting experiment with prompt for coder: Your goal is to implement the following idea: Sparsity-Guided Orthogonality Constraints for Interpretable Feature Separation. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Use existing sparsity masks to identify competing features
2. Add sparsity-weighted orthogonality loss
3. Train on google/gemma-2-2b using standard datasets
4. Compare benchmark performance against baseline and other orthogonal SAEs
5. Analyze feature competition patterns
6. Evaluate impact of competition thresholds.
The implementation plan is as follows: 1. Add function to compute mask intersections from top-k indices
2. Modify AutoEncoderTopK to use sparsity patterns
3. Add sparsity-weighted orthogonality loss
4. Add configuration for competition threshold
5. Add evaluation metrics for feature competition
6. Update training loop to use activation masks.

You can also refer to other information in the idea: {'Name': 'sparse_orthogonal_sae', 'Title': 'Sparsity-Guided Orthogonality Constraints for Interpretable Feature Separation', 'Experiment': '1. Use existing sparsity masks to identify competing features\n2. Add sparsity-weighted orthogonality loss\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and other orthogonal SAEs\n5. Analyze feature competition patterns\n6. Evaluate impact of competition thresholds', 'Technical_Details': 'The method uses a sparsity-based orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_{i,j} c_{ij} * |f_i^T f_j| where c_{ij} is the normalized intersection size of sparsity masks for features i and j over a batch. Features that frequently activate on the same inputs face stronger orthogonality constraints, encouraging them to learn distinct concepts. The competition coefficients c_{ij} are computed directly from the existing top-k activation masks with no additional overhead.', 'Implementation_Plan': '1. Add function to compute mask intersections from top-k indices\n2. Modify AutoEncoderTopK to use sparsity patterns\n3. Add sparsity-weighted orthogonality loss\n4. Add configuration for competition threshold\n5. Add evaluation metrics for feature competition\n6. Update training loop to use activation masks', 'Interestingness_Evaluation': 'Using sparsity patterns to guide orthogonality provides a direct and elegant connection between the two key mechanisms for feature separation.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation uses only existing top-k masks; no additional computation needed; standard matrix operations; easily within 30-minute limit on H100; minimal code changes required.', 'Feasibility': 10, 'Novelty_Evaluation': 'Leveraging sparsity patterns to guide orthogonality constraints is a novel and principled approach that directly targets feature competition.', 'Novelty': 9, 'Expected_Research_Impact': 'The direct connection between sparsity and orthogonality should provide more interpretable feature separation while maintaining computational efficiency.', 'Research_Impact': 9, 'Overall_Score': 9.4, 'Abstract': "We present a sparsity-guided approach to improving feature separation in sparse autoencoders through targeted orthogonality constraints. Rather than applying uniform penalties or tracking complex statistics, our method identifies competing features directly from their sparsity patterns and applies orthogonality constraints proportional to their activation overlap. This creates a natural connection between the autoencoder's sparsity mechanism and feature separation, encouraging features that activate on similar inputs to learn distinct representations. The method requires minimal modifications to existing architectures, leveraging already-computed sparsity masks to guide the orthogonality constraints. We evaluate this approach on large language models to assess its effectiveness in improving feature interpretability while maintaining reconstruction quality.", 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0001, 'steps': 4882, 'seed': 42, 'activation_dim': 2304, 'dict_size': 65536, 'k': 320, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 2314.96337890625, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '8a52411b-0254-4a79-9c7d-5ac4b0605b84', 'datetime_epoch_millis': 1738123643311, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.991702251552795, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.08349609375}, 'model_performance_preservation': {'ce_loss_score': 0.9917763157894737, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.015625, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.7890625, 'mse': 1.328125, 'cossim': 0.9375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 140.0, 'l2_ratio': 0.93359375, 'relative_reconstruction_bias': 1.0}, 'sparsity': {'l0': 320.0, 'l1': 856.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '0e9b952b-871d-4ce7-9ed9-96e818c1216c', 'datetime_epoch_millis': 1738123994045, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.13237432110574746, 'scr_metric_threshold_2': 0.06470180818617062, 'scr_dir2_threshold_2': 0.06404012878979931, 'scr_dir1_threshold_5': 0.19715911444320258, 'scr_metric_threshold_5': 0.10614614555416035, 'scr_dir2_threshold_5': 0.10750522265458036, 'scr_dir1_threshold_10': 0.20602627219129846, 'scr_metric_threshold_10': 0.13110591778208402, 'scr_dir2_threshold_10': 0.13181100792268205, 'scr_dir1_threshold_20': 0.20329280529998314, 'scr_metric_threshold_20': 0.15287015862526643, 'scr_dir2_threshold_20': 0.15768317185099726, 'scr_dir1_threshold_50': 0.2120491811053764, 'scr_metric_threshold_50': 0.1802298448942073, 'scr_dir2_threshold_50': 0.18891571918146965, 'scr_dir1_threshold_100': 0.1863658869644765, 'scr_metric_threshold_100': 0.21474250080724952, 'scr_dir2_threshold_100': 0.22145872177849663, 'scr_dir1_threshold_500': 0.13944711141292782, 'scr_metric_threshold_500': 0.22965224222697211, 'scr_dir2_threshold_500': 0.2437974632444002}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.359374374267165, 'scr_metric_threshold_2': 0.0024571168813214595, 'scr_dir2_threshold_2': 0.0024571168813214595, 'scr_dir1_threshold_5': 0.45312510186348476, 'scr_metric_threshold_5': 0.01474211549298745, 'scr_dir2_threshold_5': 0.01474211549298745, 'scr_dir1_threshold_10': 0.42187454889028175, 'scr_metric_threshold_10': 0.019656056358159712, 'scr_dir2_threshold_10': 0.019656056358159712, 'scr_dir1_threshold_20': 0.46874991268844163, 'scr_metric_threshold_20': 0.03439802540241183, 'scr_dir2_threshold_20': 0.03439802540241183, 'scr_dir1_threshold_50': 0.46874991268844163, 'scr_metric_threshold_50': 0.05405408176057155, 'scr_dir2_threshold_50': 0.05405408176057155, 'scr_dir1_threshold_100': 0.45312510186348476, 'scr_metric_threshold_100': 0.0638821099396514, 'scr_dir2_threshold_100': 0.0638821099396514, 'scr_dir1_threshold_500': 0.2968751309673376, 'scr_metric_threshold_500': 0.09582316490947711, 'scr_dir2_threshold_500': 0.09582316490947711}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 0.01941722852250481, 'scr_metric_threshold_2': 0.048710490144909316, 'scr_dir2_threshold_2': 0.048710490144909316, 'scr_dir1_threshold_5': 0.19417459996886668, 'scr_metric_threshold_5': 0.08309451623569147, 'scr_dir2_threshold_5': 0.08309451623569147, 'scr_dir1_threshold_10': 0.2038832142301191, 'scr_metric_threshold_10': 0.10601720029621291, 'scr_dir2_threshold_10': 0.10601720029621291, 'scr_dir1_threshold_20': 0.19417459996886668, 'scr_metric_threshold_20': 0.14040105560003607, 'scr_dir2_threshold_20': 0.14040105560003607, 'scr_dir1_threshold_50': 0.1844659857076143, 'scr_metric_threshold_50': 0.18338113091025351, 'scr_dir2_threshold_50': 0.18338113091025351, 'scr_dir1_threshold_100': 0.1844659857076143, 'scr_metric_threshold_100': 0.21489969340325124, 'scr_dir2_threshold_100': 0.21489969340325124, 'scr_dir1_threshold_500': -0.03883503573096427, 'scr_metric_threshold_500': 0.09455585826595218, 'scr_dir2_threshold_500': 0.09455585826595218}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.42857142857142855, 'scr_metric_threshold_2': 0.015189978091145603, 'scr_dir2_threshold_2': 0.015189978091145603, 'scr_dir1_threshold_5': 0.5396829601737049, 'scr_metric_threshold_5': 0.02784819256855401, 'scr_dir2_threshold_5': 0.02784819256855401, 'scr_dir1_threshold_10': 0.5555557658011382, 'scr_metric_threshold_10': 0.03291141800038962, 'scr_dir2_threshold_10': 0.03291141800038962, 'scr_dir1_threshold_20': 0.5555557658011382, 'scr_metric_threshold_20': 0.05316462152337083, 'scr_dir2_threshold_20': 0.05316462152337083, 'scr_dir1_threshold_50': 0.5396829601737049, 'scr_metric_threshold_50': 0.07848105047818764, 'scr_dir2_threshold_50': 0.07848105047818764, 'scr_dir1_threshold_100': 0.4603179859314168, 'scr_metric_threshold_100': 0.10379747943300446, 'scr_dir2_threshold_100': 0.10379747943300446, 'scr_dir1_threshold_500': 0.17460370021713112, 'scr_metric_threshold_500': 0.017721590807063405, 'scr_dir2_threshold_500': 0.017721590807063405}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': 0.1171878346939954, 'scr_metric_threshold_2': 0.035608379457657886, 'scr_dir2_threshold_2': 0.035608379457657886, 'scr_dir1_threshold_5': 0.17968748544808716, 'scr_metric_threshold_5': 0.09198818094766784, 'scr_dir2_threshold_5': 0.09198818094766784, 'scr_dir1_threshold_10': 0.2265628055901697, 'scr_metric_threshold_10': 0.12166174206152826, 'scr_dir2_threshold_10': 0.12166174206152826, 'scr_dir1_threshold_20': 0.14062502910382568, 'scr_metric_threshold_20': 0.14540066169996685, 'scr_dir2_threshold_20': 0.14540066169996685, 'scr_dir1_threshold_50': 0.10156257275956422, 'scr_metric_threshold_50': 0.16913958133840545, 'scr_dir2_threshold_50': 0.16913958133840545, 'scr_dir1_threshold_100': -0.046874854480871565, 'scr_metric_threshold_100': 0.20771528153377425, 'scr_dir2_threshold_100': 0.20771528153377425, 'scr_dir1_threshold_500': 0.03906245634426147, 'scr_metric_threshold_500': 0.15430280078147524, 'scr_dir2_threshold_500': 0.15430280078147524}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.00546440968122594, 'scr_metric_threshold_2': 0.2549018966193889, 'scr_dir2_threshold_2': 0.2549018966193889, 'scr_dir1_threshold_5': 0.016393554752069144, 'scr_metric_threshold_5': 0.3843138373204936, 'scr_dir2_threshold_5': 0.3843138373204936, 'scr_dir1_threshold_10': 0.03825119347697291, 'scr_metric_threshold_10': 0.466666635500846, 'scr_dir2_threshold_10': 0.466666635500846, 'scr_dir1_threshold_20': -0.027322374114521025, 'scr_metric_threshold_20': 0.5215685788689659, 'scr_dir2_threshold_20': 0.5215685788689659, 'scr_dir1_threshold_50': 0.01092881936245188, 'scr_metric_threshold_50': 0.5607842193113866, 'scr_dir2_threshold_50': 0.5607842193113866, 'scr_dir1_threshold_100': -0.1092894964580841, 'scr_metric_threshold_100': 0.5921568251627849, 'scr_dir2_threshold_100': 0.5921568251627849, 'scr_dir1_threshold_500': -0.1311474608913792, 'scr_metric_threshold_500': 0.6549020368655817, 'scr_dir2_threshold_500': 0.6549020368655817}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.0410254200064373, 'scr_metric_threshold_2': 0.06854845881151235, 'scr_dir2_threshold_2': 0.06854845881151235, 'scr_dir1_threshold_5': 0.06666642213478169, 'scr_metric_threshold_5': 0.08870972781349516, 'scr_dir2_threshold_5': 0.08870972781349516, 'scr_dir1_threshold_10': 0.05641014354938642, 'scr_metric_threshold_10': 0.12096785435318151, 'scr_dir2_threshold_10': 0.12096785435318151, 'scr_dir1_threshold_20': 0.11282028709877284, 'scr_metric_threshold_20': 0.1290322658174608, 'scr_dir2_threshold_20': 0.1290322658174608, 'scr_dir1_threshold_50': 0.158974152062764, 'scr_metric_threshold_50': 0.16935480382142643, 'scr_dir2_threshold_50': 0.16935480382142643, 'scr_dir1_threshold_100': 0.17948701489841076, 'scr_metric_threshold_100': 0.18951607282340924, 'scr_dir2_threshold_100': 0.18951607282340924, 'scr_dir1_threshold_500': 0.21538460127700665, 'scr_metric_threshold_500': 0.30241927537102686, 'scr_dir2_threshold_500': 0.30241927537102686}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.049327255975766836, 'scr_metric_threshold_2': 0.05357152835597378, 'scr_dir2_threshold_2': 0.05357152835597378, 'scr_dir1_threshold_5': 0.07174876026342582, 'scr_metric_threshold_5': 0.10267856311319266, 'scr_dir2_threshold_5': 0.10267856311319266, 'scr_dir1_threshold_10': 0.09417026455108479, 'scr_metric_threshold_10': 0.12946419424511926, 'scr_dir2_threshold_10': 0.12946419424511926, 'scr_dir1_threshold_20': 0.13452902572595388, 'scr_metric_threshold_20': 0.1517858639625321, 'scr_dir2_threshold_20': 0.1517858639625321, 'scr_dir1_threshold_50': 0.20627805327479426, 'scr_metric_threshold_50': 0.20089289871975097, 'scr_dir2_threshold_50': 0.20089289871975097, 'scr_dir1_threshold_100': 0.31390132731264025, 'scr_metric_threshold_100': 0.29017851322092003, 'scr_dir2_threshold_100': 0.29017851322092003, 'scr_dir1_threshold_500': 0.4394618582376964, 'scr_metric_threshold_500': 0.3973215699328676, 'scr_dir2_threshold_500': 0.3973215699328676}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.03862661712745569, 'scr_metric_threshold_2': 0.03862661712745569, 'scr_dir2_threshold_2': 0.033333181956485214, 'scr_dir1_threshold_5': 0.05579403094120067, 'scr_metric_threshold_5': 0.05579403094120067, 'scr_dir2_threshold_5': 0.06666664774456064, 'scr_dir1_threshold_10': 0.05150224144123495, 'scr_metric_threshold_10': 0.05150224144123495, 'scr_dir2_threshold_10': 0.05714296256601923, 'scr_dir1_threshold_20': 0.047210196127387125, 'scr_metric_threshold_20': 0.047210196127387125, 'scr_dir2_threshold_20': 0.08571430193323373, 'scr_dir1_threshold_50': 0.025750992813676425, 'scr_metric_threshold_50': 0.025750992813676425, 'scr_dir2_threshold_50': 0.09523798711177515, 'scr_dir1_threshold_100': 0.05579403094120067, 'scr_metric_threshold_100': 0.05579403094120067, 'scr_dir2_threshold_100': 0.10952379871117751, 'scr_dir1_threshold_500': 0.12017164088233277, 'scr_metric_threshold_500': 0.12017164088233277, 'scr_dir2_threshold_500': 0.2333334090217574}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'cec3b1d1-bb8b-41f8-90b8-b713eb734514', 'datetime_epoch_millis': 1738124057848, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9597812928259372, 'sae_top_1_test_accuracy': 0.7900625, 'sae_top_2_test_accuracy': 0.8382124999999999, 'sae_top_5_test_accuracy': 0.8636625, 'sae_top_10_test_accuracy': 0.89578125, 'sae_top_20_test_accuracy': 0.9201812500000001, 'sae_top_50_test_accuracy': 0.9356312500000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9636000394821167, 'sae_top_1_test_accuracy': 0.7896, 'sae_top_2_test_accuracy': 0.852, 'sae_top_5_test_accuracy': 0.8827999999999999, 'sae_top_10_test_accuracy': 0.925, 'sae_top_20_test_accuracy': 0.9328, 'sae_top_50_test_accuracy': 0.9536, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.841, 'sae_top_2_test_accuracy': 0.844, 'sae_top_5_test_accuracy': 0.8646, 'sae_top_10_test_accuracy': 0.8907999999999999, 'sae_top_20_test_accuracy': 0.9208000000000001, 'sae_top_50_test_accuracy': 0.9236000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9338000297546387, 'sae_top_1_test_accuracy': 0.7988, 'sae_top_2_test_accuracy': 0.8311999999999999, 'sae_top_5_test_accuracy': 0.8629999999999999, 'sae_top_10_test_accuracy': 0.8832000000000001, 'sae_top_20_test_accuracy': 0.8992000000000001, 'sae_top_50_test_accuracy': 0.9092, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000476837158, 'sae_top_1_test_accuracy': 0.7072, 'sae_top_2_test_accuracy': 0.7442, 'sae_top_5_test_accuracy': 0.7985999999999999, 'sae_top_10_test_accuracy': 0.8291999999999999, 'sae_top_20_test_accuracy': 0.8544, 'sae_top_50_test_accuracy': 0.8806, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000641345978, 'sae_top_1_test_accuracy': 0.797, 'sae_top_2_test_accuracy': 0.944, 'sae_top_5_test_accuracy': 0.95, 'sae_top_10_test_accuracy': 0.947, 'sae_top_20_test_accuracy': 0.952, 'sae_top_50_test_accuracy': 0.96, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9698000311851501, 'sae_top_1_test_accuracy': 0.6492, 'sae_top_2_test_accuracy': 0.6674, 'sae_top_5_test_accuracy': 0.7058, 'sae_top_10_test_accuracy': 0.8048, 'sae_top_20_test_accuracy': 0.8785999999999999, 'sae_top_50_test_accuracy': 0.931, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9567500352859497, 'sae_top_1_test_accuracy': 0.8265, 'sae_top_2_test_accuracy': 0.8704999999999999, 'sae_top_5_test_accuracy': 0.8835, 'sae_top_10_test_accuracy': 0.91125, 'sae_top_20_test_accuracy': 0.92825, 'sae_top_50_test_accuracy': 0.93025, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9982000470161438, 'sae_top_1_test_accuracy': 0.9112, 'sae_top_2_test_accuracy': 0.9523999999999999, 'sae_top_5_test_accuracy': 0.961, 'sae_top_10_test_accuracy': 0.975, 'sae_top_20_test_accuracy': 0.9954000000000001, 'sae_top_50_test_accuracy': 0.9968, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'abf22d2a-8885-4d9d-a3cf-367a20df38a5', 'datetime_epoch_millis': 1738124069976, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from jumprelu SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'JumpReLUTrainer', 'activation_dim': 2304, 'dict_size': 65536, 'lr': 0.0003, 'sparsity_penalty': 1.0, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'JumpReLUTrainer', 'jump_coeff': 0.1, 'target_l0': 20.0, 'bandwidth': 0.001}, 'final_info': {'training_steps': 4882, 'final_loss': 26473.599609375, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '5aea69e4-4345-41a4-8b18-70b17142f013', 'datetime_epoch_millis': 1738125160695, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9957540760869565, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.042724609375}, 'model_performance_preservation': {'ce_loss_score': 0.9967105263157895, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.96875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.84375, 'mse': 0.98046875, 'cossim': 0.953125}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 143.0, 'l2_ratio': 0.953125, 'relative_reconstruction_bias': 1.0}, 'sparsity': {'l0': 319.994384765625, 'l1': 924.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '5b676c82-4ea1-4c24-a45b-81285a8d0e77', 'datetime_epoch_millis': 1738125503941, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.02163248562962354, 'scr_metric_threshold_2': 0.013775576175600678, 'scr_dir2_threshold_2': 0.011747162721208801, 'scr_dir1_threshold_5': 0.00797513200245279, 'scr_metric_threshold_5': 0.012243467722626217, 'scr_dir2_threshold_5': 0.01581488514352803, 'scr_dir1_threshold_10': 0.005620844078476961, 'scr_metric_threshold_10': 0.021725976871906364, 'scr_dir2_threshold_10': 0.0254149430440832, 'scr_dir1_threshold_20': -0.0042442461395118146, 'scr_metric_threshold_20': 0.02798841060137652, 'scr_dir2_threshold_20': 0.03114087110932238, 'scr_dir1_threshold_50': -0.037368683077093755, 'scr_metric_threshold_50': 0.03933976502243071, 'scr_dir2_threshold_50': 0.04725410359859606, 'scr_dir1_threshold_100': -0.041929115762227756, 'scr_metric_threshold_100': 0.04968711624808444, 'scr_dir2_threshold_100': 0.058850707586679374, 'scr_dir1_threshold_500': -0.061147481563771526, 'scr_metric_threshold_500': 0.1546941453598473, 'scr_dir2_threshold_500': 0.15686816891559102}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.10937460709798731, 'scr_metric_threshold_2': -0.0024569704325861324, 'scr_dir2_threshold_2': -0.0024569704325861324, 'scr_dir1_threshold_5': 0.10937460709798731, 'scr_metric_threshold_5': 0.0024571168813214595, 'scr_dir2_threshold_5': 0.0024571168813214595, 'scr_dir1_threshold_10': 0.10937460709798731, 'scr_metric_threshold_10': 0.007371057746493725, 'scr_dir2_threshold_10': 0.007371057746493725, 'scr_dir1_threshold_20': 0.12499941792294417, 'scr_metric_threshold_20': 0.017199085925573582, 'scr_dir2_threshold_20': 0.017199085925573582, 'scr_dir1_threshold_50': 0.1562499708961472, 'scr_metric_threshold_50': 0.02948408453723957, 'scr_dir2_threshold_50': 0.02948408453723957, 'scr_dir1_threshold_100': 0.17187478172110407, 'scr_metric_threshold_100': 0.039312112716319424, 'scr_dir2_threshold_100': 0.039312112716319424, 'scr_dir1_threshold_500': 0.21875014551926394, 'scr_metric_threshold_500': 0.10565119308855696, 'scr_dir2_threshold_500': 0.10565119308855696}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 0.009708614261252404, 'scr_metric_threshold_2': 0.005730585621650868, 'scr_dir2_threshold_2': 0.005730585621650868, 'scr_dir1_threshold_5': -0.05825284293942373, 'scr_metric_threshold_5': -0.005730756408609849, 'scr_dir2_threshold_5': -0.005730756408609849, 'scr_dir1_threshold_10': -0.06796145720067613, 'scr_metric_threshold_10': 0.011461342030260717, 'scr_dir2_threshold_10': 0.011461342030260717, 'scr_dir1_threshold_20': -0.06796145720067613, 'scr_metric_threshold_20': 0.017191927651911583, 'scr_dir2_threshold_20': 0.017191927651911583, 'scr_dir1_threshold_50': -0.23300963569983096, 'scr_metric_threshold_50': 0.02005722046273702, 'scr_dir2_threshold_50': 0.02005722046273702, 'scr_dir1_threshold_100': -0.3495147428927238, 'scr_metric_threshold_100': 0.06876788139460532, 'scr_dir2_threshold_100': 0.06876788139460532, 'scr_dir1_threshold_500': -0.7572817500389166, 'scr_metric_threshold_500': 0.19484230215355525, 'scr_dir2_threshold_500': 0.19484230215355525}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.0634921686148548, 'scr_metric_threshold_2': -0.0025316127159178037, 'scr_dir2_threshold_2': -0.0025316127159178037, 'scr_dir1_threshold_5': 0.015873751732555005, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0, 'scr_dir1_threshold_10': 0.0, 'scr_metric_threshold_10': -0.0025316127159178037, 'scr_dir2_threshold_10': -0.0025316127159178037, 'scr_dir1_threshold_20': -0.04761841688229979, 'scr_metric_threshold_20': -0.0025316127159178037, 'scr_dir2_threshold_20': -0.0025316127159178037, 'scr_dir1_threshold_50': -0.1111105854971546, 'scr_metric_threshold_50': 0.010126601761490606, 'scr_dir2_threshold_50': 0.010126601761490606, 'scr_dir1_threshold_100': -0.1111105854971546, 'scr_metric_threshold_100': 0.02025320352298121, 'scr_dir2_threshold_100': 0.02025320352298121, 'scr_dir1_threshold_500': -0.1111105854971546, 'scr_metric_threshold_500': 0.07594943776226984, 'scr_dir2_threshold_500': 0.07594943776226984}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.015624796273220196, 'scr_metric_threshold_2': 0.005934818343797456, 'scr_dir2_threshold_2': 0.005934818343797456, 'scr_dir1_threshold_5': -0.03125005820765137, 'scr_metric_threshold_5': 0.014836957425305832, 'scr_dir2_threshold_5': 0.014836957425305832, 'scr_dir1_threshold_10': -0.046874854480871565, 'scr_metric_threshold_10': 0.011869459819219295, 'scr_dir2_threshold_10': 0.011869459819219295, 'scr_dir1_threshold_20': -0.12499976716939451, 'scr_metric_threshold_20': 0.020771598900727672, 'scr_dir2_threshold_20': 0.020771598900727672, 'scr_dir1_threshold_50': -0.22656233992895872, 'scr_metric_threshold_50': 0.011869459819219295, 'scr_dir2_threshold_50': 0.011869459819219295, 'scr_dir1_threshold_100': -0.2890624563442615, 'scr_metric_threshold_100': -0.023738742770062975, 'scr_dir2_threshold_100': -0.023738742770062975, 'scr_dir1_threshold_500': -0.5937497089617432, 'scr_metric_threshold_500': 0.07715140039073762, 'scr_dir2_threshold_500': 0.07715140039073762}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': -0.027322374114521025, 'scr_metric_threshold_2': 0.03529412314690952, 'scr_dir2_threshold_2': 0.03529412314690952, 'scr_dir1_threshold_5': -0.04371592886659017, 'scr_metric_threshold_5': 0.03137260585139841, 'scr_dir2_threshold_5': 0.03137260585139841, 'scr_dir1_threshold_10': -0.03825151918536423, 'scr_metric_threshold_10': 0.06274521170279682, 'scr_dir2_threshold_10': 0.06274521170279682, 'scr_dir1_threshold_20': -0.03825151918536423, 'scr_metric_threshold_20': 0.06666672899830793, 'scr_dir2_threshold_20': 0.06666672899830793, 'scr_dir1_threshold_50': -0.07103830298111119, 'scr_metric_threshold_50': 0.05490194336811984, 'scr_dir2_threshold_50': 0.05490194336811984, 'scr_dir1_threshold_100': -0.00546440968122594, 'scr_metric_threshold_100': 0.06274521170279682, 'scr_dir2_threshold_100': 0.06274521170279682, 'scr_dir1_threshold_500': 0.13661187057260513, 'scr_metric_threshold_500': 0.23921582743734443, 'scr_dir2_threshold_500': 0.23921582743734443}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': -0.010256584250251485, 'scr_metric_threshold_2': 0.020161269001982816, 'scr_dir2_threshold_2': 0.020161269001982816, 'scr_dir1_threshold_5': 0.03589728071373967, 'scr_metric_threshold_5': 0.028225920807546715, 'scr_dir2_threshold_5': 0.028225920807546715, 'scr_dir1_threshold_10': 0.03076914142104203, 'scr_metric_threshold_10': 0.056451601273808806, 'scr_dir2_threshold_10': 0.056451601273808806, 'scr_dir1_threshold_20': 0.03076914142104203, 'scr_metric_threshold_20': 0.060483807005948444, 'scr_dir2_threshold_20': 0.060483807005948444, 'scr_dir1_threshold_50': 0.06153828284208406, 'scr_metric_threshold_50': 0.11290320254761761, 'scr_dir2_threshold_50': 0.11290320254761761, 'scr_dir1_threshold_100': 0.08717928497042846, 'scr_metric_threshold_100': 0.1370969176230247, 'scr_dir2_threshold_100': 0.1370969176230247, 'scr_dir1_threshold_500': 0.18974359914866223, 'scr_metric_threshold_500': 0.18951607282340924, 'scr_dir2_threshold_500': 0.18951607282340924}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.0179372568872101, 'scr_metric_threshold_2': 0.02232140362529227, 'scr_dir2_threshold_2': 0.02232140362529227, 'scr_dir1_threshold_5': 0.03587424648900561, 'scr_metric_threshold_5': 0.02678589722404717, 'scr_dir2_threshold_5': 0.02678589722404717, 'scr_dir1_threshold_10': 0.049327255975766836, 'scr_metric_threshold_10': 0.017857176118657926, 'scr_dir2_threshold_10': 0.017857176118657926, 'scr_dir1_threshold_20': 0.0762330076638747, 'scr_metric_threshold_20': 0.03125012473068151, 'scr_dir2_threshold_20': 0.03125012473068151, 'scr_dir1_threshold_50': 0.11210752143829489, 'scr_metric_threshold_50': 0.062499983369242465, 'scr_dir2_threshold_50': 0.062499983369242465, 'scr_dir1_threshold_100': 0.14349778781226621, 'scr_metric_threshold_100': 0.07589293198126605, 'scr_dir2_threshold_100': 0.07589293198126605, 'scr_dir1_threshold_500': 0.26905831873732233, 'scr_metric_threshold_500': 0.19642867121311663, 'scr_dir2_threshold_500': 0.19642867121311663}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.025750992813676425, 'scr_metric_threshold_2': 0.025750992813676425, 'scr_dir2_threshold_2': 0.009523685178541423, 'scr_dir1_threshold_5': 0.0, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0285713393672145, 'scr_dir1_threshold_10': 0.008583578999931441, 'scr_metric_threshold_10': 0.008583578999931441, 'scr_dir2_threshold_10': 0.03809530837734615, 'scr_dir1_threshold_20': 0.012875624313779262, 'scr_metric_threshold_20': 0.012875624313779262, 'scr_dir2_threshold_20': 0.03809530837734615, 'scr_dir1_threshold_50': 0.012875624313779262, 'scr_metric_threshold_50': 0.012875624313779262, 'scr_dir2_threshold_50': 0.07619033292310208, 'scr_dir1_threshold_100': 0.01716741381374498, 'scr_metric_threshold_100': 0.01716741381374498, 'scr_dir2_threshold_100': 0.09047614452250444, 'scr_dir1_threshold_500': 0.15879825800978847, 'scr_metric_threshold_500': 0.15879825800978847, 'scr_dir2_threshold_500': 0.17619044645573817}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '350f2df7-bc1a-4b57-a180-ccdeedf190e0', 'datetime_epoch_millis': 1738125574126, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9616312891244888, 'sae_top_1_test_accuracy': 0.69450625, 'sae_top_2_test_accuracy': 0.7569937499999999, 'sae_top_5_test_accuracy': 0.8077375, 'sae_top_10_test_accuracy': 0.8382624999999999, 'sae_top_20_test_accuracy': 0.8718437500000001, 'sae_top_50_test_accuracy': 0.9135812499999999, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9712000370025635, 'sae_top_1_test_accuracy': 0.7774000000000001, 'sae_top_2_test_accuracy': 0.8262, 'sae_top_5_test_accuracy': 0.8396000000000001, 'sae_top_10_test_accuracy': 0.8615999999999999, 'sae_top_20_test_accuracy': 0.8918000000000001, 'sae_top_50_test_accuracy': 0.9244, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9562000513076783, 'sae_top_1_test_accuracy': 0.7114, 'sae_top_2_test_accuracy': 0.7346000000000001, 'sae_top_5_test_accuracy': 0.775, 'sae_top_10_test_accuracy': 0.8155999999999999, 'sae_top_20_test_accuracy': 0.8508000000000001, 'sae_top_50_test_accuracy': 0.9154, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9362000346183776, 'sae_top_1_test_accuracy': 0.6599999999999999, 'sae_top_2_test_accuracy': 0.7213999999999999, 'sae_top_5_test_accuracy': 0.7972, 'sae_top_10_test_accuracy': 0.8362, 'sae_top_20_test_accuracy': 0.858, 'sae_top_50_test_accuracy': 0.8814, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9216000437736511, 'sae_top_1_test_accuracy': 0.6537999999999999, 'sae_top_2_test_accuracy': 0.686, 'sae_top_5_test_accuracy': 0.758, 'sae_top_10_test_accuracy': 0.7934000000000001, 'sae_top_20_test_accuracy': 0.8168, 'sae_top_50_test_accuracy': 0.8504000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.979500025510788, 'sae_top_1_test_accuracy': 0.689, 'sae_top_2_test_accuracy': 0.772, 'sae_top_5_test_accuracy': 0.83, 'sae_top_10_test_accuracy': 0.855, 'sae_top_20_test_accuracy': 0.891, 'sae_top_50_test_accuracy': 0.948, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9702000379562378, 'sae_top_1_test_accuracy': 0.5944, 'sae_top_2_test_accuracy': 0.6974, 'sae_top_5_test_accuracy': 0.7425999999999999, 'sae_top_10_test_accuracy': 0.7882, 'sae_top_20_test_accuracy': 0.8656, 'sae_top_50_test_accuracy': 0.9100000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9587500393390656, 'sae_top_1_test_accuracy': 0.76425, 'sae_top_2_test_accuracy': 0.80575, 'sae_top_5_test_accuracy': 0.8684999999999999, 'sae_top_10_test_accuracy': 0.8795, 'sae_top_20_test_accuracy': 0.90075, 'sae_top_50_test_accuracy': 0.91825, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9994000434875489, 'sae_top_1_test_accuracy': 0.7058, 'sae_top_2_test_accuracy': 0.8126, 'sae_top_5_test_accuracy': 0.851, 'sae_top_10_test_accuracy': 0.8766, 'sae_top_20_test_accuracy': 0.9, 'sae_top_50_test_accuracy': 0.9608000000000001, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'a04e01fa-e075-47c1-b969-a23d14e827cc', 'datetime_epoch_millis': 1738125586025, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'JumpReLU', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'jumprelu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None, 'jump_coeff': 0.1}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 65536, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 554.7803955078125, 'layer': 12, 'dict_size': 65536, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'db99d2ac-f339-444f-abc3-ab3dbff3ac4f', 'datetime_epoch_millis': 1738122093122, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.009173980180944308, 'mean_num_split_features': 1.0}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01179245283018868, 'num_absorption': 30, 'num_probe_true_positives': 2544, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.007462686567164179, 'num_absorption': 12, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2773, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.0018450184501845018, 'num_absorption': 3, 'num_probe_true_positives': 1626, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.018397626112759646, 'num_absorption': 31, 'num_probe_true_positives': 1685, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.00938566552901024, 'num_absorption': 11, 'num_probe_true_positives': 1172, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.00676818950930626, 'num_absorption': 8, 'num_probe_true_positives': 1182, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.016983016983016984, 'num_absorption': 17, 'num_probe_true_positives': 1001, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.025730994152046785, 'num_absorption': 44, 'num_probe_true_positives': 1710, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.01366742596810934, 'num_absorption': 6, 'num_probe_true_positives': 439, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.009538950715421303, 'num_absorption': 6, 'num_probe_true_positives': 629, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002512562814070352, 'num_absorption': 3, 'num_probe_true_positives': 1194, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1756, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.006150061500615006, 'num_absorption': 5, 'num_probe_true_positives': 813, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.03185328185328185, 'num_absorption': 33, 'num_probe_true_positives': 1036, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.005531914893617021, 'num_absorption': 13, 'num_probe_true_positives': 2350, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 168, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.012987012987012988, 'num_absorption': 22, 'num_probe_true_positives': 1694, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0007168458781362007, 'num_absorption': 2, 'num_probe_true_positives': 2790, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0036231884057971015, 'num_absorption': 6, 'num_probe_true_positives': 1656, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.002509410288582183, 'num_absorption': 2, 'num_probe_true_positives': 797, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.015402843601895734, 'num_absorption': 13, 'num_probe_true_positives': 844, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.031818181818181815, 'num_absorption': 21, 'num_probe_true_positives': 660, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 102, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 153, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0038461538461538464, 'num_absorption': 1, 'num_probe_true_positives': 260, 'num_split_features': 1}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995905716226708, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004119873046875}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.98828125, 'mse': 0.08154296875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.9921875}, 'sparsity': {'l0': 8311.291015625, 'l1': 10624.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': '2cd18ab3-69b8-49fb-b329-542d864ccd8b', 'datetime_epoch_millis': 1738122399973, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.027608449030321582, 'scr_metric_threshold_2': 0.005305287226446962, 'scr_dir2_threshold_2': 0.0022115513779289633, 'scr_dir1_threshold_5': 0.030822806181597877, 'scr_metric_threshold_5': 0.02093731432318256, 'scr_dir2_threshold_5': 0.015513756858591759, 'scr_dir1_threshold_10': 0.018658375052698073, 'scr_metric_threshold_10': 0.027692526555416326, 'scr_dir2_threshold_10': 0.035908344251160164, 'scr_dir1_threshold_20': -0.09747346364731523, 'scr_metric_threshold_20': 0.036469946578223826, 'scr_dir2_threshold_20': 0.04206977745351752, 'scr_dir1_threshold_50': -0.22401566666710962, 'scr_metric_threshold_50': 0.09246180292099579, 'scr_dir2_threshold_50': 0.09956122701857269, 'scr_dir1_threshold_100': -0.3645932090682159, 'scr_metric_threshold_100': 0.1386625503200265, 'scr_dir2_threshold_100': 0.13324664981303627, 'scr_dir1_threshold_500': -1.1858990549174069, 'scr_metric_threshold_500': 0.24651666349430923, 'scr_dir2_threshold_500': 0.22477133045516998}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.03124962164991371, 'scr_metric_threshold_2': -0.004913940865172265, 'scr_dir2_threshold_2': -0.004913940865172265, 'scr_dir1_threshold_5': 0.04687443247487057, 'scr_metric_threshold_5': 0.009828028179079856, 'scr_dir2_threshold_5': 0.009828028179079856, 'scr_dir1_threshold_10': 0.04687443247487057, 'scr_metric_threshold_10': 0.017199085925573582, 'scr_dir2_threshold_10': 0.017199085925573582, 'scr_dir1_threshold_20': 0.015624810824956856, 'scr_metric_threshold_20': 0.036855142283733294, 'scr_dir2_threshold_20': 0.036855142283733294, 'scr_dir1_threshold_50': 0.015624810824956856, 'scr_metric_threshold_50': 0.1670761861468869, 'scr_dir2_threshold_50': 0.1670761861468869, 'scr_dir1_threshold_100': 0.06249924329982742, 'scr_metric_threshold_100': 0.1793611847585529, 'scr_dir2_threshold_100': 0.1793611847585529, 'scr_dir1_threshold_500': -0.34375049476549746, 'scr_metric_threshold_500': 0.23341526651912445, 'scr_dir2_threshold_500': 0.23341526651912445}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': -0.08737868572318094, 'scr_metric_threshold_2': 0.034383855303823166, 'scr_dir2_threshold_2': 0.034383855303823166, 'scr_dir1_threshold_5': -0.2718446714307952, 'scr_metric_threshold_5': 0.03151856249299773, 'scr_dir2_threshold_5': 0.03151856249299773, 'scr_dir1_threshold_10': -0.24271882864703803, 'scr_metric_threshold_10': 0.06876788139460532, 'scr_dir2_threshold_10': 0.06876788139460532, 'scr_dir1_threshold_20': -0.7475731357776643, 'scr_metric_threshold_20': 0.04011461171243302, 'scr_dir2_threshold_20': 0.04011461171243302, 'scr_dir1_threshold_50': -1.3009710929005072, 'scr_metric_threshold_50': 0.12034383513729906, 'scr_dir2_threshold_50': 0.12034383513729906, 'scr_dir1_threshold_100': -2.242718828647038, 'scr_metric_threshold_100': 0.18911171653190437, 'scr_dir2_threshold_100': 0.18911171653190437, 'scr_dir1_threshold_500': -3.2330102143857857, 'scr_metric_threshold_500': 0.21489969340325124, 'scr_dir2_threshold_500': 0.21489969340325124}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.04761936298742153, 'scr_metric_threshold_2': 0.005063376329654997, 'scr_dir2_threshold_2': 0.005063376329654997, 'scr_dir1_threshold_5': 0.04761936298742153, 'scr_metric_threshold_5': -0.0025316127159178037, 'scr_dir2_threshold_5': -0.0025316127159178037, 'scr_dir1_threshold_10': 0.07936497424228806, 'scr_metric_threshold_10': 0.025316579852636207, 'scr_dir2_threshold_10': 0.025316579852636207, 'scr_dir1_threshold_20': 0.07936497424228806, 'scr_metric_threshold_20': 0.03544318161412681, 'scr_dir2_threshold_20': 0.03544318161412681, 'scr_dir1_threshold_50': -0.015872805627433265, 'scr_metric_threshold_50': 0.09873425400116885, 'scr_dir2_threshold_50': 0.09873425400116885, 'scr_dir1_threshold_100': -0.3333327025965855, 'scr_metric_threshold_100': 0.13924051014931188, 'scr_dir2_threshold_100': 0.13924051014931188, 'scr_dir1_threshold_500': -4.761903185062892, 'scr_metric_threshold_500': 0.3772152742997926, 'scr_dir2_threshold_500': 0.3772152742997926}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.007812398136610098, 'scr_metric_threshold_2': 0.002967497606086536, 'scr_dir2_threshold_2': 0.002967497606086536, 'scr_dir1_threshold_5': 0.007812863797821078, 'scr_metric_threshold_5': 0.03264105871994697, 'scr_dir2_threshold_5': 0.03264105871994697, 'scr_dir1_threshold_10': -0.08593731082513303, 'scr_metric_threshold_10': 0.026706240376149513, 'scr_dir2_threshold_10': 0.026706240376149513, 'scr_dir1_threshold_20': -0.22656233992895872, 'scr_metric_threshold_20': 0.035608379457657886, 'scr_dir2_threshold_20': 0.035608379457657886, 'scr_dir1_threshold_50': -0.7734371944098303, 'scr_metric_threshold_50': 0.09792299929146529, 'scr_dir2_threshold_50': 0.09792299929146529, 'scr_dir1_threshold_100': -1.1171873690327845, 'scr_metric_threshold_100': 0.16617226060069454, 'scr_dir2_threshold_100': 0.16617226060069454, 'scr_dir1_threshold_500': -2.257811932475399, 'scr_metric_threshold_500': 0.2818991843184253, 'scr_dir2_threshold_500': 0.2818991843184253}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.1092894964580841, 'scr_metric_threshold_2': -0.03529412314690952, 'scr_dir2_threshold_2': -0.03529412314690952, 'scr_dir1_threshold_5': 0.12568305121015325, 'scr_metric_threshold_5': 0.011764785630188087, 'scr_dir2_threshold_5': 0.011764785630188087, 'scr_dir1_threshold_10': 0.05464474822904205, 'scr_metric_threshold_10': 0.04705890877709761, 'scr_dir2_threshold_10': 0.04705890877709761, 'scr_dir1_threshold_20': -0.00546440968122594, 'scr_metric_threshold_20': 0.06666672899830793, 'scr_dir2_threshold_20': 0.06666672899830793, 'scr_dir1_threshold_50': 0.027322374114521025, 'scr_metric_threshold_50': 0.08235303192400713, 'scr_dir2_threshold_50': 0.08235303192400713, 'scr_dir1_threshold_100': 0.20765017355371632, 'scr_metric_threshold_100': 0.09019606651502937, 'scr_dir2_threshold_100': 0.09019606651502937, 'scr_dir1_threshold_500': 0.4863386500885438, 'scr_metric_threshold_500': -0.14509800988314922, 'scr_dir2_threshold_500': -0.14509800988314922}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.05128200425668879, 'scr_metric_threshold_2': 0.04435498407738989, 'scr_dir2_threshold_2': 0.04435498407738989, 'scr_dir1_threshold_5': 0.07692300638503319, 'scr_metric_threshold_5': 0.032258126539686356, 'scr_dir2_threshold_5': 0.032258126539686356, 'scr_dir1_threshold_10': 0.07692300638503319, 'scr_metric_threshold_10': -0.0040322057321396385, 'scr_dir2_threshold_10': -0.0040322057321396385, 'scr_dir1_threshold_20': -0.17435918127056935, 'scr_metric_threshold_20': 0.04032253800396563, 'scr_dir2_threshold_20': 0.04032253800396563, 'scr_dir1_threshold_50': 0.07179486709233554, 'scr_metric_threshold_50': 0.12500006008532116, 'scr_dir2_threshold_50': 0.12500006008532116, 'scr_dir1_threshold_100': 0.10256400851337757, 'scr_metric_threshold_100': 0.11290320254761761, 'scr_dir2_threshold_100': 0.11290320254761761, 'scr_dir1_threshold_500': -0.07179517275719176, 'scr_metric_threshold_500': 0.4516130505317551, 'scr_dir2_threshold_500': 0.4516130505317551}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.08520176975018703, 'scr_metric_threshold_2': 0.0044642275066343415, 'scr_dir2_threshold_2': 0.0044642275066343415, 'scr_dir1_threshold_5': 0.17488778690082293, 'scr_metric_threshold_5': 0.013392948612023585, 'scr_dir2_threshold_5': 0.013392948612023585, 'scr_dir1_threshold_10': 0.22869955756245325, 'scr_metric_threshold_10': 0.04910730084933944, 'scr_dir2_threshold_10': 0.04910730084933944, 'scr_dir1_threshold_20': 0.30493256522632795, 'scr_metric_threshold_20': 0.062499983369242465, 'scr_dir2_threshold_20': 0.062499983369242465, 'scr_dir1_threshold_50': 0.27354256613777117, 'scr_metric_threshold_50': 0.13839291535050852, 'scr_dir2_threshold_50': 0.13839291535050852, 'scr_dir1_threshold_100': 0.40807159186372505, 'scr_metric_threshold_100': 0.23660725095706683, 'scr_dir2_threshold_100': 0.23660725095706683, 'scr_dir1_threshold_500': 0.5874438934504115, 'scr_metric_threshold_500': 0.4508928321967208, 'scr_dir2_threshold_500': 0.4508928321967208}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': -0.008583578999931441, 'scr_metric_threshold_2': -0.008583578999931441, 'scr_dir2_threshold_2': -0.03333346578807544, 'scr_dir1_threshold_5': 0.03862661712745569, 'scr_metric_threshold_5': 0.03862661712745569, 'scr_dir2_threshold_5': -0.004761842589270712, 'scr_dir1_threshold_10': -0.008583578999931441, 'scr_metric_threshold_10': -0.008583578999931441, 'scr_dir2_threshold_10': 0.05714296256601923, 'scr_dir1_threshold_20': -0.025750992813676425, 'scr_metric_threshold_20': -0.025750992813676425, 'scr_dir2_threshold_20': 0.019047654188673074, 'scr_dir1_threshold_50': -0.09012885856869063, 'scr_metric_threshold_50': -0.09012885856869063, 'scr_dir2_threshold_50': -0.03333346578807544, 'scr_dir1_threshold_100': -0.004291789499965721, 'scr_metric_threshold_100': -0.004291789499965721, 'scr_dir2_threshold_100': -0.04761899355588758, 'scr_dir1_threshold_500': 0.10729601656855352, 'scr_metric_threshold_500': 0.10729601656855352, 'scr_dir2_threshold_500': -0.06666664774456064}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'e6ee1070-9fb3-4e3f-b0fd-b799643b4aed', 'datetime_epoch_millis': 1738122464283, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9525750000000001, 'llm_top_1_test_accuracy': 0.6575, 'llm_top_2_test_accuracy': 0.72118125, 'llm_top_5_test_accuracy': 0.7797062499999999, 'llm_top_10_test_accuracy': 0.82980625, 'llm_top_20_test_accuracy': 0.87794375, 'llm_top_50_test_accuracy': 0.9228500000000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.95824378952384, 'sae_top_1_test_accuracy': 0.697525, 'sae_top_2_test_accuracy': 0.73530625, 'sae_top_5_test_accuracy': 0.78453125, 'sae_top_10_test_accuracy': 0.824875, 'sae_top_20_test_accuracy': 0.8698687500000001, 'sae_top_50_test_accuracy': 0.9177312499999999, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9596, 'llm_top_1_test_accuracy': 0.6432, 'llm_top_2_test_accuracy': 0.6916, 'llm_top_5_test_accuracy': 0.7912, 'llm_top_10_test_accuracy': 0.8336, 'llm_top_20_test_accuracy': 0.8962, 'llm_top_50_test_accuracy': 0.9384, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9658000469207764, 'sae_top_1_test_accuracy': 0.7064, 'sae_top_2_test_accuracy': 0.7888, 'sae_top_5_test_accuracy': 0.8160000000000001, 'sae_top_10_test_accuracy': 0.841, 'sae_top_20_test_accuracy': 0.8868, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9505999999999999, 'llm_top_1_test_accuracy': 0.6744, 'llm_top_2_test_accuracy': 0.7326, 'llm_top_5_test_accuracy': 0.7569999999999999, 'llm_top_10_test_accuracy': 0.7971999999999999, 'llm_top_20_test_accuracy': 0.8642, 'llm_top_50_test_accuracy': 0.9057999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9510000467300415, 'sae_top_1_test_accuracy': 0.6986, 'sae_top_2_test_accuracy': 0.7236000000000001, 'sae_top_5_test_accuracy': 0.7632, 'sae_top_10_test_accuracy': 0.8211999999999999, 'sae_top_20_test_accuracy': 0.8642, 'sae_top_50_test_accuracy': 0.8995999999999998, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9164, 'llm_top_1_test_accuracy': 0.6914, 'llm_top_2_test_accuracy': 0.7398, 'llm_top_5_test_accuracy': 0.768, 'llm_top_10_test_accuracy': 0.7958000000000001, 'llm_top_20_test_accuracy': 0.8513999999999999, 'llm_top_50_test_accuracy': 0.8880000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.93340003490448, 'sae_top_1_test_accuracy': 0.6783999999999999, 'sae_top_2_test_accuracy': 0.6918, 'sae_top_5_test_accuracy': 0.7516, 'sae_top_10_test_accuracy': 0.8006, 'sae_top_20_test_accuracy': 0.8476000000000001, 'sae_top_50_test_accuracy': 0.8834, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8996000000000001, 'llm_top_1_test_accuracy': 0.5984, 'llm_top_2_test_accuracy': 0.6516, 'llm_top_5_test_accuracy': 0.6696000000000001, 'llm_top_10_test_accuracy': 0.7472, 'llm_top_20_test_accuracy': 0.8126, 'llm_top_50_test_accuracy': 0.8638000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9178000450134277, 'sae_top_1_test_accuracy': 0.6314, 'sae_top_2_test_accuracy': 0.666, 'sae_top_5_test_accuracy': 0.7090000000000001, 'sae_top_10_test_accuracy': 0.7612, 'sae_top_20_test_accuracy': 0.7952000000000001, 'sae_top_50_test_accuracy': 0.8513999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.671, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.847, 'llm_top_50_test_accuracy': 0.9325000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9800000488758087, 'sae_top_1_test_accuracy': 0.887, 'sae_top_2_test_accuracy': 0.892, 'sae_top_5_test_accuracy': 0.903, 'sae_top_10_test_accuracy': 0.898, 'sae_top_20_test_accuracy': 0.91, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9686, 'llm_top_1_test_accuracy': 0.6542, 'llm_top_2_test_accuracy': 0.681, 'llm_top_5_test_accuracy': 0.7527999999999999, 'llm_top_10_test_accuracy': 0.807, 'llm_top_20_test_accuracy': 0.8692, 'llm_top_50_test_accuracy': 0.9294, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9674000382423401, 'sae_top_1_test_accuracy': 0.6392, 'sae_top_2_test_accuracy': 0.6900000000000001, 'sae_top_5_test_accuracy': 0.7327999999999999, 'sae_top_10_test_accuracy': 0.7622, 'sae_top_20_test_accuracy': 0.8298, 'sae_top_50_test_accuracy': 0.9067999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9445, 'llm_top_1_test_accuracy': 0.681, 'llm_top_2_test_accuracy': 0.7752500000000001, 'llm_top_5_test_accuracy': 0.82325, 'llm_top_10_test_accuracy': 0.8672500000000001, 'llm_top_20_test_accuracy': 0.8947499999999999, 'llm_top_50_test_accuracy': 0.9275, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9517500400543213, 'sae_top_1_test_accuracy': 0.714, 'sae_top_2_test_accuracy': 0.76525, 'sae_top_5_test_accuracy': 0.8572500000000001, 'sae_top_10_test_accuracy': 0.886, 'sae_top_20_test_accuracy': 0.9087500000000001, 'sae_top_50_test_accuracy': 0.92625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.6464, 'llm_top_2_test_accuracy': 0.7735999999999998, 'llm_top_5_test_accuracy': 0.9098, 'llm_top_10_test_accuracy': 0.9643999999999998, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9974000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9988000154495239, 'sae_top_1_test_accuracy': 0.6252, 'sae_top_2_test_accuracy': 0.665, 'sae_top_5_test_accuracy': 0.7434000000000001, 'sae_top_10_test_accuracy': 0.8288, 'sae_top_20_test_accuracy': 0.9166000000000001, 'sae_top_50_test_accuracy': 0.9894000000000001, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'bd06b69e-471f-4cf2-a376-eba3a6c0c86f', 'datetime_epoch_millis': 1738122476463, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': '4153f454e45d23297aadf56f83c2f432798680d1', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 65536, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the sparsity-guided orthogonality constraints. Let me plan out the experiments:                                                                                                                  

Experiment Plan:                                                                                                                                                                                                     

 1 Run 1: Implement basic sparsity-weighted orthogonality loss (λ_2=0.01)                                                                                                                                            
 2 Run 2: Stronger orthogonality weight (λ_2=0.1)                                                                                                                                                                    
 3 Run 3: Very strong orthogonality (λ_2=1.0)                                                                                                                                                                        
 4 Run 4: Balanced configuration (λ_2=0.05) with higher competition threshold                                                                                                                                        

Let's start with Run 1 by implementing the core functionality:                                                                                                                                                       

 1 First, let's modify experiment.py to add the sparsity-weighted orthogonality loss:                                                                                                                                

experiment.py                                                                                                                                                                                                        

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                                                                   
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                                                                       
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                                                               
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                                                                       
                                                                                                                                                                                                                     
         # Scatter topk values to form encoded activations                                                                                                                                                           
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                                                                          
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                                                                        
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                                                                          
         )                                                                                                                                                                                                           
                                                                                                                                                                                                                     
         if return_topk:                                                                                                                                                                                             
             return encoded_acts_BF, tops_acts_BK, top_indices_BK                                                                                                                                                    
         else:                                                                                                                                                                                                       
             return encoded_acts_BF                                                                                                                                                                                  
 =======                                                                                                                                                                                                             
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                                                                   
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                                                                       
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                                                               
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                                                                       
                                                                                                                                                                                                                     
         # Scatter topk values to form encoded activations                                                                                                                                                           
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                                                                          
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                                                                        
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                                                                          
         )                                                                                                                                                                                                           
                                                                                                                                                                                                                     
         # Create binary mask from indices for computing competition coefficients                                                                                                                                    
         batch_size = x.shape[0]                                                                                                                                                                                     
         binary_mask = torch.zeros((batch_size, self.dict_size), device=x.device)                                                                                                                                    
         binary_mask.scatter_(dim=-1, index=top_indices_BK, value=1.0)                                                                                                                                               
                                                                                                                                                                                                                     
         if return_topk:                                                                                                                                                                                             
             return encoded_acts_BF, tops_acts_BK, top_indices_BK, binary_mask                                                                                                                                       
         else:                                                                                                                                                                                                       
             return encoded_acts_BF                                                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
     def loss(self, x, step=None, logging=False):                                                                                                                                                                    
         # Run the SAE                                                                                                                                                                                               
         f, top_acts, top_indices = self.ae.encode(x, return_topk=True)                                                                                                                                              
         x_hat = self.ae.decode(f)                                                                                                                                                                                   
                                                                                                                                                                                                                     
         # Measure goodness of reconstruction                                                                                                                                                                        
         e = x_hat - x                                                                                                                                                                                               
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                                                              
 =======                                                                                                                                                                                                             
     def loss(self, x, step=None, logging=False):                                                                                                                                                                    
         # Run the SAE                                                                                                                                                                                               
         f, top_acts, top_indices, binary_mask = self.ae.encode(x, return_topk=True)                                                                                                                                 
         x_hat = self.ae.decode(f)                                                                                                                                                                                   
                                                                                                                                                                                                                     
         # Measure goodness of reconstruction                                                                                                                                                                        
         e = x_hat - x                                                                                                                                                                                               
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                                                              
                                                                                                                                                                                                                     
         # Compute competition coefficients from binary masks                                                                                                                                                        
         # Normalize by k to get values between 0 and 1                                                                                                                                                              
         competition = torch.mm(binary_mask.T, binary_mask) / self.k                                                                                                                                                 
                                                                                                                                                                                                                     
         # Compute orthogonality loss weighted by competition                                                                                                                                                        
         features = self.ae.W_dec.T  # [d_sae, d_in]                                                                                                                                                                 
         feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
         orthogonality_loss = (competition * normalized_dots.abs()).mean()                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                                                                       
         auxk_loss = auxk_loss.sum(dim=-1).mean()                                                                                                                                                                    
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                                                                                
 =======                                                                                                                                                                                                             
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                                                                       
         auxk_loss = auxk_loss.sum(dim=-1).mean() if 'auxk_loss' in locals() else 0.0                                                                                                                                
                                                                                                                                                                                                                     
         # Add orthogonality loss with weight 0.01 for first run                                                                                                                                                     
         loss = l2_loss + self.auxk_alpha * auxk_loss + 0.01 * orthogonality_loss                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

This implements:                                                                                                                                                                                                     

 1 Binary mask tracking for feature competition                                                                                                                                                                      
 2 Competition coefficient computation                                                                                                                                                                               
 3 Sparsity-weighted orthogonality loss                                                                                                                                                                              
 4 Initial λ_2=0.01 weight                                                                                                                                                                                           

Let's run the first experiment with this configuration:                                                                                                                                                              

                                                                                                                                                                                                                     
 python experiment.py --out_dir=run_1                                                                                                                                                                                
                                                                                                                                                                                                                     

This will give us a baseline for how the sparsity-guided orthogonality affects feature separation. Based on those results, we can then try the stronger weights in subsequent runs.                                  

Would you like me to proceed with Run 1 using these changes?                                                                                                                                                         

Tokens: 88k sent, 1.5k received. Cost: $0.29 message, $0.29 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
coder_out: I'll help implement the sparsity-guided orthogonality constraints. Let me plan out the experiments:

Experiment Plan:
1. Run 1: Implement basic sparsity-weighted orthogonality loss (λ_2=0.01)
2. Run 2: Stronger orthogonality weight (λ_2=0.1) 
3. Run 3: Very strong orthogonality (λ_2=1.0)
4. Run 4: Balanced configuration (λ_2=0.05) with higher competition threshold

Let's start with Run 1 by implementing the core functionality:

1. First, let's modify experiment.py to add the sparsity-weighted orthogonality loss:

experiment.py
```python
<<<<<<< SEARCH
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF
=======
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        # Create binary mask from indices for computing competition coefficients
        batch_size = x.shape[0]
        binary_mask = torch.zeros((batch_size, self.dict_size), device=x.device)
        binary_mask.scatter_(dim=-1, index=top_indices_BK, value=1.0)

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK, binary_mask
        else:
            return encoded_acts_BF
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)
=======
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices, binary_mask = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)

        # Compute competition coefficients from binary masks
        # Normalize by k to get values between 0 and 1
        competition = torch.mm(binary_mask.T, binary_mask) / self.k
        
        # Compute orthogonality loss weighted by competition
        features = self.ae.W_dec.T  # [d_sae, d_in]
        feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        orthogonality_loss = (competition * normalized_dots.abs()).mean()
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss
=======
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean() if 'auxk_loss' in locals() else 0.0
        
        # Add orthogonality loss with weight 0.01 for first run
        loss = l2_loss + self.auxk_alpha * auxk_loss + 0.01 * orthogonality_loss
>>>>>>> REPLACE
```

This implements:
1. Binary mask tracking for feature competition
2. Competition coefficient computation
3. Sparsity-weighted orthogonality loss
4. Initial λ_2=0.01 weight

Let's run the first experiment with this configuration:

```bash
python experiment.py --out_dir=run_1
```

This will give us a baseline for how the sparsity-guided orthogonality affects feature separation. Based on those results, we can then try the stronger weights in subsequent runs.

Would you like me to proceed with Run 1 using these changes?, type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.97s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.31s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  4.95s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.73s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 835, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 553, in run_sae_training
    loss_dict = trainer.update(step, activations)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 356, in update
    loss = self.loss(x, step=step)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 283, in loss
    orthogonality_loss = (competition * normalized_dots.abs()).mean()
                          ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (18432) must match the size of tensor b (2304) at non-singleton dimension 1

Currently on iteration 1 of run 1
The error occurs because the competition matrix and normalized dots matrix have mismatched dimensions. Let me fix this by transposing the features matrix correctly.                                                 

experiment.py                                                                                                                                                                                                        

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
         # Compute orthogonality loss weighted by competition                                                                                                                                                        
         features = self.ae.W_dec.T  # [d_sae, d_in]                                                                                                                                                                 
         feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
         orthogonality_loss = (competition * normalized_dots.abs()).mean()                                                                                                                                           
 =======                                                                                                                                                                                                             
         # Compute orthogonality loss weighted by competition                                                                                                                                                        
         features = self.ae.W_dec  # [d_in, d_sae]                                                                                                                                                                   
         feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
         orthogonality_loss = (competition * normalized_dots.abs()).mean()                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

This change fixes the dimension mismatch by:                                                                                                                                                                         

 1 Using W_dec directly instead of transposing it, since it's already in [d_in, d_sae] shape                                                                                                                         
 2 Computing feature dot products correctly with features.T @ features to get [d_sae, d_sae] matrix                                                                                                                  

Now the competition matrix [d_sae, d_sae] and normalized_dots [d_sae, d_sae] will have matching dimensions.                                                                                                          

Try running the experiment again with:                                                                                                                                                                               

                                                                                                                                                                                                                     
 python experiment.py --out_dir=run_1                                                                                                                                                                                
                                                                                                                                                                                                                     

Tokens: 51k sent, 468 received. Cost: $0.16 message, $0.45 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
coder_out: The error occurs because the competition matrix and normalized dots matrix have mismatched dimensions. Let me fix this by transposing the features matrix correctly.

experiment.py
```python
<<<<<<< SEARCH
        # Compute orthogonality loss weighted by competition
        features = self.ae.W_dec.T  # [d_sae, d_in]
        feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        orthogonality_loss = (competition * normalized_dots.abs()).mean()
=======
        # Compute orthogonality loss weighted by competition
        features = self.ae.W_dec  # [d_in, d_sae]
        feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        orthogonality_loss = (competition * normalized_dots.abs()).mean()
>>>>>>> REPLACE
```

This change fixes the dimension mismatch by:
1. Using W_dec directly instead of transposing it, since it's already in [d_in, d_sae] shape
2. Computing feature dot products correctly with features.T @ features to get [d_sae, d_sae] matrix

Now the competition matrix [d_sae, d_sae] and normalized_dots [d_sae, d_sae] will have matching dimensions.

Try running the experiment again with:

```bash
python experiment.py --out_dir=run_1
```, type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.24s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.09s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.32s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 835, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 553, in run_sae_training
    loss_dict = trainer.update(step, activations)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 356, in update
    loss = self.loss(x, step=step)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 283, in loss
    orthogonality_loss = (competition * normalized_dots.abs()).mean()
                          ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (18432) must match the size of tensor b (2304) at non-singleton dimension 1

Currently on iteration 2 of run 1
I'll fix the dimension mismatch in the competition calculation. The issue is that we need to handle the batch dimension correctly when computing the competition coefficients.                                       

experiment.py                                                                                                                                                                                                        

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
         # Compute competition coefficients from binary masks                                                                                                                                                        
         # Normalize by k to get values between 0 and 1                                                                                                                                                              
         competition = torch.mm(binary_mask.T, binary_mask) / self.k                                                                                                                                                 
                                                                                                                                                                                                                     
         # Compute orthogonality loss weighted by competition                                                                                                                                                        
         features = self.ae.W_dec  # [d_in, d_sae]                                                                                                                                                                   
         feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
         orthogonality_loss = (competition * normalized_dots.abs()).mean()                                                                                                                                           
 =======                                                                                                                                                                                                             
         # Compute competition coefficients from binary masks                                                                                                                                                        
         # Sum over batch dimension and normalize by total activations                                                                                                                                               
         competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.sum()                                                                                                                                      
                                                                                                                                                                                                                     
         # Compute feature correlations                                                                                                                                                                              
         features = self.ae.W_dec  # [d_in, d_sae]                                                                                                                                                                   
         feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
                                                                                                                                                                                                                     
         # Mask out diagonal elements since we don't want to penalize self-correlation                                                                                                                               
         mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)                                                                                                                               
         orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

The changes:                                                                                                                                                                                                         

 1 Fixed competition coefficient calculation to properly normalize by total activations                                                                                                                              
 2 Added masking of diagonal elements in the correlation matrix                                                                                                                                                      
 3 Ensured all tensors have matching dimensions                                                                                                                                                                      

Try running again with:                                                                                                                                                                                              

                                                                                                                                                                                                                     
 python experiment.py --out_dir=run_1                                                                                                                                                                                
                                                                                                                                                                                                                     

Tokens: 52k sent, 554 received. Cost: $0.16 message, $0.61 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
coder_out: I'll fix the dimension mismatch in the competition calculation. The issue is that we need to handle the batch dimension correctly when computing the competition coefficients.

experiment.py
```python
<<<<<<< SEARCH
        # Compute competition coefficients from binary masks
        # Normalize by k to get values between 0 and 1
        competition = torch.mm(binary_mask.T, binary_mask) / self.k
        
        # Compute orthogonality loss weighted by competition
        features = self.ae.W_dec  # [d_in, d_sae]
        feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        orthogonality_loss = (competition * normalized_dots.abs()).mean()
=======
        # Compute competition coefficients from binary masks
        # Sum over batch dimension and normalize by total activations
        competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.sum()
        
        # Compute feature correlations
        features = self.ae.W_dec  # [d_in, d_sae] 
        feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        
        # Mask out diagonal elements since we don't want to penalize self-correlation
        mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)
        orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()
>>>>>>> REPLACE
```

The changes:
1. Fixed competition coefficient calculation to properly normalize by total activations
2. Added masking of diagonal elements in the correlation matrix
3. Ensured all tensors have matching dimensions

Try running again with:

```bash
python experiment.py --out_dir=run_1
```, type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.23s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.31s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 838, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 556, in run_sae_training
    loss_dict = trainer.update(step, activations)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 359, in update
    loss = self.loss(x, step=step)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 286, in loss
    orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()
                          ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (18432) must match the size of tensor b (2304) at non-singleton dimension 1

Currently on iteration 3 of run 1
I'll fix the dimension mismatch in the competition calculation. The issue is that we need to handle the batch dimension correctly and ensure the competition coefficients match the feature dimensions.              

experiment.py                                                                                                                                                                                                        

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
         # Compute competition coefficients from binary masks                                                                                                                                                        
         # Sum over batch dimension and normalize by total activations                                                                                                                                               
         competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.sum()                                                                                                                                      
                                                                                                                                                                                                                     
         # Compute feature correlations                                                                                                                                                                              
         features = self.ae.W_dec  # [d_in, d_sae]                                                                                                                                                                   
         feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
                                                                                                                                                                                                                     
         # Mask out diagonal elements since we don't want to penalize self-correlation                                                                                                                               
         mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)                                                                                                                               
         orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()                                                                                                                                    
 =======                                                                                                                                                                                                             
         # Compute competition coefficients from binary masks                                                                                                                                                        
         # Sum over batch dimension and normalize by total activations                                                                                                                                               
         competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.shape[0]  # [d_sae, d_sae]                                                                                                                 
                                                                                                                                                                                                                     
         # Compute feature correlations                                                                                                                                                                              
         features = self.ae.W_dec  # [d_sae, d_in]                                                                                                                                                                   
         feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]                                                                                                                                                
         feat_norms = torch.sqrt(torch.diag(feat_dots))                                                                                                                                                              
         normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))                                                                                                                           
                                                                                                                                                                                                                     
         # Mask out diagonal elements since we don't want to penalize self-correlation                                                                                                                               
         mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)                                                                                                                               
                                                                                                                                                                                                                     
         # Weight orthogonality loss by competition                                                                                                                                                                  
         orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

The key changes:                                                                                                                                                                                                     

 1 Fixed competition coefficient normalization to use batch size                                                                                                                                                     
 2 Corrected feature matrix multiplication order for computing correlations                                                                                                                                          
 3 Ensured all tensors have matching dimensions [d_sae, d_sae]                                                                                                                                                       

Try running again with:                                                                                                                                                                                              

                                                                                                                                                                                                                     
 python experiment.py --out_dir=run_1                                                                                                                                                                                
                                                                                                                                                                                                                     

Tokens: 52k sent, 644 received. Cost: $0.16 message, $0.77 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
Step 0: 31233.34765625
Step 100: 4595.013671875
Step 200: 4046.97021484375
Step 300: 3797.37890625
Step 400: 3578.882568359375
Step 500: 3430.39111328125
Step 600: 3406.927978515625
Step 700: 3231.0283203125
Step 800: 3194.314697265625
Step 900: 3098.0595703125
Step 1000: 3218.53271484375
Step 1100: 3109.3359375
Step 1200: 3015.543212890625
Step 1300: 3045.6787109375
Step 1400: 3051.0703125
Step 1500: 3055.136962890625
Step 1600: 3001.38818359375
Step 1700: 3003.02880859375
Step 1800: 2963.102294921875
Step 1900: 2930.08056640625
Step 2000: 2938.6123046875
Step 2100: 3003.52587890625
Step 2200: 2898.22998046875
Step 2300: 2937.3544921875
Step 2400: 2901.23046875
Step 2500: 2887.71826171875
Step 2600: 2964.5322265625
Step 2700: 2926.8974609375
Step 2800: 2903.216796875
Step 2900: 2833.737548828125
Step 3000: 2784.8212890625
Step 3100: 2802.4765625
Step 3200: 2791.63818359375
Step 3300: 2800.76806640625
Step 3400: 2735.5390625
Step 3500: 2684.90576171875
Step 3600: 2728.328369140625
Step 3700: 2734.666259765625
Step 3800: 2696.2646484375
Step 3900: 2644.56103515625
Step 4000: 2664.1083984375
Step 4100: 2630.8037109375
Step 4200: 2603.201171875
Step 4300: 2623.76220703125
Step 4400: 2539.510986328125
Step 4500: 2529.447998046875
Step 4600: 2504.509765625
Step 4700: 2480.73486328125
Step 4800: 2496.23046875

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
coder_out: I'll fix the dimension mismatch in the competition calculation. The issue is that we need to handle the batch dimension correctly and ensure the competition coefficients match the feature dimensions.

experiment.py
```python
<<<<<<< SEARCH
        # Compute competition coefficients from binary masks
        # Sum over batch dimension and normalize by total activations
        competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.sum()
        
        # Compute feature correlations
        features = self.ae.W_dec  # [d_in, d_sae] 
        feat_dots = torch.mm(features.T, features)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        
        # Mask out diagonal elements since we don't want to penalize self-correlation
        mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)
        orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()
=======
        # Compute competition coefficients from binary masks
        # Sum over batch dimension and normalize by total activations
        competition = torch.mm(binary_mask.T, binary_mask) / binary_mask.shape[0]  # [d_sae, d_sae]
        
        # Compute feature correlations 
        features = self.ae.W_dec  # [d_sae, d_in]
        feat_dots = torch.mm(features, features.T)  # [d_sae, d_sae]
        feat_norms = torch.sqrt(torch.diag(feat_dots))
        normalized_dots = feat_dots / (feat_norms.unsqueeze(1) * feat_norms.unsqueeze(0))
        
        # Mask out diagonal elements since we don't want to penalize self-correlation
        mask = 1 - torch.eye(normalized_dots.shape[0], device=normalized_dots.device)
        
        # Weight orthogonality loss by competition
        orthogonality_loss = (competition * normalized_dots.abs() * mask).mean()
>>>>>>> REPLACE
```

The key changes:
1. Fixed competition coefficient normalization to use batch size
2. Corrected feature matrix multiplication order for computing correlations
3. Ensured all tensors have matching dimensions [d_sae, d_sae]

Try running again with:

```bash
python experiment.py --out_dir=run_1
```, type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.13s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.19s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.33s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.36s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.48s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.16s/it]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]
Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]
Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A

Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.06s/it][A

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.06s/it][A

Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.28s/it][A
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sae_lens/training/activations_store.py:285: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.
  warnings.warn(


Reconstruction Batches:   0%|          | 0/200 [00:00<?, ?it/s][A
Reconstruction Batches:   0%|          | 0/200 [00:00<?, ?it/s]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:16<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 898, in <module>
    evaluate_trained_sae(
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 801, in evaluate_trained_sae
    eval_runners[eval_type]()
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 717, in <lambda>
    lambda: core.multiple_evals(
            ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 955, in multiple_evals
    scalar_metrics, feature_metrics = run_evals(
                                      ^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 154, in run_evals
    reconstruction_metrics = get_downstream_reconstruction_metrics(
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 333, in get_downstream_reconstruction_metrics
    for metric_name, metric_value in get_recons_loss(
                                     ^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 671, in get_recons_loss
    recons_logits, recons_ce_loss = model.run_with_hooks(
                                    ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/hook_points.py", line 455, in run_with_hooks
    return hooked_model.forward(*model_args, **model_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py", line 575, in forward
    residual = block(
               ^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py", line 183, in forward
    resid_post = self.hook_resid_post(resid_mid + mlp_out)  # [batch, pos, d_model]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1803, in inner
    hook_result = hook(self, args, result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/hook_points.py", line 109, in full_hook
    return hook(module_output, hook=self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 581, in standard_replacement_hook
    reconstructed_activations = sae.decode(sae.encode(activations)).to(activations.dtype)
                                           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/experiment.py", line 118, in encode
    binary_mask.scatter_(dim=-1, index=top_indices_BK, value=1.0)
RuntimeError: Index tensor must have the same number of dimensions as self tensor

Currently on iteration 4 of run 1
The error occurs because the binary mask tensor has a different number of dimensions than the indices tensor. Let's fix the encode method in experiment.py to ensure proper tensor dimensions:                       

experiment.py                                                                                                                                                                                                        

                                                                                                                                                                                                                     
 <<<<<<< SEARCH                                                                                                                                                                                                      
         # Create binary mask from indices for computing competition coefficients                                                                                                                                    
         batch_size = x.shape[0]                                                                                                                                                                                     
         binary_mask = torch.zeros((batch_size, self.dict_size), device=x.device)                                                                                                                                    
         binary_mask.scatter_(dim=-1, index=top_indices_BK, value=1.0)                                                                                                                                               
 =======                                                                                                                                                                                                             
         # Create binary mask from indices for computing competition coefficients                                                                                                                                    
         batch_size = x.shape[0]                                                                                                                                                                                     
         binary_mask = torch.zeros((batch_size, self.dict_size), device=x.device)                                                                                                                                    
         binary_mask.scatter_(dim=-1, index=top_indices_BK.reshape(batch_size, -1), value=1.0)                                                                                                                       
 >>>>>>> REPLACE                                                                                                                                                                                                     
                                                                                                                                                                                                                     

This change reshapes the top_indices_BK tensor to match the dimensions of binary_mask before performing the scatter operation. The reshape ensures both tensors have the same number of dimensions (2D in this case -
batch size × dictionary size).                                                                                                                                                                                       

Let me know if you need any clarification or run into other issues!                                                                                                                                                  

Tokens: 51k sent, 294 received. Cost: $0.16 message, $0.93 session.
Applied edit to experiment.py
Step 0: 31233.34765625
Step 100: 4595.013671875
Step 200: 4046.97021484375
Step 300: 3797.37890625
Step 400: 3578.882568359375
Step 500: 3430.39111328125
Step 600: 3406.927978515625
Step 700: 3231.0283203125
Step 800: 3194.314697265625
Step 900: 3098.0595703125
Step 1000: 3218.53271484375
Step 1100: 3109.3359375
Step 1200: 3015.543212890625
Step 1300: 3045.6787109375
Step 1400: 3051.0703125
Step 1500: 3055.136962890625
Step 1600: 3001.38818359375
Step 1700: 3003.02880859375
Step 1800: 2963.102294921875
Step 1900: 2930.08056640625
Step 2000: 2938.6123046875
Step 2100: 3003.52587890625
Step 2200: 2898.22998046875
Step 2300: 2937.3544921875
Step 2400: 2901.23046875
Step 2500: 2887.71826171875
Step 2600: 2964.5322265625
Step 2700: 2926.8974609375
Step 2800: 2903.216796875
Step 2900: 2833.737548828125
Step 3000: 2784.8212890625
Step 3100: 2802.4765625
Step 3200: 2791.63818359375
Step 3300: 2800.76806640625
Step 3400: 2735.5390625
Step 3500: 2684.90576171875
Step 3600: 2728.328369140625
Step 3700: 2734.666259765625
Step 3800: 2696.2646484375
Step 3900: 2644.56103515625
Step 4000: 2664.1083984375
Step 4100: 2630.8037109375
Step 4200: 2603.201171875
Step 4300: 2623.76220703125
Step 4400: 2539.510986328125
Step 4500: 2529.447998046875
Step 4600: 2504.509765625
Step 4700: 2480.73486328125
Step 4800: 2496.23046875

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250130_014704_sparse_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9750000238418579
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9720000624656677
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9950000643730164
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9350000619888306, scr_score: 0.37500011641541114
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.940000057220459, scr_score: 0.45312510186348476
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9450000524520874, scr_score: 0.5312500873115584
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9380000233650208, scr_score: 0.42187454889028175
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9290000200271606, scr_score: 0.2812493888190914
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9310000538825989, scr_score: 0.3124999417922944
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9330000281333923, scr_score: 0.34374956344220814
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5900000333786011, scr_score: 0.017199085925573582
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5940000414848328, scr_score: 0.027027114104653437
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.597000002861023, scr_score: 0.03439802540241183
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6100000143051147, scr_score: 0.06633908037223754
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.625, scr_score: 0.1031940762072355
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6300000548362732, scr_score: 0.11547922126763682
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5860000252723694, scr_score: 0.007371057746493725
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9890000224113464
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9920000433921814
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8600000143051147, scr_score: 0.07766949277597389
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8800000548362732, scr_score: 0.2718446714307952
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8810000419616699, scr_score: 0.28155328569204763
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8770000338554382, scr_score: 0.24271824996108338
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8670000433921814, scr_score: 0.14563094997665002
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8340000510215759, scr_score: -0.1747573714463619
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.7630000114440918, scr_score: -0.864078242970557
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6880000233650208, scr_score: 0.13753576278921065
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7020000219345093, scr_score: 0.17765037450164364
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.718000054359436, scr_score: 0.2234957426226865
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.737000048160553, scr_score: 0.2779369891762057
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7580000162124634, scr_score: 0.3381088213513757
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7820000648498535, scr_score: 0.40687687353294
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6680000424385071, scr_score: 0.08022922342486603
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9830000400543213
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.984000027179718
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9730000495910645
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9720000624656677
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.999000072479248
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9350000619888306, scr_score: 0.396825817316562
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9470000267028809, scr_score: 0.5873013770560047
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9480000734329224, scr_score: 0.6031751287885597
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9390000700950623, scr_score: 0.4603179859314168
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9310000538825989, scr_score: 0.33333364870170723
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9190000295639038, scr_score: 0.14285714285714285
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.893000066280365, scr_score: -0.2698405339817307
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6010000109672546, scr_score: 0.03291141800038962
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6150000095367432, scr_score: 0.06835444871669703
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6260000467300415, scr_score: 0.09620264128525105
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6310000419616699, scr_score: 0.10886085576265946
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6430000066757202, scr_score: 0.13924051014931188
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6680000424385071, scr_score: 0.20253173343417333
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6340000033378601, scr_score: 0.11645569391041287
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9940000176429749
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9940000176429749
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9690000414848328
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.968000054359436
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8540000319480896, scr_score: 0.10156257275956422
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.878000020980835, scr_score: 0.2890624563442615
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8650000691413879, scr_score: 0.18750034924590825
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8500000238418579, scr_score: 0.07031251455191284
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8540000319480896, scr_score: 0.10156257275956422
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8560000658035278, scr_score: 0.1171878346939954
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8770000338554382, scr_score: 0.28125005820765137
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6720000505447388, scr_score: 0.044510518539166266
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6890000104904175, scr_score: 0.09495550168537875
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7020000219345093, scr_score: 0.13353120188074755
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7090000510215759, scr_score: 0.15430280078147524
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7360000610351562, scr_score: 0.23442152190992377
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7500000596046448, scr_score: 0.2759645428430035
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6460000276565552, scr_score: -0.03264088185157135
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9440000653266907
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9520000219345093
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9970000386238098
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.815000057220459, scr_score: 0.04371592886659017
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8110000491142273, scr_score: 0.021857964433295084
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8160000443458557, scr_score: 0.04918033854781611
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8130000233650208, scr_score: 0.03278678379574697
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8220000267028809, scr_score: 0.08196712234356307
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7640000581741333, scr_score: -0.23497254766823733
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7390000224113464, scr_score: -0.3715847439492338
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8280000686645508, scr_score: 0.545098150129342
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8500000238418579, scr_score: 0.6313724656052055
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.859000027179718, scr_score: 0.666666588752115
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8600000143051147, scr_score: 0.6705881060476262
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8660000562667847, scr_score: 0.6941176773080023
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8650000691413879, scr_score: 0.6901961600124913
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8700000643730164, scr_score: 0.7098039802337016
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9640000462532043
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9640000462532043
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9770000576972961
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9750000238418579
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.800000011920929, scr_score: 0.09230742426312609
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8160000443458557, scr_score: 0.17435887560571312
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8230000138282776, scr_score: 0.2102561563194528
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8330000638961792, scr_score: 0.2615384662409978
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8380000591278076, scr_score: 0.28717946836934216
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8530000448226929, scr_score: 0.36410247475437535
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8450000286102295, scr_score: 0.32307674908308187
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7410000562667847, scr_score: 0.1008065853511987
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7450000643730164, scr_score: 0.11693564862104187
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7580000162124634, scr_score: 0.16935480382142643
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7710000276565552, scr_score: 0.2217741993630956
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7930000424385071, scr_score: 0.31048392717659073
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7930000424385071, scr_score: 0.31048392717659073
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.8350000381469727, scr_score: 0.47983873099801716
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9780000448226929
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9780000448226929
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9940000176429749
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9930000305175781
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7910000085830688, scr_score: 0.16143477741406173
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8070000410079956, scr_score: 0.2331838049629021
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8300000429153442, scr_score: 0.33632283160029924
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8460000157356262, scr_score: 0.40807159186372505
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8720000386238098, scr_score: 0.5246636279878835
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8610000610351562, scr_score: 0.4753363720121166
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8160000443458557, scr_score: 0.27354256613777117
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7540000081062317, scr_score: 0.06696421087587681
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7750000357627869, scr_score: 0.1607143189758008
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7850000262260437, scr_score: 0.20535712622638533
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8090000152587891, scr_score: 0.3124999168462123
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8250000476837158, scr_score: 0.38392862132084404
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8420000672340393, scr_score: 0.4598215533021101
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8410000205039978, scr_score: 0.45535705970335516
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9620000720024109
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9940000176429749
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7430000305175781, scr_score: 0.060085820441166386
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7480000257492065, scr_score: 0.08154502375487709
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7570000290870667, scr_score: 0.12017164088233277
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.76500004529953, scr_score: 0.15450646850982275
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7680000066757202, scr_score: 0.1673818370097199
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7730000615119934, scr_score: 0.1888412961373127
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7850000262260437, scr_score: 0.24034328176466555
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7750000357627869, scr_score: 0.06666664774456064
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7820000648498535, scr_score: 0.10000011353263609
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7910000085830688, scr_score: 0.14285698066766273
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7980000376701355, scr_score: 0.17619044645573817
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7920000553131104, scr_score: 0.14761910708852366
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8030000329017639, scr_score: 0.19999994323368195
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8080000281333923, scr_score: 0.22380944001162575

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.9550000429153442
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 1: 0.9580000638961792
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 2: 0.9580000638961792
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 6: 0.9890000224113464
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 9: 0.9820000529289246
Num non-zero elements: 1
Test accuracy for 0: 0.64
Num non-zero elements: 1
Test accuracy for 1: 0.677
Num non-zero elements: 1
Test accuracy for 2: 0.855
Num non-zero elements: 1
Test accuracy for 6: 0.98
Num non-zero elements: 1
Test accuracy for 9: 0.939
Num non-zero elements: 2
Test accuracy for 0: 0.713
Num non-zero elements: 2
Test accuracy for 1: 0.751
Num non-zero elements: 2
Test accuracy for 2: 0.87
Num non-zero elements: 2
Test accuracy for 6: 0.978
Num non-zero elements: 2
Test accuracy for 9: 0.946
Num non-zero elements: 5
Test accuracy for 0: 0.826
Num non-zero elements: 5
Test accuracy for 1: 0.89
Num non-zero elements: 5
Test accuracy for 2: 0.875
Num non-zero elements: 5
Test accuracy for 6: 0.984
Num non-zero elements: 5
Test accuracy for 9: 0.945
Num non-zero elements: 10
Test accuracy for 0: 0.846
Num non-zero elements: 10
Test accuracy for 1: 0.899
Num non-zero elements: 10
Test accuracy for 2: 0.894
Num non-zero elements: 10
Test accuracy for 6: 0.982
Num non-zero elements: 10
Test accuracy for 9: 0.964
Num non-zero elements: 20
Test accuracy for 0: 0.884
Num non-zero elements: 20
Test accuracy for 1: 0.922
Num non-zero elements: 20
Test accuracy for 2: 0.934
Num non-zero elements: 20
Test accuracy for 6: 0.99
Num non-zero elements: 20
Test accuracy for 9: 0.969
Num non-zero elements: 50
Test accuracy for 0: 0.924
Num non-zero elements: 50
Test accuracy for 1: 0.952
Num non-zero elements: 50
Test accuracy for 2: 0.946
Num non-zero elements: 50
Test accuracy for 6: 0.991
Num non-zero elements: 50
Test accuracy for 9: 0.969
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.951
Num non-zero elements: 2304
Test accuracy for 13: 0.952
Num non-zero elements: 2304
Test accuracy for 14: 0.953
Num non-zero elements: 2304
Test accuracy for 18: 0.915
Num non-zero elements: 2304
Test accuracy for 19: 0.958
Num non-zero elements: 1
Test accuracy for 11: 0.555
Num non-zero elements: 1
Test accuracy for 13: 0.67
Num non-zero elements: 1
Test accuracy for 14: 0.648
Num non-zero elements: 1
Test accuracy for 18: 0.691
Num non-zero elements: 1
Test accuracy for 19: 0.787
Num non-zero elements: 2
Test accuracy for 11: 0.688
Num non-zero elements: 2
Test accuracy for 13: 0.718
Num non-zero elements: 2
Test accuracy for 14: 0.665
Num non-zero elements: 2
Test accuracy for 18: 0.722
Num non-zero elements: 2
Test accuracy for 19: 0.774
Num non-zero elements: 5
Test accuracy for 11: 0.779
Num non-zero elements: 5
Test accuracy for 13: 0.742
Num non-zero elements: 5
Test accuracy for 14: 0.722
Num non-zero elements: 5
Test accuracy for 18: 0.734
Num non-zero elements: 5
Test accuracy for 19: 0.838
Num non-zero elements: 10
Test accuracy for 11: 0.842
Num non-zero elements: 10
Test accuracy for 13: 0.775
Num non-zero elements: 10
Test accuracy for 14: 0.788
Num non-zero elements: 10
Test accuracy for 18: 0.741
Num non-zero elements: 10
Test accuracy for 19: 0.861
Num non-zero elements: 20
Test accuracy for 11: 0.908
Num non-zero elements: 20
Test accuracy for 13: 0.876
Num non-zero elements: 20
Test accuracy for 14: 0.881
Num non-zero elements: 20
Test accuracy for 18: 0.772
Num non-zero elements: 20
Test accuracy for 19: 0.903
Num non-zero elements: 50
Test accuracy for 11: 0.939
Num non-zero elements: 50
Test accuracy for 13: 0.919
Num non-zero elements: 50
Test accuracy for 14: 0.906
Num non-zero elements: 50
Test accuracy for 18: 0.847
Num non-zero elements: 50
Test accuracy for 19: 0.931
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 11: 0.9670000672340393
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 13: 0.9510000348091125
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 14: 0.9610000252723694
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 18: 0.9420000314712524
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 19: 0.9630000591278076
Num non-zero elements: 1
Test accuracy for 11: 0.729
Num non-zero elements: 1
Test accuracy for 13: 0.767
Num non-zero elements: 1
Test accuracy for 14: 0.814
Num non-zero elements: 1
Test accuracy for 18: 0.724
Num non-zero elements: 1
Test accuracy for 19: 0.844
Num non-zero elements: 2
Test accuracy for 11: 0.866
Num non-zero elements: 2
Test accuracy for 13: 0.802
Num non-zero elements: 2
Test accuracy for 14: 0.822
Num non-zero elements: 2
Test accuracy for 18: 0.716
Num non-zero elements: 2
Test accuracy for 19: 0.86
Num non-zero elements: 5
Test accuracy for 11: 0.882
Num non-zero elements: 5
Test accuracy for 13: 0.825
Num non-zero elements: 5
Test accuracy for 14: 0.836
Num non-zero elements: 5
Test accuracy for 18: 0.806
Num non-zero elements: 5
Test accuracy for 19: 0.905
Num non-zero elements: 10
Test accuracy for 11: 0.951
Num non-zero elements: 10
Test accuracy for 13: 0.83
Num non-zero elements: 10
Test accuracy for 14: 0.869
Num non-zero elements: 10
Test accuracy for 18: 0.876
Num non-zero elements: 10
Test accuracy for 19: 0.923
Num non-zero elements: 20
Test accuracy for 11: 0.956
Num non-zero elements: 20
Test accuracy for 13: 0.909
Num non-zero elements: 20
Test accuracy for 14: 0.932
Num non-zero elements: 20
Test accuracy for 18: 0.891
Num non-zero elements: 20
Test accuracy for 19: 0.926
Num non-zero elements: 50
Test accuracy for 11: 0.961
Num non-zero elements: 50
Test accuracy for 13: 0.925
Num non-zero elements: 50
Test accuracy for 14: 0.953
Num non-zero elements: 50
Test accuracy for 18: 0.894
Num non-zero elements: 50
Test accuracy for 19: 0.93
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.955
Num non-zero elements: 2304
Test accuracy for 21: 0.899
Num non-zero elements: 2304
Test accuracy for 22: 0.903
Num non-zero elements: 2304
Test accuracy for 25: 0.956
Num non-zero elements: 2304
Test accuracy for 26: 0.859
Num non-zero elements: 1
Test accuracy for 20: 0.713
Num non-zero elements: 1
Test accuracy for 21: 0.765
Num non-zero elements: 1
Test accuracy for 22: 0.65
Num non-zero elements: 1
Test accuracy for 25: 0.698
Num non-zero elements: 1
Test accuracy for 26: 0.623
Num non-zero elements: 2
Test accuracy for 20: 0.798
Num non-zero elements: 2
Test accuracy for 21: 0.769
Num non-zero elements: 2
Test accuracy for 22: 0.671
Num non-zero elements: 2
Test accuracy for 25: 0.758
Num non-zero elements: 2
Test accuracy for 26: 0.669
Num non-zero elements: 5
Test accuracy for 20: 0.871
Num non-zero elements: 5
Test accuracy for 21: 0.785
Num non-zero elements: 5
Test accuracy for 22: 0.677
Num non-zero elements: 5
Test accuracy for 25: 0.788
Num non-zero elements: 5
Test accuracy for 26: 0.677
Num non-zero elements: 10
Test accuracy for 20: 0.879
Num non-zero elements: 10
Test accuracy for 21: 0.822
Num non-zero elements: 10
Test accuracy for 22: 0.73
Num non-zero elements: 10
Test accuracy for 25: 0.83
Num non-zero elements: 10
Test accuracy for 26: 0.711
Num non-zero elements: 20
Test accuracy for 20: 0.889
Num non-zero elements: 20
Test accuracy for 21: 0.826
Num non-zero elements: 20
Test accuracy for 22: 0.782
Num non-zero elements: 20
Test accuracy for 25: 0.906
Num non-zero elements: 20
Test accuracy for 26: 0.795
Num non-zero elements: 50
Test accuracy for 20: 0.932
Num non-zero elements: 50
Test accuracy for 21: 0.88
Num non-zero elements: 50
Test accuracy for 22: 0.868
Num non-zero elements: 50
Test accuracy for 25: 0.928
Num non-zero elements: 50
Test accuracy for 26: 0.846
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 20: 0.9650000333786011
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 21: 0.937000036239624
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 22: 0.9140000343322754
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 25: 0.9660000205039978
Num non-zero elements: 18432
Training probe with dim: 18432, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 26: 0.9040000438690186
Num non-zero elements: 1
Test accuracy for 20: 0.868
Num non-zero elements: 1
Test accuracy for 21: 0.802
Num non-zero elements: 1
Test accuracy for 22: 0.708
Num non-zero elements: 1
Test accuracy for 25: 0.87
Num non-zero elements: 1
Test accuracy for 26: 0.688
Num non-zero elements: 2
Test accuracy for 20: 0.899
Num non-zero elements: 2
Test accuracy for 21: 0.81
Num non-zero elements: 2
Test accuracy for 22: 0.786
Num non-zero elements: 2
Test accuracy for 25: 0.887
Num non-zero elements: 2
Test accuracy for 26: 0.751
Num non-zero elements: 5
Test accuracy for 20: 0.91
Num non-zero elements: 5
Test accuracy for 21: 0.851
Num non-zero elements: 5
Test accuracy for 22: 0.848
Num non-zero elements: 5
Test accuracy for 25: 0.92
Num non-zero elements: 5
Test accuracy for 26: 0.778
Num non-zero elements: 10
Test accuracy for 20: 0.926
Num non-zero elements: 10
Test accuracy for 21: 0.852
Num non-zero elements: 10
Test accuracy for 22: 0.89
Num non-zero elements: 10
Test accuracy for 25: 0.914
Num non-zero elements: 10
Test accuracy for 26: 0.821
Num non-zero elements: 20
Test accuracy for 20: 0.93
Num non-zero elements: 20
Test accuracy for 21: 0.865
Num non-zero elements: 20
Test accuracy for 22: 0.884
Num non-zero elements: 20
Test accuracy for 25: 0.922
Num non-zero elements: 20
Test accuracy for 26: 0.853
Num non-zero elements: 50
Test accuracy for 20: 0.95
Num non-zero elements: 50
Test accuracy for 21: 0.88
Num non-zero elements: 50
Test accuracy for 22: 0.884
Num non-zero elements: 50
Test accuracy for 25: 0.949
Num non-zero elements: 50
Test accuracy for 26: 0.844
Removing 31 from train set
Removing 0 from train set
Removing 7 from train set
Removing 28 from train set
Removing 29 from train set
