Using GPUs: [0]
Using OpenRouter API with DeepSeek Reasoner.
ideas
Loaded existing ideas:
Processing idea: Orthogonal_sae

[2025-02-07 19:57:03]  
*Starting idea: 20250207_195701_Orthogonal_sae*

[2025-02-07 19:57:04] Starting experiments execution
Starting experiment with prompt for coder: Your goal is to implement the following idea: Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Implement adaptive orthogonality loss with controlled feature sharing
2. Add batch-wise feature grouping with periodic updates
3. Train on Pythia-70M using WMDP-bio and WikiText datasets
4. Compare unlearning performance against baseline and fixed orthogonal SAE
5. Analyze condition numbers of feature subspaces
6. Evaluate impact of different α values for controlled sharing.
The implementation plan is as follows: 1. Add AdaptiveOrthogonalityLoss with scheduling
2. Implement efficient batch-wise feature grouping
3. Modify CustomSAE to include group-specific biases
4. Add condition number calculation utilities
5. Update CustomTrainer with adaptive loss weight
6. Add evaluation metrics for controlled sharing.

You can also refer to other information in the idea: {'Name': 'Orthogonal_sae', 'Title': 'Orthogonal Feature Learning for Optimal Knowledge Separation in Sparse Autoencoders', 'Experiment': '1. Implement adaptive orthogonality loss with controlled feature sharing\n2. Add batch-wise feature grouping with periodic updates\n3. Train on Pythia-70M using WMDP-bio and WikiText datasets\n4. Compare unlearning performance against baseline and fixed orthogonal SAE\n5. Analyze condition numbers of feature subspaces\n6. Evaluate impact of different α values for controlled sharing', 'Technical_Details': 'The method uses an adaptive objective: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_ortho where λ_2(t) = λ_2_max * min(1, t/t_0) increases linearly until step t_0. L_ortho = ||W_1^T W_2 - αI||_F allows controlled feature sharing through parameter α. Feature groups are updated every n=100 steps using efficient batch statistics. Implementation includes early stopping based on condition number thresholds. The decoder uses group-specific bias terms while sharing weights to balance separation and reconstruction quality.', 'Research_Impact': 'A key challenge in selective unlearning is finding the optimal balance between knowledge separation and model performance. Current approaches either sacrifice reconstruction quality for separation or vice versa. This research addresses the challenge through adaptive training and controlled feature sharing, achieving better separation while maintaining performance. The theoretical framework provides precise control over the separation-performance trade-off.', 'Implementation_Plan': '1. Add AdaptiveOrthogonalityLoss with scheduling\n2. Implement efficient batch-wise feature grouping\n3. Modify CustomSAE to include group-specific biases\n4. Add condition number calculation utilities\n5. Update CustomTrainer with adaptive loss weight\n6. Add evaluation metrics for controlled sharing', 'Interestingness_Evaluation': 'The combination of theoretical insights about feature sharing with practical adaptive training creates a more nuanced and effective approach to knowledge separation.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The implementation remains efficient with batch-wise updates and simple matrix operations; adaptive weighting adds minimal overhead; controlled sharing through α provides easy tuning; all computations well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'The adaptive approach with controlled feature sharing provides a novel theoretical framework for knowledge separation that bridges the gap between strict orthogonality and practical performance.', 'Novelty': 9, 'Overall_Score': 9.0, 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

If the experiments in the idea is already implemented in 'experiment.py' you are given with, you should try to improve its result by further enhancing the implementation.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run (you can try to run with different hyperparameters in the same run across different iterations.).

Note that we already provide the baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0002, 'steps': 2441, 'seed': 42, 'activation_dim': 2304, 'dict_size': 16384, 'k': 40, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 4357.5732421875, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'e5d9d74a-c71a-462d-9661-5a7f2d41d4b8', 'datetime_epoch_millis': 1738885451045, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.1429159584016188, 'mean_num_split_features': 1.12}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.04216867469879518, 'num_absorption': 112, 'num_probe_true_positives': 2656, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.018471337579617834, 'num_absorption': 29, 'num_probe_true_positives': 1570, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.3883746760459089, 'num_absorption': 1049, 'num_probe_true_positives': 2701, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.11044417767106843, 'num_absorption': 184, 'num_probe_true_positives': 1666, 'num_split_features': 2}, {'first_letter': 'e', 'absorption_rate': 0.07116104868913857, 'num_absorption': 114, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.05956375838926174, 'num_absorption': 71, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.08423686405337781, 'num_absorption': 101, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.3019648397104447, 'num_absorption': 292, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.10113704368641532, 'num_absorption': 169, 'num_probe_true_positives': 1671, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.004739336492890996, 'num_absorption': 2, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.007518796992481203, 'num_absorption': 5, 'num_probe_true_positives': 665, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.030377668308702793, 'num_absorption': 37, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.27388535031847133, 'num_absorption': 473, 'num_probe_true_positives': 1727, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.04790419161676647, 'num_absorption': 40, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.32429906542056075, 'num_absorption': 347, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.4584221748400853, 'num_absorption': 1075, 'num_probe_true_positives': 2345, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.07567567567567568, 'num_absorption': 14, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.004294478527607362, 'num_absorption': 7, 'num_probe_true_positives': 1630, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.39844026940801136, 'num_absorption': 1124, 'num_probe_true_positives': 2821, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.0267639902676399, 'num_absorption': 44, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.21963824289405684, 'num_absorption': 170, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.008333333333333333, 'num_absorption': 7, 'num_probe_true_positives': 840, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.14390602055800295, 'num_absorption': 98, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.3473684210526316, 'num_absorption': 66, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.023809523809523808, 'num_absorption': 6, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9596273291925466, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.40625}, 'model_performance_preservation': {'ce_loss_score': 0.9588815789473685, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.328125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.609375, 'mse': 2.484375, 'cossim': 0.875}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 131.0, 'l2_ratio': 0.875, 'relative_reconstruction_bias': 0.99609375}, 'sparsity': {'l0': 40.0, 'l1': 364.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '0a27606c-8d4d-449f-ad67-c23bba779fc9', 'datetime_epoch_millis': 1738886529707, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.94985, 'llm_top_1_test_accuracy': 0.66250625, 'llm_top_2_test_accuracy': 0.7216312500000001, 'llm_top_5_test_accuracy': 0.7799, 'llm_top_10_test_accuracy': 0.8296999999999999, 'llm_top_20_test_accuracy': 0.87843125, 'llm_top_50_test_accuracy': 0.9207375, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9509000509977341, 'sae_top_1_test_accuracy': 0.75075625, 'sae_top_2_test_accuracy': 0.8130499999999999, 'sae_top_5_test_accuracy': 0.8722624999999999, 'sae_top_10_test_accuracy': 0.90165625, 'sae_top_20_test_accuracy': 0.9201937499999998, 'sae_top_50_test_accuracy': 0.93555625, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.96260005235672, 'sae_top_1_test_accuracy': 0.8308000000000002, 'sae_top_2_test_accuracy': 0.8662000000000001, 'sae_top_5_test_accuracy': 0.8854, 'sae_top_10_test_accuracy': 0.8987999999999999, 'sae_top_20_test_accuracy': 0.9468, 'sae_top_50_test_accuracy': 0.9583999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9454, 'llm_top_1_test_accuracy': 0.6724, 'llm_top_2_test_accuracy': 0.7358, 'llm_top_5_test_accuracy': 0.7624000000000001, 'llm_top_10_test_accuracy': 0.8026, 'llm_top_20_test_accuracy': 0.8661999999999999, 'llm_top_50_test_accuracy': 0.9042000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.941200053691864, 'sae_top_1_test_accuracy': 0.7767999999999999, 'sae_top_2_test_accuracy': 0.8154, 'sae_top_5_test_accuracy': 0.8486, 'sae_top_10_test_accuracy': 0.9022, 'sae_top_20_test_accuracy': 0.9246000000000001, 'sae_top_50_test_accuracy': 0.9346, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9109999999999999, 'llm_top_1_test_accuracy': 0.6849999999999999, 'llm_top_2_test_accuracy': 0.7438, 'llm_top_5_test_accuracy': 0.7618, 'llm_top_10_test_accuracy': 0.7934, 'llm_top_20_test_accuracy': 0.851, 'llm_top_50_test_accuracy': 0.8879999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000596046448, 'sae_top_1_test_accuracy': 0.8310000000000001, 'sae_top_2_test_accuracy': 0.8448, 'sae_top_5_test_accuracy': 0.8725999999999999, 'sae_top_10_test_accuracy': 0.8865999999999999, 'sae_top_20_test_accuracy': 0.8985999999999998, 'sae_top_50_test_accuracy': 0.9106, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8964000000000001, 'llm_top_1_test_accuracy': 0.6024, 'llm_top_2_test_accuracy': 0.6397999999999999, 'llm_top_5_test_accuracy': 0.6666, 'llm_top_10_test_accuracy': 0.7491999999999999, 'llm_top_20_test_accuracy': 0.8138, 'llm_top_50_test_accuracy': 0.8566, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9076000452041626, 'sae_top_1_test_accuracy': 0.7055999999999999, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.818, 'sae_top_10_test_accuracy': 0.8550000000000001, 'sae_top_20_test_accuracy': 0.8708, 'sae_top_50_test_accuracy': 0.883, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9625000655651093, 'sae_top_1_test_accuracy': 0.649, 'sae_top_2_test_accuracy': 0.813, 'sae_top_5_test_accuracy': 0.875, 'sae_top_10_test_accuracy': 0.928, 'sae_top_20_test_accuracy': 0.935, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9662000000000001, 'llm_top_1_test_accuracy': 0.666, 'llm_top_2_test_accuracy': 0.6900000000000001, 'llm_top_5_test_accuracy': 0.7613999999999999, 'llm_top_10_test_accuracy': 0.8039999999999999, 'llm_top_20_test_accuracy': 0.8622, 'llm_top_50_test_accuracy': 0.923, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9688000440597534, 'sae_top_1_test_accuracy': 0.6042, 'sae_top_2_test_accuracy': 0.6302, 'sae_top_5_test_accuracy': 0.8632, 'sae_top_10_test_accuracy': 0.8856000000000002, 'sae_top_20_test_accuracy': 0.9151999999999999, 'sae_top_50_test_accuracy': 0.9425999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9380000000000001, 'llm_top_1_test_accuracy': 0.63325, 'llm_top_2_test_accuracy': 0.76825, 'llm_top_5_test_accuracy': 0.827, 'llm_top_10_test_accuracy': 0.869, 'llm_top_20_test_accuracy': 0.89925, 'llm_top_50_test_accuracy': 0.9265000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9455000460147858, 'sae_top_1_test_accuracy': 0.79725, 'sae_top_2_test_accuracy': 0.85, 'sae_top_5_test_accuracy': 0.8865, 'sae_top_10_test_accuracy': 0.90925, 'sae_top_20_test_accuracy': 0.91675, 'sae_top_50_test_accuracy': 0.93625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.7243999999999999, 'llm_top_2_test_accuracy': 0.7802, 'llm_top_5_test_accuracy': 0.9019999999999999, 'llm_top_10_test_accuracy': 0.96, 'llm_top_20_test_accuracy': 0.99, 'llm_top_50_test_accuracy': 0.9978, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9940000414848328, 'sae_top_1_test_accuracy': 0.8114000000000001, 'sae_top_2_test_accuracy': 0.8938, 'sae_top_5_test_accuracy': 0.9288000000000001, 'sae_top_10_test_accuracy': 0.9478, 'sae_top_20_test_accuracy': 0.9538, 'sae_top_50_test_accuracy': 0.966, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'e8be4b3c-dfd3-49dd-91ca-e1b0d08e9055', 'datetime_epoch_millis': 1738886775914, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.026217222213745117}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 16384, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 701.0477905273438, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b5f3e18f-7844-4d8a-ac26-9b99d357cf9a', 'datetime_epoch_millis': 1738804062497, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.016138604243358595, 'mean_num_split_features': 1.4166666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2656, 'num_split_features': 2}, {'first_letter': 'b', 'absorption_rate': 0.0006369426751592356, 'num_absorption': 1, 'num_probe_true_positives': 1570, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2701, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.009603841536614645, 'num_absorption': 16, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.05305867665418227, 'num_absorption': 85, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.013422818791946308, 'num_absorption': 16, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.021684737281067557, 'num_absorption': 26, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.006582884500299222, 'num_absorption': 11, 'num_probe_true_positives': 1671, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.09770114942528736, 'num_absorption': 119, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0005790387955993051, 'num_absorption': 1, 'num_probe_true_positives': 1727, 'num_split_features': 4}, {'first_letter': 'n', 'absorption_rate': 0.013173652694610778, 'num_absorption': 11, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.022429906542056073, 'num_absorption': 24, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.008955223880597015, 'num_absorption': 21, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0006134969325153375, 'num_absorption': 1, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.0857851825593761, 'num_absorption': 242, 'num_probe_true_positives': 2821, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.011557177615571776, 'num_absorption': 19, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.021963824289405683, 'num_absorption': 17, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.0014684287812041115, 'num_absorption': 1, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.011904761904761904, 'num_absorption': 3, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995723748059007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004302978515625}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.984375, 'mse': 0.0888671875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.98828125}, 'sparsity': {'l0': 8895.810546875, 'l1': 12608.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'fc35f1d1-c27c-402b-abd2-fe38f12c05aa', 'datetime_epoch_millis': 1738804969188, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95075625, 'llm_top_1_test_accuracy': 0.65444375, 'llm_top_2_test_accuracy': 0.71966875, 'llm_top_5_test_accuracy': 0.7860875, 'llm_top_10_test_accuracy': 0.83056875, 'llm_top_20_test_accuracy': 0.8764437500000001, 'llm_top_50_test_accuracy': 0.9228625, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9578625451773405, 'sae_top_1_test_accuracy': 0.6448624999999999, 'sae_top_2_test_accuracy': 0.6875375000000001, 'sae_top_5_test_accuracy': 0.7562874999999999, 'sae_top_10_test_accuracy': 0.8066500000000001, 'sae_top_20_test_accuracy': 0.8664625000000001, 'sae_top_50_test_accuracy': 0.9122750000000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6862, 'sae_top_2_test_accuracy': 0.6885999999999999, 'sae_top_5_test_accuracy': 0.7842, 'sae_top_10_test_accuracy': 0.8386000000000001, 'sae_top_20_test_accuracy': 0.8806, 'sae_top_50_test_accuracy': 0.9252, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9492, 'llm_top_1_test_accuracy': 0.6794, 'llm_top_2_test_accuracy': 0.717, 'llm_top_5_test_accuracy': 0.767, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8614, 'llm_top_50_test_accuracy': 0.9086000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.6844, 'sae_top_2_test_accuracy': 0.6834, 'sae_top_5_test_accuracy': 0.7402000000000001, 'sae_top_10_test_accuracy': 0.7884, 'sae_top_20_test_accuracy': 0.8503999999999999, 'sae_top_50_test_accuracy': 0.8998000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6862, 'llm_top_2_test_accuracy': 0.732, 'llm_top_5_test_accuracy': 0.7722, 'llm_top_10_test_accuracy': 0.8008, 'llm_top_20_test_accuracy': 0.8532, 'llm_top_50_test_accuracy': 0.8922000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9368000507354737, 'sae_top_1_test_accuracy': 0.5932000000000001, 'sae_top_2_test_accuracy': 0.725, 'sae_top_5_test_accuracy': 0.7638, 'sae_top_10_test_accuracy': 0.8056000000000001, 'sae_top_20_test_accuracy': 0.8465999999999999, 'sae_top_50_test_accuracy': 0.8838000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8948, 'llm_top_1_test_accuracy': 0.6039999999999999, 'llm_top_2_test_accuracy': 0.6508, 'llm_top_5_test_accuracy': 0.6878, 'llm_top_10_test_accuracy': 0.745, 'llm_top_20_test_accuracy': 0.8051999999999999, 'llm_top_50_test_accuracy': 0.8628, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9118000507354737, 'sae_top_1_test_accuracy': 0.5828, 'sae_top_2_test_accuracy': 0.6594, 'sae_top_5_test_accuracy': 0.7343999999999999, 'sae_top_10_test_accuracy': 0.768, 'sae_top_20_test_accuracy': 0.8114000000000001, 'sae_top_50_test_accuracy': 0.8452, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000343322754, 'sae_top_1_test_accuracy': 0.699, 'sae_top_2_test_accuracy': 0.701, 'sae_top_5_test_accuracy': 0.792, 'sae_top_10_test_accuracy': 0.814, 'sae_top_20_test_accuracy': 0.908, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6634, 'llm_top_2_test_accuracy': 0.7002, 'llm_top_5_test_accuracy': 0.7648, 'llm_top_10_test_accuracy': 0.8024000000000001, 'llm_top_20_test_accuracy': 0.8608, 'llm_top_50_test_accuracy': 0.9258, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6342000000000001, 'sae_top_2_test_accuracy': 0.6708000000000001, 'sae_top_5_test_accuracy': 0.7096000000000001, 'sae_top_10_test_accuracy': 0.7712, 'sae_top_20_test_accuracy': 0.8295999999999999, 'sae_top_50_test_accuracy': 0.9026, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.93825, 'llm_top_1_test_accuracy': 0.63375, 'llm_top_2_test_accuracy': 0.7607499999999999, 'llm_top_5_test_accuracy': 0.8265, 'llm_top_10_test_accuracy': 0.8727499999999999, 'llm_top_20_test_accuracy': 0.8977499999999999, 'llm_top_50_test_accuracy': 0.9265, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9530000537633896, 'sae_top_1_test_accuracy': 0.6285, 'sae_top_2_test_accuracy': 0.7015, 'sae_top_5_test_accuracy': 0.7515000000000001, 'sae_top_10_test_accuracy': 0.856, 'sae_top_20_test_accuracy': 0.8875, 'sae_top_50_test_accuracy': 0.9199999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6522, 'llm_top_2_test_accuracy': 0.7814, 'llm_top_5_test_accuracy': 0.9123999999999999, 'llm_top_10_test_accuracy': 0.9642, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9972000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9990000247955322, 'sae_top_1_test_accuracy': 0.6506000000000001, 'sae_top_2_test_accuracy': 0.6706, 'sae_top_5_test_accuracy': 0.7745999999999998, 'sae_top_10_test_accuracy': 0.8114000000000001, 'sae_top_20_test_accuracy': 0.9176, 'sae_top_50_test_accuracy': 0.9895999999999999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'b48744e1-4f43-4d25-af92-4f1a46abab60', 'datetime_epoch_millis': 1738805225729, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll plan 5 focused runs to implement and evaluate the orthogonal feature learning approach:                                                                             

Run 1: Implement core adaptive orthogonality mechanism                                                                                                                   

 • Add AdaptiveOrthogonalityLoss with λ2 scheduling (λ2_max=0.1, t0=2000 steps)                                                                                          
 • Set α=0.2 for controlled sharing                                                                                                                                      
 • Add basic condition number tracking                                                                                                                                   

Run 2: Add batch-wise feature grouping                                                                                                                                   

 • Implement grouping every 100 steps                                                                                                                                    
 • Use k-means clustering on W_enc with k=d_sae/64                                                                                                                       
 • Add group-specific biases in decoder                                                                                                                                  
 • Keep α=0.2                                                                                                                                                            

Run 3: Compare α values (ablation study)                                                                                                                                 

 • Run 3a: α=0 (strict orthogonality)                                                                                                                                    
 • Run 3b: α=0.4 (more sharing)                                                                                                                                          
 • Run 3c: α=0.1 (less sharing)                                                                                                                                          

Run 4: Early stopping based on condition number                                                                                                                          

 • Set threshold κ_max=50                                                                                                                                                
 • Add validation check every 500 steps                                                                                                                                  
 • Compare against fixed orthogonal SAE baseline                                                                                                                         

Run 5: Final optimized configuration                                                                                                                                     

 • Combine best α from Run 3                                                                                                                                             
 • Fine-tune λ2_max=0.2                                                                                                                                                  
 • Increase grouping frequency to 50 steps                                                                                                                               
 • Add feature subspace analysis metrics                                                                                                                                 

Let's start with Run 1. I'll need to modify these components:                                                                                                            

 1 Add AdaptiveOrthogonalityLoss class                                                                                                                                   
 2 Update TrainerTopK loss calculation                                                                                                                                   
 3 Add condition number tracking                                                                                                                                         
 4 Modify config to include ortho params                                                                                                                                 

Here are the code changes needed for Run 1:                                                                                                                              

experiment.py changes:                                                                                                                                                   

                                                                                                                                                                         
 class AdaptiveOrthogonalityLoss(nn.Module):                                                                                                                             
     def __init__(self, alpha=0.2):                                                                                                                                      
         super().__init__()                                                                                                                                              
         self.alpha = alpha                                                                                                                                              
                                                                                                                                                                         
     def forward(self, W_enc):                                                                                                                                           
         ortho_term = torch.mm(W_enc.T, W_enc)                                                                                                                           
         identity = torch.eye(W_enc.size(1), device=W_enc.device)                                                                                                        
         return torch.norm(ortho_term - self.alpha*identity, p='fro')**2                                                                                                 
                                                                                                                                                                         
 # Modify TrainerTopK __init__:                                                                                                                                          
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,                                                                                                                                              
         decay_start=24000,                                                                                                                                              
         steps=30000,                                                                                                                                                    
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
         # New params:                                                                                                                                                   
         lambda2_max=0.1,                                                                                                                                                
         t0=2000,                                                                                                                                                        
         ortho_alpha=0.2,                                                                                                                                                
     ):                                                                                                                                                                  
         # ... existing init code ...                                                                                                                                    
         self.ortho_loss_fn = AdaptiveOrthogonalityLoss(alpha=ortho_alpha)                                                                                               
         self.lambda2_max = lambda2_max                                                                                                                                  
         self.t0 = t0                                                                                                                                                    
         self.logging_parameters += ["condition_number", "ortho_loss"]                                                                                                   
                                                                                                                                                                         
 # Update TrainerTopK loss method:                                                                                                                                       
     def loss(self, x, step=None, logging=False):                                                                                                                        
         # ... existing loss calculation ...                                                                                                                             
                                                                                                                                                                         
         # Calculate orthogonality loss                                                                                                                                  
         ortho_loss = self.ortho_loss_fn(self.ae.W_enc)                                                                                                                  
         current_lambda2 = self.lambda2_max * min(1.0, (step/self.t0)) if step else 0                                                                                    
         loss = loss + current_lambda2 * ortho_loss                                                                                                                      
                                                                                                                                                                         
         if logging:                                                                                                                                                     
             # Track condition number                                                                                                                                    
             with torch.no_grad():                                                                                                                                       
                 W = self.ae.W_enc                                                                                                                                       
                 u,s,_ = torch.svd(W @ W.T)                                                                                                                              
                 cond = s[0]/s[-1]                                                                                                                                       
                 self.condition_number = cond.item()                                                                                                                     
                                                                                                                                                                         
             losses["ortho_loss"] = ortho_loss.item()                                                                                                                    
             losses["condition_number"] = self.condition_number                                                                                                          
                                                                                                                                                                         
 # Update config in TrainerTopK:                                                                                                                                         
     @property                                                                                                                                                           
     def config(self):                                                                                                                                                   
         return {                                                                                                                                                        
             # ... existing config ...                                                                                                                                   
             "lambda2_max": self.lambda2_max,                                                                                                                            
             "t0": self.t0,                                                                                                                                              
             "ortho_alpha": self.ortho_loss_fn.alpha,                                                                                                                    
         }                                                                                                                                                               
                                                                                                                                                                         

This implements the core adaptive orthogonality mechanism. Let's run with: python experiment.py --out_dir=run_1                                                          

After validation, I'll proceed with Run 2 for feature grouping.                                                                                                          

Tokens: 33k sent, 1.4k received.
I'll help implement Run 1 with proper SEARCH/REPLACE blocks. Let's make these changes step by step:                                                                      

 1 First, let's add the AdaptiveOrthogonalityLoss class to experiment.py:                                                                                                

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 import einops                                                                                                                                                           
 import torch as t                                                                                                                                                       
 import torch.nn as nn                                                                                                                                                   
 from collections import namedtuple                                                                                                                                      
                                                                                                                                                                         
 from dictionary_learning.config import DEBUG                                                                                                                            
 from dictionary_learning.dictionary import Dictionary                                                                                                                   
 from dictionary_learning.trainers.trainer import SAETrainer                                                                                                             
 =======                                                                                                                                                                 
 import einops                                                                                                                                                           
 import torch as t                                                                                                                                                       
 import torch.nn as nn                                                                                                                                                   
 from collections import namedtuple                                                                                                                                      
                                                                                                                                                                         
 from dictionary_learning.config import DEBUG                                                                                                                            
 from dictionary_learning.dictionary import Dictionary                                                                                                                   
 from dictionary_learning.trainers.trainer import SAETrainer                                                                                                             
                                                                                                                                                                         
 class AdaptiveOrthogonalityLoss(nn.Module):                                                                                                                             
     def __init__(self, alpha=0.2):                                                                                                                                      
         super().__init__()                                                                                                                                              
         self.alpha = alpha                                                                                                                                              
                                                                                                                                                                         
     def forward(self, W_enc):                                                                                                                                           
         ortho_term = t.mm(W_enc.T, W_enc)                                                                                                                               
         identity = t.eye(W_enc.size(1), device=W_enc.device)                                                                                                            
         return t.norm(ortho_term - self.alpha*identity, p='fro')**2                                                                                                     
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

 2 Now update the TrainerTopK init method:                                                                                                                               

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
     ):                                                                                                                                                                  
 =======                                                                                                                                                                 
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
         # Orthogonality params                                                                                                                                          
         lambda2_max=0.1,                                                                                                                                                
         t0=2000,                                                                                                                                                        
         ortho_alpha=0.2,                                                                                                                                                
     ):                                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

 3 Add the orthogonality components to init:                                                                                                                             

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features"]                                                                                                     
         self.effective_l0 = -1                                                                                                                                          
         self.dead_features = -1                                                                                                                                         
 =======                                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "condition_number", "ortho_loss"]                                                                   
         self.effective_l0 = -1                                                                                                                                          
         self.dead_features = -1                                                                                                                                         
         self.condition_number = -1                                                                                                                                      
                                                                                                                                                                         
         # Initialize orthogonality components                                                                                                                           
         self.ortho_loss_fn = AdaptiveOrthogonalityLoss(alpha=ortho_alpha)                                                                                               
         self.lambda2_max = lambda2_max                                                                                                                                  
         self.t0 = t0                                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

 4 Update the loss method:                                                                                                                                               

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                                    
                                                                                                                                                                         
         if not logging:                                                                                                                                                 
             return loss                                                                                                                                                 
         else:                                                                                                                                                           
             return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(                                                                                                
                 x,                                                                                                                                                      
                 x_hat,                                                                                                                                                  
                 f,                                                                                                                                                      
                 {"l2_loss": l2_loss.item(), "auxk_loss": auxk_loss.item(), "loss": loss.item()},                                                                        
             )                                                                                                                                                           
 =======                                                                                                                                                                 
         # Calculate orthogonality loss                                                                                                                                  
         ortho_loss = self.ortho_loss_fn(self.ae.W_enc)                                                                                                                  
         current_lambda2 = self.lambda2_max * min(1.0, (step/self.t0)) if step else 0                                                                                    
         loss = l2_loss + self.auxk_alpha * auxk_loss + current_lambda2 * ortho_loss                                                                                     
                                                                                                                                                                         
         if not logging:                                                                                                                                                 
             return loss                                                                                                                                                 
         else:                                                                                                                                                           
             # Track condition number                                                                                                                                    
             with t.no_grad():                                                                                                                                           
                 W = self.ae.W_enc                                                                                                                                       
                 u,s,_ = t.svd(W @ W.T)                                                                                                                                  
                 self.condition_number = (s[0]/s[-1]).item()                                                                                                             
                                                                                                                                                                         
             return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(                                                                                                
                 x,                                                                                                                                                      
                 x_hat,                                                                                                                                                  
                 f,                                                                                                                                                      
                 {                                                                                                                                                       
                     "l2_loss": l2_loss.item(),                                                                                                                          
                     "auxk_loss": auxk_loss.item(),                                                                                                                      
                     "ortho_loss": ortho_loss.item(),                                                                                                                    
                     "condition_number": self.condition_number,                                                                                                          
                     "loss": loss.item()                                                                                                                                 
                 },                                                                                                                                                      
             )                                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

 5 Update the config property:                                                                                                                                           

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     @property                                                                                                                                                           
     def config(self):                                                                                                                                                   
         return {                                                                                                                                                        
             "trainer_class": "TrainerTopK",                                                                                                                             
             "dict_class": "AutoEncoderTopK",                                                                                                                            
             "lr": self.lr,                                                                                                                                              
             "steps": self.steps,                                                                                                                                        
             "seed": self.seed,                                                                                                                                          
             "activation_dim": self.ae.activation_dim,                                                                                                                   
             "dict_size": self.ae.dict_size,                                                                                                                             
             "k": self.ae.k,                                                                                                                                             
             "device": self.device,                                                                                                                                      
             "layer": self.layer,                                                                                                                                        
             "lm_name": self.lm_name,                                                                                                                                    
             "wandb_name": self.wandb_name,                                                                                                                              
             "submodule_name": self.submodule_name,                                                                                                                      
         }                                                                                                                                                               
 =======                                                                                                                                                                 
     @property                                                                                                                                                           
     def config(self):                                                                                                                                                   
         return {                                                                                                                                                        
             "trainer_class": "TrainerTopK",                                                                                                                             
             "dict_class": "AutoEncoderTopK",                                                                                                                            
             "lr": self.lr,                                                                                                                                              
             "steps": self.steps,                                                                                                                                        
             "seed": self.seed,                                                                                                                                          
             "activation_dim": self.ae.activation_dim,                                                                                                                   
             "dict_size": self.ae.dict_size,                                                                                                                             
             "k": self.ae.k,                                                                                                                                             
             "device": self.device,                                                                                                                                      
             "layer": self.layer,                                                                                                                                        
             "lm_name": self.lm_name,                                                                                                                                    
             "wandb_name": self.wandb_name,                                                                                                                              
             "submodule_name": self.submodule_name,                                                                                                                      
             "lambda2_max": self.lambda2_max,                                                                                                                            
             "t0": self.t0,                                                                                                                                              
             "ortho_alpha": self.ortho_loss_fn.alpha,                                                                                                                    
         }                                                                                                                                                               
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

These changes implement:                                                                                                                                                 

 1 AdaptiveOrthogonalityLoss class with controlled sharing via alpha parameter                                                                                           
 2 Scheduled orthogonality loss weight (lambda2) that ramps up over t0 steps                                                                                             
 3 Condition number tracking via SVD                                                                                                                                     
 4 Configuration updates to include new parameters                                                                                                                       

The orthogonality loss will gradually increase its influence as training progresses, helping to maintain feature independence while allowing some controlled sharing     
through the alpha parameter.                                                                                                                                             

Would you like me to proceed with implementing Run 2 (batch-wise feature grouping) after you've applied and tested these changes?                                        

Tokens: 27k sent, 1.9k received. Cost: $0.11 message, $0.11 session.
Applied edit to experiment.py
Step 0: 45201.3515625
Step 100: 17948.892578125
Step 200: 7536.07177734375
Step 300: 6043.4072265625
Step 400: 5587.6982421875
Step 500: 5420.94873046875
Step 600: 5070.75634765625
Step 700: 5083.974609375
Step 800: 4931.11865234375
Step 900: 4942.318359375
Step 1000: 4833.72900390625
Step 1100: 4887.81591796875
Step 1200: 4925.70458984375
Step 1300: 4892.98291015625
Step 1400: 4852.8427734375
Step 1500: 4974.18603515625
Step 1600: 4800.91162109375
Step 1700: 4758.28173828125
Step 1800: 4813.43896484375
Step 1900: 4972.36669921875
Step 2000: 4750.56787109375
Step 2100: 4771.83984375
Step 2200: 4822.15771484375
Step 2300: 4797.38916015625
Step 2400: 4804.9443359375

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250207_195701_Orthogonal_sae/run_1/final_info.json

[2025-02-07 20:14:23]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 559.67s

[2025-02-07 20:23:43]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 301.35s

[2025-02-07 20:28:44]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.937
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.783
Num non-zero elements: 1
Test accuracy for 9: 0.55
Num non-zero elements: 2
Test accuracy for 0: 0.581
Num non-zero elements: 2
Test accuracy for 1: 0.659
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.803
Num non-zero elements: 2
Test accuracy for 9: 0.7
Num non-zero elements: 5
Test accuracy for 0: 0.724
Num non-zero elements: 5
Test accuracy for 1: 0.699
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.904
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.783
Num non-zero elements: 10
Test accuracy for 1: 0.796
Num non-zero elements: 10
Test accuracy for 2: 0.802
Num non-zero elements: 10
Test accuracy for 6: 0.885
Num non-zero elements: 10
Test accuracy for 9: 0.901
Num non-zero elements: 20
Test accuracy for 0: 0.816
Num non-zero elements: 20
Test accuracy for 1: 0.881
Num non-zero elements: 20
Test accuracy for 2: 0.878
Num non-zero elements: 20
Test accuracy for 6: 0.974
Num non-zero elements: 20
Test accuracy for 9: 0.936
Num non-zero elements: 50
Test accuracy for 0: 0.909
Num non-zero elements: 50
Test accuracy for 1: 0.924
Num non-zero elements: 50
Test accuracy for 2: 0.92
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 0: 0.9410000443458557
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 1: 0.9580000638961792
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 2: 0.937000036239624
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 6: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 9: 0.9670000672340393
Num non-zero elements: 1
Test accuracy for 0: 0.69
Num non-zero elements: 1
Test accuracy for 1: 0.82
Num non-zero elements: 1
Test accuracy for 2: 0.626
Num non-zero elements: 1
Test accuracy for 6: 0.975
Num non-zero elements: 1
Test accuracy for 9: 0.934
Num non-zero elements: 2
Test accuracy for 0: 0.753
Num non-zero elements: 2
Test accuracy for 1: 0.828
Num non-zero elements: 2
Test accuracy for 2: 0.89
Num non-zero elements: 2
Test accuracy for 6: 0.98
Num non-zero elements: 2
Test accuracy for 9: 0.939
Num non-zero elements: 5
Test accuracy for 0: 0.776
Num non-zero elements: 5
Test accuracy for 1: 0.838
Num non-zero elements: 5
Test accuracy for 2: 0.904
Num non-zero elements: 5
Test accuracy for 6: 0.977
Num non-zero elements: 5
Test accuracy for 9: 0.95
Num non-zero elements: 10
Test accuracy for 0: 0.831
Num non-zero elements: 10
Test accuracy for 1: 0.933
Num non-zero elements: 10
Test accuracy for 2: 0.923
Num non-zero elements: 10
Test accuracy for 6: 0.987
Num non-zero elements: 10
Test accuracy for 9: 0.95
Num non-zero elements: 20
Test accuracy for 0: 0.863
Num non-zero elements: 20
Test accuracy for 1: 0.949
Num non-zero elements: 20
Test accuracy for 2: 0.934
Num non-zero elements: 20
Test accuracy for 6: 0.986
Num non-zero elements: 20
Test accuracy for 9: 0.952
Num non-zero elements: 50
Test accuracy for 0: 0.914
Num non-zero elements: 50
Test accuracy for 1: 0.958
Num non-zero elements: 50
Test accuracy for 2: 0.93
Num non-zero elements: 50
Test accuracy for 6: 0.992
Num non-zero elements: 50
Test accuracy for 9: 0.968
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.965
Num non-zero elements: 2304
Test accuracy for 13: 0.947
Num non-zero elements: 2304
Test accuracy for 14: 0.949
Num non-zero elements: 2304
Test accuracy for 18: 0.927
Num non-zero elements: 2304
Test accuracy for 19: 0.961
Num non-zero elements: 1
Test accuracy for 11: 0.549
Num non-zero elements: 1
Test accuracy for 13: 0.664
Num non-zero elements: 1
Test accuracy for 14: 0.648
Num non-zero elements: 1
Test accuracy for 18: 0.709
Num non-zero elements: 1
Test accuracy for 19: 0.805
Num non-zero elements: 2
Test accuracy for 11: 0.687
Num non-zero elements: 2
Test accuracy for 13: 0.718
Num non-zero elements: 2
Test accuracy for 14: 0.673
Num non-zero elements: 2
Test accuracy for 18: 0.731
Num non-zero elements: 2
Test accuracy for 19: 0.769
Num non-zero elements: 5
Test accuracy for 11: 0.787
Num non-zero elements: 5
Test accuracy for 13: 0.745
Num non-zero elements: 5
Test accuracy for 14: 0.724
Num non-zero elements: 5
Test accuracy for 18: 0.722
Num non-zero elements: 5
Test accuracy for 19: 0.838
Num non-zero elements: 10
Test accuracy for 11: 0.824
Num non-zero elements: 10
Test accuracy for 13: 0.793
Num non-zero elements: 10
Test accuracy for 14: 0.769
Num non-zero elements: 10
Test accuracy for 18: 0.745
Num non-zero elements: 10
Test accuracy for 19: 0.861
Num non-zero elements: 20
Test accuracy for 11: 0.913
Num non-zero elements: 20
Test accuracy for 13: 0.889
Num non-zero elements: 20
Test accuracy for 14: 0.868
Num non-zero elements: 20
Test accuracy for 18: 0.778
Num non-zero elements: 20
Test accuracy for 19: 0.892
Num non-zero elements: 50
Test accuracy for 11: 0.931
Num non-zero elements: 50
Test accuracy for 13: 0.925
Num non-zero elements: 50
Test accuracy for 14: 0.904
Num non-zero elements: 50
Test accuracy for 18: 0.833
Num non-zero elements: 50
Test accuracy for 19: 0.929
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 11: 0.9510000348091125
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 13: 0.9390000700950623
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 14: 0.9430000185966492
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 18: 0.9140000343322754
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 19: 0.956000030040741
Num non-zero elements: 1
Test accuracy for 11: 0.723
Num non-zero elements: 1
Test accuracy for 13: 0.702
Num non-zero elements: 1
Test accuracy for 14: 0.824
Num non-zero elements: 1
Test accuracy for 18: 0.728
Num non-zero elements: 1
Test accuracy for 19: 0.842
Num non-zero elements: 2
Test accuracy for 11: 0.882
Num non-zero elements: 2
Test accuracy for 13: 0.701
Num non-zero elements: 2
Test accuracy for 14: 0.829
Num non-zero elements: 2
Test accuracy for 18: 0.735
Num non-zero elements: 2
Test accuracy for 19: 0.856
Num non-zero elements: 5
Test accuracy for 11: 0.879
Num non-zero elements: 5
Test accuracy for 13: 0.801
Num non-zero elements: 5
Test accuracy for 14: 0.845
Num non-zero elements: 5
Test accuracy for 18: 0.841
Num non-zero elements: 5
Test accuracy for 19: 0.883
Num non-zero elements: 10
Test accuracy for 11: 0.945
Num non-zero elements: 10
Test accuracy for 13: 0.906
Num non-zero elements: 10
Test accuracy for 14: 0.844
Num non-zero elements: 10
Test accuracy for 18: 0.855
Num non-zero elements: 10
Test accuracy for 19: 0.901
Num non-zero elements: 20
Test accuracy for 11: 0.952
Num non-zero elements: 20
Test accuracy for 13: 0.922
Num non-zero elements: 20
Test accuracy for 14: 0.942
Num non-zero elements: 20
Test accuracy for 18: 0.897
Num non-zero elements: 20
Test accuracy for 19: 0.912
Num non-zero elements: 50
Test accuracy for 11: 0.963
Num non-zero elements: 50
Test accuracy for 13: 0.932
Num non-zero elements: 50
Test accuracy for 14: 0.939
Num non-zero elements: 50
Test accuracy for 18: 0.904
Num non-zero elements: 50
Test accuracy for 19: 0.935
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.948
Num non-zero elements: 2304
Test accuracy for 21: 0.9
Num non-zero elements: 2304
Test accuracy for 22: 0.893
Num non-zero elements: 2304
Test accuracy for 25: 0.956
Num non-zero elements: 2304
Test accuracy for 26: 0.863
Num non-zero elements: 1
Test accuracy for 20: 0.724
Num non-zero elements: 1
Test accuracy for 21: 0.763
Num non-zero elements: 1
Test accuracy for 22: 0.64
Num non-zero elements: 1
Test accuracy for 25: 0.699
Num non-zero elements: 1
Test accuracy for 26: 0.623
Num non-zero elements: 2
Test accuracy for 20: 0.803
Num non-zero elements: 2
Test accuracy for 21: 0.753
Num non-zero elements: 2
Test accuracy for 22: 0.662
Num non-zero elements: 2
Test accuracy for 25: 0.771
Num non-zero elements: 2
Test accuracy for 26: 0.678
Num non-zero elements: 5
Test accuracy for 20: 0.82
Num non-zero elements: 5
Test accuracy for 21: 0.798
Num non-zero elements: 5
Test accuracy for 22: 0.722
Num non-zero elements: 5
Test accuracy for 25: 0.811
Num non-zero elements: 5
Test accuracy for 26: 0.672
Num non-zero elements: 10
Test accuracy for 20: 0.873
Num non-zero elements: 10
Test accuracy for 21: 0.82
Num non-zero elements: 10
Test accuracy for 22: 0.746
Num non-zero elements: 10
Test accuracy for 25: 0.847
Num non-zero elements: 10
Test accuracy for 26: 0.763
Num non-zero elements: 20
Test accuracy for 20: 0.911
Num non-zero elements: 20
Test accuracy for 21: 0.84
Num non-zero elements: 20
Test accuracy for 22: 0.78
Num non-zero elements: 20
Test accuracy for 25: 0.911
Num non-zero elements: 20
Test accuracy for 26: 0.804
Num non-zero elements: 50
Test accuracy for 20: 0.929
Num non-zero elements: 50
Test accuracy for 21: 0.871
Num non-zero elements: 50
Test accuracy for 22: 0.87
Num non-zero elements: 50
Test accuracy for 25: 0.925
Num non-zero elements: 50
Test accuracy for 26: 0.851
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 20: 0.9460000395774841
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 21: 0.9290000200271606
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 22: 0.909000039100647
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 25: 0.9590000510215759
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 26: 0.89000004529953
Num non-zero elements: 1
Test accuracy for 20: 0.827
Num non-zero elements: 1
Test accuracy for 21: 0.78
Num non-zero elements: 1
Test accuracy for 22: 0.882
Num non-zero elements: 1
Test accuracy for 25: 0.872
Num non-zero elements: 1
Test accuracy for 26: 0.708
Num non-zero elements: 2
Test accuracy for 20: 0.877
Num non-zero elements: 2
Test accuracy for 21: 0.829
Num non-zero elements: 2
Test accuracy for 22: 0.877
Num non-zero elements: 2
Test accuracy for 25: 0.88
Num non-zero elements: 2
Test accuracy for 26: 0.77
Num non-zero elements: 5
Test accuracy for 20: 0.905
Num non-zero elements: 5
Test accuracy for 21: 0.845
Num non-zero elements: 5
Test accuracy for 22: 0.885
Num non-zero elements: 5
Test accuracy for 25: 0.895
Num non-zero elements: 5
Test accuracy for 26: 0.803
Num non-zero elements: 10
Test accuracy for 20: 0.923
Num non-zero elements: 10
Test accuracy for 21: 0.847
Num non-zero elements: 10
Test accuracy for 22: 0.896
Num non-zero elements: 10
Test accuracy for 25: 0.915
Num non-zero elements: 10
Test accuracy for 26: 0.848
Num non-zero elements: 20
Test accuracy for 20: 0.936
Num non-zero elements: 20
Test accuracy for 21: 0.873
Num non-zero elements: 20
Test accuracy for 22: 0.891
Num non-zero elements: 20
Test accuracy for 25: 0.943
Num non-zero elements: 20
Test accuracy for 26: 0.865
Num non-zero elements: 50
Test accuracy for 20: 0.939
Num non-zero elements: 50
Test accuracy for 21: 0.896
Num non-zero elements: 50
Test accuracy for 22: 0.896
Num non-zero elements: 50
Test accuracy for 25: 0.957
Num non-zero elements: 50
Test accuracy for 26: 0.884
Removing 28 from train set
Removing 0 from train set
Removing 31 from train set
Removing 7 from train set
Removing 29 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.949
Num non-zero elements: 2304
Test accuracy for 2: 0.915
Num non-zero elements: 2304
Test accuracy for 3: 0.912
Num non-zero elements: 2304
Test accuracy for 5: 0.912
Num non-zero elements: 2304
Test accuracy for 6: 0.822
Num non-zero elements: 1
Test accuracy for 1: 0.677
Num non-zero elements: 1
Test accuracy for 2: 0.594
Num non-zero elements: 1
Test accuracy for 3: 0.583
Num non-zero elements: 1
Test accuracy for 5: 0.556
Num non-zero elements: 1
Test accuracy for 6: 0.591
Num non-zero elements: 2
Test accuracy for 1: 0.74
Num non-zero elements: 2
Test accuracy for 2: 0.632
Num non-zero elements: 2
Test accuracy for 3: 0.613
Num non-zero elements: 2
Test accuracy for 5: 0.622
Num non-zero elements: 2
Test accuracy for 6: 0.634
Num non-zero elements: 5
Test accuracy for 1: 0.782
Num non-zero elements: 5
Test accuracy for 2: 0.661
Num non-zero elements: 5
Test accuracy for 3: 0.653
Num non-zero elements: 5
Test accuracy for 5: 0.649
Num non-zero elements: 5
Test accuracy for 6: 0.67
Num non-zero elements: 10
Test accuracy for 1: 0.819
Num non-zero elements: 10
Test accuracy for 2: 0.767
Num non-zero elements: 10
Test accuracy for 3: 0.74
Num non-zero elements: 10
Test accuracy for 5: 0.717
Num non-zero elements: 10
Test accuracy for 6: 0.716
Num non-zero elements: 20
Test accuracy for 1: 0.863
Num non-zero elements: 20
Test accuracy for 2: 0.83
Num non-zero elements: 20
Test accuracy for 3: 0.787
Num non-zero elements: 20
Test accuracy for 5: 0.8
Num non-zero elements: 20
Test accuracy for 6: 0.748
Num non-zero elements: 50
Test accuracy for 1: 0.914
Num non-zero elements: 50
Test accuracy for 2: 0.89
Num non-zero elements: 50
Test accuracy for 3: 0.842
Num non-zero elements: 50
Test accuracy for 5: 0.847
Num non-zero elements: 50
Test accuracy for 6: 0.779
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 1: 0.9300000667572021
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 2: 0.9050000309944153
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 3: 0.9130000472068787
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 5: 0.906000018119812
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 6: 0.8500000238418579
Num non-zero elements: 1
Test accuracy for 1: 0.738
Num non-zero elements: 1
Test accuracy for 2: 0.638
Num non-zero elements: 1
Test accuracy for 3: 0.619
Num non-zero elements: 1
Test accuracy for 5: 0.599
Num non-zero elements: 1
Test accuracy for 6: 0.666
Num non-zero elements: 2
Test accuracy for 1: 0.855
Num non-zero elements: 2
Test accuracy for 2: 0.705
Num non-zero elements: 2
Test accuracy for 3: 0.621
Num non-zero elements: 2
Test accuracy for 5: 0.581
Num non-zero elements: 2
Test accuracy for 6: 0.731
Num non-zero elements: 5
Test accuracy for 1: 0.911
Num non-zero elements: 5
Test accuracy for 2: 0.765
Num non-zero elements: 5
Test accuracy for 3: 0.704
Num non-zero elements: 5
Test accuracy for 5: 0.811
Num non-zero elements: 5
Test accuracy for 6: 0.746
Num non-zero elements: 10
Test accuracy for 1: 0.923
Num non-zero elements: 10
Test accuracy for 2: 0.771
Num non-zero elements: 10
Test accuracy for 3: 0.76
Num non-zero elements: 10
Test accuracy for 5: 0.843
Num non-zero elements: 10
Test accuracy for 6: 0.761
Num non-zero elements: 20
Test accuracy for 1: 0.931
Num non-zero elements: 20
Test accuracy for 2: 0.862
Num non-zero elements: 20
Test accuracy for 3: 0.841
Num non-zero elements: 20
Test accuracy for 5: 0.86
Num non-zero elements: 20
Test accuracy for 6: 0.796
Num non-zero elements: 50
Test accuracy for 1: 0.923
Num non-zero elements: 50
Test accuracy for 2: 0.894
Num non-zero elements: 50
Test accuracy for 3: 0.906
Num non-zero elements: 50
Test accuracy for 5: 0.885
Num non-zero elements: 50
Test accuracy for 6: 0.781
Num non-zero elements: 2304
Test accuracy for 1.0: 0.982
Num non-zero elements: 2304
Test accuracy for 5.0: 0.982
Num non-zero elements: 1
Test accuracy for 1.0: 0.673
Num non-zero elements: 1
Test accuracy for 5.0: 0.673
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.848
Num non-zero elements: 20
Test accuracy for 5.0: 0.848
Num non-zero elements: 50
Test accuracy for 1.0: 0.932
Num non-zero elements: 50
Test accuracy for 5.0: 0.932
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 1.0: 0.9620000720024109
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 5.0: 0.9610000252723694
Num non-zero elements: 1
Test accuracy for 1.0: 0.826
Num non-zero elements: 1
Test accuracy for 5.0: 0.826
Num non-zero elements: 2
Test accuracy for 1.0: 0.82
Num non-zero elements: 2
Test accuracy for 5.0: 0.82
Num non-zero elements: 5
Test accuracy for 1.0: 0.892
Num non-zero elements: 5
Test accuracy for 5.0: 0.892
Num non-zero elements: 10
Test accuracy for 1.0: 0.907
Num non-zero elements: 10
Test accuracy for 5.0: 0.907
Num non-zero elements: 20
Test accuracy for 1.0: 0.932
Num non-zero elements: 20
Test accuracy for 5.0: 0.932
Num non-zero elements: 50
Test accuracy for 1.0: 0.947
Num non-zero elements: 50
Test accuracy for 5.0: 0.947
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.946
Num non-zero elements: 2304
Test accuracy for Python: 0.988
Num non-zero elements: 2304
Test accuracy for HTML: 0.982
Num non-zero elements: 2304
Test accuracy for Java: 0.962
Num non-zero elements: 2304
Test accuracy for PHP: 0.955
Num non-zero elements: 1
Test accuracy for C: 0.656
Num non-zero elements: 1
Test accuracy for Python: 0.649
Num non-zero elements: 1
Test accuracy for HTML: 0.785
Num non-zero elements: 1
Test accuracy for Java: 0.634
Num non-zero elements: 1
Test accuracy for PHP: 0.585
Num non-zero elements: 2
Test accuracy for C: 0.658
Num non-zero elements: 2
Test accuracy for Python: 0.67
Num non-zero elements: 2
Test accuracy for HTML: 0.81
Num non-zero elements: 2
Test accuracy for Java: 0.685
Num non-zero elements: 2
Test accuracy for PHP: 0.656
Num non-zero elements: 5
Test accuracy for C: 0.749
Num non-zero elements: 5
Test accuracy for Python: 0.722
Num non-zero elements: 5
Test accuracy for HTML: 0.897
Num non-zero elements: 5
Test accuracy for Java: 0.742
Num non-zero elements: 5
Test accuracy for PHP: 0.696
Num non-zero elements: 10
Test accuracy for C: 0.784
Num non-zero elements: 10
Test accuracy for Python: 0.754
Num non-zero elements: 10
Test accuracy for HTML: 0.954
Num non-zero elements: 10
Test accuracy for Java: 0.774
Num non-zero elements: 10
Test accuracy for PHP: 0.724
Num non-zero elements: 20
Test accuracy for C: 0.871
Num non-zero elements: 20
Test accuracy for Python: 0.861
Num non-zero elements: 20
Test accuracy for HTML: 0.96
Num non-zero elements: 20
Test accuracy for Java: 0.841
Num non-zero elements: 20
Test accuracy for PHP: 0.827
Num non-zero elements: 50
Test accuracy for C: 0.915
Num non-zero elements: 50
Test accuracy for Python: 0.953
Num non-zero elements: 50
Test accuracy for HTML: 0.967
Num non-zero elements: 50
Test accuracy for Java: 0.907
Num non-zero elements: 50
Test accuracy for PHP: 0.904
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for C: 0.9450000524520874
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for Python: 0.9760000705718994
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for HTML: 0.9880000352859497
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 36 epochs
Test accuracy for Java: 0.9610000252723694
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for PHP: 0.9550000429153442
Num non-zero elements: 1
Test accuracy for C: 0.598
Num non-zero elements: 1
Test accuracy for Python: 0.618
Num non-zero elements: 1
Test accuracy for HTML: 0.904
Num non-zero elements: 1
Test accuracy for Java: 0.646
Num non-zero elements: 1
Test accuracy for PHP: 0.607
Num non-zero elements: 2
Test accuracy for C: 0.616
Num non-zero elements: 2
Test accuracy for Python: 0.606
Num non-zero elements: 2
Test accuracy for HTML: 0.911
Num non-zero elements: 2
Test accuracy for Java: 0.691
Num non-zero elements: 2
Test accuracy for PHP: 0.891
Num non-zero elements: 5
Test accuracy for C: 0.696
Num non-zero elements: 5
Test accuracy for Python: 0.843
Num non-zero elements: 5
Test accuracy for HTML: 0.936
Num non-zero elements: 5
Test accuracy for Java: 0.702
Num non-zero elements: 5
Test accuracy for PHP: 0.918
Num non-zero elements: 10
Test accuracy for C: 0.843
Num non-zero elements: 10
Test accuracy for Python: 0.863
Num non-zero elements: 10
Test accuracy for HTML: 0.951
Num non-zero elements: 10
Test accuracy for Java: 0.82
Num non-zero elements: 10
Test accuracy for PHP: 0.909
Num non-zero elements: 20
Test accuracy for C: 0.878
Num non-zero elements: 20
Test accuracy for Python: 0.897
Num non-zero elements: 20
Test accuracy for HTML: 0.958
Num non-zero elements: 20
Test accuracy for Java: 0.837
Num non-zero elements: 20
Test accuracy for PHP: 0.925
Num non-zero elements: 50
Test accuracy for C: 0.877
Num non-zero elements: 50
Test accuracy for Python: 0.95
Num non-zero elements: 50
Test accuracy for HTML: 0.963
Num non-zero elements: 50
Test accuracy for Java: 0.894
Num non-zero elements: 50
Test accuracy for PHP: 0.928
Num non-zero elements: 2304
Test accuracy for 0: 0.923
Num non-zero elements: 2304
Test accuracy for 1: 0.987
Num non-zero elements: 2304
Test accuracy for 2: 0.899
Num non-zero elements: 2304
Test accuracy for 3: 0.933
Num non-zero elements: 1
Test accuracy for 0: 0.684
Num non-zero elements: 1
Test accuracy for 1: 0.672
Num non-zero elements: 1
Test accuracy for 2: 0.66
Num non-zero elements: 1
Test accuracy for 3: 0.737
Num non-zero elements: 2
Test accuracy for 0: 0.711
Num non-zero elements: 2
Test accuracy for 1: 0.802
Num non-zero elements: 2
Test accuracy for 2: 0.683
Num non-zero elements: 2
Test accuracy for 3: 0.749
Num non-zero elements: 5
Test accuracy for 0: 0.812
Num non-zero elements: 5
Test accuracy for 1: 0.885
Num non-zero elements: 5
Test accuracy for 2: 0.748
Num non-zero elements: 5
Test accuracy for 3: 0.843
Num non-zero elements: 10
Test accuracy for 0: 0.867
Num non-zero elements: 10
Test accuracy for 1: 0.929
Num non-zero elements: 10
Test accuracy for 2: 0.789
Num non-zero elements: 10
Test accuracy for 3: 0.877
Num non-zero elements: 20
Test accuracy for 0: 0.884
Num non-zero elements: 20
Test accuracy for 1: 0.96
Num non-zero elements: 20
Test accuracy for 2: 0.868
Num non-zero elements: 20
Test accuracy for 3: 0.893
Num non-zero elements: 50
Test accuracy for 0: 0.903
Num non-zero elements: 50
Test accuracy for 1: 0.967
Num non-zero elements: 50
Test accuracy for 2: 0.895
Num non-zero elements: 50
Test accuracy for 3: 0.929
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 0: 0.9330000281333923
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 1: 0.9850000739097595
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 2: 0.9270000457763672
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for 3: 0.940000057220459
Num non-zero elements: 1
Test accuracy for 0: 0.795
Num non-zero elements: 1
Test accuracy for 1: 0.947
Num non-zero elements: 1
Test accuracy for 2: 0.838
Num non-zero elements: 1
Test accuracy for 3: 0.725
Num non-zero elements: 2
Test accuracy for 0: 0.816
Num non-zero elements: 2
Test accuracy for 1: 0.961
Num non-zero elements: 2
Test accuracy for 2: 0.843
Num non-zero elements: 2
Test accuracy for 3: 0.832
Num non-zero elements: 5
Test accuracy for 0: 0.887
Num non-zero elements: 5
Test accuracy for 1: 0.977
Num non-zero elements: 5
Test accuracy for 2: 0.859
Num non-zero elements: 5
Test accuracy for 3: 0.886
Num non-zero elements: 10
Test accuracy for 0: 0.913
Num non-zero elements: 10
Test accuracy for 1: 0.975
Num non-zero elements: 10
Test accuracy for 2: 0.864
Num non-zero elements: 10
Test accuracy for 3: 0.912
Num non-zero elements: 20
Test accuracy for 0: 0.926
Num non-zero elements: 20
Test accuracy for 1: 0.974
Num non-zero elements: 20
Test accuracy for 2: 0.903
Num non-zero elements: 20
Test accuracy for 3: 0.925
Num non-zero elements: 50
Test accuracy for 0: 0.913
Num non-zero elements: 50
Test accuracy for 1: 0.976
Num non-zero elements: 50
Test accuracy for 2: 0.913
Num non-zero elements: 50
Test accuracy for 3: 0.923
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.998
Num non-zero elements: 2304
Test accuracy for fr: 1.0
Num non-zero elements: 2304
Test accuracy for de: 0.999
Num non-zero elements: 2304
Test accuracy for es: 1.0
Num non-zero elements: 2304
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.734
Num non-zero elements: 1
Test accuracy for fr: 0.591
Num non-zero elements: 1
Test accuracy for de: 0.747
Num non-zero elements: 1
Test accuracy for es: 0.497
Num non-zero elements: 1
Test accuracy for nl: 0.646
Num non-zero elements: 2
Test accuracy for en: 0.832
Num non-zero elements: 2
Test accuracy for fr: 0.597
Num non-zero elements: 2
Test accuracy for de: 0.819
Num non-zero elements: 2
Test accuracy for es: 0.916
Num non-zero elements: 2
Test accuracy for nl: 0.74
Num non-zero elements: 5
Test accuracy for en: 0.89
Num non-zero elements: 5
Test accuracy for fr: 0.908
Num non-zero elements: 5
Test accuracy for de: 0.906
Num non-zero elements: 5
Test accuracy for es: 0.979
Num non-zero elements: 5
Test accuracy for nl: 0.868
Num non-zero elements: 10
Test accuracy for en: 0.98
Num non-zero elements: 10
Test accuracy for fr: 0.971
Num non-zero elements: 10
Test accuracy for de: 0.964
Num non-zero elements: 10
Test accuracy for es: 0.988
Num non-zero elements: 10
Test accuracy for nl: 0.921
Num non-zero elements: 20
Test accuracy for en: 0.996
Num non-zero elements: 20
Test accuracy for fr: 0.991
Num non-zero elements: 20
Test accuracy for de: 0.989
Num non-zero elements: 20
Test accuracy for es: 0.998
Num non-zero elements: 20
Test accuracy for nl: 0.976
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 0.999
Num non-zero elements: 50
Test accuracy for de: 0.996
Num non-zero elements: 50
Test accuracy for es: 1.0
Num non-zero elements: 50
Test accuracy for nl: 0.994
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for en: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for fr: 0.9940000176429749
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for de: 0.9790000319480896
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for es: 0.984000027179718
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for nl: 0.9720000624656677
Num non-zero elements: 1
Test accuracy for en: 0.999
Num non-zero elements: 1
Test accuracy for fr: 0.978
Num non-zero elements: 1
Test accuracy for de: 0.712
Num non-zero elements: 1
Test accuracy for es: 0.574
Num non-zero elements: 1
Test accuracy for nl: 0.639
Num non-zero elements: 2
Test accuracy for en: 0.999
Num non-zero elements: 2
Test accuracy for fr: 0.98
Num non-zero elements: 2
Test accuracy for de: 0.77
Num non-zero elements: 2
Test accuracy for es: 0.688
Num non-zero elements: 2
Test accuracy for nl: 0.794
Num non-zero elements: 5
Test accuracy for en: 1.0
Num non-zero elements: 5
Test accuracy for fr: 0.991
Num non-zero elements: 5
Test accuracy for de: 0.818
Num non-zero elements: 5
Test accuracy for es: 0.963
Num non-zero elements: 5
Test accuracy for nl: 0.832
Num non-zero elements: 10
Test accuracy for en: 0.998
Num non-zero elements: 10
Test accuracy for fr: 0.992
Num non-zero elements: 10
Test accuracy for de: 0.841
Num non-zero elements: 10
Test accuracy for es: 0.975
Num non-zero elements: 10
Test accuracy for nl: 0.833
Num non-zero elements: 20
Test accuracy for en: 0.998
Num non-zero elements: 20
Test accuracy for fr: 0.984
Num non-zero elements: 20
Test accuracy for de: 0.904
Num non-zero elements: 20
Test accuracy for es: 0.981
Num non-zero elements: 20
Test accuracy for nl: 0.866
Num non-zero elements: 50
Test accuracy for en: 0.997
Num non-zero elements: 50
Test accuracy for fr: 0.988
Num non-zero elements: 50
Test accuracy for de: 0.935
Num non-zero elements: 50
Test accuracy for es: 0.987
Num non-zero elements: 50
Test accuracy for nl: 0.888
Time spent on sparse_probing: 702.77s

[2025-02-07 20:40:27]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
24432 16027
1962 859
torch.Size([6378, 1024]) torch.Size([275, 1024])
Forget sparsity: [0.       0.000179 0.006867 ... 0.       0.       0.      ]
(16384,)
Retain sparsity: [0.       0.000254 0.005989 ... 0.       0.       0.      ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15089  7910 10405 ...   890  7894 13590]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15089,  7910, 10405,  1419, 13500, 14163,  7019, 12806,  4247,
       11057]), array([15089,  7910, 10405,  1419, 13500, 14163,  7019, 12806,  4247,
       11057, 13171,    87, 11403,  2209, 12639,  4267, 13120,  4187,
       11134, 13296])], 'multiplier': [25, 50, 100, 200]}
Saving target question ids for gemma-2-2b-it on wmdp-bio...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct.csv saved, with 267 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct.csv saved, with 267 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct.csv saved, with 534 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct-iff-question.csv saved, with 244 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct-iff-question.csv saved, with 244 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct-iff-question.csv saved, with 488 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct-no-tricks.csv saved, with 263 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct-no-tricks.csv saved, with 263 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct-no-tricks.csv saved, with 526 questions
Saving target question ids for gemma-2-2b-it on high_school_us_history...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct.csv saved, with 54 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct.csv saved, with 109 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct-iff-question.csv saved, with 53 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct-iff-question.csv saved, with 54 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct-iff-question.csv saved, with 107 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 54 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 109 questions
Saving target question ids for gemma-2-2b-it on college_computer_science...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct.csv saved, with 9 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct-iff-question.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct-iff-question.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct-iff-question.csv saved, with 9 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct-no-tricks.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct-no-tricks.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct-no-tricks.csv saved, with 9 questions
Saving target question ids for gemma-2-2b-it on high_school_geography...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct.csv saved, with 51 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct.csv saved, with 52 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct.csv saved, with 103 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct-iff-question.csv saved, with 50 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct-iff-question.csv saved, with 50 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct-iff-question.csv saved, with 100 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct-no-tricks.csv saved, with 51 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct-no-tricks.csv saved, with 52 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct-no-tricks.csv saved, with 103 questions
Saving target question ids for gemma-2-2b-it on human_aging...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct.csv saved, with 84 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct-iff-question.csv saved, with 40 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct-iff-question.csv saved, with 41 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct-iff-question.csv saved, with 81 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct-no-tricks.csv saved, with 41 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct-no-tricks.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct-no-tricks.csv saved, with 83 questions
Calculating metrics for retain threshold: 0.01
Top features for ablation: [11777  4358 10253 ...  3312 16237  6063]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([11777,  4358, 10253,  1675,  1053,   727,  2904,  4747,  9377,
       15089]), array([11777,  4358, 10253,  1675,  1053,   727,  2904,  4747,  9377,
       15089,  7910, 10405, 12659, 14838,  8331,  1419, 13500,  5157,
       16284, 13934])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   0.998127                1.000000  ...          25              1.000000
1   0.941948                0.954128  ...         200              0.963934
2   0.973783                1.000000  ...         100              0.990164
3   0.945693                1.000000  ...         200              0.996721
4   0.882023                0.963303  ...         200              0.957377
5   0.992509                1.000000  ...          50              0.996721
6   0.986891                1.000000  ...         100              1.000000
7   0.985019                1.000000  ...         100              1.000000
8   1.000000                1.000000  ...          50              1.000000
9   0.994382                1.000000  ...          50              1.000000
10  0.996255                1.000000  ...          25              1.000000
11  0.945693                1.000000  ...         100              0.983607
12  1.000000                1.000000  ...          25              1.000000
13  1.000000                1.000000  ...          25              1.000000
14  0.958802                1.000000  ...         200              0.996721
15  0.996255                1.000000  ...          50              1.000000

[16 rows x 10 columns]
