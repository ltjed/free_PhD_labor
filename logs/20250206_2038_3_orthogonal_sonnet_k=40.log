Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.

Generating idea 1/3
Iteration 1/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces sparsity-weighted orthogonality constraints, which is promising but could be refined further. I notice that the competition coefficients c_(ij) are static within each batch, which may not capture the dynamic nature of feature competition. Additionally, the binary intersection of sparsity masks might be too coarse-grained.

I propose enhancing this approach by introducing "activation-weighted orthogonality" - instead of just using binary masks, we weight the orthogonality constraints by the actual activation magnitudes. This creates a more nuanced measure of feature competition that considers not just which features co-activate, but how strongly they co-activate. Features with stronger co-activation will face stronger pressure to become orthogonal.

This refinement maintains the core insight and computational efficiency of the prototype while providing more granular control over feature separation. It requires minimal additional computation since we already have the activation values from the forward pass. The key advantage is that it should better handle cases where features partially overlap or compete with different intensities, leading to more natural feature separation.

NEW IDEA JSON:
```json
{
    "Name": "activation_weighted_orthogonal_sae",
    "Title": "Activation-Weighted Orthogonality for Dynamic Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Modify loss function to use activation magnitudes as weights\n2. Implement efficient computation of activation-weighted feature similarities\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and sparsity-weighted orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method enhances sparsity-weighted orthogonality with activation magnitudes: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) w_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (max_k |a_k|)^2 for activations a_i, a_j of features i,j. The weights w_(ij) dynamically adjust orthogonality pressure based on activation strengths, naturally handling partial feature overlap. The normalization by max activation prevents numerical instability while preserving relative activation strengths.",
    "Implementation_Plan": "1. Modify compute_loss in AutoEncoderTopK to track activation magnitudes\n2. Add function to compute activation-weighted similarities\n3. Update orthogonality loss computation\n4. Add configuration for activation weighting\n5. Add metrics to track dynamic feature competition\n6. Optimize matrix operations for efficiency",
    "Interestingness_Evaluation": "Using continuous activation values rather than binary masks provides more nuanced control over feature separation while maintaining computational simplicity.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only minor modifications to existing loss computation; uses standard tensor operations; maintains O(n^2) complexity; easily runs within 30-minute limit on H100; implementation complexity suitable for 1-month timeline.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing orthogonality constraints, the dynamic activation-weighted approach is novel and specifically tailored for sparse autoencoder interpretability.",
    "Novelty": 7,
    "Expected_Research_Impact": "The more granular control over feature separation should improve sparse_probing scores by learning cleaner features, while maintaining good reconstruction performance for core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We propose an activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on activation magnitudes. Unlike previous approaches that use binary sparsity masks to guide orthogonality constraints, our method leverages the continuous values of feature activations to create a more nuanced measure of feature competition. This allows the model to naturally handle cases of partial feature overlap and varying degrees of competition intensity. The approach maintains computational efficiency by utilizing activation values already computed in the forward pass, requiring minimal additional computation. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'activation_weighted_orthogonal_sae', 'Title': 'Activation-Weighted Orthogonality for Dynamic Feature Separation in Sparse Autoencoders', 'Experiment': '1. Modify loss function to use activation magnitudes as weights\n2. Implement efficient computation of activation-weighted feature similarities\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and sparsity-weighted orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality with activation magnitudes: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) w_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (max_k |a_k|)^2 for activations a_i, a_j of features i,j. The weights w_(ij) dynamically adjust orthogonality pressure based on activation strengths, naturally handling partial feature overlap. The normalization by max activation prevents numerical instability while preserving relative activation strengths.', 'Implementation_Plan': '1. Modify compute_loss in AutoEncoderTopK to track activation magnitudes\n2. Add function to compute activation-weighted similarities\n3. Update orthogonality loss computation\n4. Add configuration for activation weighting\n5. Add metrics to track dynamic feature competition\n6. Optimize matrix operations for efficiency', 'Interestingness_Evaluation': 'Using continuous activation values rather than binary masks provides more nuanced control over feature separation while maintaining computational simplicity.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only minor modifications to existing loss computation; uses standard tensor operations; maintains O(n^2) complexity; easily runs within 30-minute limit on H100; implementation complexity suitable for 1-month timeline.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing orthogonality constraints, the dynamic activation-weighted approach is novel and specifically tailored for sparse autoencoder interpretability.', 'Novelty': 7, 'Expected_Research_Impact': 'The more granular control over feature separation should improve sparse_probing scores by learning cleaner features, while maintaining good reconstruction performance for core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We propose an activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on activation magnitudes. Unlike previous approaches that use binary sparsity masks to guide orthogonality constraints, our method leverages the continuous values of feature activations to create a more nuanced measure of feature competition. This allows the model to naturally handle cases of partial feature overlap and varying degrees of competition intensity. The approach maintains computational efficiency by utilizing activation values already computed in the forward pass, requiring minimal additional computation. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model.'}
Iteration 2/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at my previous idea, I realize several areas for improvement:

1. The ratings were too conservative - the idea maintains the simplicity of the prototype while adding meaningful refinement
2. The technical details could be more precise about implementation efficiency
3. The weighting scheme could be simplified while maintaining effectiveness

The key insight remains using activation magnitudes, but we can make it more elegant by using a simpler normalization scheme: instead of normalizing by max activation squared, we can normalize by the L2 norm of the activations vector. This maintains numerical stability while being more theoretically grounded (cosine similarity interpretation) and computationally efficient (can use existing norm computations from the SAE).

Additionally, we can leverage the existing top-k sparsity mechanism to only compute weights for the k most active features, maintaining efficiency. This creates a natural bridge between the sparsity mechanism and orthogonality constraints.

These refinements make the idea more theoretically sound and practically efficient while staying true to the prototype's spirit of using activation patterns to guide feature separation.

NEW IDEA JSON:
```json
{
    "Name": "activation_weighted_orthogonal_sae",
    "Title": "Activation-Weighted Orthogonality for Dynamic Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Modify loss function to use L2-normalized activation magnitudes\n2. Implement efficient top-k activation-weighted similarities\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and sparsity-weighted orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method enhances sparsity-weighted orthogonality using L2-normalized activation magnitudes: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 for activations a. By restricting to top-k activations and using L2 normalization, we maintain O(k^2) complexity and numerical stability while creating a natural connection to cosine similarity. The weights w_(ij) dynamically adjust orthogonality pressure based on normalized activation strengths, effectively balancing feature separation with reconstruction quality.",
    "Implementation_Plan": "1. Add L2 normalization to activation computation\n2. Modify top-k selection to track normalized activations\n3. Implement efficient top-k weighted similarity matrix\n4. Update orthogonality loss computation\n5. Add configuration for weighting scheme\n6. Optimize matrix operations for batch processing",
    "Interestingness_Evaluation": "Using L2-normalized activation magnitudes creates an elegant theoretical connection to cosine similarity while maintaining computational efficiency and improving feature separation.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only standard tensor operations; maintains O(k^2) complexity through top-k restriction; uses existing L2 normalization; easily runs within 30-minute limit on H100; straightforward modifications to existing code.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of L2-normalized activation weighting with top-k sparsity creates a novel and theoretically grounded approach to feature separation in SAEs.",
    "Novelty": 8,
    "Expected_Research_Impact": "The theoretically grounded approach to feature separation should significantly improve sparse_probing benchmarks while maintaining strong core metrics through careful balance of reconstruction and orthogonality.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We propose an activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on L2-normalized activation magnitudes. Unlike previous approaches that use binary sparsity masks, our method leverages normalized continuous activation values restricted to top-k features, creating a theoretically grounded measure of feature competition that naturally connects to cosine similarity. The approach maintains computational efficiency through careful integration with existing top-k sparsity mechanisms and L2 normalization. By dynamically adjusting orthogonality constraints based on normalized activation strengths, the method provides principled control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'activation_weighted_orthogonal_sae', 'Title': 'Activation-Weighted Orthogonality for Dynamic Feature Separation in Sparse Autoencoders', 'Experiment': '1. Modify loss function to use L2-normalized activation magnitudes\n2. Implement efficient top-k activation-weighted similarities\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and sparsity-weighted orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality using L2-normalized activation magnitudes: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 for activations a. By restricting to top-k activations and using L2 normalization, we maintain O(k^2) complexity and numerical stability while creating a natural connection to cosine similarity. The weights w_(ij) dynamically adjust orthogonality pressure based on normalized activation strengths, effectively balancing feature separation with reconstruction quality.', 'Implementation_Plan': '1. Add L2 normalization to activation computation\n2. Modify top-k selection to track normalized activations\n3. Implement efficient top-k weighted similarity matrix\n4. Update orthogonality loss computation\n5. Add configuration for weighting scheme\n6. Optimize matrix operations for batch processing', 'Interestingness_Evaluation': 'Using L2-normalized activation magnitudes creates an elegant theoretical connection to cosine similarity while maintaining computational efficiency and improving feature separation.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only standard tensor operations; maintains O(k^2) complexity through top-k restriction; uses existing L2 normalization; easily runs within 30-minute limit on H100; straightforward modifications to existing code.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of L2-normalized activation weighting with top-k sparsity creates a novel and theoretically grounded approach to feature separation in SAEs.', 'Novelty': 8, 'Expected_Research_Impact': 'The theoretically grounded approach to feature separation should significantly improve sparse_probing benchmarks while maintaining strong core metrics through careful balance of reconstruction and orthogonality.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We propose an activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on L2-normalized activation magnitudes. Unlike previous approaches that use binary sparsity masks, our method leverages normalized continuous activation values restricted to top-k features, creating a theoretically grounded measure of feature competition that naturally connects to cosine similarity. The approach maintains computational efficiency through careful integration with existing top-k sparsity mechanisms and L2 normalization. By dynamically adjusting orthogonality constraints based on normalized activation strengths, the method provides principled control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model.'}
Iteration 3/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The previous iteration made good progress, but there's one key refinement we can make to further improve efficiency and interpretability: adaptive weighting based on feature activation frequency. Currently, we normalize all activations uniformly, but some features naturally activate more frequently than others. We can make the orthogonality constraints more targeted by incorporating activation frequency statistics.

Specifically, we can maintain exponential moving averages of feature activation frequencies and use these to scale the weights. This creates stronger orthogonality pressure between frequently co-activating features while being more lenient with rarely co-activating ones. This is computationally efficient (just one extra vector of statistics) and theoretically motivated (focusing separation effort where it matters most).

This refinement maintains all the benefits of L2-normalized activation weighting while adding adaptive behavior that should improve feature separation where it matters most. It's a natural extension that requires minimal additional computation while potentially providing significant benefits for interpretability.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_activation_weighted_orthogonal_sae",
    "Title": "Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Implement activation frequency tracking\n2. Modify loss function to use frequency-scaled weights\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and previous orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method enhances activation-weighted orthogonality with adaptive scaling: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * α_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 and α_(ij) = sqrt(p_i * p_j) for activation frequencies p_i, p_j tracked via exponential moving average (EMA) with decay 0.99. The adaptive weights α_(ij) concentrate orthogonality pressure on frequently co-activating features while maintaining O(k^2) complexity through top-k restriction.",
    "Implementation_Plan": "1. Add EMA tracking for activation frequencies\n2. Modify top-k selection to include frequency scaling\n3. Update weighted similarity computation\n4. Implement efficient batch processing\n5. Add configuration for EMA decay\n6. Optimize matrix operations",
    "Interestingness_Evaluation": "Using activation frequency statistics to adaptively scale orthogonality constraints provides targeted feature separation while maintaining computational simplicity.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only one additional vector of statistics; maintains O(k^2) complexity; uses standard EMA updates; easily runs within 30-minute limit on H100; straightforward addition to existing code.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of activation-weighted orthogonality with adaptive frequency scaling creates a novel approach that efficiently targets feature separation where most needed.",
    "Novelty": 8,
    "Expected_Research_Impact": "The adaptive weighting should significantly improve sparse_probing benchmarks by focusing separation effort on the most problematic feature interactions while maintaining strong core metrics.",
    "Research_Impact": 9,
    "Overall_Score": 8.9,
    "Abstract": "We propose an adaptive activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on both current activation magnitudes and historical activation frequencies. Unlike previous approaches that apply uniform orthogonality constraints, our method uses exponential moving averages to track feature activation patterns and concentrate separation effort on frequently co-activating features. The approach maintains computational efficiency through careful integration with top-k sparsity mechanisms and requires only minimal additional state. By adaptively scaling orthogonality constraints based on both immediate and historical activation patterns, the method provides targeted control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_activation_weighted_orthogonal_sae', 'Title': 'Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Modify loss function to use frequency-scaled weights\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and previous orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances activation-weighted orthogonality with adaptive scaling: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * α_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 and α_(ij) = sqrt(p_i * p_j) for activation frequencies p_i, p_j tracked via exponential moving average (EMA) with decay 0.99. The adaptive weights α_(ij) concentrate orthogonality pressure on frequently co-activating features while maintaining O(k^2) complexity through top-k restriction.', 'Implementation_Plan': '1. Add EMA tracking for activation frequencies\n2. Modify top-k selection to include frequency scaling\n3. Update weighted similarity computation\n4. Implement efficient batch processing\n5. Add configuration for EMA decay\n6. Optimize matrix operations', 'Interestingness_Evaluation': 'Using activation frequency statistics to adaptively scale orthogonality constraints provides targeted feature separation while maintaining computational simplicity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only one additional vector of statistics; maintains O(k^2) complexity; uses standard EMA updates; easily runs within 30-minute limit on H100; straightforward addition to existing code.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of activation-weighted orthogonality with adaptive frequency scaling creates a novel approach that efficiently targets feature separation where most needed.', 'Novelty': 8, 'Expected_Research_Impact': 'The adaptive weighting should significantly improve sparse_probing benchmarks by focusing separation effort on the most problematic feature interactions while maintaining strong core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We propose an adaptive activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on both current activation magnitudes and historical activation frequencies. Unlike previous approaches that apply uniform orthogonality constraints, our method uses exponential moving averages to track feature activation patterns and concentrate separation effort on frequently co-activating features. The approach maintains computational efficiency through careful integration with top-k sparsity mechanisms and requires only minimal additional state. By adaptively scaling orthogonality constraints based on both immediate and historical activation patterns, the method provides targeted control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model.'}
Iteration 4/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current idea is well-developed and strikes an excellent balance between effectiveness and simplicity. Looking at potential improvements:

1. We could add more complex tracking mechanisms - but this would violate our principle of minimizing complexity
2. We could modify the architecture - but this would deviate from the prototype idea
3. We could adjust the weighting formulas - but the current approach using EMA and L2 normalization is already theoretically sound and computationally efficient

The current implementation:
- Maintains O(k^2) complexity
- Uses standard operations (EMA, L2 norm)
- Requires minimal additional state
- Has clear theoretical motivation
- Stays true to the prototype idea
- Is feasible to implement within time constraints
- Should effectively improve benchmark performance

Any further modifications would likely add complexity without proportional benefits. The idea has reached a sweet spot between sophistication and simplicity.

I am done.

```json
{
    "Name": "adaptive_activation_weighted_orthogonal_sae",
    "Title": "Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Implement activation frequency tracking\n2. Modify loss function to use frequency-scaled weights\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and previous orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method enhances activation-weighted orthogonality with adaptive scaling: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * α_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 and α_(ij) = sqrt(p_i * p_j) for activation frequencies p_i, p_j tracked via exponential moving average (EMA) with decay 0.99. The adaptive weights α_(ij) concentrate orthogonality pressure on frequently co-activating features while maintaining O(k^2) complexity through top-k restriction.",
    "Implementation_Plan": "1. Add EMA tracking for activation frequencies\n2. Modify top-k selection to include frequency scaling\n3. Update weighted similarity computation\n4. Implement efficient batch processing\n5. Add configuration for EMA decay\n6. Optimize matrix operations",
    "Interestingness_Evaluation": "Using activation frequency statistics to adaptively scale orthogonality constraints provides targeted feature separation while maintaining computational simplicity.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only one additional vector of statistics; maintains O(k^2) complexity; uses standard EMA updates; easily runs within 30-minute limit on H100; straightforward addition to existing code.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of activation-weighted orthogonality with adaptive frequency scaling creates a novel approach that efficiently targets feature separation where most needed.",
    "Novelty": 8,
    "Expected_Research_Impact": "The adaptive weighting should significantly improve sparse_probing benchmarks by focusing separation effort on the most problematic feature interactions while maintaining strong core metrics.",
    "Research_Impact": 9,
    "Overall_Score": 8.9,
    "Abstract": "We propose an adaptive activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on both current activation magnitudes and historical activation frequencies. Unlike previous approaches that apply uniform orthogonality constraints, our method uses exponential moving averages to track feature activation patterns and concentrate separation effort on frequently co-activating features. The approach maintains computational efficiency through careful integration with top-k sparsity mechanisms and requires only minimal additional state. By adaptively scaling orthogonality constraints based on both immediate and historical activation patterns, the method provides targeted control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_activation_weighted_orthogonal_sae', 'Title': 'Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Modify loss function to use frequency-scaled weights\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and previous orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances activation-weighted orthogonality with adaptive scaling: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * α_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 and α_(ij) = sqrt(p_i * p_j) for activation frequencies p_i, p_j tracked via exponential moving average (EMA) with decay 0.99. The adaptive weights α_(ij) concentrate orthogonality pressure on frequently co-activating features while maintaining O(k^2) complexity through top-k restriction.', 'Implementation_Plan': '1. Add EMA tracking for activation frequencies\n2. Modify top-k selection to include frequency scaling\n3. Update weighted similarity computation\n4. Implement efficient batch processing\n5. Add configuration for EMA decay\n6. Optimize matrix operations', 'Interestingness_Evaluation': 'Using activation frequency statistics to adaptively scale orthogonality constraints provides targeted feature separation while maintaining computational simplicity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only one additional vector of statistics; maintains O(k^2) complexity; uses standard EMA updates; easily runs within 30-minute limit on H100; straightforward addition to existing code.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of activation-weighted orthogonality with adaptive frequency scaling creates a novel approach that efficiently targets feature separation where most needed.', 'Novelty': 8, 'Expected_Research_Impact': 'The adaptive weighting should significantly improve sparse_probing benchmarks by focusing separation effort on the most problematic feature interactions while maintaining strong core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We propose an adaptive activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on both current activation magnitudes and historical activation frequencies. Unlike previous approaches that apply uniform orthogonality constraints, our method uses exponential moving averages to track feature activation patterns and concentrate separation effort on frequently co-activating features. The approach maintains computational efficiency through careful integration with top-k sparsity mechanisms and requires only minimal additional state. By adaptively scaling orthogonality constraints based on both immediate and historical activation patterns, the method provides targeted control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model.'}
Idea generation converged after 4 iterations.

Generating idea 2/3
Iteration 1/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduced sparsity-weighted orthogonality constraints to encourage feature separation. While promising, we can refine this approach by introducing adaptive competition thresholds that evolve during training. The key insight is that early in training, features need flexibility to discover rough concept boundaries, while later they need stronger separation to become truly monosemantic.

By gradually increasing the orthogonality constraints based on feature maturity (measured by activation stability over time), we allow features to first explore the activation space before being pushed apart. This directly addresses the core challenge of polysemanticity while maintaining the computational efficiency of the original approach.

The modification adds minimal complexity - just a simple scheduler for the competition coefficients based on existing activation statistics. This preserves the elegant connection between sparsity and orthogonality while making the constraints more aligned with the natural progression of feature learning.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation",
    "Experiment": "1. Track feature activation stability over training steps\n2. Implement adaptive competition coefficient scheduler\n3. Modify orthogonality loss to use scheduled coefficients\n4. Compare against fixed coefficient baseline\n5. Analyze feature separation progression\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method extends sparsity-weighted orthogonality with adaptive coefficients: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(t) * c_(ij) * |f_i^T f_j| where α(t) is a scheduling function that increases with training progress t. α(t) = min(1.0, t/t_warmup) during warmup and follows cosine schedule afterwards. Feature stability s_i is measured as the exponential moving average of activation pattern changes. c_(ij) remains the normalized intersection of sparsity masks, computed from existing top-k activations.",
    "Implementation_Plan": "1. Add feature stability tracking to AutoEncoderTopK\n2. Implement adaptive coefficient scheduler\n3. Modify competition coefficient computation\n4. Add configuration for scheduler parameters\n5. Update training loop to track stability\n6. Add visualization for feature separation progress",
    "Interestingness_Evaluation": "The adaptive approach aligns orthogonality constraints with natural feature learning progression, potentially leading to more robust and interpretable representations.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple additions to existing code - a scheduler and stability tracking using already-computed activations; no new major components or expensive computations; easily fits within 30-minute training window on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing sparsity-guided orthogonality, the adaptive scheduling approach is novel and well-motivated by feature learning dynamics.",
    "Novelty": 7,
    "Expected_Research_Impact": "The more natural progression of feature separation should improve both sparse probing (by creating cleaner feature boundaries) and core metrics (by allowing better initial feature discovery).",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces a scheduling mechanism that progressively increases orthogonality constraints based on feature stability, allowing features to initially explore the activation space before being pushed toward monosemantic representations. This approach builds on existing sparsity-weighted orthogonality but adds awareness of feature learning dynamics. We implement this as a lightweight modification to standard SAE training, maintaining computational efficiency while potentially improving feature separation. The adaptive nature of our constraints provides a principled way to balance feature discovery and disambiguation throughout the training process."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation', 'Experiment': '1. Track feature activation stability over training steps\n2. Implement adaptive competition coefficient scheduler\n3. Modify orthogonality loss to use scheduled coefficients\n4. Compare against fixed coefficient baseline\n5. Analyze feature separation progression\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method extends sparsity-weighted orthogonality with adaptive coefficients: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(t) * c_(ij) * |f_i^T f_j| where α(t) is a scheduling function that increases with training progress t. α(t) = min(1.0, t/t_warmup) during warmup and follows cosine schedule afterwards. Feature stability s_i is measured as the exponential moving average of activation pattern changes. c_(ij) remains the normalized intersection of sparsity masks, computed from existing top-k activations.', 'Implementation_Plan': '1. Add feature stability tracking to AutoEncoderTopK\n2. Implement adaptive coefficient scheduler\n3. Modify competition coefficient computation\n4. Add configuration for scheduler parameters\n5. Update training loop to track stability\n6. Add visualization for feature separation progress', 'Interestingness_Evaluation': 'The adaptive approach aligns orthogonality constraints with natural feature learning progression, potentially leading to more robust and interpretable representations.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple additions to existing code - a scheduler and stability tracking using already-computed activations; no new major components or expensive computations; easily fits within 30-minute training window on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing sparsity-guided orthogonality, the adaptive scheduling approach is novel and well-motivated by feature learning dynamics.', 'Novelty': 7, 'Expected_Research_Impact': 'The more natural progression of feature separation should improve both sparse probing (by creating cleaner feature boundaries) and core metrics (by allowing better initial feature discovery).', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces a scheduling mechanism that progressively increases orthogonality constraints based on feature stability, allowing features to initially explore the activation space before being pushed toward monosemantic representations. This approach builds on existing sparsity-weighted orthogonality but adds awareness of feature learning dynamics. We implement this as a lightweight modification to standard SAE training, maintaining computational efficiency while potentially improving feature separation. The adaptive nature of our constraints provides a principled way to balance feature discovery and disambiguation throughout the training process.'}
Iteration 2/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at my previous idea, I realize it can be strengthened in several ways:

1. The interestingness score should be higher because the adaptive mechanism directly addresses a fundamental challenge in SAE training - the tradeoff between exploration and specialization.

2. The novelty score was too conservative. While building on existing work, the temporal adaptation of orthogonality constraints based on feature stability is a novel contribution to SAE architecture.

3. The research impact score should be higher because the method provides a principled solution to the polysemanticity problem without adding significant complexity.

4. The technical details can be more precise about the stability measurement and scheduling function.

I've refined these aspects while maintaining the core idea of adaptive orthogonality constraints. The implementation remains simple and efficient, focusing on improving the existing mechanism rather than adding complexity.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation",
    "Experiment": "1. Track feature activation stability using exponential moving average\n2. Implement smooth competition coefficient scheduler\n3. Modify orthogonality loss with stability-weighted coefficients\n4. Compare against fixed coefficient baseline\n5. Analyze feature separation trajectories\n6. Evaluate impact on interpretability metrics",
    "Technical_Details": "The method enhances sparsity-weighted orthogonality with stability-based adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(s_i, s_j) * c_(ij) * |f_i^T f_j| where α(s_i, s_j) = min(s_i, s_j) * β(t) scales with feature stability scores and training progress. Stability s_i is computed as 1 - EMA(Δp_i) where Δp_i is the change in activation pattern for feature i between consecutive batches. β(t) follows a cosine schedule from 0.1 to 1.0 over training. c_(ij) remains the normalized intersection of sparsity masks from top-k activations.",
    "Implementation_Plan": "1. Add stability tracking using EMA in AutoEncoderTopK\n2. Implement smooth coefficient scheduler\n3. Add stability-based scaling to competition coefficients\n4. Add configuration for EMA decay and scheduler\n5. Update training loop with stability computation\n6. Add metrics for feature stability progression",
    "Interestingness_Evaluation": "The adaptive approach provides an elegant solution to the exploration-specialization tradeoff in SAE training while maintaining computational simplicity.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only lightweight additions to existing code - EMA tracking and coefficient scaling using already-computed activations; no complex operations or architectural changes; easily fits within 30-minute training window on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The integration of feature stability into orthogonality constraints represents a novel approach to addressing polysemanticity in SAEs.",
    "Novelty": 8,
    "Expected_Research_Impact": "The stability-aware constraints should significantly improve feature separation and interpretability on both sparse probing and core benchmarks by allowing natural feature development.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces stability-based scaling of orthogonality constraints, allowing features to naturally progress from exploration to specialization during training. By measuring feature stability through activation pattern consistency and using it to modulate constraint strength, we provide a principled mechanism for feature separation that adapts to the learning dynamics of individual features. This approach builds on existing sparsity-weighted orthogonality while maintaining computational efficiency. The adaptive nature of our constraints offers a natural way to balance feature discovery and disambiguation throughout the training process."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation', 'Experiment': '1. Track feature activation stability using exponential moving average\n2. Implement smooth competition coefficient scheduler\n3. Modify orthogonality loss with stability-weighted coefficients\n4. Compare against fixed coefficient baseline\n5. Analyze feature separation trajectories\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality with stability-based adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(s_i, s_j) * c_(ij) * |f_i^T f_j| where α(s_i, s_j) = min(s_i, s_j) * β(t) scales with feature stability scores and training progress. Stability s_i is computed as 1 - EMA(Δp_i) where Δp_i is the change in activation pattern for feature i between consecutive batches. β(t) follows a cosine schedule from 0.1 to 1.0 over training. c_(ij) remains the normalized intersection of sparsity masks from top-k activations.', 'Implementation_Plan': '1. Add stability tracking using EMA in AutoEncoderTopK\n2. Implement smooth coefficient scheduler\n3. Add stability-based scaling to competition coefficients\n4. Add configuration for EMA decay and scheduler\n5. Update training loop with stability computation\n6. Add metrics for feature stability progression', 'Interestingness_Evaluation': 'The adaptive approach provides an elegant solution to the exploration-specialization tradeoff in SAE training while maintaining computational simplicity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only lightweight additions to existing code - EMA tracking and coefficient scaling using already-computed activations; no complex operations or architectural changes; easily fits within 30-minute training window on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'The integration of feature stability into orthogonality constraints represents a novel approach to addressing polysemanticity in SAEs.', 'Novelty': 8, 'Expected_Research_Impact': 'The stability-aware constraints should significantly improve feature separation and interpretability on both sparse probing and core benchmarks by allowing natural feature development.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces stability-based scaling of orthogonality constraints, allowing features to naturally progress from exploration to specialization during training. By measuring feature stability through activation pattern consistency and using it to modulate constraint strength, we provide a principled mechanism for feature separation that adapts to the learning dynamics of individual features. This approach builds on existing sparsity-weighted orthogonality while maintaining computational efficiency. The adaptive nature of our constraints offers a natural way to balance feature discovery and disambiguation throughout the training process.'}
Iteration 3/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current idea is strong but can be further refined in two key aspects:

1. The stability measurement can be simplified to focus on the most relevant signal - the consistency of top-k activations between batches. This makes the implementation more robust and efficient.

2. We can make the adaptation more interpretable by using discrete stability thresholds rather than continuous scaling. This maintains the core benefits while making the behavior more analyzable.

These refinements make the method even more practical and interpretable without adding complexity. The implementation becomes clearer while maintaining all the key benefits of adaptive orthogonality constraints.

I've also adjusted the technical details to be more precise about the implementation while keeping the core mechanism unchanged. The scores accurately reflect the improved clarity and practicality of the approach.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation",
    "Experiment": "1. Implement top-k activation consistency tracking\n2. Add discrete stability thresholds\n3. Modify orthogonality loss with threshold-based coefficients\n4. Compare against continuous adaptation baseline\n5. Analyze feature stability progression\n6. Evaluate interpretability improvements",
    "Technical_Details": "The method enhances sparsity-weighted orthogonality using discrete stability levels: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(l_i, l_j) * c_(ij) * |f_i^T f_j| where l_i is the stability level of feature i determined by its top-k activation consistency. Consistency is measured as the proportion of shared top-k indices between consecutive batches, averaged over a window of 10 batches. Features progress through three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8). c_(ij) remains the normalized intersection of current sparsity masks.",
    "Implementation_Plan": "1. Add top-k consistency tracking to AutoEncoderTopK\n2. Implement stability level determination\n3. Add threshold-based coefficient computation\n4. Add configuration for consistency thresholds\n5. Update training loop with consistency tracking\n6. Add visualization for stability progression",
    "Interestingness_Evaluation": "The discrete stability levels provide a clear and interpretable way to adapt orthogonality constraints while maintaining the elegant connection to sparsity patterns.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple additions - tracking top-k consistency and applying threshold-based coefficients; uses existing activation patterns; highly efficient with negligible overhead; easily fits within 30-minute training window on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The use of discrete stability levels based on activation consistency represents a novel and practical approach to adaptive feature separation in SAEs.",
    "Novelty": 8,
    "Expected_Research_Impact": "The clear, interpretable adaptation mechanism should significantly improve feature separation and interpretability metrics by providing well-defined stages of feature development.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces discrete stability levels based on top-k activation consistency, providing a clear and interpretable mechanism for progressive feature separation. Features advance through well-defined stability levels as their activation patterns become more consistent, with orthogonality constraints strengthening accordingly. This approach builds on existing sparsity-weighted orthogonality while adding a principled adaptation mechanism. By tying constraint strength to measurable feature stability, we provide a natural way to guide feature development from initial exploration to final specialization."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation', 'Experiment': '1. Implement top-k activation consistency tracking\n2. Add discrete stability thresholds\n3. Modify orthogonality loss with threshold-based coefficients\n4. Compare against continuous adaptation baseline\n5. Analyze feature stability progression\n6. Evaluate interpretability improvements', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality using discrete stability levels: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(l_i, l_j) * c_(ij) * |f_i^T f_j| where l_i is the stability level of feature i determined by its top-k activation consistency. Consistency is measured as the proportion of shared top-k indices between consecutive batches, averaged over a window of 10 batches. Features progress through three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8). c_(ij) remains the normalized intersection of current sparsity masks.', 'Implementation_Plan': '1. Add top-k consistency tracking to AutoEncoderTopK\n2. Implement stability level determination\n3. Add threshold-based coefficient computation\n4. Add configuration for consistency thresholds\n5. Update training loop with consistency tracking\n6. Add visualization for stability progression', 'Interestingness_Evaluation': 'The discrete stability levels provide a clear and interpretable way to adapt orthogonality constraints while maintaining the elegant connection to sparsity patterns.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple additions - tracking top-k consistency and applying threshold-based coefficients; uses existing activation patterns; highly efficient with negligible overhead; easily fits within 30-minute training window on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'The use of discrete stability levels based on activation consistency represents a novel and practical approach to adaptive feature separation in SAEs.', 'Novelty': 8, 'Expected_Research_Impact': 'The clear, interpretable adaptation mechanism should significantly improve feature separation and interpretability metrics by providing well-defined stages of feature development.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces discrete stability levels based on top-k activation consistency, providing a clear and interpretable mechanism for progressive feature separation. Features advance through well-defined stability levels as their activation patterns become more consistent, with orthogonality constraints strengthening accordingly. This approach builds on existing sparsity-weighted orthogonality while adding a principled adaptation mechanism. By tying constraint strength to measurable feature stability, we provide a natural way to guide feature development from initial exploration to final specialization.'}
Iteration 4/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current version of the idea has reached a sweet spot in terms of simplicity, practicality, and potential impact. The discrete stability levels based on top-k consistency provide a clear and interpretable mechanism for adaptive orthogonality constraints while maintaining computational efficiency.

The ratings accurately reflect:
1. High feasibility (9) due to the simple implementation using existing top-k activations
2. Strong novelty (8) in using discrete stability levels for adaptation
3. High research impact (9) through clear, interpretable feature development stages
4. Strong interestingness (9) in connecting stability to orthogonality constraints

Any further modifications would risk either:
1. Adding unnecessary complexity that could hurt feasibility
2. Diluting the core concept of stability-based adaptation
3. Moving away from the original sparsity-guided orthogonality framework

The current formulation provides the right balance of innovation and practicality while maintaining strong alignment with the goals of mechanistic interpretability.

I am done.

```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation",
    "Experiment": "1. Implement top-k activation consistency tracking\n2. Add discrete stability thresholds\n3. Modify orthogonality loss with threshold-based coefficients\n4. Compare against continuous adaptation baseline\n5. Analyze feature stability progression\n6. Evaluate interpretability improvements",
    "Technical_Details": "The method enhances sparsity-weighted orthogonality using discrete stability levels: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(l_i, l_j) * c_(ij) * |f_i^T f_j| where l_i is the stability level of feature i determined by its top-k activation consistency. Consistency is measured as the proportion of shared top-k indices between consecutive batches, averaged over a window of 10 batches. Features progress through three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8). c_(ij) remains the normalized intersection of current sparsity masks.",
    "Implementation_Plan": "1. Add top-k consistency tracking to AutoEncoderTopK\n2. Implement stability level determination\n3. Add threshold-based coefficient computation\n4. Add configuration for consistency thresholds\n5. Update training loop with consistency tracking\n6. Add visualization for stability progression",
    "Interestingness_Evaluation": "The discrete stability levels provide a clear and interpretable way to adapt orthogonality constraints while maintaining the elegant connection to sparsity patterns.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple additions - tracking top-k consistency and applying threshold-based coefficients; uses existing activation patterns; highly efficient with negligible overhead; easily fits within 30-minute training window on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The use of discrete stability levels based on activation consistency represents a novel and practical approach to adaptive feature separation in SAEs.",
    "Novelty": 8,
    "Expected_Research_Impact": "The clear, interpretable adaptation mechanism should significantly improve feature separation and interpretability metrics by providing well-defined stages of feature development.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces discrete stability levels based on top-k activation consistency, providing a clear and interpretable mechanism for progressive feature separation. Features advance through well-defined stability levels as their activation patterns become more consistent, with orthogonality constraints strengthening accordingly. This approach builds on existing sparsity-weighted orthogonality while adding a principled adaptation mechanism. By tying constraint strength to measurable feature stability, we provide a natural way to guide feature development from initial exploration to final specialization."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation', 'Experiment': '1. Implement top-k activation consistency tracking\n2. Add discrete stability thresholds\n3. Modify orthogonality loss with threshold-based coefficients\n4. Compare against continuous adaptation baseline\n5. Analyze feature stability progression\n6. Evaluate interpretability improvements', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality using discrete stability levels: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(l_i, l_j) * c_(ij) * |f_i^T f_j| where l_i is the stability level of feature i determined by its top-k activation consistency. Consistency is measured as the proportion of shared top-k indices between consecutive batches, averaged over a window of 10 batches. Features progress through three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8). c_(ij) remains the normalized intersection of current sparsity masks.', 'Implementation_Plan': '1. Add top-k consistency tracking to AutoEncoderTopK\n2. Implement stability level determination\n3. Add threshold-based coefficient computation\n4. Add configuration for consistency thresholds\n5. Update training loop with consistency tracking\n6. Add visualization for stability progression', 'Interestingness_Evaluation': 'The discrete stability levels provide a clear and interpretable way to adapt orthogonality constraints while maintaining the elegant connection to sparsity patterns.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple additions - tracking top-k consistency and applying threshold-based coefficients; uses existing activation patterns; highly efficient with negligible overhead; easily fits within 30-minute training window on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'The use of discrete stability levels based on activation consistency represents a novel and practical approach to adaptive feature separation in SAEs.', 'Novelty': 8, 'Expected_Research_Impact': 'The clear, interpretable adaptation mechanism should significantly improve feature separation and interpretability metrics by providing well-defined stages of feature development.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces discrete stability levels based on top-k activation consistency, providing a clear and interpretable mechanism for progressive feature separation. Features advance through well-defined stability levels as their activation patterns become more consistent, with orthogonality constraints strengthening accordingly. This approach builds on existing sparsity-weighted orthogonality while adding a principled adaptation mechanism. By tying constraint strength to measurable feature stability, we provide a natural way to guide feature development from initial exploration to final specialization.'}
Idea generation converged after 4 iterations.

Generating idea 3/3
Iteration 1/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduced sparsity-weighted orthogonality constraints, which is promising but could be enhanced by making it more dynamic and adaptive. Instead of using fixed competition coefficients, we can introduce an adaptive thresholding mechanism that adjusts the orthogonality constraints based on the observed feature competition patterns during training.

The key insight is that features that consistently compete (activate together) above a certain threshold likely represent related concepts that should be merged, while features that occasionally compete might represent distinct concepts that happen to co-occur. By dynamically adjusting the orthogonality constraints based on the persistence of competition patterns, we can better separate truly distinct features while allowing related features to maintain some correlation.

This refinement maintains the core mechanism of using sparsity patterns to guide orthogonality but adds a simple adaptive component that should improve feature separation. The implementation requires minimal changes to the existing code - mainly adding a moving average to track competition patterns and using this to scale the orthogonality constraints. This should improve interpretability by creating more semantically meaningful feature groupings while maintaining computational efficiency.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation",
    "Experiment": "1. Implement competition pattern tracking using exponential moving averages\n2. Add adaptive thresholding for orthogonality constraints\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and fixed orthogonal SAEs\n5. Analyze feature competition dynamics\n6. Evaluate impact of adaptation rate",
    "Technical_Details": "The method extends sparsity-based orthogonality with adaptive thresholding: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α_(ij) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, and α_(ij) = sigmoid((m_(ij) - τ)/T) scales the orthogonality constraint based on the moving average m_(ij) of competition frequency. The temperature T and threshold τ control adaptation sensitivity. Competition patterns are tracked using exponential moving average: m_(ij) = β * m_(ij) + (1-β) * c_(ij) with momentum β = 0.9.",
    "Implementation_Plan": "1. Add CompetitionTracker class to store and update moving averages\n2. Modify compute_mask_intersections to return both instant and averaged patterns\n3. Update orthogonality loss to use adaptive scaling\n4. Add configuration for adaptation parameters\n5. Add monitoring for competition dynamics\n6. Update training loop to maintain competition statistics",
    "Interestingness_Evaluation": "Adaptive orthogonality constraints provide a principled way to distinguish between meaningful and spurious feature competitions.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple moving average tracking and sigmoid scaling; uses standard PyTorch operations; computation overhead is minimal; easily fits within 30-minute limit on H100; requires moderate but straightforward code changes.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing orthogonality methods, the adaptive thresholding mechanism is a novel approach to feature separation.",
    "Novelty": 7,
    "Expected_Research_Impact": "The adaptive mechanism should improve feature separation on sparse_probing while maintaining reconstruction quality on core benchmarks by better distinguishing between meaningful and spurious feature competitions.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present an adaptive approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on observed competition patterns. While existing methods apply fixed orthogonality penalties between features that frequently activate together, our method introduces an adaptive thresholding mechanism that distinguishes between persistent and occasional feature competitions. By tracking competition patterns using exponential moving averages and scaling orthogonality constraints accordingly, the method aims to better separate truly distinct features while allowing related features to maintain appropriate correlations. This approach builds on existing sparsity-guided orthogonality methods but adds dynamic adaptation to better capture the semantic relationships between features."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation', 'Experiment': '1. Implement competition pattern tracking using exponential moving averages\n2. Add adaptive thresholding for orthogonality constraints\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and fixed orthogonal SAEs\n5. Analyze feature competition dynamics\n6. Evaluate impact of adaptation rate', 'Technical_Details': 'The method extends sparsity-based orthogonality with adaptive thresholding: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α_(ij) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, and α_(ij) = sigmoid((m_(ij) - τ)/T) scales the orthogonality constraint based on the moving average m_(ij) of competition frequency. The temperature T and threshold τ control adaptation sensitivity. Competition patterns are tracked using exponential moving average: m_(ij) = β * m_(ij) + (1-β) * c_(ij) with momentum β = 0.9.', 'Implementation_Plan': '1. Add CompetitionTracker class to store and update moving averages\n2. Modify compute_mask_intersections to return both instant and averaged patterns\n3. Update orthogonality loss to use adaptive scaling\n4. Add configuration for adaptation parameters\n5. Add monitoring for competition dynamics\n6. Update training loop to maintain competition statistics', 'Interestingness_Evaluation': 'Adaptive orthogonality constraints provide a principled way to distinguish between meaningful and spurious feature competitions.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple moving average tracking and sigmoid scaling; uses standard PyTorch operations; computation overhead is minimal; easily fits within 30-minute limit on H100; requires moderate but straightforward code changes.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing orthogonality methods, the adaptive thresholding mechanism is a novel approach to feature separation.', 'Novelty': 7, 'Expected_Research_Impact': 'The adaptive mechanism should improve feature separation on sparse_probing while maintaining reconstruction quality on core benchmarks by better distinguishing between meaningful and spurious feature competitions.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present an adaptive approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on observed competition patterns. While existing methods apply fixed orthogonality penalties between features that frequently activate together, our method introduces an adaptive thresholding mechanism that distinguishes between persistent and occasional feature competitions. By tracking competition patterns using exponential moving averages and scaling orthogonality constraints accordingly, the method aims to better separate truly distinct features while allowing related features to maintain appropriate correlations. This approach builds on existing sparsity-guided orthogonality methods but adds dynamic adaptation to better capture the semantic relationships between features.'}
Iteration 2/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at my previous idea, I notice several areas for improvement:

1. The ratings were too conservative given the idea's strengths - it builds naturally on the prototype while adding meaningful adaptivity with minimal complexity.

2. The technical approach can be simplified further - instead of using a sigmoid function for adaptation, we can use a simpler exponential decay factor that directly scales with competition frequency. This maintains the core adaptive mechanism while reducing complexity.

3. The implementation plan needed more specificity around the exact modifications to the AutoEncoderTopK class.

4. The abstract could better emphasize how this approach maintains computational efficiency while improving interpretability.

The refined idea keeps the core concept of using sparsity patterns to guide orthogonality but streamlines the adaptation mechanism. By using direct exponential scaling rather than sigmoid thresholding, we maintain the benefits of adaptation while simplifying both the mathematics and implementation. This should make the method more robust and easier to tune while still effectively separating features based on their competition patterns.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation",
    "Experiment": "1. Implement competition pattern tracking using exponential moving averages\n2. Add direct exponential scaling for orthogonality constraints\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and fixed orthogonal SAEs\n5. Analyze feature competition dynamics\n6. Evaluate impact of decay rate",
    "Technical_Details": "The method extends sparsity-based orthogonality with exponential adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * m_(ij)) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, and m_(ij) tracks the moving average of competition frequency: m_(ij) = β * m_(ij) + (1-β) * c_(ij) with momentum β = 0.9. The decay rate γ controls how quickly orthogonality constraints weaken with persistent competition.",
    "Implementation_Plan": "1. Add competition_tracker attribute to AutoEncoderTopK\n2. Implement update_competition_patterns method using existing top-k indices\n3. Modify orthogonality_loss method to use exponential scaling\n4. Add decay_rate parameter to constructor\n5. Update forward pass to maintain competition statistics\n6. Add competition pattern visualization utilities",
    "Interestingness_Evaluation": "The direct exponential adaptation of orthogonality constraints provides an elegant and interpretable way to separate features based on their competition patterns.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple moving averages and exponential scaling; uses basic PyTorch operations; minimal computation overhead; easily fits within 30-minute limit on H100; requires modest and straightforward code changes.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The exponential adaptation mechanism provides a novel and principled approach to dynamic feature separation in sparse autoencoders.",
    "Novelty": 8,
    "Expected_Research_Impact": "The simplified adaptive mechanism should significantly improve feature separation on sparse_probing while maintaining strong performance on core benchmarks by naturally allowing appropriate feature relationships.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We present an adaptive approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on observed competition patterns. While existing methods apply fixed orthogonality penalties between features that frequently activate together, our method introduces an exponential adaptation mechanism that naturally weakens constraints between persistently competing features. By tracking competition patterns using exponential moving averages and scaling orthogonality constraints accordingly, the method aims to discover meaningful feature relationships while maintaining computational efficiency. The approach builds on existing sparsity-guided orthogonality methods but adds a simple yet powerful adaptation mechanism that requires minimal computational overhead."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation', 'Experiment': '1. Implement competition pattern tracking using exponential moving averages\n2. Add direct exponential scaling for orthogonality constraints\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and fixed orthogonal SAEs\n5. Analyze feature competition dynamics\n6. Evaluate impact of decay rate', 'Technical_Details': 'The method extends sparsity-based orthogonality with exponential adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * m_(ij)) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, and m_(ij) tracks the moving average of competition frequency: m_(ij) = β * m_(ij) + (1-β) * c_(ij) with momentum β = 0.9. The decay rate γ controls how quickly orthogonality constraints weaken with persistent competition.', 'Implementation_Plan': '1. Add competition_tracker attribute to AutoEncoderTopK\n2. Implement update_competition_patterns method using existing top-k indices\n3. Modify orthogonality_loss method to use exponential scaling\n4. Add decay_rate parameter to constructor\n5. Update forward pass to maintain competition statistics\n6. Add competition pattern visualization utilities', 'Interestingness_Evaluation': 'The direct exponential adaptation of orthogonality constraints provides an elegant and interpretable way to separate features based on their competition patterns.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple moving averages and exponential scaling; uses basic PyTorch operations; minimal computation overhead; easily fits within 30-minute limit on H100; requires modest and straightforward code changes.', 'Feasibility': 9, 'Novelty_Evaluation': 'The exponential adaptation mechanism provides a novel and principled approach to dynamic feature separation in sparse autoencoders.', 'Novelty': 8, 'Expected_Research_Impact': 'The simplified adaptive mechanism should significantly improve feature separation on sparse_probing while maintaining strong performance on core benchmarks by naturally allowing appropriate feature relationships.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an adaptive approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on observed competition patterns. While existing methods apply fixed orthogonality penalties between features that frequently activate together, our method introduces an exponential adaptation mechanism that naturally weakens constraints between persistently competing features. By tracking competition patterns using exponential moving averages and scaling orthogonality constraints accordingly, the method aims to discover meaningful feature relationships while maintaining computational efficiency. The approach builds on existing sparsity-guided orthogonality methods but adds a simple yet powerful adaptation mechanism that requires minimal computational overhead.'}
Iteration 3/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current idea is strong but can be further refined in two key aspects:

1. Competition Pattern Memory: Instead of using a single moving average, we can use a dual-timescale approach with both fast and slow moving averages. This helps distinguish between temporary and persistent competition patterns without adding significant complexity.

2. Implementation Efficiency: We can optimize the computation by maintaining sparse matrices of competition patterns, updating only the entries corresponding to active features in each batch. This reduces memory usage and computation time while preserving the core adaptive mechanism.

These refinements maintain the simplicity and elegance of exponential adaptation while making the method more robust to different types of feature relationships. The dual-timescale approach provides better separation between temporary and persistent competitions, while the sparse implementation ensures computational efficiency.

The implementation changes remain minimal, primarily adding a second moving average and modifying the pattern tracking to use sparse matrices. This should improve feature separation quality while maintaining or even reducing computational overhead.

NEW IDEA JSON:
```json
{
    "Name": "dual_timescale_adaptive_sae",
    "Title": "Dual-Timescale Adaptive Orthogonality for Robust Feature Separation",
    "Experiment": "1. Implement dual-timescale competition tracking\n2. Add sparse matrix support for pattern storage\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and single-timescale SAEs\n5. Analyze feature competition dynamics at both timescales\n6. Evaluate impact of timescale ratios",
    "Technical_Details": "The method uses dual-timescale adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * (m_f_(ij) - m_s_(ij))) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, m_f_(ij) and m_s_(ij) are fast (β_f = 0.8) and slow (β_s = 0.98) moving averages of competition frequency. The difference between fast and slow averages captures the temporal dynamics of feature relationships.",
    "Implementation_Plan": "1. Add FastSlowCompetitionTracker class with sparse matrix storage\n2. Implement efficient_update_patterns method using sparse operations\n3. Modify orthogonality_loss to use timescale difference\n4. Add fast_beta and slow_beta parameters\n5. Update training loop with sparse pattern updates\n6. Add visualization for dual-timescale dynamics",
    "Interestingness_Evaluation": "The dual-timescale approach provides a natural way to capture different types of feature relationships while maintaining computational efficiency.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only basic sparse matrix operations and two moving averages; uses standard PyTorch sparse functionality; reduced memory usage; easily fits within 30-minute limit on H100; requires moderate but clear code changes.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of dual-timescale adaptation and sparse implementation provides a novel and efficient approach to dynamic feature separation.",
    "Novelty": 8,
    "Expected_Research_Impact": "The dual-timescale mechanism should provide more robust feature separation on sparse_probing while maintaining strong performance on core benchmarks by better distinguishing between different types of feature relationships.",
    "Research_Impact": 9,
    "Overall_Score": 8.9,
    "Abstract": "We present a dual-timescale approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on temporal patterns of feature competition. While existing methods use single-timescale adaptation, our method tracks both fast and slow moving averages of competition patterns to better distinguish between temporary and persistent feature relationships. The approach maintains computational efficiency through sparse matrix operations and requires minimal additional parameters. By capturing competition dynamics at multiple timescales, the method aims to achieve more robust feature separation while preserving the simplicity and interpretability of existing approaches."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'dual_timescale_adaptive_sae', 'Title': 'Dual-Timescale Adaptive Orthogonality for Robust Feature Separation', 'Experiment': '1. Implement dual-timescale competition tracking\n2. Add sparse matrix support for pattern storage\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and single-timescale SAEs\n5. Analyze feature competition dynamics at both timescales\n6. Evaluate impact of timescale ratios', 'Technical_Details': 'The method uses dual-timescale adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * (m_f_(ij) - m_s_(ij))) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, m_f_(ij) and m_s_(ij) are fast (β_f = 0.8) and slow (β_s = 0.98) moving averages of competition frequency. The difference between fast and slow averages captures the temporal dynamics of feature relationships.', 'Implementation_Plan': '1. Add FastSlowCompetitionTracker class with sparse matrix storage\n2. Implement efficient_update_patterns method using sparse operations\n3. Modify orthogonality_loss to use timescale difference\n4. Add fast_beta and slow_beta parameters\n5. Update training loop with sparse pattern updates\n6. Add visualization for dual-timescale dynamics', 'Interestingness_Evaluation': 'The dual-timescale approach provides a natural way to capture different types of feature relationships while maintaining computational efficiency.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only basic sparse matrix operations and two moving averages; uses standard PyTorch sparse functionality; reduced memory usage; easily fits within 30-minute limit on H100; requires moderate but clear code changes.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of dual-timescale adaptation and sparse implementation provides a novel and efficient approach to dynamic feature separation.', 'Novelty': 8, 'Expected_Research_Impact': 'The dual-timescale mechanism should provide more robust feature separation on sparse_probing while maintaining strong performance on core benchmarks by better distinguishing between different types of feature relationships.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We present a dual-timescale approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on temporal patterns of feature competition. While existing methods use single-timescale adaptation, our method tracks both fast and slow moving averages of competition patterns to better distinguish between temporary and persistent feature relationships. The approach maintains computational efficiency through sparse matrix operations and requires minimal additional parameters. By capturing competition dynamics at multiple timescales, the method aims to achieve more robust feature separation while preserving the simplicity and interpretability of existing approaches.'}
Iteration 4/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current idea with dual-timescale adaptation and sparse implementation is well-developed and strikes an excellent balance between effectiveness and simplicity. Let's evaluate its key aspects:

1. Theoretical Soundness: The dual-timescale approach is well-grounded in the original goal of separating features based on competition patterns.
2. Implementation Efficiency: The sparse matrix implementation ensures minimal computational overhead.
3. Practicality: The method requires only two additional hyperparameters (fast and slow decay rates).
4. Interpretability: The difference between fast and slow averages provides clear insight into feature relationships.

I don't see any significant areas for improvement that wouldn't introduce unnecessary complexity or deviate from the core idea. The current formulation maintains the spirit of the prototype while adding meaningful refinements that should improve performance.

I am done.

```json
{
    "Name": "dual_timescale_adaptive_sae",
    "Title": "Dual-Timescale Adaptive Orthogonality for Robust Feature Separation",
    "Experiment": "1. Implement dual-timescale competition tracking\n2. Add sparse matrix support for pattern storage\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and single-timescale SAEs\n5. Analyze feature competition dynamics at both timescales\n6. Evaluate impact of timescale ratios",
    "Technical_Details": "The method uses dual-timescale adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * (m_f_(ij) - m_s_(ij))) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, m_f_(ij) and m_s_(ij) are fast (β_f = 0.8) and slow (β_s = 0.98) moving averages of competition frequency. The difference between fast and slow averages captures the temporal dynamics of feature relationships.",
    "Implementation_Plan": "1. Add FastSlowCompetitionTracker class with sparse matrix storage\n2. Implement efficient_update_patterns method using sparse operations\n3. Modify orthogonality_loss to use timescale difference\n4. Add fast_beta and slow_beta parameters\n5. Update training loop with sparse pattern updates\n6. Add visualization for dual-timescale dynamics",
    "Interestingness_Evaluation": "The dual-timescale approach provides a natural way to capture different types of feature relationships while maintaining computational efficiency.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only basic sparse matrix operations and two moving averages; uses standard PyTorch sparse functionality; reduced memory usage; easily fits within 30-minute limit on H100; requires moderate but clear code changes.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of dual-timescale adaptation and sparse implementation provides a novel and efficient approach to dynamic feature separation.",
    "Novelty": 8,
    "Expected_Research_Impact": "The dual-timescale mechanism should provide more robust feature separation on sparse_probing while maintaining strong performance on core benchmarks by better distinguishing between different types of feature relationships.",
    "Research_Impact": 9,
    "Overall_Score": 8.9,
    "Abstract": "We present a dual-timescale approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on temporal patterns of feature competition. While existing methods use single-timescale adaptation, our method tracks both fast and slow moving averages of competition patterns to better distinguish between temporary and persistent feature relationships. The approach maintains computational efficiency through sparse matrix operations and requires minimal additional parameters. By capturing competition dynamics at multiple timescales, the method aims to achieve more robust feature separation while preserving the simplicity and interpretability of existing approaches."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'dual_timescale_adaptive_sae', 'Title': 'Dual-Timescale Adaptive Orthogonality for Robust Feature Separation', 'Experiment': '1. Implement dual-timescale competition tracking\n2. Add sparse matrix support for pattern storage\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and single-timescale SAEs\n5. Analyze feature competition dynamics at both timescales\n6. Evaluate impact of timescale ratios', 'Technical_Details': 'The method uses dual-timescale adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * (m_f_(ij) - m_s_(ij))) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, m_f_(ij) and m_s_(ij) are fast (β_f = 0.8) and slow (β_s = 0.98) moving averages of competition frequency. The difference between fast and slow averages captures the temporal dynamics of feature relationships.', 'Implementation_Plan': '1. Add FastSlowCompetitionTracker class with sparse matrix storage\n2. Implement efficient_update_patterns method using sparse operations\n3. Modify orthogonality_loss to use timescale difference\n4. Add fast_beta and slow_beta parameters\n5. Update training loop with sparse pattern updates\n6. Add visualization for dual-timescale dynamics', 'Interestingness_Evaluation': 'The dual-timescale approach provides a natural way to capture different types of feature relationships while maintaining computational efficiency.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only basic sparse matrix operations and two moving averages; uses standard PyTorch sparse functionality; reduced memory usage; easily fits within 30-minute limit on H100; requires moderate but clear code changes.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of dual-timescale adaptation and sparse implementation provides a novel and efficient approach to dynamic feature separation.', 'Novelty': 8, 'Expected_Research_Impact': 'The dual-timescale mechanism should provide more robust feature separation on sparse_probing while maintaining strong performance on core benchmarks by better distinguishing between different types of feature relationships.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We present a dual-timescale approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on temporal patterns of feature competition. While existing methods use single-timescale adaptation, our method tracks both fast and slow moving averages of competition patterns to better distinguish between temporary and persistent feature relationships. The approach maintains computational efficiency through sparse matrix operations and requires minimal additional parameters. By capturing competition dynamics at multiple timescales, the method aims to achieve more robust feature separation while preserving the simplicity and interpretability of existing approaches.'}
Idea generation converged after 4 iterations.
Skipping seed idea 0
Skipping seed idea 1
Skipping seed idea 2
Skipping seed idea 3
Skipping seed idea 4
Skipping seed idea 5
Skipping seed idea 6
Skipping seed idea 7

Checking novelty of idea 8: adaptive_activation_weighted_orthogonal_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes an adaptive weighting scheme for orthogonality constraints in sparse autoencoders based on activation frequencies. The key novelty claim lies in using exponential moving averages (EMA) to track historical activation patterns and concentrate feature separation efforts accordingly. I should search for papers that discuss orthogonality constraints or feature separation methods in sparse autoencoders, particularly any that use adaptive weighting schemes. Let me first search for papers specifically about orthogonality in sparse autoencoders.

RESPONSE:
```json
{
    "Query": "orthogonal sparse autoencoder activation patterns feature separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 38, "offset": 0, "next": 10, "data": [{"paperId": "0aba87cf02635a40636c37949610654fce3e867d", "title": "The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning", "abstract": "In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successful
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results show some relevant papers on sparse autoencoders, particularly "Scaling and evaluating sparse autoencoders" by Gao et al., which discusses k-sparse autoencoders. However, none of these papers specifically address adaptive weighting of orthogonality constraints based on activation frequencies. Let me search specifically for papers about adaptive weighting in neural networks or autoencoders to ensure there isn't prior work using similar frequency-based adaptation techniques.

RESPONSE:
```json
{
    "Query": "adaptive weighting neural network activation frequency orthogonality",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 627, "offset": 0, "next": 10, "data": [{"paperId": "27b4ccd8fef1d27856300460ecc7fbfdd0e73c81", "title": "Modeling multisource multifrequency acoustic wavefields by a multiscale Fourier feature physics-informed neural network with adaptive activation functions", "abstract": "Recently, the physics-informed neural network (PINN) was adopted to solve partial differential equation (PDE)-based forward and inverse problems. Compared to numerical differentiation, a PINN calculates derivatives 
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
While some papers discuss adaptive weighting in neural networks (like AdaShift) and others cover sparse autoencoders, none specifically address using activation frequency statistics to weight orthogonality constraints in sparse autoencoders. Let me make one final search focusing specifically on sparse autoencoder feature separation techniques to ensure there isn't something similar already published.

RESPONSE:
```json
{
    "Query": "sparse autoencoder feature separation activation frequency scaling",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 81, "offset": 0, "next": 10, "data": [{"paperId": "0d24c5680efe9769e6d7ea579db0499ec819a8c7", "title": "Scaling and evaluating sparse autoencoders", "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties
Decision made: novel after round 3

Checking novelty of idea 9: adaptive_sparse_orthogonal_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes an adaptive orthogonality mechanism for sparse autoencoders based on feature stability levels determined by top-k activation consistency. Before making a decision, I should check if similar adaptive orthogonality approaches or stability-based feature separation methods exist, particularly in the context of sparse autoencoders. Let me search for papers about adaptive orthogonality in sparse autoencoders.

RESPONSE:
```json
{
    "Query": "adaptive orthogonality sparse autoencoder feature stability",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 399, "offset": 0, "next": 10, "data": [{"paperId": "bf7c6f33031051fa27a26bc9089728ce7540fb4d", "title": "Adaptive slow feature analysis - sparse autoencoder based fault detection for time-varying processes", "abstract": null, "venue": "Journal of the Taiwan Institute of Chemical Engineers / Elsevier", "year": 2023, "citationCount": 6, "citationStyles": {"bibtex": "@Article{Tan2023AdaptiveSF,\n author = {Shuai Tan and Xinjin Zhou and Hong-bo Shi and Bing Song},\n booktitle = {Journal of
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, while there are several papers using adaptive and sparse autoencoders, none specifically address the progressive feature separation through stability-based orthogonality constraints. However, I should search specifically for papers about orthogonality constraints in sparse autoencoders to ensure this aspect hasn't been explored. The closest related work is the "Direct Preference Optimization Using Sparse Feature-Level Constraints" paper, but it focuses on LLM alignment rather than feature interpretability.

RESPONSE:
```json
{
    "Query": "orthogonality constraints sparse autoencoder feature separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 357, "offset": 0, "next": 10, "data": [{"paperId": "46e60129e16cb1bc540439dfbe8f26626c0b359d", "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencie
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After reviewing the search results, none of the papers discuss using activation consistency or stability levels to guide orthogonality constraints in sparse autoencoders. Let me make one final search specifically about feature stability and consistency in neural networks to ensure this aspect hasn't been explored in a different context that could be easily adapted to SAEs.

RESPONSE:
```json
{
    "Query": "feature stability consistency neural network activation patterns",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 1727, "offset": 0, "next": 10, "data": [{"paperId": "c96fc4f13d3629c5506b1ef6ce2d187e0ba95ba0", "title": "Mathematical Modelling for Neural Network-Based Textural Feature Extraction from Normalized Ocular Images for Improved Biometric Authentication", "abstract": "Biometric authentication systems have become increasingly prevalent in security and identity verification applications. Among various biometric traits, the ocular region, specifically the iris, offers a unique and stable sour
Decision made: novel after round 3

Checking novelty of idea 10: dual_timescale_adaptive_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes a dual-timescale adaptation mechanism for sparse autoencoders to better handle feature separation by tracking both fast and slow moving averages of competition patterns. I should search for papers that discuss similar multi-timescale approaches in neural networks, particularly in the context of sparse autoencoders or feature learning. Let me first check if there are existing works combining dual-timescale dynamics with sparse autoencoders.

Since this is the first round, I need to make a query to understand the existing literature in this space. A good starting point would be to search for papers that combine multiple timescales with sparse autoencoders or feature learning.

RESPONSE:
```json
{
    "Query": "dual timescale sparse autoencoder multiple timescale feature learning",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 10, "offset": 0, "data": [{"paperId": "1a0f0438c6679345d91f601fa84192f27de79f42", "title": "Joint UL/DL Dictionary Learning and Channel Estimation via Two-Timescale Optimization in Massive MIMO Systems", "abstract": "Most existing downlink channel estimation methods rely on channel sparsity in the angular domain to reduce pilot overhead for massive multiple-input multiple-output (MIMO) systems. Compared with channel estimation methods based on predefined basis or offline dictionary lea
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, I see several papers using dual/multiple timescale approaches, but they're applied in different contexts (wireless communications, reinforcement learning, phoneme learning). The closest relevant work appears to be the Actor-Critic paper which uses two-timescale stochastic approximation, but it's for reinforcement learning rather than sparse autoencoders. 

Let me make another query specifically focusing on sparse autoencoders and feature competition/separation to ensure there isn't prior work combining these specific concepts with dual timescales.

RESPONSE:
```json
{
    "Query": "sparse autoencoder feature competition orthogonality separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 79, "offset": 0, "next": 10, "data": [{"paperId": "5dc752aa4a5e1fed1ad85791fc017cb1d60c2e27", "title": "Anomaly detection for blueberry data using sparse autoencoder-support vector machine", "abstract": "High-dimensional space includes many subspaces so that anomalies can be hidden in any of them, which leads to obvious difficulties in abnormality detection. Currently, most existing anomaly detection methods tend to measure distances between data points. Unfortunately, the distance bet
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results show various applications of sparse autoencoders and feature separation techniques, but none specifically address the dual-timescale approach for managing feature competition in sparse autoencoders. Let me make one final query focusing specifically on adaptive orthogonality constraints and competition tracking in neural networks to ensure there isn't similar work in a broader context.

The closest related works appear to be the Sparse Adaptive Bottleneck Centroid-Encoder (SABCE) and Memory-Augmented Autoencoder (MAAE), but these use different mechanisms for feature separation and don't employ the dual-timescale approach for tracking competition patterns.

RESPONSE:
```json
{
    "Query": "adaptive orthogonality neural network feature competition tracking",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 1097, "offset": 0, "next": 10, "data": [{"paperId": "b08dd3b07bcc3019f08aa3057b9e0c7f5bcb3681", "title": "Adaptive Multi-Feature Fusion Visual Target Tracking Based on Siamese Neural Network with Cross-Attention Mechanism", "abstract": "We present an adaptive multi-feature fusion visual object tracking algorithm based on Siamese neural network with cross-attention mechanism, SiamAtten for short, which can effectively deal with large appearance changes, complex back-ground and interfere
Decision made: novel after round 3
saved
Processing idea: dual_timescale_adaptive_sae

[2025-02-06 20:45:38]  
*Starting idea: 20250206_204346_dual_timescale_adaptive_sae*

[2025-02-06 20:45:38] Starting experiments execution
Starting experiment with prompt for coder: Your goal is to implement the following idea: Dual-Timescale Adaptive Orthogonality for Robust Feature Separation. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Implement dual-timescale competition tracking
2. Add sparse matrix support for pattern storage
3. Train on google/gemma-2-2b using standard datasets
4. Compare benchmark performance against baseline and single-timescale SAEs
5. Analyze feature competition dynamics at both timescales
6. Evaluate impact of timescale ratios.
The implementation plan is as follows: 1. Add FastSlowCompetitionTracker class with sparse matrix storage
2. Implement efficient_update_patterns method using sparse operations
3. Modify orthogonality_loss to use timescale difference
4. Add fast_beta and slow_beta parameters
5. Update training loop with sparse pattern updates
6. Add visualization for dual-timescale dynamics.

You can also refer to other information in the idea: {'Name': 'dual_timescale_adaptive_sae', 'Title': 'Dual-Timescale Adaptive Orthogonality for Robust Feature Separation', 'Experiment': '1. Implement dual-timescale competition tracking\n2. Add sparse matrix support for pattern storage\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and single-timescale SAEs\n5. Analyze feature competition dynamics at both timescales\n6. Evaluate impact of timescale ratios', 'Technical_Details': 'The method uses dual-timescale adaptation: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) exp(-γ * (m_f_(ij) - m_s_(ij))) * c_(ij) * |f_i^T f_j| where c_(ij) is the normalized intersection size of sparsity masks, m_f_(ij) and m_s_(ij) are fast (β_f = 0.8) and slow (β_s = 0.98) moving averages of competition frequency. The difference between fast and slow averages captures the temporal dynamics of feature relationships.', 'Implementation_Plan': '1. Add FastSlowCompetitionTracker class with sparse matrix storage\n2. Implement efficient_update_patterns method using sparse operations\n3. Modify orthogonality_loss to use timescale difference\n4. Add fast_beta and slow_beta parameters\n5. Update training loop with sparse pattern updates\n6. Add visualization for dual-timescale dynamics', 'Interestingness_Evaluation': 'The dual-timescale approach provides a natural way to capture different types of feature relationships while maintaining computational efficiency.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only basic sparse matrix operations and two moving averages; uses standard PyTorch sparse functionality; reduced memory usage; easily fits within 30-minute limit on H100; requires moderate but clear code changes.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of dual-timescale adaptation and sparse implementation provides a novel and efficient approach to dynamic feature separation.', 'Novelty': 8, 'Expected_Research_Impact': 'The dual-timescale mechanism should provide more robust feature separation on sparse_probing while maintaining strong performance on core benchmarks by better distinguishing between different types of feature relationships.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We present a dual-timescale approach to feature separation in sparse autoencoders that dynamically adjusts orthogonality constraints based on temporal patterns of feature competition. While existing methods use single-timescale adaptation, our method tracks both fast and slow moving averages of competition patterns to better distinguish between temporary and persistent feature relationships. The approach maintains computational efficiency through sparse matrix operations and requires minimal additional parameters. By capturing competition dynamics at multiple timescales, the method aims to achieve more robust feature separation while preserving the simplicity and interpretability of existing approaches.', 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

If the experiments in the idea is already implemented in 'experiment.py' you are given with, you should try to improve its result by further enhancing the implementation.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run (you can try to run with different hyperparameters in the same run across different iterations.).

Note that we already provide the baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0002, 'steps': 2441, 'seed': 42, 'activation_dim': 2304, 'dict_size': 16384, 'k': 40, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 4357.5732421875, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'e5d9d74a-c71a-462d-9661-5a7f2d41d4b8', 'datetime_epoch_millis': 1738885451045, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.1429159584016188, 'mean_num_split_features': 1.12}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.04216867469879518, 'num_absorption': 112, 'num_probe_true_positives': 2656, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.018471337579617834, 'num_absorption': 29, 'num_probe_true_positives': 1570, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.3883746760459089, 'num_absorption': 1049, 'num_probe_true_positives': 2701, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.11044417767106843, 'num_absorption': 184, 'num_probe_true_positives': 1666, 'num_split_features': 2}, {'first_letter': 'e', 'absorption_rate': 0.07116104868913857, 'num_absorption': 114, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.05956375838926174, 'num_absorption': 71, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.08423686405337781, 'num_absorption': 101, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.3019648397104447, 'num_absorption': 292, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.10113704368641532, 'num_absorption': 169, 'num_probe_true_positives': 1671, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.004739336492890996, 'num_absorption': 2, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.007518796992481203, 'num_absorption': 5, 'num_probe_true_positives': 665, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.030377668308702793, 'num_absorption': 37, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.27388535031847133, 'num_absorption': 473, 'num_probe_true_positives': 1727, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.04790419161676647, 'num_absorption': 40, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.32429906542056075, 'num_absorption': 347, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.4584221748400853, 'num_absorption': 1075, 'num_probe_true_positives': 2345, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.07567567567567568, 'num_absorption': 14, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.004294478527607362, 'num_absorption': 7, 'num_probe_true_positives': 1630, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.39844026940801136, 'num_absorption': 1124, 'num_probe_true_positives': 2821, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.0267639902676399, 'num_absorption': 44, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.21963824289405684, 'num_absorption': 170, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.008333333333333333, 'num_absorption': 7, 'num_probe_true_positives': 840, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.14390602055800295, 'num_absorption': 98, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.3473684210526316, 'num_absorption': 66, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.023809523809523808, 'num_absorption': 6, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9596273291925466, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.40625}, 'model_performance_preservation': {'ce_loss_score': 0.9588815789473685, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.328125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.609375, 'mse': 2.484375, 'cossim': 0.875}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 131.0, 'l2_ratio': 0.875, 'relative_reconstruction_bias': 0.99609375}, 'sparsity': {'l0': 40.0, 'l1': 364.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '0a27606c-8d4d-449f-ad67-c23bba779fc9', 'datetime_epoch_millis': 1738886529707, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.94985, 'llm_top_1_test_accuracy': 0.66250625, 'llm_top_2_test_accuracy': 0.7216312500000001, 'llm_top_5_test_accuracy': 0.7799, 'llm_top_10_test_accuracy': 0.8296999999999999, 'llm_top_20_test_accuracy': 0.87843125, 'llm_top_50_test_accuracy': 0.9207375, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9509000509977341, 'sae_top_1_test_accuracy': 0.75075625, 'sae_top_2_test_accuracy': 0.8130499999999999, 'sae_top_5_test_accuracy': 0.8722624999999999, 'sae_top_10_test_accuracy': 0.90165625, 'sae_top_20_test_accuracy': 0.9201937499999998, 'sae_top_50_test_accuracy': 0.93555625, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.96260005235672, 'sae_top_1_test_accuracy': 0.8308000000000002, 'sae_top_2_test_accuracy': 0.8662000000000001, 'sae_top_5_test_accuracy': 0.8854, 'sae_top_10_test_accuracy': 0.8987999999999999, 'sae_top_20_test_accuracy': 0.9468, 'sae_top_50_test_accuracy': 0.9583999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9454, 'llm_top_1_test_accuracy': 0.6724, 'llm_top_2_test_accuracy': 0.7358, 'llm_top_5_test_accuracy': 0.7624000000000001, 'llm_top_10_test_accuracy': 0.8026, 'llm_top_20_test_accuracy': 0.8661999999999999, 'llm_top_50_test_accuracy': 0.9042000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.941200053691864, 'sae_top_1_test_accuracy': 0.7767999999999999, 'sae_top_2_test_accuracy': 0.8154, 'sae_top_5_test_accuracy': 0.8486, 'sae_top_10_test_accuracy': 0.9022, 'sae_top_20_test_accuracy': 0.9246000000000001, 'sae_top_50_test_accuracy': 0.9346, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9109999999999999, 'llm_top_1_test_accuracy': 0.6849999999999999, 'llm_top_2_test_accuracy': 0.7438, 'llm_top_5_test_accuracy': 0.7618, 'llm_top_10_test_accuracy': 0.7934, 'llm_top_20_test_accuracy': 0.851, 'llm_top_50_test_accuracy': 0.8879999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000596046448, 'sae_top_1_test_accuracy': 0.8310000000000001, 'sae_top_2_test_accuracy': 0.8448, 'sae_top_5_test_accuracy': 0.8725999999999999, 'sae_top_10_test_accuracy': 0.8865999999999999, 'sae_top_20_test_accuracy': 0.8985999999999998, 'sae_top_50_test_accuracy': 0.9106, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8964000000000001, 'llm_top_1_test_accuracy': 0.6024, 'llm_top_2_test_accuracy': 0.6397999999999999, 'llm_top_5_test_accuracy': 0.6666, 'llm_top_10_test_accuracy': 0.7491999999999999, 'llm_top_20_test_accuracy': 0.8138, 'llm_top_50_test_accuracy': 0.8566, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9076000452041626, 'sae_top_1_test_accuracy': 0.7055999999999999, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.818, 'sae_top_10_test_accuracy': 0.8550000000000001, 'sae_top_20_test_accuracy': 0.8708, 'sae_top_50_test_accuracy': 0.883, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9625000655651093, 'sae_top_1_test_accuracy': 0.649, 'sae_top_2_test_accuracy': 0.813, 'sae_top_5_test_accuracy': 0.875, 'sae_top_10_test_accuracy': 0.928, 'sae_top_20_test_accuracy': 0.935, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9662000000000001, 'llm_top_1_test_accuracy': 0.666, 'llm_top_2_test_accuracy': 0.6900000000000001, 'llm_top_5_test_accuracy': 0.7613999999999999, 'llm_top_10_test_accuracy': 0.8039999999999999, 'llm_top_20_test_accuracy': 0.8622, 'llm_top_50_test_accuracy': 0.923, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9688000440597534, 'sae_top_1_test_accuracy': 0.6042, 'sae_top_2_test_accuracy': 0.6302, 'sae_top_5_test_accuracy': 0.8632, 'sae_top_10_test_accuracy': 0.8856000000000002, 'sae_top_20_test_accuracy': 0.9151999999999999, 'sae_top_50_test_accuracy': 0.9425999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9380000000000001, 'llm_top_1_test_accuracy': 0.63325, 'llm_top_2_test_accuracy': 0.76825, 'llm_top_5_test_accuracy': 0.827, 'llm_top_10_test_accuracy': 0.869, 'llm_top_20_test_accuracy': 0.89925, 'llm_top_50_test_accuracy': 0.9265000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9455000460147858, 'sae_top_1_test_accuracy': 0.79725, 'sae_top_2_test_accuracy': 0.85, 'sae_top_5_test_accuracy': 0.8865, 'sae_top_10_test_accuracy': 0.90925, 'sae_top_20_test_accuracy': 0.91675, 'sae_top_50_test_accuracy': 0.93625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.7243999999999999, 'llm_top_2_test_accuracy': 0.7802, 'llm_top_5_test_accuracy': 0.9019999999999999, 'llm_top_10_test_accuracy': 0.96, 'llm_top_20_test_accuracy': 0.99, 'llm_top_50_test_accuracy': 0.9978, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9940000414848328, 'sae_top_1_test_accuracy': 0.8114000000000001, 'sae_top_2_test_accuracy': 0.8938, 'sae_top_5_test_accuracy': 0.9288000000000001, 'sae_top_10_test_accuracy': 0.9478, 'sae_top_20_test_accuracy': 0.9538, 'sae_top_50_test_accuracy': 0.966, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'e8be4b3c-dfd3-49dd-91ca-e1b0d08e9055', 'datetime_epoch_millis': 1738886775914, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.026217222213745117}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 16384, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 701.0477905273438, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b5f3e18f-7844-4d8a-ac26-9b99d357cf9a', 'datetime_epoch_millis': 1738804062497, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.016138604243358595, 'mean_num_split_features': 1.4166666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2656, 'num_split_features': 2}, {'first_letter': 'b', 'absorption_rate': 0.0006369426751592356, 'num_absorption': 1, 'num_probe_true_positives': 1570, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2701, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.009603841536614645, 'num_absorption': 16, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.05305867665418227, 'num_absorption': 85, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.013422818791946308, 'num_absorption': 16, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.021684737281067557, 'num_absorption': 26, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.006582884500299222, 'num_absorption': 11, 'num_probe_true_positives': 1671, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.09770114942528736, 'num_absorption': 119, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0005790387955993051, 'num_absorption': 1, 'num_probe_true_positives': 1727, 'num_split_features': 4}, {'first_letter': 'n', 'absorption_rate': 0.013173652694610778, 'num_absorption': 11, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.022429906542056073, 'num_absorption': 24, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.008955223880597015, 'num_absorption': 21, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0006134969325153375, 'num_absorption': 1, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.0857851825593761, 'num_absorption': 242, 'num_probe_true_positives': 2821, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.011557177615571776, 'num_absorption': 19, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.021963824289405683, 'num_absorption': 17, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.0014684287812041115, 'num_absorption': 1, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.011904761904761904, 'num_absorption': 3, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995723748059007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004302978515625}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.984375, 'mse': 0.0888671875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.98828125}, 'sparsity': {'l0': 8895.810546875, 'l1': 12608.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'fc35f1d1-c27c-402b-abd2-fe38f12c05aa', 'datetime_epoch_millis': 1738804969188, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95075625, 'llm_top_1_test_accuracy': 0.65444375, 'llm_top_2_test_accuracy': 0.71966875, 'llm_top_5_test_accuracy': 0.7860875, 'llm_top_10_test_accuracy': 0.83056875, 'llm_top_20_test_accuracy': 0.8764437500000001, 'llm_top_50_test_accuracy': 0.9228625, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9578625451773405, 'sae_top_1_test_accuracy': 0.6448624999999999, 'sae_top_2_test_accuracy': 0.6875375000000001, 'sae_top_5_test_accuracy': 0.7562874999999999, 'sae_top_10_test_accuracy': 0.8066500000000001, 'sae_top_20_test_accuracy': 0.8664625000000001, 'sae_top_50_test_accuracy': 0.9122750000000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6862, 'sae_top_2_test_accuracy': 0.6885999999999999, 'sae_top_5_test_accuracy': 0.7842, 'sae_top_10_test_accuracy': 0.8386000000000001, 'sae_top_20_test_accuracy': 0.8806, 'sae_top_50_test_accuracy': 0.9252, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9492, 'llm_top_1_test_accuracy': 0.6794, 'llm_top_2_test_accuracy': 0.717, 'llm_top_5_test_accuracy': 0.767, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8614, 'llm_top_50_test_accuracy': 0.9086000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.6844, 'sae_top_2_test_accuracy': 0.6834, 'sae_top_5_test_accuracy': 0.7402000000000001, 'sae_top_10_test_accuracy': 0.7884, 'sae_top_20_test_accuracy': 0.8503999999999999, 'sae_top_50_test_accuracy': 0.8998000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6862, 'llm_top_2_test_accuracy': 0.732, 'llm_top_5_test_accuracy': 0.7722, 'llm_top_10_test_accuracy': 0.8008, 'llm_top_20_test_accuracy': 0.8532, 'llm_top_50_test_accuracy': 0.8922000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9368000507354737, 'sae_top_1_test_accuracy': 0.5932000000000001, 'sae_top_2_test_accuracy': 0.725, 'sae_top_5_test_accuracy': 0.7638, 'sae_top_10_test_accuracy': 0.8056000000000001, 'sae_top_20_test_accuracy': 0.8465999999999999, 'sae_top_50_test_accuracy': 0.8838000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8948, 'llm_top_1_test_accuracy': 0.6039999999999999, 'llm_top_2_test_accuracy': 0.6508, 'llm_top_5_test_accuracy': 0.6878, 'llm_top_10_test_accuracy': 0.745, 'llm_top_20_test_accuracy': 0.8051999999999999, 'llm_top_50_test_accuracy': 0.8628, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9118000507354737, 'sae_top_1_test_accuracy': 0.5828, 'sae_top_2_test_accuracy': 0.6594, 'sae_top_5_test_accuracy': 0.7343999999999999, 'sae_top_10_test_accuracy': 0.768, 'sae_top_20_test_accuracy': 0.8114000000000001, 'sae_top_50_test_accuracy': 0.8452, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000343322754, 'sae_top_1_test_accuracy': 0.699, 'sae_top_2_test_accuracy': 0.701, 'sae_top_5_test_accuracy': 0.792, 'sae_top_10_test_accuracy': 0.814, 'sae_top_20_test_accuracy': 0.908, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6634, 'llm_top_2_test_accuracy': 0.7002, 'llm_top_5_test_accuracy': 0.7648, 'llm_top_10_test_accuracy': 0.8024000000000001, 'llm_top_20_test_accuracy': 0.8608, 'llm_top_50_test_accuracy': 0.9258, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6342000000000001, 'sae_top_2_test_accuracy': 0.6708000000000001, 'sae_top_5_test_accuracy': 0.7096000000000001, 'sae_top_10_test_accuracy': 0.7712, 'sae_top_20_test_accuracy': 0.8295999999999999, 'sae_top_50_test_accuracy': 0.9026, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.93825, 'llm_top_1_test_accuracy': 0.63375, 'llm_top_2_test_accuracy': 0.7607499999999999, 'llm_top_5_test_accuracy': 0.8265, 'llm_top_10_test_accuracy': 0.8727499999999999, 'llm_top_20_test_accuracy': 0.8977499999999999, 'llm_top_50_test_accuracy': 0.9265, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9530000537633896, 'sae_top_1_test_accuracy': 0.6285, 'sae_top_2_test_accuracy': 0.7015, 'sae_top_5_test_accuracy': 0.7515000000000001, 'sae_top_10_test_accuracy': 0.856, 'sae_top_20_test_accuracy': 0.8875, 'sae_top_50_test_accuracy': 0.9199999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6522, 'llm_top_2_test_accuracy': 0.7814, 'llm_top_5_test_accuracy': 0.9123999999999999, 'llm_top_10_test_accuracy': 0.9642, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9972000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9990000247955322, 'sae_top_1_test_accuracy': 0.6506000000000001, 'sae_top_2_test_accuracy': 0.6706, 'sae_top_5_test_accuracy': 0.7745999999999998, 'sae_top_10_test_accuracy': 0.8114000000000001, 'sae_top_20_test_accuracy': 0.9176, 'sae_top_50_test_accuracy': 0.9895999999999999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'b48744e1-4f43-4d25-af92-4f1a46abab60', 'datetime_epoch_millis': 1738805225729, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the dual-timescale adaptive orthogonality approach. Looking at the baseline results, we can see that the TopK SAE has a higher absorption score      
(0.143) compared to the standard SAE (0.016), indicating room for improvement in feature separation.                                                                     

Here's my experiment plan for the 10 runs:                                                                                                                               

  1 Implement basic FastSlowCompetitionTracker with default timescales (β_f=0.8, β_s=0.98)                                                                               
  2 Test with faster timescales (β_f=0.6, β_s=0.95)                                                                                                                      
  3 Test with slower timescales (β_f=0.9, β_s=0.99)                                                                                                                      
  4 Adjust competition strength (γ=0.5)                                                                                                                                  
  5 Adjust competition strength (γ=2.0)                                                                                                                                  
  6 Combine best timescales with best γ                                                                                                                                  
  7 Add sparse matrix optimizations                                                                                                                                      
  8 Fine-tune intersection threshold                                                                                                                                     
  9 Adjust orthogonality weight λ_2                                                                                                                                      
 10 Final run with best parameters                                                                                                                                       

Let's start with implementing the basic FastSlowCompetitionTracker. We need to modify experiment.py to add the new class and update the training loop.                   

Here are the changes for Run 1:                                                                                                                                          

plot.py                                                                                                                                                                  

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 # write your code here                                                                                                                                                  
 =======                                                                                                                                                                 
 import matplotlib.pyplot as plt                                                                                                                                         
 import numpy as np                                                                                                                                                      
                                                                                                                                                                         
 def plot_competition_dynamics(fast_avg, slow_avg, save_path):                                                                                                           
     """Plot fast and slow moving averages of feature competition"""                                                                                                     
     plt.figure(figsize=(10, 6))                                                                                                                                         
     plt.plot(fast_avg, label='Fast MA', alpha=0.7)                                                                                                                      
     plt.plot(slow_avg, label='Slow MA', alpha=0.7)                                                                                                                      
     plt.xlabel('Training Steps')                                                                                                                                        
     plt.ylabel('Competition Strength')                                                                                                                                  
     plt.title('Dual-Timescale Feature Competition Dynamics')                                                                                                            
     plt.legend()                                                                                                                                                        
     plt.savefig(save_path)                                                                                                                                              
     plt.close()                                                                                                                                                         
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 import sys                                                                                                                                                              
 import os                                                                                                                                                               
 =======                                                                                                                                                                 
 import sys                                                                                                                                                              
 import os                                                                                                                                                               
 import torch.sparse as sparse                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 class TrainerTopK(SAETrainer):                                                                                                                                          
 =======                                                                                                                                                                 
 class FastSlowCompetitionTracker:                                                                                                                                       
     """Tracks feature competition patterns at two timescales"""                                                                                                         
     def __init__(self, n_features, fast_beta=0.8, slow_beta=0.98, device='cuda'):                                                                                       
         self.fast_ma = torch.zeros((n_features, n_features), device=device)                                                                                             
         self.slow_ma = torch.zeros((n_features, n_features), device=device)                                                                                             
         self.fast_beta = fast_beta                                                                                                                                      
         self.slow_beta = slow_beta                                                                                                                                      
         self.device = device                                                                                                                                            
                                                                                                                                                                         
     def update(self, feature_mask):                                                                                                                                     
         """Update competition tracking based on feature co-activation mask"""                                                                                           
         # Compute current competition matrix                                                                                                                            
         comp_matrix = torch.mm(feature_mask.T.float(), feature_mask.float())                                                                                            
                                                                                                                                                                         
         # Update moving averages                                                                                                                                        
         self.fast_ma = self.fast_beta * self.fast_ma + (1 - self.fast_beta) * comp_matrix                                                                               
         self.slow_ma = self.slow_beta * self.slow_ma + (1 - self.slow_beta) * comp_matrix                                                                               
                                                                                                                                                                         
     def get_competition_strength(self, gamma=1.0):                                                                                                                      
         """Compute competition strength based on difference between fast and slow averages"""                                                                           
         return torch.exp(-gamma * (self.fast_ma - self.slow_ma))                                                                                                        
                                                                                                                                                                         
 class TrainerTopK(SAETrainer):                                                                                                                                          
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
     ):                                                                                                                                                                  
 =======                                                                                                                                                                 
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
         competition_gamma=1.0,                                                                                                                                          
         orthogonality_weight=0.1                                                                                                                                        
     ):                                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features"]                                                                                                     
 =======                                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "competition_loss"]                                                                                 
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.dead_features = -1                                                                                                                                         
 =======                                                                                                                                                                 
         self.dead_features = -1                                                                                                                                         
         self.competition_loss = 0.0                                                                                                                                     
                                                                                                                                                                         
         # Initialize competition tracker                                                                                                                                
         self.competition_tracker = FastSlowCompetitionTracker(dict_size, device=device)                                                                                 
         self.competition_gamma = competition_gamma                                                                                                                      
         self.orthogonality_weight = orthogonality_weight                                                                                                                
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def loss(self, x, step=None, logging=False):                                                                                                                        
 =======                                                                                                                                                                 
     def compute_orthogonality_loss(self, feature_acts, feature_mask):                                                                                                   
         """Compute orthogonality loss using competition tracking"""                                                                                                     
         # Update competition tracking                                                                                                                                   
         self.competition_tracker.update(feature_mask)                                                                                                                   
                                                                                                                                                                         
         # Get competition strengths                                                                                                                                     
         comp_strength = self.competition_tracker.get_competition_strength(self.competition_gamma)                                                                       
                                                                                                                                                                         
         # Compute normalized feature correlations                                                                                                                       
         feat_norm = torch.norm(feature_acts, dim=0, keepdim=True)                                                                                                       
         normalized_feats = feature_acts / (feat_norm + 1e-8)                                                                                                            
         feat_corr = torch.mm(normalized_feats.T, normalized_feats)                                                                                                      
                                                                                                                                                                         
         # Weight correlations by competition strength                                                                                                                   
         loss = (comp_strength * feat_corr.abs()).sum()                                                                                                                  
                                                                                                                                                                         
         return loss                                                                                                                                                     
                                                                                                                                                                         
     def loss(self, x, step=None, logging=False):                                                                                                                        
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                                    
 =======                                                                                                                                                                 
         # Get feature activations and mask                                                                                                                              
         feature_mask = (f > 0).float()                                                                                                                                  
                                                                                                                                                                         
         # Compute orthogonality loss                                                                                                                                    
         orth_loss = self.compute_orthogonality_loss(f, feature_mask)                                                                                                    
         self.competition_loss = orth_loss.item()                                                                                                                        
                                                                                                                                                                         
         loss = l2_loss + self.auxk_alpha * auxk_loss + self.orthogonality_weight * orth_loss                                                                            
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

This implementation:                                                                                                                                                     

 1 Adds FastSlowCompetitionTracker class to track feature competition                                                                                                    
 2 Implements dual-timescale moving averages                                                                                                                             
 3 Adds orthogonality loss based on competition patterns                                                                                                                 
 4 Adds visualization support                                                                                                                                            
 5 Uses default timescales from the technical details                                                                                                                    

Let's run this first version:                                                                                                                                            

                                                                                                                                                                         
 python experiment.py --out_dir=run_1                                                                                                                                    
                                                                                                                                                                         

This will give us a baseline for the dual-timescale approach that we can then optimize in subsequent runs. The key metrics to watch are:                                 

 1 Absorption score (lower is better)                                                                                                                                    
 2 Sparse probing accuracy                                                                                                                                               
 3 Competition loss values                                                                                                                                               
 4 Core metrics (reconstruction quality, etc.)                                                                                                                           

Tokens: 43k sent, 2.0k received. Cost: $0.16 message, $0.16 session.
Applied edit to plot.py
Applied edit to experiment.py

python experiment.py --out_dir=run_1
Step 0: 91667.125
Step 100: 54796.0546875
Step 200: 1195461.625
Step 300: 55582.625
Step 400: 618108.875
Step 500: 251244.09375
Step 600: 914828886016.0
Step 700: 49814904.0
Step 800: 4103239150600192.0
Step 900: nan
Step 1000: nan
Step 1100: nan
Step 1200: nan
Step 1300: nan
Step 1400: nan
Step 1500: nan
Step 1600: nan
Step 1700: nan
Step 1800: nan
Step 1900: nan
Step 2000: nan
Step 2100: nan
Step 2200: nan
Step 2300: nan
Step 2400: nan

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_204346_dual_timescale_adaptive_sae/run_1/final_info.json

[2025-02-06 21:03:20]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
coder_out: I'll help implement the dual-timescale adaptive orthogonality approach. Looking at the baseline results, we can see that the TopK SAE has a higher absorption score (0.143) compared to the standard SAE (0.016), indicating room for improvement in feature separation.

Here's my experiment plan for the 10 runs:

1. Implement basic FastSlowCompetitionTracker with default timescales (β_f=0.8, β_s=0.98)
2. Test with faster timescales (β_f=0.6, β_s=0.95) 
3. Test with slower timescales (β_f=0.9, β_s=0.99)
4. Adjust competition strength (γ=0.5)
5. Adjust competition strength (γ=2.0)
6. Combine best timescales with best γ
7. Add sparse matrix optimizations
8. Fine-tune intersection threshold
9. Adjust orthogonality weight λ_2
10. Final run with best parameters

Let's start with implementing the basic FastSlowCompetitionTracker. We need to modify experiment.py to add the new class and update the training loop.

Here are the changes for Run 1:

plot.py
```python
<<<<<<< SEARCH
# write your code here
=======
import matplotlib.pyplot as plt
import numpy as np

def plot_competition_dynamics(fast_avg, slow_avg, save_path):
    """Plot fast and slow moving averages of feature competition"""
    plt.figure(figsize=(10, 6))
    plt.plot(fast_avg, label='Fast MA', alpha=0.7)
    plt.plot(slow_avg, label='Slow MA', alpha=0.7)
    plt.xlabel('Training Steps')
    plt.ylabel('Competition Strength')
    plt.title('Dual-Timescale Feature Competition Dynamics')
    plt.legend()
    plt.savefig(save_path)
    plt.close()
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
import sys
import os
=======
import sys
import os
import torch.sparse as sparse
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
class TrainerTopK(SAETrainer):
=======
class FastSlowCompetitionTracker:
    """Tracks feature competition patterns at two timescales"""
    def __init__(self, n_features, fast_beta=0.8, slow_beta=0.98, device='cuda'):
        self.fast_ma = torch.zeros((n_features, n_features), device=device)
        self.slow_ma = torch.zeros((n_features, n_features), device=device)
        self.fast_beta = fast_beta
        self.slow_beta = slow_beta
        self.device = device
        
    def update(self, feature_mask):
        """Update competition tracking based on feature co-activation mask"""
        # Compute current competition matrix
        comp_matrix = torch.mm(feature_mask.T.float(), feature_mask.float())
        
        # Update moving averages
        self.fast_ma = self.fast_beta * self.fast_ma + (1 - self.fast_beta) * comp_matrix
        self.slow_ma = self.slow_beta * self.slow_ma + (1 - self.slow_beta) * comp_matrix
        
    def get_competition_strength(self, gamma=1.0):
        """Compute competition strength based on difference between fast and slow averages"""
        return torch.exp(-gamma * (self.fast_ma - self.slow_ma))

class TrainerTopK(SAETrainer):
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
    ):
=======
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
        competition_gamma=1.0,
        orthogonality_weight=0.1
    ):
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        self.logging_parameters = ["effective_l0", "dead_features"]
=======
        self.logging_parameters = ["effective_l0", "dead_features", "competition_loss"]
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        self.dead_features = -1
=======
        self.dead_features = -1
        self.competition_loss = 0.0
        
        # Initialize competition tracker
        self.competition_tracker = FastSlowCompetitionTracker(dict_size, device=device)
        self.competition_gamma = competition_gamma
        self.orthogonality_weight = orthogonality_weight
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
    def loss(self, x, step=None, logging=False):
=======
    def compute_orthogonality_loss(self, feature_acts, feature_mask):
        """Compute orthogonality loss using competition tracking"""
        # Update competition tracking
        self.competition_tracker.update(feature_mask)
        
        # Get competition strengths
        comp_strength = self.competition_tracker.get_competition_strength(self.competition_gamma)
        
        # Compute normalized feature correlations
        feat_norm = torch.norm(feature_acts, dim=0, keepdim=True)
        normalized_feats = feature_acts / (feat_norm + 1e-8)
        feat_corr = torch.mm(normalized_feats.T, normalized_feats)
        
        # Weight correlations by competition strength
        loss = (comp_strength * feat_corr.abs()).sum()
        
        return loss

    def loss(self, x, step=None, logging=False):
>>>>>>> REPLACE
```

experiment.py
```python
<<<<<<< SEARCH
        loss = l2_loss + self.auxk_alpha * auxk_loss
=======
        # Get feature activations and mask
        feature_mask = (f > 0).float()
        
        # Compute orthogonality loss
        orth_loss = self.compute_orthogonality_loss(f, feature_mask)
        self.competition_loss = orth_loss.item()
        
        loss = l2_loss + self.auxk_alpha * auxk_loss + self.orthogonality_weight * orth_loss
>>>>>>> REPLACE
```

This implementation:
1. Adds FastSlowCompetitionTracker class to track feature competition
2. Implements dual-timescale moving averages
3. Adds orthogonality loss based on competition patterns
4. Adds visualization support
5. Uses default timescales from the technical details

Let's run this first version:

```bash
python experiment.py --out_dir=run_1
```

This will give us a baseline for the dual-timescale approach that we can then optimize in subsequent runs. The key metrics to watch are:
1. Absorption score (lower is better)
2. Sparse probing accuracy
3. Competition loss values
4. Core metrics (reconstruction quality, etc.), type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.70s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.21s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  4.97s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.80s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.94s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.85s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  1.74s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  probe = torch.load(


Epochs:   0%|          | 0/50 [00:00<?, ?it/s][A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.98it/s, Loss=nan][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.98it/s, Loss=nan][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.98it/s, Loss=nan][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.98it/s, Loss=nan][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.98it/s, Loss=nan][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 35.01it/s, Loss=nan][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 35.01it/s, Loss=nan][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 35.01it/s, Loss=nan][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 35.01it/s, Loss=nan][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 35.01it/s, Loss=nan][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.58it/s, Loss=nan][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.58it/s, Loss=nan][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.58it/s, Loss=nan][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.58it/s, Loss=nan][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.58it/s, Loss=nan][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.44it/s, Loss=nan][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.44it/s, Loss=nan][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.44it/s, Loss=nan][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.44it/s, Loss=nan][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.44it/s, Loss=nan][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 27.79it/s, Loss=nan][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 27.79it/s, Loss=nan][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 27.79it/s, Loss=nan][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 27.79it/s, Loss=nan][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:01<00:00, 27.79it/s, Loss=nan][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 21.37it/s, Loss=nan][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 21.37it/s, Loss=nan][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 21.37it/s, Loss=nan][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 21.37it/s, Loss=nan][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 21.37it/s, Loss=nan][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 24.73it/s, Loss=nan][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 24.73it/s, Loss=nan][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 24.73it/s, Loss=nan][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 24.73it/s, Loss=nan][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 24.73it/s, Loss=nan][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:   0%|          | 0/50 [00:01<?, ?it/s, Mean Loss=nan, LR=1.00e-02][A

Epochs:   2%|▏         | 1/50 [00:01<01:01,  1.26s/it, Mean Loss=nan, LR=1.00e-02][A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 27.01it/s, Loss=nan][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 27.01it/s, Loss=nan][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 27.01it/s, Loss=nan][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 27.01it/s, Loss=nan][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 27.01it/s, Loss=nan][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.36it/s, Loss=nan][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.36it/s, Loss=nan][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.36it/s, Loss=nan][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.36it/s, Loss=nan][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.36it/s, Loss=nan][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.61it/s, Loss=nan][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.61it/s, Loss=nan][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=nan][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=nan][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=nan][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=nan][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=nan][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=nan][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=nan][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=nan][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=nan][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:   2%|▏         | 1/50 [00:02<01:01,  1.26s/it, Mean Loss=nan, LR=8.71e-03][A

Epochs:   4%|▍         | 2/50 [00:02<01:03,  1.33s/it, Mean Loss=nan, LR=8.71e-03][A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.89it/s, Loss=nan][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.89it/s, Loss=nan][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.89it/s, Loss=nan][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.89it/s, Loss=nan][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.89it/s, Loss=nan][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=nan][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=nan][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=nan][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=nan][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=nan][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.51it/s, Loss=nan][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.51it/s, Loss=nan][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.51it/s, Loss=nan][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.51it/s, Loss=nan][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.51it/s, Loss=nan][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.64it/s, Loss=nan][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.64it/s, Loss=nan][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:   4%|▍         | 2/50 [00:04<01:03,  1.33s/it, Mean Loss=nan, LR=7.59e-03][A

Epochs:   6%|▌         | 3/50 [00:04<01:03,  1.35s/it, Mean Loss=nan, LR=7.59e-03][A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:01, 23.88it/s, Loss=nan][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:01, 23.88it/s, Loss=nan][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:01, 23.88it/s, Loss=nan][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:01, 23.88it/s, Loss=nan][A[A


Epoch 4/50:  38%|███▊      | 13/34 [00:00<00:01, 17.68it/s, Loss=nan][A[A


Epoch 4/50:  38%|███▊      | 13/34 [00:00<00:01, 17.68it/s, Loss=nan][A[A


Epoch 4/50:  38%|███▊      | 13/34 [00:00<00:01, 17.68it/s, Loss=nan][A[A


Epoch 4/50:  38%|███▊      | 13/34 [00:00<00:01, 17.68it/s, Loss=nan][A[A


Epoch 4/50:  38%|███▊      | 13/34 [00:00<00:01, 17.68it/s, Loss=nan][A[A


Epoch 4/50:  50%|█████     | 17/34 [00:00<00:00, 22.39it/s, Loss=nan][A[A


Epoch 4/50:  50%|█████     | 17/34 [00:00<00:00, 22.39it/s, Loss=nan][A[A


Epoch 4/50:  50%|█████     | 17/34 [00:00<00:00, 22.39it/s, Loss=nan][A[A


Epoch 4/50:  50%|█████     | 17/34 [00:00<00:00, 22.39it/s, Loss=nan][A[A


Epoch 4/50:  50%|█████     | 17/34 [00:00<00:00, 22.39it/s, Loss=nan][A[A


Epoch 4/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.13it/s, Loss=nan][A[A


Epoch 4/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.13it/s, Loss=nan][A[A


Epoch 4/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.13it/s, Loss=nan][A[A


Epoch 4/50:  62%|██████▏   | 21/34 [00:01<00:00, 26.13it/s, Loss=nan][A[A


Epoch 4/50:  62%|██████▏   | 21/34 [00:01<00:00, 26.13it/s, Loss=nan][A[A


Epoch 4/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.80it/s, Loss=nan][A[A


Epoch 4/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.80it/s, Loss=nan][A[A


Epoch 4/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.80it/s, Loss=nan][A[A


Epoch 4/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.80it/s, Loss=nan][A[A


Epoch 4/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.80it/s, Loss=nan][A[A


Epoch 4/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.32it/s, Loss=nan][A[A


Epoch 4/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.32it/s, Loss=nan][A[A


Epoch 4/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.32it/s, Loss=nan][A[A


Epoch 4/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.32it/s, Loss=nan][A[A


Epoch 4/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.32it/s, Loss=nan][A[A


Epoch 4/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.12it/s, Loss=nan][A[A


Epoch 4/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.12it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:   6%|▌         | 3/50 [00:05<01:03,  1.35s/it, Mean Loss=nan, LR=6.61e-03][A

Epochs:   8%|▊         | 4/50 [00:05<01:03,  1.38s/it, Mean Loss=nan, LR=6.61e-03][A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  8.73it/s, Loss=nan][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  8.73it/s, Loss=nan][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  8.73it/s, Loss=nan][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  8.73it/s, Loss=nan][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  8.73it/s, Loss=nan][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 19.87it/s, Loss=nan][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 19.87it/s, Loss=nan][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 19.87it/s, Loss=nan][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 19.87it/s, Loss=nan][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 19.87it/s, Loss=nan][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 25.92it/s, Loss=nan][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 25.92it/s, Loss=nan][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 25.92it/s, Loss=nan][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 25.92it/s, Loss=nan][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 25.92it/s, Loss=nan][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=nan][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=nan][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=nan][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=nan][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=nan][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=nan][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=nan][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=nan][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=nan][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=nan][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:   8%|▊         | 4/50 [00:06<01:03,  1.38s/it, Mean Loss=nan, LR=5.75e-03][A

Epochs:  10%|█         | 5/50 [00:06<01:02,  1.38s/it, Mean Loss=nan, LR=5.75e-03][A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.51it/s, Loss=nan][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.51it/s, Loss=nan][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.51it/s, Loss=nan][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.51it/s, Loss=nan][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.51it/s, Loss=nan][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.57it/s, Loss=nan][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.57it/s, Loss=nan][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.57it/s, Loss=nan][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.57it/s, Loss=nan][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.57it/s, Loss=nan][A[A


Epoch 6/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 6/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 6/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 6/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 6/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 6/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=nan][A[A


Epoch 6/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=nan][A[A


Epoch 6/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=nan][A[A


Epoch 6/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=nan][A[A


Epoch 6/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=nan][A[A


Epoch 6/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.76it/s, Loss=nan][A[A


Epoch 6/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.76it/s, Loss=nan][A[A


Epoch 6/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.76it/s, Loss=nan][A[A


Epoch 6/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.76it/s, Loss=nan][A[A


Epoch 6/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.76it/s, Loss=nan][A[A


Epoch 6/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 6/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 6/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 6/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 6/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 6/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 6/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 6/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 6/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 6/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:  10%|█         | 5/50 [00:08<01:02,  1.38s/it, Mean Loss=nan, LR=5.01e-03][A

Epochs:  12%|█▏        | 6/50 [00:08<01:00,  1.38s/it, Mean Loss=nan, LR=5.01e-03][A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=nan][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=nan][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=nan][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=nan][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=nan][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.23it/s, Loss=nan][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.23it/s, Loss=nan][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.23it/s, Loss=nan][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.23it/s, Loss=nan][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.23it/s, Loss=nan][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=nan][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=nan][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.77it/s, Loss=nan][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.77it/s, Loss=nan][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.77it/s, Loss=nan][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.77it/s, Loss=nan][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.77it/s, Loss=nan][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.03it/s, Loss=nan][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.03it/s, Loss=nan][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.03it/s, Loss=nan][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.03it/s, Loss=nan][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.03it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:  12%|█▏        | 6/50 [00:09<01:00,  1.38s/it, Mean Loss=nan, LR=4.37e-03][A

Epochs:  14%|█▍        | 7/50 [00:09<00:59,  1.38s/it, Mean Loss=nan, LR=4.37e-03][A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.74it/s, Loss=nan][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.74it/s, Loss=nan][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.74it/s, Loss=nan][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.74it/s, Loss=nan][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.74it/s, Loss=nan][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:01, 19.75it/s, Loss=nan][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:01, 19.75it/s, Loss=nan][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:01, 19.75it/s, Loss=nan][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:01, 19.75it/s, Loss=nan][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:01, 19.75it/s, Loss=nan][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=nan][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=nan][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=nan][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=nan][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=nan][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.25it/s, Loss=nan][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.25it/s, Loss=nan][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.94it/s, Loss=nan][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.94it/s, Loss=nan][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.94it/s, Loss=nan][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.94it/s, Loss=nan][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.94it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:  14%|█▍        | 7/50 [00:10<00:59,  1.38s/it, Mean Loss=nan, LR=3.80e-03][A

Epochs:  16%|█▌        | 8/50 [00:10<00:58,  1.38s/it, Mean Loss=nan, LR=3.80e-03][A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 9/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 9/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 9/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 9/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 9/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 9/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 9/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 9/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 9/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 9/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 9/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 9/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 9/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 9/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 9/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 9/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=nan][A[A


Epoch 9/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=nan][A[A


Epoch 9/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=nan][A[A


Epoch 9/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=nan][A[A


Epoch 9/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=nan][A[A


Epoch 9/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.67it/s, Loss=nan][A[A


Epoch 9/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.67it/s, Loss=nan][A[A


Epoch 9/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.67it/s, Loss=nan][A[A


Epoch 9/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.67it/s, Loss=nan][A[A


Epoch 9/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.67it/s, Loss=nan][A[A


Epoch 9/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 9/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 9/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 9/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 9/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


                                                                     [A[A

Epochs:  16%|█▌        | 8/50 [00:12<00:58,  1.38s/it, Mean Loss=nan, LR=3.31e-03][A

Epochs:  18%|█▊        | 9/50 [00:12<00:56,  1.38s/it, Mean Loss=nan, LR=3.31e-03][A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 19.76it/s, Loss=nan][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 19.76it/s, Loss=nan][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 19.76it/s, Loss=nan][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 19.76it/s, Loss=nan][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 19.76it/s, Loss=nan][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 25.05it/s, Loss=nan][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 25.05it/s, Loss=nan][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 25.05it/s, Loss=nan][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 25.05it/s, Loss=nan][A[A


Epoch 10/50:  38%|███▊      | 13/34 [00:00<00:01, 18.05it/s, Loss=nan][A[A


Epoch 10/50:  38%|███▊      | 13/34 [00:00<00:01, 18.05it/s, Loss=nan][A[A


Epoch 10/50:  38%|███▊      | 13/34 [00:00<00:01, 18.05it/s, Loss=nan][A[A


Epoch 10/50:  38%|███▊      | 13/34 [00:00<00:01, 18.05it/s, Loss=nan][A[A


Epoch 10/50:  38%|███▊      | 13/34 [00:00<00:01, 18.05it/s, Loss=nan][A[A


Epoch 10/50:  50%|█████     | 17/34 [00:00<00:00, 22.59it/s, Loss=nan][A[A


Epoch 10/50:  50%|█████     | 17/34 [00:00<00:00, 22.59it/s, Loss=nan][A[A


Epoch 10/50:  50%|█████     | 17/34 [00:00<00:00, 22.59it/s, Loss=nan][A[A


Epoch 10/50:  50%|█████     | 17/34 [00:00<00:00, 22.59it/s, Loss=nan][A[A


Epoch 10/50:  50%|█████     | 17/34 [00:00<00:00, 22.59it/s, Loss=nan][A[A


Epoch 10/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.29it/s, Loss=nan][A[A


Epoch 10/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.29it/s, Loss=nan][A[A


Epoch 10/50:  62%|██████▏   | 21/34 [00:00<00:00, 26.29it/s, Loss=nan][A[A


Epoch 10/50:  62%|██████▏   | 21/34 [00:01<00:00, 26.29it/s, Loss=nan][A[A


Epoch 10/50:  62%|██████▏   | 21/34 [00:01<00:00, 26.29it/s, Loss=nan][A[A


Epoch 10/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.90it/s, Loss=nan][A[A


Epoch 10/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.90it/s, Loss=nan][A[A


Epoch 10/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.90it/s, Loss=nan][A[A


Epoch 10/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.90it/s, Loss=nan][A[A


Epoch 10/50:  74%|███████▎  | 25/34 [00:01<00:00, 20.90it/s, Loss=nan][A[A


Epoch 10/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.44it/s, Loss=nan][A[A


Epoch 10/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.44it/s, Loss=nan][A[A


Epoch 10/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.44it/s, Loss=nan][A[A


Epoch 10/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.44it/s, Loss=nan][A[A


Epoch 10/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.44it/s, Loss=nan][A[A


Epoch 10/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 10/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  18%|█▊        | 9/50 [00:13<00:56,  1.38s/it, Mean Loss=nan, LR=2.88e-03][A

Epochs:  20%|██        | 10/50 [00:13<00:55,  1.39s/it, Mean Loss=nan, LR=2.88e-03][A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=nan][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=nan][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=nan][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=nan][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=nan][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 26.81it/s, Loss=nan][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 26.81it/s, Loss=nan][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 26.81it/s, Loss=nan][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 26.81it/s, Loss=nan][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 26.81it/s, Loss=nan][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=nan][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=nan][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  20%|██        | 10/50 [00:15<00:55,  1.39s/it, Mean Loss=nan, LR=2.51e-03][A

Epochs:  22%|██▏       | 11/50 [00:15<00:54,  1.39s/it, Mean Loss=nan, LR=2.51e-03][A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 12/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=nan][A[A


Epoch 12/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=nan][A[A


Epoch 12/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=nan][A[A


Epoch 12/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=nan][A[A


Epoch 12/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=nan][A[A


Epoch 12/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 12/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 12/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 12/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 12/50:  41%|████      | 14/34 [00:00<00:01, 19.94it/s, Loss=nan][A[A


Epoch 12/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.03it/s, Loss=nan][A[A


Epoch 12/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.03it/s, Loss=nan][A[A


Epoch 12/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.03it/s, Loss=nan][A[A


Epoch 12/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.03it/s, Loss=nan][A[A


Epoch 12/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.03it/s, Loss=nan][A[A


Epoch 12/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.28it/s, Loss=nan][A[A


Epoch 12/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.28it/s, Loss=nan][A[A


Epoch 12/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.28it/s, Loss=nan][A[A


Epoch 12/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.28it/s, Loss=nan][A[A


Epoch 12/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.28it/s, Loss=nan][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.79it/s, Loss=nan][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.79it/s, Loss=nan][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.79it/s, Loss=nan][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.79it/s, Loss=nan][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.79it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  22%|██▏       | 11/50 [00:16<00:54,  1.39s/it, Mean Loss=nan, LR=2.19e-03][A

Epochs:  24%|██▍       | 12/50 [00:16<00:52,  1.39s/it, Mean Loss=nan, LR=2.19e-03][A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.10it/s, Loss=nan][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.10it/s, Loss=nan][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.10it/s, Loss=nan][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.10it/s, Loss=nan][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.10it/s, Loss=nan][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.07it/s, Loss=nan][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.07it/s, Loss=nan][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.07it/s, Loss=nan][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.07it/s, Loss=nan][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.07it/s, Loss=nan][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.21it/s, Loss=nan][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.52it/s, Loss=nan][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.52it/s, Loss=nan][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.52it/s, Loss=nan][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.52it/s, Loss=nan][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.52it/s, Loss=nan][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=nan][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=nan][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=nan][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=nan][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  24%|██▍       | 12/50 [00:17<00:52,  1.39s/it, Mean Loss=nan, LR=1.91e-03][A

Epochs:  26%|██▌       | 13/50 [00:17<00:51,  1.38s/it, Mean Loss=nan, LR=1.91e-03][A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.51it/s, Loss=nan][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.51it/s, Loss=nan][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.35it/s, Loss=nan][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.35it/s, Loss=nan][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.35it/s, Loss=nan][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.35it/s, Loss=nan][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.35it/s, Loss=nan][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.62it/s, Loss=nan][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.62it/s, Loss=nan][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.62it/s, Loss=nan][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.62it/s, Loss=nan][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.62it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  26%|██▌       | 13/50 [00:19<00:51,  1.38s/it, Mean Loss=nan, LR=1.66e-03][A

Epochs:  28%|██▊       | 14/50 [00:19<00:49,  1.38s/it, Mean Loss=nan, LR=1.66e-03][A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=nan][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.84it/s, Loss=nan][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.84it/s, Loss=nan][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.84it/s, Loss=nan][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.84it/s, Loss=nan][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.84it/s, Loss=nan][A[A


Epoch 15/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=nan][A[A


Epoch 15/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=nan][A[A


Epoch 15/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=nan][A[A


Epoch 15/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=nan][A[A


Epoch 15/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=nan][A[A


Epoch 15/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 15/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 15/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 15/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 15/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 15/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 15/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 15/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 15/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 15/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 15/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.57it/s, Loss=nan][A[A


Epoch 15/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.57it/s, Loss=nan][A[A


Epoch 15/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 15/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 15/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 15/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=nan][A[A


Epoch 15/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=nan][A[A


Epoch 15/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=nan][A[A


Epoch 15/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=nan][A[A


Epoch 15/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=nan][A[A


Epoch 15/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 15/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 15/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 15/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 15/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  28%|██▊       | 14/50 [00:20<00:49,  1.38s/it, Mean Loss=nan, LR=1.45e-03][A

Epochs:  30%|███       | 15/50 [00:20<00:48,  1.38s/it, Mean Loss=nan, LR=1.45e-03][A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.88it/s, Loss=nan][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=nan][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=nan][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=nan][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=nan][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=nan][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=nan][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=nan][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=nan][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=nan][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=nan][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=nan][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=nan][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  30%|███       | 15/50 [00:22<00:48,  1.38s/it, Mean Loss=nan, LR=1.26e-03][A

Epochs:  32%|███▏      | 16/50 [00:22<00:46,  1.38s/it, Mean Loss=nan, LR=1.26e-03][A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.33it/s, Loss=nan][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.33it/s, Loss=nan][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.33it/s, Loss=nan][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.33it/s, Loss=nan][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.33it/s, Loss=nan][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.60it/s, Loss=nan][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.60it/s, Loss=nan][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.60it/s, Loss=nan][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.60it/s, Loss=nan][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.60it/s, Loss=nan][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.90it/s, Loss=nan][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.90it/s, Loss=nan][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.90it/s, Loss=nan][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.90it/s, Loss=nan][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.90it/s, Loss=nan][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  32%|███▏      | 16/50 [00:23<00:46,  1.38s/it, Mean Loss=nan, LR=1.10e-03][A

Epochs:  34%|███▍      | 17/50 [00:23<00:45,  1.38s/it, Mean Loss=nan, LR=1.10e-03][A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 18/50:  29%|██▉       | 10/34 [00:00<00:00, 26.54it/s, Loss=nan][A[A


Epoch 18/50:  29%|██▉       | 10/34 [00:00<00:00, 26.54it/s, Loss=nan][A[A


Epoch 18/50:  29%|██▉       | 10/34 [00:00<00:00, 26.54it/s, Loss=nan][A[A


Epoch 18/50:  29%|██▉       | 10/34 [00:00<00:00, 26.54it/s, Loss=nan][A[A


Epoch 18/50:  29%|██▉       | 10/34 [00:00<00:00, 26.54it/s, Loss=nan][A[A


Epoch 18/50:  41%|████      | 14/34 [00:00<00:00, 20.04it/s, Loss=nan][A[A


Epoch 18/50:  41%|████      | 14/34 [00:00<00:00, 20.04it/s, Loss=nan][A[A


Epoch 18/50:  41%|████      | 14/34 [00:00<00:00, 20.04it/s, Loss=nan][A[A


Epoch 18/50:  41%|████      | 14/34 [00:00<00:00, 20.04it/s, Loss=nan][A[A


Epoch 18/50:  41%|████      | 14/34 [00:00<00:00, 20.04it/s, Loss=nan][A[A


Epoch 18/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 18/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 18/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 18/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 18/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 18/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.59it/s, Loss=nan][A[A


Epoch 18/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.59it/s, Loss=nan][A[A


Epoch 18/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.59it/s, Loss=nan][A[A


Epoch 18/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.59it/s, Loss=nan][A[A


Epoch 18/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.59it/s, Loss=nan][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.09it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  34%|███▍      | 17/50 [00:24<00:45,  1.38s/it, Mean Loss=nan, LR=9.55e-04][A

Epochs:  36%|███▌      | 18/50 [00:24<00:44,  1.38s/it, Mean Loss=nan, LR=9.55e-04][A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.66it/s, Loss=nan][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.66it/s, Loss=nan][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.66it/s, Loss=nan][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.66it/s, Loss=nan][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.66it/s, Loss=nan][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=nan][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.54it/s, Loss=nan][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.54it/s, Loss=nan][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=nan][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=nan][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=nan][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.74it/s, Loss=nan][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.15it/s, Loss=nan][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.15it/s, Loss=nan][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.15it/s, Loss=nan][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.15it/s, Loss=nan][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.15it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  36%|███▌      | 18/50 [00:26<00:44,  1.38s/it, Mean Loss=nan, LR=8.32e-04][A

Epochs:  38%|███▊      | 19/50 [00:26<00:42,  1.38s/it, Mean Loss=nan, LR=8.32e-04][A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=nan][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=nan][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=nan][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=nan][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=nan][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=nan][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.46it/s, Loss=nan][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.46it/s, Loss=nan][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.46it/s, Loss=nan][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.46it/s, Loss=nan][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.46it/s, Loss=nan][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  38%|███▊      | 19/50 [00:27<00:42,  1.38s/it, Mean Loss=nan, LR=7.24e-04][A

Epochs:  40%|████      | 20/50 [00:27<00:41,  1.38s/it, Mean Loss=nan, LR=7.24e-04][A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=nan][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=nan][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=nan][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=nan][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=nan][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=nan][A[A


Epoch 21/50:  29%|██▉       | 10/34 [00:00<00:00, 26.88it/s, Loss=nan][A[A


Epoch 21/50:  29%|██▉       | 10/34 [00:00<00:00, 26.88it/s, Loss=nan][A[A


Epoch 21/50:  29%|██▉       | 10/34 [00:00<00:00, 26.88it/s, Loss=nan][A[A


Epoch 21/50:  29%|██▉       | 10/34 [00:00<00:00, 26.88it/s, Loss=nan][A[A


Epoch 21/50:  29%|██▉       | 10/34 [00:00<00:00, 26.88it/s, Loss=nan][A[A


Epoch 21/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 21/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 21/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 21/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 21/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 21/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 21/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 21/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 21/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 21/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 21/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.29it/s, Loss=nan][A[A


Epoch 21/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.29it/s, Loss=nan][A[A


Epoch 21/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=nan][A[A


Epoch 21/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=nan][A[A


Epoch 21/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=nan][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.67it/s, Loss=nan][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.67it/s, Loss=nan][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.67it/s, Loss=nan][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.67it/s, Loss=nan][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.67it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  40%|████      | 20/50 [00:28<00:41,  1.38s/it, Mean Loss=nan, LR=6.31e-04][A

Epochs:  42%|████▏     | 21/50 [00:28<00:40,  1.38s/it, Mean Loss=nan, LR=6.31e-04][A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=nan][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=nan][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=nan][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=nan][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=nan][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 21.16it/s, Loss=nan][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 21.16it/s, Loss=nan][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 21.16it/s, Loss=nan][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 21.16it/s, Loss=nan][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 21.16it/s, Loss=nan][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 27.16it/s, Loss=nan][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 27.16it/s, Loss=nan][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 27.16it/s, Loss=nan][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 27.16it/s, Loss=nan][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 27.16it/s, Loss=nan][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:00, 20.23it/s, Loss=nan][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:00, 20.23it/s, Loss=nan][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:00, 20.23it/s, Loss=nan][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:00, 20.23it/s, Loss=nan][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:00, 20.23it/s, Loss=nan][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=nan][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=nan][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=nan][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=nan][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=nan][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=nan][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=nan][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.42it/s, Loss=nan][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  42%|████▏     | 21/50 [00:30<00:40,  1.38s/it, Mean Loss=nan, LR=5.50e-04][A

Epochs:  44%|████▍     | 22/50 [00:30<00:38,  1.38s/it, Mean Loss=nan, LR=5.50e-04][A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 21.25it/s, Loss=nan][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 21.25it/s, Loss=nan][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 21.25it/s, Loss=nan][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 21.25it/s, Loss=nan][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 21.25it/s, Loss=nan][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=nan][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=nan][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=nan][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=nan][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=nan][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=nan][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=nan][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=nan][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=nan][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=nan][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=nan][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=nan][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  44%|████▍     | 22/50 [00:31<00:38,  1.38s/it, Mean Loss=nan, LR=4.79e-04][A

Epochs:  46%|████▌     | 23/50 [00:31<00:37,  1.38s/it, Mean Loss=nan, LR=4.79e-04][A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=nan][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=nan][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=nan][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=nan][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=nan][A[A


Epoch 24/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 24/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 24/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 24/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 24/50:  29%|██▉       | 10/34 [00:00<00:00, 26.94it/s, Loss=nan][A[A


Epoch 24/50:  41%|████      | 14/34 [00:00<00:01, 18.77it/s, Loss=nan][A[A


Epoch 24/50:  41%|████      | 14/34 [00:00<00:01, 18.77it/s, Loss=nan][A[A


Epoch 24/50:  41%|████      | 14/34 [00:00<00:01, 18.77it/s, Loss=nan][A[A


Epoch 24/50:  41%|████      | 14/34 [00:00<00:01, 18.77it/s, Loss=nan][A[A


Epoch 24/50:  41%|████      | 14/34 [00:00<00:01, 18.77it/s, Loss=nan][A[A


Epoch 24/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.09it/s, Loss=nan][A[A


Epoch 24/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.09it/s, Loss=nan][A[A


Epoch 24/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.09it/s, Loss=nan][A[A


Epoch 24/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.09it/s, Loss=nan][A[A


Epoch 24/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.09it/s, Loss=nan][A[A


Epoch 24/50:  65%|██████▍   | 22/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 24/50:  65%|██████▍   | 22/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 24/50:  65%|██████▍   | 22/34 [00:01<00:00, 26.56it/s, Loss=nan][A[A


Epoch 24/50:  65%|██████▍   | 22/34 [00:01<00:00, 26.56it/s, Loss=nan][A[A


Epoch 24/50:  65%|██████▍   | 22/34 [00:01<00:00, 26.56it/s, Loss=nan][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.06it/s, Loss=nan][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.06it/s, Loss=nan][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.06it/s, Loss=nan][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.06it/s, Loss=nan][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.06it/s, Loss=nan][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.34it/s, Loss=nan][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.34it/s, Loss=nan][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.34it/s, Loss=nan][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.34it/s, Loss=nan][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.34it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  46%|████▌     | 23/50 [00:33<00:37,  1.38s/it, Mean Loss=nan, LR=4.17e-04][A

Epochs:  48%|████▊     | 24/50 [00:33<00:36,  1.39s/it, Mean Loss=nan, LR=4.17e-04][A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.86it/s, Loss=nan][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 27.00it/s, Loss=nan][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:00, 20.30it/s, Loss=nan][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:00, 20.30it/s, Loss=nan][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:00, 20.30it/s, Loss=nan][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:00, 20.30it/s, Loss=nan][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:00, 20.30it/s, Loss=nan][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=nan][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=nan][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=nan][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  48%|████▊     | 24/50 [00:34<00:36,  1.39s/it, Mean Loss=nan, LR=3.63e-04][A

Epochs:  50%|█████     | 25/50 [00:34<00:34,  1.39s/it, Mean Loss=nan, LR=3.63e-04][A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=nan][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=nan][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=nan][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=nan][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=nan][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.80it/s, Loss=nan][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=nan][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=nan][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=nan][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=nan][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=nan][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=nan][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=nan][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=nan][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=nan][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=nan][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.67it/s, Loss=nan][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.67it/s, Loss=nan][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.67it/s, Loss=nan][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.67it/s, Loss=nan][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.67it/s, Loss=nan][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=nan][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  50%|█████     | 25/50 [00:35<00:34,  1.39s/it, Mean Loss=nan, LR=3.16e-04][A

Epochs:  52%|█████▏    | 26/50 [00:35<00:33,  1.38s/it, Mean Loss=nan, LR=3.16e-04][A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.64it/s, Loss=nan][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.64it/s, Loss=nan][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.64it/s, Loss=nan][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.64it/s, Loss=nan][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.64it/s, Loss=nan][A[A


Epoch 27/50:  29%|██▉       | 10/34 [00:00<00:00, 26.72it/s, Loss=nan][A[A


Epoch 27/50:  29%|██▉       | 10/34 [00:00<00:00, 26.72it/s, Loss=nan][A[A


Epoch 27/50:  29%|██▉       | 10/34 [00:00<00:00, 26.72it/s, Loss=nan][A[A


Epoch 27/50:  29%|██▉       | 10/34 [00:00<00:00, 26.72it/s, Loss=nan][A[A


Epoch 27/50:  29%|██▉       | 10/34 [00:00<00:00, 26.72it/s, Loss=nan][A[A


Epoch 27/50:  41%|████      | 14/34 [00:00<00:01, 19.55it/s, Loss=nan][A[A


Epoch 27/50:  41%|████      | 14/34 [00:00<00:01, 19.55it/s, Loss=nan][A[A


Epoch 27/50:  41%|████      | 14/34 [00:00<00:01, 19.55it/s, Loss=nan][A[A


Epoch 27/50:  41%|████      | 14/34 [00:00<00:01, 19.55it/s, Loss=nan][A[A


Epoch 27/50:  41%|████      | 14/34 [00:00<00:01, 19.55it/s, Loss=nan][A[A


Epoch 27/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.87it/s, Loss=nan][A[A


Epoch 27/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.87it/s, Loss=nan][A[A


Epoch 27/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.87it/s, Loss=nan][A[A


Epoch 27/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.87it/s, Loss=nan][A[A


Epoch 27/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.87it/s, Loss=nan][A[A


Epoch 27/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.24it/s, Loss=nan][A[A


Epoch 27/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.24it/s, Loss=nan][A[A


Epoch 27/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.24it/s, Loss=nan][A[A


Epoch 27/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.24it/s, Loss=nan][A[A


Epoch 27/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.24it/s, Loss=nan][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.33it/s, Loss=nan][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.33it/s, Loss=nan][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.33it/s, Loss=nan][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.33it/s, Loss=nan][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.33it/s, Loss=nan][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=nan][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=nan][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=nan][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=nan][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  52%|█████▏    | 26/50 [00:37<00:33,  1.38s/it, Mean Loss=nan, LR=2.75e-04][A

Epochs:  54%|█████▍    | 27/50 [00:37<00:31,  1.39s/it, Mean Loss=nan, LR=2.75e-04][A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 20.50it/s, Loss=nan][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 20.50it/s, Loss=nan][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 20.50it/s, Loss=nan][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 20.50it/s, Loss=nan][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 20.50it/s, Loss=nan][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=nan][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=nan][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.26it/s, Loss=nan][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.26it/s, Loss=nan][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.26it/s, Loss=nan][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.26it/s, Loss=nan][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.26it/s, Loss=nan][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.68it/s, Loss=nan][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.68it/s, Loss=nan][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=nan][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=nan][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=nan][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=nan][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=nan][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  54%|█████▍    | 27/50 [00:38<00:31,  1.39s/it, Mean Loss=nan, LR=2.40e-04][A

Epochs:  56%|█████▌    | 28/50 [00:38<00:30,  1.38s/it, Mean Loss=nan, LR=2.40e-04][A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=nan][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=nan][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=nan][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=nan][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=nan][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:01, 19.95it/s, Loss=nan][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:01, 19.95it/s, Loss=nan][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:01, 19.95it/s, Loss=nan][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:01, 19.95it/s, Loss=nan][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:01, 19.95it/s, Loss=nan][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=nan][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=nan][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=nan][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=nan][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=nan][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.51it/s, Loss=nan][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.51it/s, Loss=nan][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.51it/s, Loss=nan][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.73it/s, Loss=nan][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.73it/s, Loss=nan][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.73it/s, Loss=nan][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.73it/s, Loss=nan][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.73it/s, Loss=nan][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  56%|█████▌    | 28/50 [00:40<00:30,  1.38s/it, Mean Loss=nan, LR=2.09e-04][A

Epochs:  58%|█████▊    | 29/50 [00:40<00:29,  1.38s/it, Mean Loss=nan, LR=2.09e-04][A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 30/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 30/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 30/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 30/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 30/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 30/50:  41%|████      | 14/34 [00:00<00:01, 19.91it/s, Loss=nan][A[A


Epoch 30/50:  41%|████      | 14/34 [00:00<00:01, 19.91it/s, Loss=nan][A[A


Epoch 30/50:  41%|████      | 14/34 [00:00<00:01, 19.91it/s, Loss=nan][A[A


Epoch 30/50:  41%|████      | 14/34 [00:00<00:01, 19.91it/s, Loss=nan][A[A


Epoch 30/50:  41%|████      | 14/34 [00:00<00:01, 19.91it/s, Loss=nan][A[A


Epoch 30/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 30/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 30/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 30/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 30/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 30/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 30/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 30/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 30/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 30/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 30/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 30/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 30/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 30/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 30/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 30/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.08it/s, Loss=nan][A[A


Epoch 30/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.08it/s, Loss=nan][A[A


Epoch 30/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.08it/s, Loss=nan][A[A


Epoch 30/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.08it/s, Loss=nan][A[A


Epoch 30/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.08it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  58%|█████▊    | 29/50 [00:41<00:29,  1.38s/it, Mean Loss=nan, LR=1.82e-04][A

Epochs:  60%|██████    | 30/50 [00:41<00:27,  1.38s/it, Mean Loss=nan, LR=1.82e-04][A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.14it/s, Loss=nan][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=nan][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=nan][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=nan][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=nan][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=nan][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=nan][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.38it/s, Loss=nan][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.38it/s, Loss=nan][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.38it/s, Loss=nan][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.38it/s, Loss=nan][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.38it/s, Loss=nan][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  60%|██████    | 30/50 [00:42<00:27,  1.38s/it, Mean Loss=nan, LR=1.58e-04][A

Epochs:  62%|██████▏   | 31/50 [00:42<00:26,  1.38s/it, Mean Loss=nan, LR=1.58e-04][A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=nan][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=nan][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=nan][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=nan][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=nan][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 26.79it/s, Loss=nan][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.00it/s, Loss=nan][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.00it/s, Loss=nan][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.00it/s, Loss=nan][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.00it/s, Loss=nan][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.00it/s, Loss=nan][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.25it/s, Loss=nan][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.25it/s, Loss=nan][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.25it/s, Loss=nan][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.31it/s, Loss=nan][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.31it/s, Loss=nan][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.31it/s, Loss=nan][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.31it/s, Loss=nan][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.31it/s, Loss=nan][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  62%|██████▏   | 31/50 [00:44<00:26,  1.38s/it, Mean Loss=nan, LR=1.38e-04][A

Epochs:  64%|██████▍   | 32/50 [00:44<00:24,  1.38s/it, Mean Loss=nan, LR=1.38e-04][A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=nan][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=nan][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=nan][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=nan][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=nan][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 21.13it/s, Loss=nan][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 21.13it/s, Loss=nan][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 21.13it/s, Loss=nan][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 21.13it/s, Loss=nan][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 21.13it/s, Loss=nan][A[A


Epoch 33/50:  29%|██▉       | 10/34 [00:00<00:00, 26.95it/s, Loss=nan][A[A


Epoch 33/50:  29%|██▉       | 10/34 [00:00<00:00, 26.95it/s, Loss=nan][A[A


Epoch 33/50:  29%|██▉       | 10/34 [00:00<00:00, 26.95it/s, Loss=nan][A[A


Epoch 33/50:  29%|██▉       | 10/34 [00:00<00:00, 26.95it/s, Loss=nan][A[A


Epoch 33/50:  29%|██▉       | 10/34 [00:00<00:00, 26.95it/s, Loss=nan][A[A


Epoch 33/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 33/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 33/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 33/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 33/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 33/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 33/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 33/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 33/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 33/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=nan][A[A


Epoch 33/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 33/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 33/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 33/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 33/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 33/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.34it/s, Loss=nan][A[A


Epoch 33/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.34it/s, Loss=nan][A[A


Epoch 33/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.34it/s, Loss=nan][A[A


Epoch 33/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.34it/s, Loss=nan][A[A


Epoch 33/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.34it/s, Loss=nan][A[A


Epoch 33/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 33/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 33/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 33/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


Epoch 33/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.72it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  64%|██████▍   | 32/50 [00:45<00:24,  1.38s/it, Mean Loss=nan, LR=1.20e-04][A

Epochs:  66%|██████▌   | 33/50 [00:45<00:23,  1.38s/it, Mean Loss=nan, LR=1.20e-04][A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=nan][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 21.18it/s, Loss=nan][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 21.18it/s, Loss=nan][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 21.18it/s, Loss=nan][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 21.18it/s, Loss=nan][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 21.18it/s, Loss=nan][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=nan][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=nan][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=nan][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=nan][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=nan][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.57it/s, Loss=nan][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.57it/s, Loss=nan][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.57it/s, Loss=nan][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  66%|██████▌   | 33/50 [00:46<00:23,  1.38s/it, Mean Loss=nan, LR=1.05e-04][A

Epochs:  68%|██████▊   | 34/50 [00:46<00:22,  1.38s/it, Mean Loss=nan, LR=1.05e-04][A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.95it/s, Loss=nan][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.95it/s, Loss=nan][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.95it/s, Loss=nan][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.95it/s, Loss=nan][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.95it/s, Loss=nan][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.25it/s, Loss=nan][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.25it/s, Loss=nan][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.25it/s, Loss=nan][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.25it/s, Loss=nan][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.25it/s, Loss=nan][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.32it/s, Loss=nan][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.61it/s, Loss=nan][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.61it/s, Loss=nan][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.61it/s, Loss=nan][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=nan][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  68%|██████▊   | 34/50 [00:48<00:22,  1.38s/it, Mean Loss=nan, LR=9.12e-05][A

Epochs:  70%|███████   | 35/50 [00:48<00:20,  1.38s/it, Mean Loss=nan, LR=9.12e-05][A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=nan][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 20.90it/s, Loss=nan][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 20.90it/s, Loss=nan][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 20.90it/s, Loss=nan][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 20.90it/s, Loss=nan][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 20.90it/s, Loss=nan][A[A


Epoch 36/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=nan][A[A


Epoch 36/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=nan][A[A


Epoch 36/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=nan][A[A


Epoch 36/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=nan][A[A


Epoch 36/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=nan][A[A


Epoch 36/50:  41%|████      | 14/34 [00:00<00:00, 20.35it/s, Loss=nan][A[A


Epoch 36/50:  41%|████      | 14/34 [00:00<00:00, 20.35it/s, Loss=nan][A[A


Epoch 36/50:  41%|████      | 14/34 [00:00<00:00, 20.35it/s, Loss=nan][A[A


Epoch 36/50:  41%|████      | 14/34 [00:00<00:00, 20.35it/s, Loss=nan][A[A


Epoch 36/50:  41%|████      | 14/34 [00:00<00:00, 20.35it/s, Loss=nan][A[A


Epoch 36/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 36/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 36/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 36/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 36/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 36/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.70it/s, Loss=nan][A[A


Epoch 36/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.70it/s, Loss=nan][A[A


Epoch 36/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.70it/s, Loss=nan][A[A


Epoch 36/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.70it/s, Loss=nan][A[A


Epoch 36/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.70it/s, Loss=nan][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.91it/s, Loss=nan][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.91it/s, Loss=nan][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.91it/s, Loss=nan][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.91it/s, Loss=nan][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.91it/s, Loss=nan][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.46it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  70%|███████   | 35/50 [00:49<00:20,  1.38s/it, Mean Loss=nan, LR=7.94e-05][A

Epochs:  72%|███████▏  | 36/50 [00:49<00:19,  1.41s/it, Mean Loss=nan, LR=7.94e-05][A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=nan][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.81it/s, Loss=nan][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.97it/s, Loss=nan][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.97it/s, Loss=nan][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.97it/s, Loss=nan][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.97it/s, Loss=nan][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.97it/s, Loss=nan][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.36it/s, Loss=nan][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.49it/s, Loss=nan][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.68it/s, Loss=nan][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.68it/s, Loss=nan][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.68it/s, Loss=nan][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=nan][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.96it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  72%|███████▏  | 36/50 [00:51<00:19,  1.41s/it, Mean Loss=nan, LR=6.92e-05][A

Epochs:  74%|███████▍  | 37/50 [00:51<00:18,  1.40s/it, Mean Loss=nan, LR=6.92e-05][A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=nan][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=nan][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=nan][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=nan][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=nan][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.70it/s, Loss=nan][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.65it/s, Loss=nan][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.65it/s, Loss=nan][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.65it/s, Loss=nan][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.65it/s, Loss=nan][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.65it/s, Loss=nan][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=nan][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=nan][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=nan][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=nan][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=nan][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.52it/s, Loss=nan][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.81it/s, Loss=nan][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.81it/s, Loss=nan][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.81it/s, Loss=nan][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.81it/s, Loss=nan][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.81it/s, Loss=nan][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=nan][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=nan][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=nan][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=nan][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=nan][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  74%|███████▍  | 37/50 [00:52<00:18,  1.40s/it, Mean Loss=nan, LR=6.03e-05][A

Epochs:  76%|███████▌  | 38/50 [00:52<00:16,  1.39s/it, Mean Loss=nan, LR=6.03e-05][A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=nan][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=nan][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=nan][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=nan][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=nan][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=nan][A[A


Epoch 39/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 39/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 39/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 39/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 39/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=nan][A[A


Epoch 39/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 39/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 39/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 39/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 39/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=nan][A[A


Epoch 39/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 39/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 39/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 39/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 39/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 39/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.82it/s, Loss=nan][A[A


Epoch 39/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.82it/s, Loss=nan][A[A


Epoch 39/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.82it/s, Loss=nan][A[A


Epoch 39/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.82it/s, Loss=nan][A[A


Epoch 39/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.82it/s, Loss=nan][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.11it/s, Loss=nan][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.11it/s, Loss=nan][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.11it/s, Loss=nan][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.11it/s, Loss=nan][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.11it/s, Loss=nan][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.61it/s, Loss=nan][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.61it/s, Loss=nan][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.61it/s, Loss=nan][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.61it/s, Loss=nan][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.61it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  76%|███████▌  | 38/50 [00:54<00:16,  1.39s/it, Mean Loss=nan, LR=5.25e-05][A

Epochs:  78%|███████▊  | 39/50 [00:54<00:15,  1.42s/it, Mean Loss=nan, LR=5.25e-05][A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.13it/s, Loss=nan][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=nan][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=nan][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=nan][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=nan][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=nan][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.64it/s, Loss=nan][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:00, 20.13it/s, Loss=nan][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:00, 20.13it/s, Loss=nan][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:00, 20.13it/s, Loss=nan][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:00, 20.13it/s, Loss=nan][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:00, 20.13it/s, Loss=nan][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.42it/s, Loss=nan][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.83it/s, Loss=nan][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.83it/s, Loss=nan][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.83it/s, Loss=nan][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.83it/s, Loss=nan][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.83it/s, Loss=nan][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.69it/s, Loss=nan][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.69it/s, Loss=nan][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.69it/s, Loss=nan][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.69it/s, Loss=nan][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.69it/s, Loss=nan][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.98it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  78%|███████▊  | 39/50 [00:55<00:15,  1.42s/it, Mean Loss=nan, LR=4.57e-05][A

Epochs:  80%|████████  | 40/50 [00:55<00:14,  1.41s/it, Mean Loss=nan, LR=4.57e-05][A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=nan][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=nan][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=nan][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=nan][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=nan][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.63it/s, Loss=nan][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.63it/s, Loss=nan][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.63it/s, Loss=nan][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.63it/s, Loss=nan][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.63it/s, Loss=nan][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.48it/s, Loss=nan][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.48it/s, Loss=nan][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.48it/s, Loss=nan][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.48it/s, Loss=nan][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.48it/s, Loss=nan][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=nan][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=nan][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=nan][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=nan][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=nan][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=nan][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=nan][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=nan][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=nan][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=nan][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.35it/s, Loss=nan][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.35it/s, Loss=nan][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.35it/s, Loss=nan][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.35it/s, Loss=nan][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.35it/s, Loss=nan][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=nan][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=nan][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=nan][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=nan][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=nan][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  80%|████████  | 40/50 [00:56<00:14,  1.41s/it, Mean Loss=nan, LR=3.98e-05][A

Epochs:  82%|████████▏ | 41/50 [00:56<00:12,  1.40s/it, Mean Loss=nan, LR=3.98e-05][A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=nan][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=nan][A[A


Epoch 42/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=nan][A[A


Epoch 42/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=nan][A[A


Epoch 42/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=nan][A[A


Epoch 42/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=nan][A[A


Epoch 42/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=nan][A[A


Epoch 42/50:  41%|████      | 14/34 [00:00<00:01, 19.90it/s, Loss=nan][A[A


Epoch 42/50:  41%|████      | 14/34 [00:00<00:01, 19.90it/s, Loss=nan][A[A


Epoch 42/50:  41%|████      | 14/34 [00:00<00:01, 19.90it/s, Loss=nan][A[A


Epoch 42/50:  41%|████      | 14/34 [00:00<00:01, 19.90it/s, Loss=nan][A[A


Epoch 42/50:  41%|████      | 14/34 [00:00<00:01, 19.90it/s, Loss=nan][A[A


Epoch 42/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 42/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 42/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 42/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 42/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.12it/s, Loss=nan][A[A


Epoch 42/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=nan][A[A


Epoch 42/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=nan][A[A


Epoch 42/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 42/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 42/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=nan][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.13it/s, Loss=nan][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.13it/s, Loss=nan][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.13it/s, Loss=nan][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.13it/s, Loss=nan][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 18.13it/s, Loss=nan][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.68it/s, Loss=nan][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.68it/s, Loss=nan][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.68it/s, Loss=nan][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.68it/s, Loss=nan][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.68it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  82%|████████▏ | 41/50 [00:58<00:12,  1.40s/it, Mean Loss=nan, LR=3.47e-05][A

Epochs:  84%|████████▍ | 42/50 [00:58<00:11,  1.43s/it, Mean Loss=nan, LR=3.47e-05][A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.23it/s, Loss=nan][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=nan][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.82it/s, Loss=nan][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.82it/s, Loss=nan][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.82it/s, Loss=nan][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.82it/s, Loss=nan][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.82it/s, Loss=nan][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=nan][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=nan][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=nan][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=nan][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=nan][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=nan][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=nan][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=nan][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=nan][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.06it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  84%|████████▍ | 42/50 [00:59<00:11,  1.43s/it, Mean Loss=nan, LR=3.02e-05][A

Epochs:  86%|████████▌ | 43/50 [00:59<00:09,  1.41s/it, Mean Loss=nan, LR=3.02e-05][A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=nan][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.79it/s, Loss=nan][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=nan][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=nan][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=nan][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=nan][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=nan][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=nan][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=nan][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=nan][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=nan][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=nan][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.10it/s, Loss=nan][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=nan][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=nan][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=nan][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=nan][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=nan][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=nan][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.84it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  86%|████████▌ | 43/50 [01:01<00:09,  1.41s/it, Mean Loss=nan, LR=2.63e-05][A

Epochs:  88%|████████▊ | 44/50 [01:01<00:08,  1.40s/it, Mean Loss=nan, LR=2.63e-05][A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=nan][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=nan][A[A


Epoch 45/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 45/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 45/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 45/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 45/50:  29%|██▉       | 10/34 [00:00<00:00, 26.73it/s, Loss=nan][A[A


Epoch 45/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 45/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 45/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 45/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 45/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=nan][A[A


Epoch 45/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 45/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 45/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 45/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 45/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=nan][A[A


Epoch 45/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=nan][A[A


Epoch 45/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=nan][A[A


Epoch 45/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=nan][A[A


Epoch 45/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=nan][A[A


Epoch 45/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=nan][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.58it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  88%|████████▊ | 44/50 [01:02<00:08,  1.40s/it, Mean Loss=nan, LR=2.29e-05][A

Epochs:  90%|█████████ | 45/50 [01:02<00:07,  1.43s/it, Mean Loss=nan, LR=2.29e-05][A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.15it/s, Loss=nan][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.15it/s, Loss=nan][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.15it/s, Loss=nan][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.15it/s, Loss=nan][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.15it/s, Loss=nan][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 27.13it/s, Loss=nan][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.12it/s, Loss=nan][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.12it/s, Loss=nan][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.12it/s, Loss=nan][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.12it/s, Loss=nan][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.12it/s, Loss=nan][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.30it/s, Loss=nan][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.30it/s, Loss=nan][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.30it/s, Loss=nan][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.30it/s, Loss=nan][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.30it/s, Loss=nan][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=nan][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=nan][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=nan][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  90%|█████████ | 45/50 [01:03<00:07,  1.43s/it, Mean Loss=nan, LR=2.00e-05][A

Epochs:  92%|█████████▏| 46/50 [01:03<00:05,  1.41s/it, Mean Loss=nan, LR=2.00e-05][A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=nan][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=nan][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=nan][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=nan][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=nan][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=nan][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=nan][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=nan][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=nan][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=nan][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.39it/s, Loss=nan][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.39it/s, Loss=nan][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.39it/s, Loss=nan][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.39it/s, Loss=nan][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.39it/s, Loss=nan][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.70it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  92%|█████████▏| 46/50 [01:05<00:05,  1.41s/it, Mean Loss=nan, LR=1.74e-05][A

Epochs:  94%|█████████▍| 47/50 [01:05<00:04,  1.40s/it, Mean Loss=nan, LR=1.74e-05][A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=nan][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 21.01it/s, Loss=nan][A[A


Epoch 48/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 48/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 48/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 48/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 48/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 48/50:  41%|████      | 14/34 [00:00<00:00, 20.18it/s, Loss=nan][A[A


Epoch 48/50:  41%|████      | 14/34 [00:00<00:00, 20.18it/s, Loss=nan][A[A


Epoch 48/50:  41%|████      | 14/34 [00:00<00:00, 20.18it/s, Loss=nan][A[A


Epoch 48/50:  41%|████      | 14/34 [00:00<00:00, 20.18it/s, Loss=nan][A[A


Epoch 48/50:  41%|████      | 14/34 [00:00<00:00, 20.18it/s, Loss=nan][A[A


Epoch 48/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 48/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 48/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 48/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 48/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=nan][A[A


Epoch 48/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=nan][A[A


Epoch 48/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=nan][A[A


Epoch 48/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 48/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 48/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=nan][A[A


Epoch 48/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 48/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 48/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 48/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 48/50:  76%|███████▋  | 26/34 [00:01<00:00, 17.99it/s, Loss=nan][A[A


Epoch 48/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.51it/s, Loss=nan][A[A


Epoch 48/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.51it/s, Loss=nan][A[A


Epoch 48/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.51it/s, Loss=nan][A[A


Epoch 48/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.51it/s, Loss=nan][A[A


Epoch 48/50:  88%|████████▊ | 30/34 [00:01<00:00, 21.51it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  94%|█████████▍| 47/50 [01:06<00:04,  1.40s/it, Mean Loss=nan, LR=1.51e-05][A

Epochs:  96%|█████████▌| 48/50 [01:06<00:02,  1.43s/it, Mean Loss=nan, LR=1.51e-05][A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=nan][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=nan][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=nan][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=nan][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=nan][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=nan][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 27.15it/s, Loss=nan][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=nan][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=nan][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=nan][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=nan][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=nan][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=nan][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.64it/s, Loss=nan][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.64it/s, Loss=nan][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.64it/s, Loss=nan][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.49it/s, Loss=nan][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.49it/s, Loss=nan][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.49it/s, Loss=nan][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.49it/s, Loss=nan][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.49it/s, Loss=nan][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.74it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  96%|█████████▌| 48/50 [01:08<00:02,  1.43s/it, Mean Loss=nan, LR=1.32e-05][A

Epochs:  98%|█████████▊| 49/50 [01:08<00:01,  1.41s/it, Mean Loss=nan, LR=1.32e-05][A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=nan][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.27it/s, Loss=nan][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.27it/s, Loss=nan][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.27it/s, Loss=nan][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.27it/s, Loss=nan][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.27it/s, Loss=nan][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=nan][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=nan][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=nan][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=nan][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=nan][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=nan][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=nan][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.45it/s, Loss=nan][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.45it/s, Loss=nan][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.45it/s, Loss=nan][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.45it/s, Loss=nan][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.45it/s, Loss=nan][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.72it/s, Loss=nan][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.72it/s, Loss=nan][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=nan][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=nan][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=nan][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=nan][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=nan][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=nan][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=nan][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=nan][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=nan][A[A


                                                                      [A[A

Epochs:  98%|█████████▊| 49/50 [01:09<00:01,  1.41s/it, Mean Loss=nan, LR=1.15e-05][A

Epochs: 100%|██████████| 50/50 [01:09<00:00,  1.40s/it, Mean Loss=nan, LR=1.15e-05][A
Epochs: 100%|██████████| 50/50 [01:09<00:00,  1.39s/it, Mean Loss=nan, LR=1.15e-05]


training k-probes:   0%|          | 0/260 [00:00<?, ?it/s][A
training k-probes:   0%|          | 0/260 [00:00<?, ?it/s]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [01:16<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_204346_dual_timescale_adaptive_sae/experiment.py", line 959, in <module>
    evaluate_trained_sae(
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_204346_dual_timescale_adaptive_sae/experiment.py", line 860, in evaluate_trained_sae
    eval_runners[eval_type]()
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_204346_dual_timescale_adaptive_sae/experiment.py", line 726, in <lambda>
    lambda: absorption.run_eval(
            ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/main.py", line 76, in run_eval
    k_sparse_probing_results = run_k_sparse_probing_experiment(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 485, in run_k_sparse_probing_experiment
    metrics_df = load_df_or_run(
                 ^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py", line 191, in load_df_or_run
    return load_dfs_or_run(lambda: [fn()], [path], force)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py", line 200, in load_dfs_or_run
    dfs = fn()
          ^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py", line 191, in <lambda>
    return load_dfs_or_run(lambda: [fn()], [path], force)[0]
                                    ^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 486, in <lambda>
    lambda: build_metrics_df(*get_raw_results_df(), max_k_value=max_k_value),
                              ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 465, in get_raw_results_df
    return load_dfs_or_run(
           ^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py", line 200, in load_dfs_or_run
    dfs = fn()
          ^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 466, in <lambda>
    lambda: load_and_run_eval_probe_and_sae_k_sparse_raw_scores(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 301, in load_and_run_eval_probe_and_sae_k_sparse_raw_scores
    k_sparse_probes = train_k_sparse_probes(
                      ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/k_sparse_probing.py", line 163, in train_k_sparse_probes
    ).fit(train_k_x, (train_k_y == label).astype(np.int64))
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py", line 1223, in fit
    X, y = self._validate_data(
           ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
        ^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Error during experiments: cannot access local variable 'results' where it is not associated with a value
Experiments failed for idea 20250206_204346_dual_timescale_adaptive_sae
FINISHED IDEA
Completed idea: dual_timescale_adaptive_sae, Success: False
Processing idea: adaptive_sparse_orthogonal_sae

[2025-02-06 21:08:21]  
*Starting idea: 20250206_210632_adaptive_sparse_orthogonal_sae*

[2025-02-06 21:08:21] Starting experiments execution
Starting experiment with prompt for coder: Your goal is to implement the following idea: Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Implement top-k activation consistency tracking
2. Add discrete stability thresholds
3. Modify orthogonality loss with threshold-based coefficients
4. Compare against continuous adaptation baseline
5. Analyze feature stability progression
6. Evaluate interpretability improvements.
The implementation plan is as follows: 1. Add top-k consistency tracking to AutoEncoderTopK
2. Implement stability level determination
3. Add threshold-based coefficient computation
4. Add configuration for consistency thresholds
5. Update training loop with consistency tracking
6. Add visualization for stability progression.

You can also refer to other information in the idea: {'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Progressive Feature Separation', 'Experiment': '1. Implement top-k activation consistency tracking\n2. Add discrete stability thresholds\n3. Modify orthogonality loss with threshold-based coefficients\n4. Compare against continuous adaptation baseline\n5. Analyze feature stability progression\n6. Evaluate interpretability improvements', 'Technical_Details': 'The method enhances sparsity-weighted orthogonality using discrete stability levels: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j) α(l_i, l_j) * c_(ij) * |f_i^T f_j| where l_i is the stability level of feature i determined by its top-k activation consistency. Consistency is measured as the proportion of shared top-k indices between consecutive batches, averaged over a window of 10 batches. Features progress through three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8). c_(ij) remains the normalized intersection of current sparsity masks.', 'Implementation_Plan': '1. Add top-k consistency tracking to AutoEncoderTopK\n2. Implement stability level determination\n3. Add threshold-based coefficient computation\n4. Add configuration for consistency thresholds\n5. Update training loop with consistency tracking\n6. Add visualization for stability progression', 'Interestingness_Evaluation': 'The discrete stability levels provide a clear and interpretable way to adapt orthogonality constraints while maintaining the elegant connection to sparsity patterns.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple additions - tracking top-k consistency and applying threshold-based coefficients; uses existing activation patterns; highly efficient with negligible overhead; easily fits within 30-minute training window on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'The use of discrete stability levels based on activation consistency represents a novel and practical approach to adaptive feature separation in SAEs.', 'Novelty': 8, 'Expected_Research_Impact': 'The clear, interpretable adaptation mechanism should significantly improve feature separation and interpretability metrics by providing well-defined stages of feature development.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an adaptive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs) that aims to improve feature interpretability. Our method introduces discrete stability levels based on top-k activation consistency, providing a clear and interpretable mechanism for progressive feature separation. Features advance through well-defined stability levels as their activation patterns become more consistent, with orthogonality constraints strengthening accordingly. This approach builds on existing sparsity-weighted orthogonality while adding a principled adaptation mechanism. By tying constraint strength to measurable feature stability, we provide a natural way to guide feature development from initial exploration to final specialization.', 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

If the experiments in the idea is already implemented in 'experiment.py' you are given with, you should try to improve its result by further enhancing the implementation.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run (you can try to run with different hyperparameters in the same run across different iterations.).

Note that we already provide the baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0002, 'steps': 2441, 'seed': 42, 'activation_dim': 2304, 'dict_size': 16384, 'k': 40, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 4357.5732421875, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'e5d9d74a-c71a-462d-9661-5a7f2d41d4b8', 'datetime_epoch_millis': 1738885451045, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.1429159584016188, 'mean_num_split_features': 1.12}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.04216867469879518, 'num_absorption': 112, 'num_probe_true_positives': 2656, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.018471337579617834, 'num_absorption': 29, 'num_probe_true_positives': 1570, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.3883746760459089, 'num_absorption': 1049, 'num_probe_true_positives': 2701, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.11044417767106843, 'num_absorption': 184, 'num_probe_true_positives': 1666, 'num_split_features': 2}, {'first_letter': 'e', 'absorption_rate': 0.07116104868913857, 'num_absorption': 114, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.05956375838926174, 'num_absorption': 71, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.08423686405337781, 'num_absorption': 101, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.3019648397104447, 'num_absorption': 292, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.10113704368641532, 'num_absorption': 169, 'num_probe_true_positives': 1671, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.004739336492890996, 'num_absorption': 2, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.007518796992481203, 'num_absorption': 5, 'num_probe_true_positives': 665, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.030377668308702793, 'num_absorption': 37, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.27388535031847133, 'num_absorption': 473, 'num_probe_true_positives': 1727, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.04790419161676647, 'num_absorption': 40, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.32429906542056075, 'num_absorption': 347, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.4584221748400853, 'num_absorption': 1075, 'num_probe_true_positives': 2345, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.07567567567567568, 'num_absorption': 14, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.004294478527607362, 'num_absorption': 7, 'num_probe_true_positives': 1630, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.39844026940801136, 'num_absorption': 1124, 'num_probe_true_positives': 2821, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.0267639902676399, 'num_absorption': 44, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.21963824289405684, 'num_absorption': 170, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.008333333333333333, 'num_absorption': 7, 'num_probe_true_positives': 840, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.14390602055800295, 'num_absorption': 98, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.3473684210526316, 'num_absorption': 66, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.023809523809523808, 'num_absorption': 6, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9596273291925466, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.40625}, 'model_performance_preservation': {'ce_loss_score': 0.9588815789473685, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.328125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.609375, 'mse': 2.484375, 'cossim': 0.875}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 131.0, 'l2_ratio': 0.875, 'relative_reconstruction_bias': 0.99609375}, 'sparsity': {'l0': 40.0, 'l1': 364.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '0a27606c-8d4d-449f-ad67-c23bba779fc9', 'datetime_epoch_millis': 1738886529707, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.94985, 'llm_top_1_test_accuracy': 0.66250625, 'llm_top_2_test_accuracy': 0.7216312500000001, 'llm_top_5_test_accuracy': 0.7799, 'llm_top_10_test_accuracy': 0.8296999999999999, 'llm_top_20_test_accuracy': 0.87843125, 'llm_top_50_test_accuracy': 0.9207375, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9509000509977341, 'sae_top_1_test_accuracy': 0.75075625, 'sae_top_2_test_accuracy': 0.8130499999999999, 'sae_top_5_test_accuracy': 0.8722624999999999, 'sae_top_10_test_accuracy': 0.90165625, 'sae_top_20_test_accuracy': 0.9201937499999998, 'sae_top_50_test_accuracy': 0.93555625, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.96260005235672, 'sae_top_1_test_accuracy': 0.8308000000000002, 'sae_top_2_test_accuracy': 0.8662000000000001, 'sae_top_5_test_accuracy': 0.8854, 'sae_top_10_test_accuracy': 0.8987999999999999, 'sae_top_20_test_accuracy': 0.9468, 'sae_top_50_test_accuracy': 0.9583999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9454, 'llm_top_1_test_accuracy': 0.6724, 'llm_top_2_test_accuracy': 0.7358, 'llm_top_5_test_accuracy': 0.7624000000000001, 'llm_top_10_test_accuracy': 0.8026, 'llm_top_20_test_accuracy': 0.8661999999999999, 'llm_top_50_test_accuracy': 0.9042000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.941200053691864, 'sae_top_1_test_accuracy': 0.7767999999999999, 'sae_top_2_test_accuracy': 0.8154, 'sae_top_5_test_accuracy': 0.8486, 'sae_top_10_test_accuracy': 0.9022, 'sae_top_20_test_accuracy': 0.9246000000000001, 'sae_top_50_test_accuracy': 0.9346, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9109999999999999, 'llm_top_1_test_accuracy': 0.6849999999999999, 'llm_top_2_test_accuracy': 0.7438, 'llm_top_5_test_accuracy': 0.7618, 'llm_top_10_test_accuracy': 0.7934, 'llm_top_20_test_accuracy': 0.851, 'llm_top_50_test_accuracy': 0.8879999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000596046448, 'sae_top_1_test_accuracy': 0.8310000000000001, 'sae_top_2_test_accuracy': 0.8448, 'sae_top_5_test_accuracy': 0.8725999999999999, 'sae_top_10_test_accuracy': 0.8865999999999999, 'sae_top_20_test_accuracy': 0.8985999999999998, 'sae_top_50_test_accuracy': 0.9106, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8964000000000001, 'llm_top_1_test_accuracy': 0.6024, 'llm_top_2_test_accuracy': 0.6397999999999999, 'llm_top_5_test_accuracy': 0.6666, 'llm_top_10_test_accuracy': 0.7491999999999999, 'llm_top_20_test_accuracy': 0.8138, 'llm_top_50_test_accuracy': 0.8566, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9076000452041626, 'sae_top_1_test_accuracy': 0.7055999999999999, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.818, 'sae_top_10_test_accuracy': 0.8550000000000001, 'sae_top_20_test_accuracy': 0.8708, 'sae_top_50_test_accuracy': 0.883, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9625000655651093, 'sae_top_1_test_accuracy': 0.649, 'sae_top_2_test_accuracy': 0.813, 'sae_top_5_test_accuracy': 0.875, 'sae_top_10_test_accuracy': 0.928, 'sae_top_20_test_accuracy': 0.935, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9662000000000001, 'llm_top_1_test_accuracy': 0.666, 'llm_top_2_test_accuracy': 0.6900000000000001, 'llm_top_5_test_accuracy': 0.7613999999999999, 'llm_top_10_test_accuracy': 0.8039999999999999, 'llm_top_20_test_accuracy': 0.8622, 'llm_top_50_test_accuracy': 0.923, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9688000440597534, 'sae_top_1_test_accuracy': 0.6042, 'sae_top_2_test_accuracy': 0.6302, 'sae_top_5_test_accuracy': 0.8632, 'sae_top_10_test_accuracy': 0.8856000000000002, 'sae_top_20_test_accuracy': 0.9151999999999999, 'sae_top_50_test_accuracy': 0.9425999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9380000000000001, 'llm_top_1_test_accuracy': 0.63325, 'llm_top_2_test_accuracy': 0.76825, 'llm_top_5_test_accuracy': 0.827, 'llm_top_10_test_accuracy': 0.869, 'llm_top_20_test_accuracy': 0.89925, 'llm_top_50_test_accuracy': 0.9265000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9455000460147858, 'sae_top_1_test_accuracy': 0.79725, 'sae_top_2_test_accuracy': 0.85, 'sae_top_5_test_accuracy': 0.8865, 'sae_top_10_test_accuracy': 0.90925, 'sae_top_20_test_accuracy': 0.91675, 'sae_top_50_test_accuracy': 0.93625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.7243999999999999, 'llm_top_2_test_accuracy': 0.7802, 'llm_top_5_test_accuracy': 0.9019999999999999, 'llm_top_10_test_accuracy': 0.96, 'llm_top_20_test_accuracy': 0.99, 'llm_top_50_test_accuracy': 0.9978, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9940000414848328, 'sae_top_1_test_accuracy': 0.8114000000000001, 'sae_top_2_test_accuracy': 0.8938, 'sae_top_5_test_accuracy': 0.9288000000000001, 'sae_top_10_test_accuracy': 0.9478, 'sae_top_20_test_accuracy': 0.9538, 'sae_top_50_test_accuracy': 0.966, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'e8be4b3c-dfd3-49dd-91ca-e1b0d08e9055', 'datetime_epoch_millis': 1738886775914, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.026217222213745117}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 16384, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 701.0477905273438, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b5f3e18f-7844-4d8a-ac26-9b99d357cf9a', 'datetime_epoch_millis': 1738804062497, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.016138604243358595, 'mean_num_split_features': 1.4166666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2656, 'num_split_features': 2}, {'first_letter': 'b', 'absorption_rate': 0.0006369426751592356, 'num_absorption': 1, 'num_probe_true_positives': 1570, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2701, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.009603841536614645, 'num_absorption': 16, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.05305867665418227, 'num_absorption': 85, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.013422818791946308, 'num_absorption': 16, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.021684737281067557, 'num_absorption': 26, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.006582884500299222, 'num_absorption': 11, 'num_probe_true_positives': 1671, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.09770114942528736, 'num_absorption': 119, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0005790387955993051, 'num_absorption': 1, 'num_probe_true_positives': 1727, 'num_split_features': 4}, {'first_letter': 'n', 'absorption_rate': 0.013173652694610778, 'num_absorption': 11, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.022429906542056073, 'num_absorption': 24, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.008955223880597015, 'num_absorption': 21, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0006134969325153375, 'num_absorption': 1, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.0857851825593761, 'num_absorption': 242, 'num_probe_true_positives': 2821, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.011557177615571776, 'num_absorption': 19, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.021963824289405683, 'num_absorption': 17, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.0014684287812041115, 'num_absorption': 1, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.011904761904761904, 'num_absorption': 3, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995723748059007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004302978515625}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.984375, 'mse': 0.0888671875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.98828125}, 'sparsity': {'l0': 8895.810546875, 'l1': 12608.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'fc35f1d1-c27c-402b-abd2-fe38f12c05aa', 'datetime_epoch_millis': 1738804969188, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95075625, 'llm_top_1_test_accuracy': 0.65444375, 'llm_top_2_test_accuracy': 0.71966875, 'llm_top_5_test_accuracy': 0.7860875, 'llm_top_10_test_accuracy': 0.83056875, 'llm_top_20_test_accuracy': 0.8764437500000001, 'llm_top_50_test_accuracy': 0.9228625, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9578625451773405, 'sae_top_1_test_accuracy': 0.6448624999999999, 'sae_top_2_test_accuracy': 0.6875375000000001, 'sae_top_5_test_accuracy': 0.7562874999999999, 'sae_top_10_test_accuracy': 0.8066500000000001, 'sae_top_20_test_accuracy': 0.8664625000000001, 'sae_top_50_test_accuracy': 0.9122750000000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6862, 'sae_top_2_test_accuracy': 0.6885999999999999, 'sae_top_5_test_accuracy': 0.7842, 'sae_top_10_test_accuracy': 0.8386000000000001, 'sae_top_20_test_accuracy': 0.8806, 'sae_top_50_test_accuracy': 0.9252, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9492, 'llm_top_1_test_accuracy': 0.6794, 'llm_top_2_test_accuracy': 0.717, 'llm_top_5_test_accuracy': 0.767, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8614, 'llm_top_50_test_accuracy': 0.9086000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.6844, 'sae_top_2_test_accuracy': 0.6834, 'sae_top_5_test_accuracy': 0.7402000000000001, 'sae_top_10_test_accuracy': 0.7884, 'sae_top_20_test_accuracy': 0.8503999999999999, 'sae_top_50_test_accuracy': 0.8998000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6862, 'llm_top_2_test_accuracy': 0.732, 'llm_top_5_test_accuracy': 0.7722, 'llm_top_10_test_accuracy': 0.8008, 'llm_top_20_test_accuracy': 0.8532, 'llm_top_50_test_accuracy': 0.8922000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9368000507354737, 'sae_top_1_test_accuracy': 0.5932000000000001, 'sae_top_2_test_accuracy': 0.725, 'sae_top_5_test_accuracy': 0.7638, 'sae_top_10_test_accuracy': 0.8056000000000001, 'sae_top_20_test_accuracy': 0.8465999999999999, 'sae_top_50_test_accuracy': 0.8838000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8948, 'llm_top_1_test_accuracy': 0.6039999999999999, 'llm_top_2_test_accuracy': 0.6508, 'llm_top_5_test_accuracy': 0.6878, 'llm_top_10_test_accuracy': 0.745, 'llm_top_20_test_accuracy': 0.8051999999999999, 'llm_top_50_test_accuracy': 0.8628, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9118000507354737, 'sae_top_1_test_accuracy': 0.5828, 'sae_top_2_test_accuracy': 0.6594, 'sae_top_5_test_accuracy': 0.7343999999999999, 'sae_top_10_test_accuracy': 0.768, 'sae_top_20_test_accuracy': 0.8114000000000001, 'sae_top_50_test_accuracy': 0.8452, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000343322754, 'sae_top_1_test_accuracy': 0.699, 'sae_top_2_test_accuracy': 0.701, 'sae_top_5_test_accuracy': 0.792, 'sae_top_10_test_accuracy': 0.814, 'sae_top_20_test_accuracy': 0.908, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6634, 'llm_top_2_test_accuracy': 0.7002, 'llm_top_5_test_accuracy': 0.7648, 'llm_top_10_test_accuracy': 0.8024000000000001, 'llm_top_20_test_accuracy': 0.8608, 'llm_top_50_test_accuracy': 0.9258, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6342000000000001, 'sae_top_2_test_accuracy': 0.6708000000000001, 'sae_top_5_test_accuracy': 0.7096000000000001, 'sae_top_10_test_accuracy': 0.7712, 'sae_top_20_test_accuracy': 0.8295999999999999, 'sae_top_50_test_accuracy': 0.9026, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.93825, 'llm_top_1_test_accuracy': 0.63375, 'llm_top_2_test_accuracy': 0.7607499999999999, 'llm_top_5_test_accuracy': 0.8265, 'llm_top_10_test_accuracy': 0.8727499999999999, 'llm_top_20_test_accuracy': 0.8977499999999999, 'llm_top_50_test_accuracy': 0.9265, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9530000537633896, 'sae_top_1_test_accuracy': 0.6285, 'sae_top_2_test_accuracy': 0.7015, 'sae_top_5_test_accuracy': 0.7515000000000001, 'sae_top_10_test_accuracy': 0.856, 'sae_top_20_test_accuracy': 0.8875, 'sae_top_50_test_accuracy': 0.9199999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6522, 'llm_top_2_test_accuracy': 0.7814, 'llm_top_5_test_accuracy': 0.9123999999999999, 'llm_top_10_test_accuracy': 0.9642, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9972000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9990000247955322, 'sae_top_1_test_accuracy': 0.6506000000000001, 'sae_top_2_test_accuracy': 0.6706, 'sae_top_5_test_accuracy': 0.7745999999999998, 'sae_top_10_test_accuracy': 0.8114000000000001, 'sae_top_20_test_accuracy': 0.9176, 'sae_top_50_test_accuracy': 0.9895999999999999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'b48744e1-4f43-4d25-af92-4f1a46abab60', 'datetime_epoch_millis': 1738805225729, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the adaptive sparsity-guided orthogonality approach. Let's plan the experiments:                                                                     

Run 1: Implement basic consistency tracking and stability levels                                                                                                         

 • Add top-k consistency tracking                                                                                                                                        
 • Implement 3 stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8)                                                                               
 • Add basic orthogonality loss with stability coefficients                                                                                                              

Run 2: Tune consistency window and thresholds                                                                                                                            

 • Test different window sizes (5, 10, 15) for consistency tracking                                                                                                      
 • Adjust stability thresholds (0.4/0.7, 0.5/0.8, 0.6/0.9)                                                                                                               

Run 3: Optimize orthogonality coefficients                                                                                                                               

 • Test different coefficient scales for stability levels                                                                                                                
 • Fine-tune the balance between reconstruction and orthogonality losses                                                                                                 

Let's start with Run 1. We need to modify the AutoEncoderTopK class to add consistency tracking and stability-based orthogonality:                                       

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         d_in: int,                                                                                                                                                      
         d_sae: int,                                                                                                                                                     
         hook_layer: int,                                                                                                                                                
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                              
         hook_name: Optional[str] = None,                                                                                                                                
         k: int = 100,                                                                                                                                                   
     ):                                                                                                                                                                  
         super().__init__()                                                                                                                                              
         self.activation_dim = d_in                                                                                                                                      
         self.dict_size = d_sae                                                                                                                                          
         self.k = k                                                                                                                                                      
 =======                                                                                                                                                                 
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         d_in: int,                                                                                                                                                      
         d_sae: int,                                                                                                                                                     
         hook_layer: int,                                                                                                                                                
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                              
         hook_name: Optional[str] = None,                                                                                                                                
         k: int = 100,                                                                                                                                                   
         consistency_window: int = 10,                                                                                                                                   
         stability_thresholds: tuple = (0.5, 0.8),                                                                                                                       
         stability_levels: tuple = (0.2, 0.6, 1.0)                                                                                                                       
     ):                                                                                                                                                                  
         super().__init__()                                                                                                                                              
         self.activation_dim = d_in                                                                                                                                      
         self.dict_size = d_sae                                                                                                                                          
         self.k = k                                                                                                                                                      
                                                                                                                                                                         
         # Consistency tracking                                                                                                                                          
         self.consistency_window = consistency_window                                                                                                                    
         self.prev_indices = []                                                                                                                                          
         self.consistency_scores = torch.zeros(d_sae, device=self.device)                                                                                                
         self.stability_thresholds = stability_thresholds                                                                                                                
         self.stability_levels = stability_levels                                                                                                                        
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                       
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                           
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                   
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                           
                                                                                                                                                                         
         # Scatter topk values to form encoded activations                                                                                                               
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                              
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                            
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                              
         )                                                                                                                                                               
                                                                                                                                                                         
         if return_topk:                                                                                                                                                 
             return encoded_acts_BF, tops_acts_BK, top_indices_BK                                                                                                        
         else:                                                                                                                                                           
             return encoded_acts_BF                                                                                                                                      
 =======                                                                                                                                                                 
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                       
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                           
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                   
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                           
                                                                                                                                                                         
         # Scatter topk values to form encoded activations                                                                                                               
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                              
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                            
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                              
         )                                                                                                                                                               
                                                                                                                                                                         
         # Update consistency tracking                                                                                                                                   
         if self.training:                                                                                                                                               
             self.update_consistency_scores(top_indices_BK)                                                                                                              
                                                                                                                                                                         
         if return_topk:                                                                                                                                                 
             return encoded_acts_BF, tops_acts_BK, top_indices_BK                                                                                                        
         else:                                                                                                                                                           
             return encoded_acts_BF                                                                                                                                      
                                                                                                                                                                         
     def update_consistency_scores(self, current_indices):                                                                                                               
         """Update feature consistency scores based on activation patterns"""                                                                                            
         if len(self.prev_indices) >= self.consistency_window:                                                                                                           
             self.prev_indices.pop(0)                                                                                                                                    
         self.prev_indices.append(current_indices)                                                                                                                       
                                                                                                                                                                         
         if len(self.prev_indices) > 1:                                                                                                                                  
             # Calculate consistency as average overlap with previous batches                                                                                            
             consistency = torch.zeros(self.dict_size, device=self.device)                                                                                               
             for prev_idx in self.prev_indices[:-1]:                                                                                                                     
                 overlap = torch.zeros(self.dict_size, device=self.device)                                                                                               
                 for i in range(len(current_indices)):                                                                                                                   
                     curr_set = set(current_indices[i].cpu().numpy())                                                                                                    
                     prev_set = set(prev_idx[i].cpu().numpy())                                                                                                           
                     intersection = curr_set.intersection(prev_set)                                                                                                      
                     for idx in intersection:                                                                                                                            
                         overlap[idx] += 1                                                                                                                               
                 consistency += overlap / len(current_indices)                                                                                                           
             self.consistency_scores = consistency / (len(self.prev_indices) - 1)                                                                                        
                                                                                                                                                                         
     def get_stability_coefficients(self):                                                                                                                               
         """Get per-feature stability coefficients based on consistency scores"""                                                                                        
         coeffs = torch.ones_like(self.consistency_scores)                                                                                                               
         for threshold, level in zip(self.stability_thresholds, self.stability_levels):                                                                                  
             coeffs = torch.where(self.consistency_scores >= threshold,                                                                                                  
                                torch.tensor(level, device=self.device),                                                                                                 
                                coeffs)                                                                                                                                  
         return coeffs                                                                                                                                                   
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def forward(self, x: torch.Tensor, output_features: bool = False):                                                                                                  
         encoded_acts = self.encode(x)                                                                                                                                   
         x_hat = self.decode(encoded_acts)                                                                                                                               
         return (x_hat, encoded_acts) if output_features else x_hat                                                                                                      
 =======                                                                                                                                                                 
     def forward(self, x: torch.Tensor, output_features: bool = False):                                                                                                  
         encoded_acts = self.encode(x)                                                                                                                                   
         x_hat = self.decode(encoded_acts)                                                                                                                               
                                                                                                                                                                         
         # Calculate orthogonality loss if training                                                                                                                      
         if self.training:                                                                                                                                               
             stability_coeffs = self.get_stability_coefficients()                                                                                                        
             ortho_loss = self.orthogonality_loss(encoded_acts, stability_coeffs)                                                                                        
             self.current_ortho_loss = ortho_loss                                                                                                                        
                                                                                                                                                                         
         return (x_hat, encoded_acts) if output_features else x_hat                                                                                                      
                                                                                                                                                                         
     def orthogonality_loss(self, encoded_acts, stability_coeffs):                                                                                                       
         """Calculate orthogonality loss weighted by stability coefficients"""                                                                                           
         # Get feature correlations                                                                                                                                      
         features = encoded_acts.T  # [F x B]                                                                                                                            
         norm_features = features / (features.norm(dim=1, keepdim=True) + 1e-8)                                                                                          
         cosine_sim = torch.mm(norm_features, norm_features.T)                                                                                                           
                                                                                                                                                                         
         # Apply stability coefficients                                                                                                                                  
         coeff_matrix = stability_coeffs.unsqueeze(0) * stability_coeffs.unsqueeze(1)                                                                                    
         weighted_cosine = coeff_matrix * torch.abs(cosine_sim)                                                                                                          
                                                                                                                                                                         
         # Zero out diagonal                                                                                                                                             
         mask = 1 - torch.eye(weighted_cosine.shape[0], device=weighted_cosine.device)                                                                                   
         return (weighted_cosine * mask).sum() / (mask.sum() + 1e-8)                                                                                                     
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

Now let's modify the TrainerTopK class to use the new orthogonality loss:                                                                                                

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def loss(self, x, step=None, logging=False):                                                                                                                        
         # Run the SAE                                                                                                                                                   
         f, top_acts, top_indices = self.ae.encode(x, return_topk=True)                                                                                                  
         x_hat = self.ae.decode(f)                                                                                                                                       
                                                                                                                                                                         
         # Measure goodness of reconstruction                                                                                                                            
         e = x_hat - x                                                                                                                                                   
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                  
 =======                                                                                                                                                                 
     def loss(self, x, step=None, logging=False):                                                                                                                        
         # Run the SAE                                                                                                                                                   
         f, top_acts, top_indices = self.ae.encode(x, return_topk=True)                                                                                                  
         x_hat = self.ae.decode(f)                                                                                                                                       
                                                                                                                                                                         
         # Measure goodness of reconstruction                                                                                                                            
         e = x_hat - x                                                                                                                                                   
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                  
                                                                                                                                                                         
         # Get orthogonality loss                                                                                                                                        
         ortho_loss = self.ae.current_ortho_loss if hasattr(self.ae, 'current_ortho_loss') else 0                                                                        
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                           
         auxk_loss = auxk_loss.sum(dim=-1).mean()                                                                                                                        
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                                    
 =======                                                                                                                                                                 
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                           
         auxk_loss = auxk_loss.sum(dim=-1).mean()                                                                                                                        
         loss = l2_loss + self.auxk_alpha * auxk_loss + 0.1 * ortho_loss                                                                                                 
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

Let's run the first experiment with these changes:                                                                                                                       

                                                                                                                                                                         
 python experiment.py --out_dir=run_1                                                                                                                                    
                                                                                                                                                                         

This implements:                                                                                                                                                         

 1 Top-k consistency tracking with a 10-batch window                                                                                                                     
 2 Three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8)                                                                                     
 3 Stability-weighted orthogonality loss                                                                                                                                 

The key improvements are:                                                                                                                                                

 • Features develop gradually through stability levels                                                                                                                   
 • More consistent features get stronger orthogonality constraints                                                                                                       
 • Orthogonality is guided by actual feature usage patterns                                                                                                              

After seeing the results from run_1, we can tune the parameters in subsequent runs.                                                                                      

Tokens: 42k sent, 2.6k received. Cost: $0.17 message, $0.17 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
coder_out: I'll help implement the adaptive sparsity-guided orthogonality approach. Let's plan the experiments:

Run 1: Implement basic consistency tracking and stability levels
- Add top-k consistency tracking
- Implement 3 stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8)
- Add basic orthogonality loss with stability coefficients

Run 2: Tune consistency window and thresholds
- Test different window sizes (5, 10, 15) for consistency tracking
- Adjust stability thresholds (0.4/0.7, 0.5/0.8, 0.6/0.9)

Run 3: Optimize orthogonality coefficients
- Test different coefficient scales for stability levels
- Fine-tune the balance between reconstruction and orthogonality losses

Let's start with Run 1. We need to modify the AutoEncoderTopK class to add consistency tracking and stability-based orthogonality:

experiment.py
```python
<<<<<<< SEARCH
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "EleutherAI/pythia-70m-deduped",
        hook_name: Optional[str] = None,
        k: int = 100,
    ):
        super().__init__()
        self.activation_dim = d_in
        self.dict_size = d_sae
        self.k = k
=======
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "EleutherAI/pythia-70m-deduped",
        hook_name: Optional[str] = None,
        k: int = 100,
        consistency_window: int = 10,
        stability_thresholds: tuple = (0.5, 0.8),
        stability_levels: tuple = (0.2, 0.6, 1.0)
    ):
        super().__init__()
        self.activation_dim = d_in
        self.dict_size = d_sae
        self.k = k
        
        # Consistency tracking
        self.consistency_window = consistency_window
        self.prev_indices = []
        self.consistency_scores = torch.zeros(d_sae, device=self.device)
        self.stability_thresholds = stability_thresholds
        self.stability_levels = stability_levels
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF
=======
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        # Update consistency tracking
        if self.training:
            self.update_consistency_scores(top_indices_BK)

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF

    def update_consistency_scores(self, current_indices):
        """Update feature consistency scores based on activation patterns"""
        if len(self.prev_indices) >= self.consistency_window:
            self.prev_indices.pop(0)
        self.prev_indices.append(current_indices)
        
        if len(self.prev_indices) > 1:
            # Calculate consistency as average overlap with previous batches
            consistency = torch.zeros(self.dict_size, device=self.device)
            for prev_idx in self.prev_indices[:-1]:
                overlap = torch.zeros(self.dict_size, device=self.device)
                for i in range(len(current_indices)):
                    curr_set = set(current_indices[i].cpu().numpy())
                    prev_set = set(prev_idx[i].cpu().numpy())
                    intersection = curr_set.intersection(prev_set)
                    for idx in intersection:
                        overlap[idx] += 1
                consistency += overlap / len(current_indices)
            self.consistency_scores = consistency / (len(self.prev_indices) - 1)

    def get_stability_coefficients(self):
        """Get per-feature stability coefficients based on consistency scores"""
        coeffs = torch.ones_like(self.consistency_scores)
        for threshold, level in zip(self.stability_thresholds, self.stability_levels):
            coeffs = torch.where(self.consistency_scores >= threshold, 
                               torch.tensor(level, device=self.device),
                               coeffs)
        return coeffs
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def forward(self, x: torch.Tensor, output_features: bool = False):
        encoded_acts = self.encode(x)
        x_hat = self.decode(encoded_acts)
        return (x_hat, encoded_acts) if output_features else x_hat
=======
    def forward(self, x: torch.Tensor, output_features: bool = False):
        encoded_acts = self.encode(x)
        x_hat = self.decode(encoded_acts)
        
        # Calculate orthogonality loss if training
        if self.training:
            stability_coeffs = self.get_stability_coefficients()
            ortho_loss = self.orthogonality_loss(encoded_acts, stability_coeffs)
            self.current_ortho_loss = ortho_loss
            
        return (x_hat, encoded_acts) if output_features else x_hat
        
    def orthogonality_loss(self, encoded_acts, stability_coeffs):
        """Calculate orthogonality loss weighted by stability coefficients"""
        # Get feature correlations
        features = encoded_acts.T  # [F x B]
        norm_features = features / (features.norm(dim=1, keepdim=True) + 1e-8)
        cosine_sim = torch.mm(norm_features, norm_features.T)
        
        # Apply stability coefficients
        coeff_matrix = stability_coeffs.unsqueeze(0) * stability_coeffs.unsqueeze(1)
        weighted_cosine = coeff_matrix * torch.abs(cosine_sim)
        
        # Zero out diagonal
        mask = 1 - torch.eye(weighted_cosine.shape[0], device=weighted_cosine.device)
        return (weighted_cosine * mask).sum() / (mask.sum() + 1e-8)
>>>>>>> REPLACE
```

Now let's modify the TrainerTopK class to use the new orthogonality loss:

```python
<<<<<<< SEARCH
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)
=======
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)
        
        # Get orthogonality loss
        ortho_loss = self.ae.current_ortho_loss if hasattr(self.ae, 'current_ortho_loss') else 0
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss
=======
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss + 0.1 * ortho_loss
>>>>>>> REPLACE
```

Let's run the first experiment with these changes:

```bash
python experiment.py --out_dir=run_1
```

This implements:
1. Top-k consistency tracking with a 10-batch window
2. Three stability levels (0.2, 0.6, 1.0) based on consistency thresholds (0.5, 0.8)
3. Stability-weighted orthogonality loss

The key improvements are:
- Features develop gradually through stability levels
- More consistent features get stronger orthogonality constraints
- Orthogonality is guided by actual feature usage patterns

After seeing the results from run_1, we can tune the parameters in subsequent runs., type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210632_adaptive_sparse_orthogonal_sae/experiment.py", line 912, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210632_adaptive_sparse_orthogonal_sae/experiment.py", line 591, in run_sae_training
    trainer = TrainerTopK(
              ^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210632_adaptive_sparse_orthogonal_sae/experiment.py", line 289, in __init__
    self.ae =  AutoEncoderTopK(
               ^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210632_adaptive_sparse_orthogonal_sae/experiment.py", line 88, in __init__
    self.consistency_scores = torch.zeros(d_sae, device=self.device)
                                                        ^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'AutoEncoderTopK' object has no attribute 'device'

Error during experiments: cannot access local variable 'results' where it is not associated with a value
Experiments failed for idea 20250206_210632_adaptive_sparse_orthogonal_sae
FINISHED IDEA
Completed idea: adaptive_sparse_orthogonal_sae, Success: False
Processing idea: adaptive_activation_weighted_orthogonal_sae

[2025-02-06 21:11:04]  
*Starting idea: 20250206_210915_adaptive_activation_weighted_orthogonal_sae*

[2025-02-06 21:11:04] Starting experiments execution
Starting experiment with prompt for coder: Your goal is to implement the following idea: Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Implement activation frequency tracking
2. Modify loss function to use frequency-scaled weights
3. Train on google/gemma-2-2b using standard datasets
4. Compare benchmark performance against baseline and previous orthogonal SAEs
5. Analyze feature separation patterns
6. Evaluate impact on interpretability metrics.
The implementation plan is as follows: 1. Add EMA tracking for activation frequencies
2. Modify top-k selection to include frequency scaling
3. Update weighted similarity computation
4. Implement efficient batch processing
5. Add configuration for EMA decay
6. Optimize matrix operations.

You can also refer to other information in the idea: {'Name': 'adaptive_activation_weighted_orthogonal_sae', 'Title': 'Adaptive Activation-Weighted Orthogonality for Efficient Feature Separation in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Modify loss function to use frequency-scaled weights\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare benchmark performance against baseline and previous orthogonal SAEs\n5. Analyze feature separation patterns\n6. Evaluate impact on interpretability metrics', 'Technical_Details': 'The method enhances activation-weighted orthogonality with adaptive scaling: L = L_recon + λ_1 * L_sparse + λ_2 * Σ_(i,j in top-k) w_(ij) * α_(ij) * |f_i^T f_j| where w_(ij) = (|a_i| * |a_j|) / (||a||_2)^2 and α_(ij) = sqrt(p_i * p_j) for activation frequencies p_i, p_j tracked via exponential moving average (EMA) with decay 0.99. The adaptive weights α_(ij) concentrate orthogonality pressure on frequently co-activating features while maintaining O(k^2) complexity through top-k restriction.', 'Implementation_Plan': '1. Add EMA tracking for activation frequencies\n2. Modify top-k selection to include frequency scaling\n3. Update weighted similarity computation\n4. Implement efficient batch processing\n5. Add configuration for EMA decay\n6. Optimize matrix operations', 'Interestingness_Evaluation': 'Using activation frequency statistics to adaptively scale orthogonality constraints provides targeted feature separation while maintaining computational simplicity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only one additional vector of statistics; maintains O(k^2) complexity; uses standard EMA updates; easily runs within 30-minute limit on H100; straightforward addition to existing code.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of activation-weighted orthogonality with adaptive frequency scaling creates a novel approach that efficiently targets feature separation where most needed.', 'Novelty': 8, 'Expected_Research_Impact': 'The adaptive weighting should significantly improve sparse_probing benchmarks by focusing separation effort on the most problematic feature interactions while maintaining strong core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We propose an adaptive activation-weighted orthogonality constraint for sparse autoencoders that dynamically adjusts feature separation pressure based on both current activation magnitudes and historical activation frequencies. Unlike previous approaches that apply uniform orthogonality constraints, our method uses exponential moving averages to track feature activation patterns and concentrate separation effort on frequently co-activating features. The approach maintains computational efficiency through careful integration with top-k sparsity mechanisms and requires only minimal additional state. By adaptively scaling orthogonality constraints based on both immediate and historical activation patterns, the method provides targeted control over feature separation while preserving reconstruction quality. We evaluate our method on mechanistic interpretability benchmarks using the google/gemma-2-2b language model.', 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

If the experiments in the idea is already implemented in 'experiment.py' you are given with, you should try to improve its result by further enhancing the implementation.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run (you can try to run with different hyperparameters in the same run across different iterations.).

Note that we already provide the baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0002, 'steps': 2441, 'seed': 42, 'activation_dim': 2304, 'dict_size': 16384, 'k': 40, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 4357.5732421875, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'e5d9d74a-c71a-462d-9661-5a7f2d41d4b8', 'datetime_epoch_millis': 1738885451045, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.1429159584016188, 'mean_num_split_features': 1.12}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.04216867469879518, 'num_absorption': 112, 'num_probe_true_positives': 2656, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.018471337579617834, 'num_absorption': 29, 'num_probe_true_positives': 1570, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.3883746760459089, 'num_absorption': 1049, 'num_probe_true_positives': 2701, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.11044417767106843, 'num_absorption': 184, 'num_probe_true_positives': 1666, 'num_split_features': 2}, {'first_letter': 'e', 'absorption_rate': 0.07116104868913857, 'num_absorption': 114, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.05956375838926174, 'num_absorption': 71, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.08423686405337781, 'num_absorption': 101, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.3019648397104447, 'num_absorption': 292, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.10113704368641532, 'num_absorption': 169, 'num_probe_true_positives': 1671, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.004739336492890996, 'num_absorption': 2, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.007518796992481203, 'num_absorption': 5, 'num_probe_true_positives': 665, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.030377668308702793, 'num_absorption': 37, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.27388535031847133, 'num_absorption': 473, 'num_probe_true_positives': 1727, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.04790419161676647, 'num_absorption': 40, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.32429906542056075, 'num_absorption': 347, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.4584221748400853, 'num_absorption': 1075, 'num_probe_true_positives': 2345, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.07567567567567568, 'num_absorption': 14, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.004294478527607362, 'num_absorption': 7, 'num_probe_true_positives': 1630, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.39844026940801136, 'num_absorption': 1124, 'num_probe_true_positives': 2821, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.0267639902676399, 'num_absorption': 44, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.21963824289405684, 'num_absorption': 170, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.008333333333333333, 'num_absorption': 7, 'num_probe_true_positives': 840, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.14390602055800295, 'num_absorption': 98, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.3473684210526316, 'num_absorption': 66, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.023809523809523808, 'num_absorption': 6, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9596273291925466, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.40625}, 'model_performance_preservation': {'ce_loss_score': 0.9588815789473685, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.328125, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.609375, 'mse': 2.484375, 'cossim': 0.875}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 131.0, 'l2_ratio': 0.875, 'relative_reconstruction_bias': 0.99609375}, 'sparsity': {'l0': 40.0, 'l1': 364.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': '0a27606c-8d4d-449f-ad67-c23bba779fc9', 'datetime_epoch_millis': 1738886529707, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.94985, 'llm_top_1_test_accuracy': 0.66250625, 'llm_top_2_test_accuracy': 0.7216312500000001, 'llm_top_5_test_accuracy': 0.7799, 'llm_top_10_test_accuracy': 0.8296999999999999, 'llm_top_20_test_accuracy': 0.87843125, 'llm_top_50_test_accuracy': 0.9207375, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9509000509977341, 'sae_top_1_test_accuracy': 0.75075625, 'sae_top_2_test_accuracy': 0.8130499999999999, 'sae_top_5_test_accuracy': 0.8722624999999999, 'sae_top_10_test_accuracy': 0.90165625, 'sae_top_20_test_accuracy': 0.9201937499999998, 'sae_top_50_test_accuracy': 0.93555625, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.96260005235672, 'sae_top_1_test_accuracy': 0.8308000000000002, 'sae_top_2_test_accuracy': 0.8662000000000001, 'sae_top_5_test_accuracy': 0.8854, 'sae_top_10_test_accuracy': 0.8987999999999999, 'sae_top_20_test_accuracy': 0.9468, 'sae_top_50_test_accuracy': 0.9583999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9454, 'llm_top_1_test_accuracy': 0.6724, 'llm_top_2_test_accuracy': 0.7358, 'llm_top_5_test_accuracy': 0.7624000000000001, 'llm_top_10_test_accuracy': 0.8026, 'llm_top_20_test_accuracy': 0.8661999999999999, 'llm_top_50_test_accuracy': 0.9042000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.941200053691864, 'sae_top_1_test_accuracy': 0.7767999999999999, 'sae_top_2_test_accuracy': 0.8154, 'sae_top_5_test_accuracy': 0.8486, 'sae_top_10_test_accuracy': 0.9022, 'sae_top_20_test_accuracy': 0.9246000000000001, 'sae_top_50_test_accuracy': 0.9346, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9109999999999999, 'llm_top_1_test_accuracy': 0.6849999999999999, 'llm_top_2_test_accuracy': 0.7438, 'llm_top_5_test_accuracy': 0.7618, 'llm_top_10_test_accuracy': 0.7934, 'llm_top_20_test_accuracy': 0.851, 'llm_top_50_test_accuracy': 0.8879999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9250000596046448, 'sae_top_1_test_accuracy': 0.8310000000000001, 'sae_top_2_test_accuracy': 0.8448, 'sae_top_5_test_accuracy': 0.8725999999999999, 'sae_top_10_test_accuracy': 0.8865999999999999, 'sae_top_20_test_accuracy': 0.8985999999999998, 'sae_top_50_test_accuracy': 0.9106, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8964000000000001, 'llm_top_1_test_accuracy': 0.6024, 'llm_top_2_test_accuracy': 0.6397999999999999, 'llm_top_5_test_accuracy': 0.6666, 'llm_top_10_test_accuracy': 0.7491999999999999, 'llm_top_20_test_accuracy': 0.8138, 'llm_top_50_test_accuracy': 0.8566, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9076000452041626, 'sae_top_1_test_accuracy': 0.7055999999999999, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.818, 'sae_top_10_test_accuracy': 0.8550000000000001, 'sae_top_20_test_accuracy': 0.8708, 'sae_top_50_test_accuracy': 0.883, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9625000655651093, 'sae_top_1_test_accuracy': 0.649, 'sae_top_2_test_accuracy': 0.813, 'sae_top_5_test_accuracy': 0.875, 'sae_top_10_test_accuracy': 0.928, 'sae_top_20_test_accuracy': 0.935, 'sae_top_50_test_accuracy': 0.953, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9662000000000001, 'llm_top_1_test_accuracy': 0.666, 'llm_top_2_test_accuracy': 0.6900000000000001, 'llm_top_5_test_accuracy': 0.7613999999999999, 'llm_top_10_test_accuracy': 0.8039999999999999, 'llm_top_20_test_accuracy': 0.8622, 'llm_top_50_test_accuracy': 0.923, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9688000440597534, 'sae_top_1_test_accuracy': 0.6042, 'sae_top_2_test_accuracy': 0.6302, 'sae_top_5_test_accuracy': 0.8632, 'sae_top_10_test_accuracy': 0.8856000000000002, 'sae_top_20_test_accuracy': 0.9151999999999999, 'sae_top_50_test_accuracy': 0.9425999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9380000000000001, 'llm_top_1_test_accuracy': 0.63325, 'llm_top_2_test_accuracy': 0.76825, 'llm_top_5_test_accuracy': 0.827, 'llm_top_10_test_accuracy': 0.869, 'llm_top_20_test_accuracy': 0.89925, 'llm_top_50_test_accuracy': 0.9265000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9455000460147858, 'sae_top_1_test_accuracy': 0.79725, 'sae_top_2_test_accuracy': 0.85, 'sae_top_5_test_accuracy': 0.8865, 'sae_top_10_test_accuracy': 0.90925, 'sae_top_20_test_accuracy': 0.91675, 'sae_top_50_test_accuracy': 0.93625, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9998000000000001, 'llm_top_1_test_accuracy': 0.7243999999999999, 'llm_top_2_test_accuracy': 0.7802, 'llm_top_5_test_accuracy': 0.9019999999999999, 'llm_top_10_test_accuracy': 0.96, 'llm_top_20_test_accuracy': 0.99, 'llm_top_50_test_accuracy': 0.9978, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9940000414848328, 'sae_top_1_test_accuracy': 0.8114000000000001, 'sae_top_2_test_accuracy': 0.8938, 'sae_top_5_test_accuracy': 0.9288000000000001, 'sae_top_10_test_accuracy': 0.9478, 'sae_top_20_test_accuracy': 0.9538, 'sae_top_50_test_accuracy': 0.966, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'e8be4b3c-dfd3-49dd-91ca-e1b0d08e9055', 'datetime_epoch_millis': 1738886775914, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.026217222213745117}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'ed9b5672e8e2efb4d1319445d042023416923af6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 16384, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 701.0477905273438, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b5f3e18f-7844-4d8a-ac26-9b99d357cf9a', 'datetime_epoch_millis': 1738804062497, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.016138604243358595, 'mean_num_split_features': 1.4166666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2656, 'num_split_features': 2}, {'first_letter': 'b', 'absorption_rate': 0.0006369426751592356, 'num_absorption': 1, 'num_probe_true_positives': 1570, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2701, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.009603841536614645, 'num_absorption': 16, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.05305867665418227, 'num_absorption': 85, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.013422818791946308, 'num_absorption': 16, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.021684737281067557, 'num_absorption': 26, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.006582884500299222, 'num_absorption': 11, 'num_probe_true_positives': 1671, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.09770114942528736, 'num_absorption': 119, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0005790387955993051, 'num_absorption': 1, 'num_probe_true_positives': 1727, 'num_split_features': 4}, {'first_letter': 'n', 'absorption_rate': 0.013173652694610778, 'num_absorption': 11, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.022429906542056073, 'num_absorption': 24, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.008955223880597015, 'num_absorption': 21, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0006134969325153375, 'num_absorption': 1, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.0857851825593761, 'num_absorption': 242, 'num_probe_true_positives': 2821, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.011557177615571776, 'num_absorption': 19, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.021963824289405683, 'num_absorption': 17, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.0014684287812041115, 'num_absorption': 1, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.011904761904761904, 'num_absorption': 3, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995723748059007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004302978515625}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.984375, 'mse': 0.0888671875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.98828125}, 'sparsity': {'l0': 8895.810546875, 'l1': 12608.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'fc35f1d1-c27c-402b-abd2-fe38f12c05aa', 'datetime_epoch_millis': 1738804969188, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95075625, 'llm_top_1_test_accuracy': 0.65444375, 'llm_top_2_test_accuracy': 0.71966875, 'llm_top_5_test_accuracy': 0.7860875, 'llm_top_10_test_accuracy': 0.83056875, 'llm_top_20_test_accuracy': 0.8764437500000001, 'llm_top_50_test_accuracy': 0.9228625, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9578625451773405, 'sae_top_1_test_accuracy': 0.6448624999999999, 'sae_top_2_test_accuracy': 0.6875375000000001, 'sae_top_5_test_accuracy': 0.7562874999999999, 'sae_top_10_test_accuracy': 0.8066500000000001, 'sae_top_20_test_accuracy': 0.8664625000000001, 'sae_top_50_test_accuracy': 0.9122750000000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6862, 'sae_top_2_test_accuracy': 0.6885999999999999, 'sae_top_5_test_accuracy': 0.7842, 'sae_top_10_test_accuracy': 0.8386000000000001, 'sae_top_20_test_accuracy': 0.8806, 'sae_top_50_test_accuracy': 0.9252, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9492, 'llm_top_1_test_accuracy': 0.6794, 'llm_top_2_test_accuracy': 0.717, 'llm_top_5_test_accuracy': 0.767, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8614, 'llm_top_50_test_accuracy': 0.9086000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.6844, 'sae_top_2_test_accuracy': 0.6834, 'sae_top_5_test_accuracy': 0.7402000000000001, 'sae_top_10_test_accuracy': 0.7884, 'sae_top_20_test_accuracy': 0.8503999999999999, 'sae_top_50_test_accuracy': 0.8998000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6862, 'llm_top_2_test_accuracy': 0.732, 'llm_top_5_test_accuracy': 0.7722, 'llm_top_10_test_accuracy': 0.8008, 'llm_top_20_test_accuracy': 0.8532, 'llm_top_50_test_accuracy': 0.8922000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9368000507354737, 'sae_top_1_test_accuracy': 0.5932000000000001, 'sae_top_2_test_accuracy': 0.725, 'sae_top_5_test_accuracy': 0.7638, 'sae_top_10_test_accuracy': 0.8056000000000001, 'sae_top_20_test_accuracy': 0.8465999999999999, 'sae_top_50_test_accuracy': 0.8838000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8948, 'llm_top_1_test_accuracy': 0.6039999999999999, 'llm_top_2_test_accuracy': 0.6508, 'llm_top_5_test_accuracy': 0.6878, 'llm_top_10_test_accuracy': 0.745, 'llm_top_20_test_accuracy': 0.8051999999999999, 'llm_top_50_test_accuracy': 0.8628, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9118000507354737, 'sae_top_1_test_accuracy': 0.5828, 'sae_top_2_test_accuracy': 0.6594, 'sae_top_5_test_accuracy': 0.7343999999999999, 'sae_top_10_test_accuracy': 0.768, 'sae_top_20_test_accuracy': 0.8114000000000001, 'sae_top_50_test_accuracy': 0.8452, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000343322754, 'sae_top_1_test_accuracy': 0.699, 'sae_top_2_test_accuracy': 0.701, 'sae_top_5_test_accuracy': 0.792, 'sae_top_10_test_accuracy': 0.814, 'sae_top_20_test_accuracy': 0.908, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6634, 'llm_top_2_test_accuracy': 0.7002, 'llm_top_5_test_accuracy': 0.7648, 'llm_top_10_test_accuracy': 0.8024000000000001, 'llm_top_20_test_accuracy': 0.8608, 'llm_top_50_test_accuracy': 0.9258, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6342000000000001, 'sae_top_2_test_accuracy': 0.6708000000000001, 'sae_top_5_test_accuracy': 0.7096000000000001, 'sae_top_10_test_accuracy': 0.7712, 'sae_top_20_test_accuracy': 0.8295999999999999, 'sae_top_50_test_accuracy': 0.9026, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.93825, 'llm_top_1_test_accuracy': 0.63375, 'llm_top_2_test_accuracy': 0.7607499999999999, 'llm_top_5_test_accuracy': 0.8265, 'llm_top_10_test_accuracy': 0.8727499999999999, 'llm_top_20_test_accuracy': 0.8977499999999999, 'llm_top_50_test_accuracy': 0.9265, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9530000537633896, 'sae_top_1_test_accuracy': 0.6285, 'sae_top_2_test_accuracy': 0.7015, 'sae_top_5_test_accuracy': 0.7515000000000001, 'sae_top_10_test_accuracy': 0.856, 'sae_top_20_test_accuracy': 0.8875, 'sae_top_50_test_accuracy': 0.9199999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6522, 'llm_top_2_test_accuracy': 0.7814, 'llm_top_5_test_accuracy': 0.9123999999999999, 'llm_top_10_test_accuracy': 0.9642, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9972000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9990000247955322, 'sae_top_1_test_accuracy': 0.6506000000000001, 'sae_top_2_test_accuracy': 0.6706, 'sae_top_5_test_accuracy': 0.7745999999999998, 'sae_top_10_test_accuracy': 0.8114000000000001, 'sae_top_20_test_accuracy': 0.9176, 'sae_top_50_test_accuracy': 0.9895999999999999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'b48744e1-4f43-4d25-af92-4f1a46abab60', 'datetime_epoch_millis': 1738805225729, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the adaptive activation-weighted orthogonality approach. Let me plan out the experiments:                                                            

Experiment Plan:                                                                                                                                                         

 1 Run 1: Implement basic EMA tracking and activation-weighted orthogonality                                                                                             
    • Add EMA tracking for activation frequencies                                                                                                                        
    • Add basic orthogonality loss with activation weighting                                                                                                             
    • EMA decay = 0.99, orthogonality weight = 0.1                                                                                                                       
 2 Run 2: Tune orthogonality weight                                                                                                                                      
    • Keep EMA decay = 0.99                                                                                                                                              
    • Increase orthogonality weight to 0.5 to strengthen feature separation                                                                                              
 3 Run 3: Optimize EMA decay                                                                                                                                             
    • Keep best orthogonality weight from Run 2                                                                                                                          
    • Try faster EMA decay = 0.95 for more dynamic adaptation                                                                                                            
 4 Run 4: Add frequency-scaled weights                                                                                                                                   
    • Keep best params from Run 3                                                                                                                                        
    • Add sqrt(p_i * p_j) frequency scaling to orthogonality weights                                                                                                     

Let's start with Run 1. I'll modify the experiment.py file to add EMA tracking and basic orthogonality loss:                                                             

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 class AutoEncoderTopK(nn.Module):                                                                                                                                       
     """                                                                                                                                                                 
     The top-k autoencoder architecture using parameters instead of nn.Linear layers.                                                                                    
     """                                                                                                                                                                 
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         d_in: int,                                                                                                                                                      
         d_sae: int,                                                                                                                                                     
         hook_layer: int,                                                                                                                                                
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                              
         hook_name: Optional[str] = None,                                                                                                                                
         k: int = 100,                                                                                                                                                   
     ):                                                                                                                                                                  
 =======                                                                                                                                                                 
 class AutoEncoderTopK(nn.Module):                                                                                                                                       
     """                                                                                                                                                                 
     The top-k autoencoder architecture using parameters instead of nn.Linear layers.                                                                                    
     Includes activation frequency tracking and orthogonality constraints.                                                                                               
     """                                                                                                                                                                 
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         d_in: int,                                                                                                                                                      
         d_sae: int,                                                                                                                                                     
         hook_layer: int,                                                                                                                                                
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                              
         hook_name: Optional[str] = None,                                                                                                                                
         k: int = 100,                                                                                                                                                   
         ema_decay: float = 0.99,                                                                                                                                        
         ortho_weight: float = 0.1,                                                                                                                                      
     ):                                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.set_decoder_norm_to_unit_norm()                                                                                                                            
                                                                                                                                                                         
                                                                                                                                                                         
         if hook_name is None:                                                                                                                                           
 =======                                                                                                                                                                 
         self.set_decoder_norm_to_unit_norm()                                                                                                                            
                                                                                                                                                                         
         # Initialize EMA tracking for activation frequencies                                                                                                            
         self.register_buffer('activation_ema', torch.zeros(d_sae))                                                                                                      
         self.ema_decay = ema_decay                                                                                                                                      
         self.ortho_weight = ortho_weight                                                                                                                                
                                                                                                                                                                         
         if hook_name is None:                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                       
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                           
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                   
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                           
                                                                                                                                                                         
         # Scatter topk values to form encoded activations                                                                                                               
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                              
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                            
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                              
         )                                                                                                                                                               
                                                                                                                                                                         
         if return_topk:                                                                                                                                                 
             return encoded_acts_BF, tops_acts_BK, top_indices_BK                                                                                                        
         else:                                                                                                                                                           
             return encoded_acts_BF                                                                                                                                      
 =======                                                                                                                                                                 
     def encode(self, x: torch.Tensor, return_topk: bool = False):                                                                                                       
         pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc                                                                                                           
         post_relu_feat_acts_BF = torch.relu(pre_acts)                                                                                                                   
         post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)                                                                                           
                                                                                                                                                                         
         # Scatter topk values to form encoded activations                                                                                                               
         tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices                                                                                              
         encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(                                                                                            
             dim=-1, index=top_indices_BK, src=tops_acts_BK                                                                                                              
         )                                                                                                                                                               
                                                                                                                                                                         
         # Update activation EMA                                                                                                                                         
         with torch.no_grad():                                                                                                                                           
             batch_activations = (encoded_acts_BF > 0).float().mean(0)                                                                                                   
             self.activation_ema = self.ema_decay * self.activation_ema + (1 - self.ema_decay) * batch_activations                                                       
                                                                                                                                                                         
         if return_topk:                                                                                                                                                 
             return encoded_acts_BF, tops_acts_BK, top_indices_BK                                                                                                        
         else:                                                                                                                                                           
             return encoded_acts_BF                                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 class TrainerTopK(SAETrainer):                                                                                                                                          
     """                                                                                                                                                                 
     Top-K SAE training scheme.                                                                                                                                          
     """                                                                                                                                                                 
                                                                                                                                                                         
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
     ):                                                                                                                                                                  
 =======                                                                                                                                                                 
 class TrainerTopK(SAETrainer):                                                                                                                                          
     """                                                                                                                                                                 
     Top-K SAE training scheme with activation-weighted orthogonality.                                                                                                   
     """                                                                                                                                                                 
                                                                                                                                                                         
     def __init__(                                                                                                                                                       
         self,                                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                                     
         activation_dim=512,                                                                                                                                             
         dict_size=64 * 512,                                                                                                                                             
         k=100,                                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                                              
         steps=30000,  # when when does training end                                                                                                                     
         seed=None,                                                                                                                                                      
         device=None,                                                                                                                                                    
         layer=None,                                                                                                                                                     
         lm_name=None,                                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                                   
         submodule_name=None,                                                                                                                                            
         ema_decay=0.99,                                                                                                                                                 
         ortho_weight=0.1,                                                                                                                                               
     ):                                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
             k = k                                                                                                                                                       
             )                                                                                                                                                           
                                                                                                                                                                         
                                                                                                                                                                         
         if device is None:                                                                                                                                              
 =======                                                                                                                                                                 
             k = k,                                                                                                                                                      
             ema_decay = ema_decay,                                                                                                                                      
             ortho_weight = ortho_weight                                                                                                                                 
             )                                                                                                                                                           
                                                                                                                                                                         
         if device is None:                                                                                                                                              
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def loss(self, x, step=None, logging=False):                                                                                                                        
         # Run the SAE                                                                                                                                                   
         f, top_acts, top_indices = self.ae.encode(x, return_topk=True)                                                                                                  
         x_hat = self.ae.decode(f)                                                                                                                                       
                                                                                                                                                                         
         # Measure goodness of reconstruction                                                                                                                            
         e = x_hat - x                                                                                                                                                   
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                  
 =======                                                                                                                                                                 
     def loss(self, x, step=None, logging=False):                                                                                                                        
         # Run the SAE                                                                                                                                                   
         f, top_acts, top_indices = self.ae.encode(x, return_topk=True)                                                                                                  
         x_hat = self.ae.decode(f)                                                                                                                                       
                                                                                                                                                                         
         # Measure goodness of reconstruction                                                                                                                            
         e = x_hat - x                                                                                                                                                   
         total_variance = (x - x.mean(0)).pow(2).sum(0)                                                                                                                  
                                                                                                                                                                         
         # Compute activation-weighted orthogonality loss                                                                                                                
         ortho_loss = 0                                                                                                                                                  
         if self.ae.ortho_weight > 0:                                                                                                                                    
             # Get unique pairs of active features                                                                                                                       
             active_features = torch.unique(top_indices)                                                                                                                 
             n_active = len(active_features)                                                                                                                             
                                                                                                                                                                         
             if n_active > 1:                                                                                                                                            
                 # Compute pairwise similarities                                                                                                                         
                 W = self.ae.W_dec[active_features]                                                                                                                      
                 sims = torch.mm(W, W.t())                                                                                                                               
                                                                                                                                                                         
                 # Compute activation weights                                                                                                                            
                 acts = f[:, active_features].abs().mean(0)                                                                                                              
                 act_weights = torch.outer(acts, acts)                                                                                                                   
                                                                                                                                                                         
                 # Mask out diagonal                                                                                                                                     
                 mask = ~torch.eye(n_active, dtype=torch.bool, device=sims.device)                                                                                       
                 ortho_loss = (act_weights[mask] * sims[mask].pow(2)).mean()                                                                                             
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                           
         auxk_loss = auxk_loss.sum(dim=-1).mean()                                                                                                                        
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                                    
 =======                                                                                                                                                                 
         l2_loss = e.pow(2).sum(dim=-1).mean()                                                                                                                           
         auxk_loss = auxk_loss.sum(dim=-1).mean()                                                                                                                        
         loss = l2_loss + self.auxk_alpha * auxk_loss + self.ae.ortho_weight * ortho_loss                                                                                
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

Let's run this first version with basic activation-weighted orthogonality:                                                                                               

                                                                                                                                                                         
 python experiment.py --out_dir=run_1                                                                                                                                    
                                                                                                                                                                         

This implements:                                                                                                                                                         

 1 EMA tracking of activation frequencies                                                                                                                                
 2 Basic activation-weighted orthogonality loss                                                                                                                          
 3 Conservative initial parameters (ema_decay=0.99, ortho_weight=0.1)                                                                                                    

The orthogonality loss focuses on pairs of features that are both:                                                                                                       

 1 Currently active (via top-k selection)                                                                                                                                
 2 Strongly activated (via activation magnitude weighting)                                                                                                               

We'll analyze the results and tune parameters in subsequent runs.                                                                                                        

Tokens: 43k sent, 2.5k received. Cost: $0.17 message, $0.17 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
Step 0: 45201.3515625
Step 100: 17867.94921875
Step 200: 7436.96484375
Step 300: 6015.94775390625
Step 400: 5554.48828125
Step 500: 5375.22607421875
Step 600: 4985.509765625
Step 700: 4972.078125
Step 800: 4773.4130859375
Step 900: 4751.6220703125
Step 1000: 4616.40234375
Step 1100: 4627.3359375
Step 1200: 4492.208984375
Step 1300: 4422.0986328125
Step 1400: 4524.443359375
Step 1500: 4632.974609375
Step 1600: 4456.7900390625
Step 1700: 4385.822265625
Step 1800: 4441.6494140625
Step 1900: 4556.5390625
Step 2000: 4351.4326171875
Step 2100: 4360.6181640625
Step 2200: 4412.9248046875
Step 2300: 4360.83935546875
Step 2400: 4382.0458984375

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210915_adaptive_activation_weighted_orthogonal_sae/run_1/final_info.json

[2025-02-06 21:30:57]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 514.53s

[2025-02-06 21:39:32]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
coder_out: I'll help implement the adaptive activation-weighted orthogonality approach. Let me plan out the experiments:

Experiment Plan:
1. Run 1: Implement basic EMA tracking and activation-weighted orthogonality
   - Add EMA tracking for activation frequencies
   - Add basic orthogonality loss with activation weighting
   - EMA decay = 0.99, orthogonality weight = 0.1

2. Run 2: Tune orthogonality weight
   - Keep EMA decay = 0.99
   - Increase orthogonality weight to 0.5 to strengthen feature separation

3. Run 3: Optimize EMA decay
   - Keep best orthogonality weight from Run 2
   - Try faster EMA decay = 0.95 for more dynamic adaptation

4. Run 4: Add frequency-scaled weights
   - Keep best params from Run 3
   - Add sqrt(p_i * p_j) frequency scaling to orthogonality weights

Let's start with Run 1. I'll modify the experiment.py file to add EMA tracking and basic orthogonality loss:

experiment.py
```python
<<<<<<< SEARCH
class AutoEncoderTopK(nn.Module):
    """
    The top-k autoencoder architecture using parameters instead of nn.Linear layers.
    """
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "EleutherAI/pythia-70m-deduped",
        hook_name: Optional[str] = None,
        k: int = 100,
    ):
=======
class AutoEncoderTopK(nn.Module):
    """
    The top-k autoencoder architecture using parameters instead of nn.Linear layers.
    Includes activation frequency tracking and orthogonality constraints.
    """
    def __init__(
        self,
        d_in: int,
        d_sae: int,
        hook_layer: int,
        model_name: str = "EleutherAI/pythia-70m-deduped",
        hook_name: Optional[str] = None,
        k: int = 100,
        ema_decay: float = 0.99,
        ortho_weight: float = 0.1,
    ):
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.set_decoder_norm_to_unit_norm()


        if hook_name is None:
=======
        self.set_decoder_norm_to_unit_norm()

        # Initialize EMA tracking for activation frequencies
        self.register_buffer('activation_ema', torch.zeros(d_sae))
        self.ema_decay = ema_decay
        self.ortho_weight = ortho_weight

        if hook_name is None:
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF
=======
    def encode(self, x: torch.Tensor, return_topk: bool = False):
        pre_acts = (x - self.b_dec) @ self.W_enc + self.b_enc
        post_relu_feat_acts_BF = torch.relu(pre_acts)
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)

        # Scatter topk values to form encoded activations
        tops_acts_BK, top_indices_BK = post_topk.values, post_topk.indices
        encoded_acts_BF = torch.zeros_like(post_relu_feat_acts_BF).scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )

        # Update activation EMA
        with torch.no_grad():
            batch_activations = (encoded_acts_BF > 0).float().mean(0)
            self.activation_ema = self.ema_decay * self.activation_ema + (1 - self.ema_decay) * batch_activations

        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
class TrainerTopK(SAETrainer):
    """
    Top-K SAE training scheme.
    """

    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
    ):
=======
class TrainerTopK(SAETrainer):
    """
    Top-K SAE training scheme with activation-weighted orthogonality.
    """

    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
        ema_decay=0.99,
        ortho_weight=0.1,
    ):
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
            k = k
            )


        if device is None:
=======
            k = k,
            ema_decay = ema_decay,
            ortho_weight = ortho_weight
            )

        if device is None:
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)
=======
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)

        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)

        # Compute activation-weighted orthogonality loss
        ortho_loss = 0
        if self.ae.ortho_weight > 0:
            # Get unique pairs of active features
            active_features = torch.unique(top_indices)
            n_active = len(active_features)
            
            if n_active > 1:
                # Compute pairwise similarities
                W = self.ae.W_dec[active_features]
                sims = torch.mm(W, W.t())
                
                # Compute activation weights
                acts = f[:, active_features].abs().mean(0)
                act_weights = torch.outer(acts, acts)
                
                # Mask out diagonal
                mask = ~torch.eye(n_active, dtype=torch.bool, device=sims.device)
                ortho_loss = (act_weights[mask] * sims[mask].pow(2)).mean()
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss
=======
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss + self.ae.ortho_weight * ortho_loss
>>>>>>> REPLACE
```

Let's run this first version with basic activation-weighted orthogonality:

```bash
python experiment.py --out_dir=run_1
```

This implements:
1. EMA tracking of activation frequencies
2. Basic activation-weighted orthogonality loss
3. Conservative initial parameters (ema_decay=0.99, ortho_weight=0.1)

The orthogonality loss focuses on pairs of features that are both:
1. Currently active (via top-k selection)
2. Strongly activated (via activation magnitude weighting)

We'll analyze the results and tune parameters in subsequent runs., type: <class 'str'>
Run 1 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.90s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.62s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  5.83s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.78s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.19s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  1.88s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.22s/it]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  probe = torch.load(


Epochs:   0%|          | 0/50 [00:00<?, ?it/s][A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=1.96875000][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=2.43750000][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=2.07812500][A[A


Epoch 1/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=1.57031250][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.75it/s, Loss=1.57031250][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.75it/s, Loss=1.50000000][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.75it/s, Loss=1.57812500][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.75it/s, Loss=1.46875000][A[A


Epoch 1/50:  12%|█▏        | 4/34 [00:00<00:00, 31.75it/s, Loss=1.22656250][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 34.49it/s, Loss=1.22656250][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 34.49it/s, Loss=1.14062500][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 34.49it/s, Loss=1.23437500][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 34.49it/s, Loss=1.20312500][A[A


Epoch 1/50:  24%|██▎       | 8/34 [00:00<00:00, 34.49it/s, Loss=1.14062500][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.71it/s, Loss=1.14062500][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.71it/s, Loss=1.08593750][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.71it/s, Loss=1.07031250][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.71it/s, Loss=1.01562500][A[A


Epoch 1/50:  35%|███▌      | 12/34 [00:00<00:01, 21.71it/s, Loss=0.96093750][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.94it/s, Loss=0.96093750][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.94it/s, Loss=0.96875000][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.94it/s, Loss=0.96484375][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.94it/s, Loss=0.92187500][A[A


Epoch 1/50:  47%|████▋     | 16/34 [00:00<00:00, 25.94it/s, Loss=0.89062500][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 28.98it/s, Loss=0.89062500][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 28.98it/s, Loss=0.85937500][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 28.98it/s, Loss=0.86718750][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:00<00:00, 28.98it/s, Loss=0.86718750][A[A


Epoch 1/50:  59%|█████▉    | 20/34 [00:01<00:00, 28.98it/s, Loss=0.83593750][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 20.77it/s, Loss=0.83593750][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 20.77it/s, Loss=0.82812500][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 20.77it/s, Loss=0.79687500][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 20.77it/s, Loss=0.76562500][A[A


Epoch 1/50:  71%|███████   | 24/34 [00:01<00:00, 20.77it/s, Loss=0.76562500][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.29it/s, Loss=0.76562500][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.29it/s, Loss=0.77343750][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.29it/s, Loss=0.75781250][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.29it/s, Loss=0.73828125][A[A


Epoch 1/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.29it/s, Loss=0.72656250][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.23it/s, Loss=0.72656250][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.23it/s, Loss=0.69531250][A[A


Epoch 1/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.23it/s, Loss=0.65625000][A[A


                                                                            [A[A

Epochs:   0%|          | 0/50 [00:01<?, ?it/s, Mean Loss=1.09880515, LR=1.00e-02][A

Epochs:   2%|▏         | 1/50 [00:01<01:03,  1.29s/it, Mean Loss=1.09880515, LR=1.00e-02][A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.70312500][A[A


Epoch 2/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.67968750][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=0.67968750][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=0.67187500][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=0.65234375][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=0.64062500][A[A


Epoch 2/50:   6%|▌         | 2/34 [00:00<00:03,  9.17it/s, Loss=0.62500000][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=0.62500000][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=0.64062500][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=0.65625000][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=0.63281250][A[A


Epoch 2/50:  18%|█▊        | 6/34 [00:00<00:01, 20.55it/s, Loss=0.60546875][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.60546875][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.58593750][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.59375000][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.60937500][A[A


Epoch 2/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.58593750][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=0.58593750][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=0.61328125][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=0.61718750][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=0.57031250][A[A


Epoch 2/50:  41%|████      | 14/34 [00:00<00:00, 20.15it/s, Loss=0.56640625][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=0.56640625][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=0.58203125][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=0.57812500][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=0.60156250][A[A


Epoch 2/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.40it/s, Loss=0.55468750][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.55468750][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.57031250][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.57031250][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.57421875][A[A


Epoch 2/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.55078125][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.11it/s, Loss=0.55078125][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.11it/s, Loss=0.56250000][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.11it/s, Loss=0.57031250][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.11it/s, Loss=0.55468750][A[A


Epoch 2/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.11it/s, Loss=0.53125000][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.52it/s, Loss=0.53125000][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.52it/s, Loss=0.56250000][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.52it/s, Loss=0.56250000][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.52it/s, Loss=0.56250000][A[A


Epoch 2/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.52it/s, Loss=0.51171875][A[A


                                                                            [A[A

Epochs:   2%|▏         | 1/50 [00:02<01:03,  1.29s/it, Mean Loss=0.59558824, LR=8.71e-03][A

Epochs:   4%|▍         | 2/50 [00:02<01:04,  1.35s/it, Mean Loss=0.59558824, LR=8.71e-03][A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.51953125][A[A


Epoch 3/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.53515625][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=0.53515625][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=0.55078125][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=0.53906250][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=0.53125000][A[A


Epoch 3/50:   6%|▌         | 2/34 [00:00<00:03,  9.22it/s, Loss=0.52343750][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.52343750][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.53125000][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.51562500][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.52343750][A[A


Epoch 3/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.52343750][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 26.55it/s, Loss=0.52343750][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 26.55it/s, Loss=0.51953125][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 26.55it/s, Loss=0.50390625][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 26.55it/s, Loss=0.50390625][A[A


Epoch 3/50:  29%|██▉       | 10/34 [00:00<00:00, 26.55it/s, Loss=0.52343750][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=0.52343750][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=0.53125000][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=0.51953125][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=0.51562500][A[A


Epoch 3/50:  41%|████      | 14/34 [00:00<00:00, 20.10it/s, Loss=0.53125000][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.53125000][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.50781250][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.48828125][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.51562500][A[A


Epoch 3/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.51562500][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.72it/s, Loss=0.51562500][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.72it/s, Loss=0.52343750][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=0.49023438][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=0.49609375][A[A


Epoch 3/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.72it/s, Loss=0.51562500][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.51562500][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.51562500][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.50000000][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.50781250][A[A


Epoch 3/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.50781250][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.50781250][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.49414062][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.47656250][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.49218750][A[A


Epoch 3/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.51171875][A[A


                                                                            [A[A

Epochs:   4%|▍         | 2/50 [00:04<01:04,  1.35s/it, Mean Loss=0.51470588, LR=7.59e-03][A

Epochs:   6%|▌         | 3/50 [00:04<01:04,  1.36s/it, Mean Loss=0.51470588, LR=7.59e-03][A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.50000000][A[A


Epoch 4/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.47851562][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=0.47851562][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=0.48046875][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=0.48828125][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=0.48437500][A[A


Epoch 4/50:   6%|▌         | 2/34 [00:00<00:03,  9.25it/s, Loss=0.48046875][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.66it/s, Loss=0.48046875][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.66it/s, Loss=0.50390625][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.66it/s, Loss=0.48437500][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.66it/s, Loss=0.48242188][A[A


Epoch 4/50:  18%|█▊        | 6/34 [00:00<00:01, 20.66it/s, Loss=0.47656250][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:00, 26.49it/s, Loss=0.47656250][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:00, 26.49it/s, Loss=0.48046875][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:00, 26.49it/s, Loss=0.48437500][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:00, 26.49it/s, Loss=0.48828125][A[A


Epoch 4/50:  29%|██▉       | 10/34 [00:00<00:00, 26.49it/s, Loss=0.46875000][A[A


Epoch 4/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=0.46875000][A[A


Epoch 4/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=0.48242188][A[A


Epoch 4/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=0.48046875][A[A


Epoch 4/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=0.48437500][A[A


Epoch 4/50:  41%|████      | 14/34 [00:00<00:00, 20.02it/s, Loss=0.47265625][A[A


Epoch 4/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s, Loss=0.47265625][A[A


Epoch 4/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s, Loss=0.48046875][A[A


Epoch 4/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s, Loss=0.46093750][A[A


Epoch 4/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s, Loss=0.46875000][A[A


Epoch 4/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s, Loss=0.46289062][A[A


Epoch 4/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=0.46289062][A[A


Epoch 4/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=0.46093750][A[A


Epoch 4/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.47460938][A[A


Epoch 4/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.47460938][A[A


Epoch 4/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.47265625][A[A


Epoch 4/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.47265625][A[A


Epoch 4/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.46875000][A[A


Epoch 4/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.46484375][A[A


Epoch 4/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.48046875][A[A


Epoch 4/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.47265625][A[A


Epoch 4/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.47265625][A[A


Epoch 4/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.47460938][A[A


Epoch 4/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.47460938][A[A


Epoch 4/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.46875000][A[A


Epoch 4/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.37304688][A[A


                                                                            [A[A

Epochs:   6%|▌         | 3/50 [00:05<01:04,  1.36s/it, Mean Loss=0.47455193, LR=6.61e-03][A

Epochs:   8%|▊         | 4/50 [00:05<01:03,  1.37s/it, Mean Loss=0.47455193, LR=6.61e-03][A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.46484375][A[A


Epoch 5/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.45703125][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=0.45703125][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=0.45898438][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=0.44921875][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=0.44531250][A[A


Epoch 5/50:   6%|▌         | 2/34 [00:00<00:03,  9.28it/s, Loss=0.44921875][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=0.44921875][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=0.46875000][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=0.45117188][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=0.46093750][A[A


Epoch 5/50:  18%|█▊        | 6/34 [00:00<00:01, 20.74it/s, Loss=0.44531250][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.44531250][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.44140625][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.43945312][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.45312500][A[A


Epoch 5/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.44921875][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.44921875][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.44726562][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.44140625][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.44531250][A[A


Epoch 5/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.44140625][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=0.44140625][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=0.44726562][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=0.44531250][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=0.44921875][A[A


Epoch 5/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.17it/s, Loss=0.44531250][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.49it/s, Loss=0.44531250][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.49it/s, Loss=0.43750000][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.49it/s, Loss=0.44531250][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.49it/s, Loss=0.44531250][A[A


Epoch 5/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.49it/s, Loss=0.43750000][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.68it/s, Loss=0.43750000][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.68it/s, Loss=0.44921875][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.68it/s, Loss=0.42968750][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.68it/s, Loss=0.43945312][A[A


Epoch 5/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.68it/s, Loss=0.44140625][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.07it/s, Loss=0.44140625][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.07it/s, Loss=0.44921875][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.07it/s, Loss=0.42578125][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.07it/s, Loss=0.44531250][A[A


Epoch 5/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.07it/s, Loss=0.49609375][A[A


                                                                            [A[A

Epochs:   8%|▊         | 4/50 [00:06<01:03,  1.37s/it, Mean Loss=0.44818474, LR=5.75e-03][A

Epochs:  10%|█         | 5/50 [00:06<01:01,  1.37s/it, Mean Loss=0.44818474, LR=5.75e-03][A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.44531250][A[A


Epoch 6/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.43554688][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.43554688][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.42578125][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.43554688][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.43945312][A[A


Epoch 6/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.42578125][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.42578125][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.43359375][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.44531250][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.42382812][A[A


Epoch 6/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.42578125][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.39it/s, Loss=0.42578125][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.39it/s, Loss=0.42773438][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.39it/s, Loss=0.42968750][A[A


Epoch 6/50:  29%|██▉       | 10/34 [00:00<00:00, 26.39it/s, Loss=0.42773438][A[A


Epoch 6/50:  38%|███▊      | 13/34 [00:00<00:01, 18.79it/s, Loss=0.42773438][A[A


Epoch 6/50:  38%|███▊      | 13/34 [00:00<00:01, 18.79it/s, Loss=0.42968750][A[A


Epoch 6/50:  38%|███▊      | 13/34 [00:00<00:01, 18.79it/s, Loss=0.42968750][A[A


Epoch 6/50:  38%|███▊      | 13/34 [00:00<00:01, 18.79it/s, Loss=0.42578125][A[A


Epoch 6/50:  38%|███▊      | 13/34 [00:00<00:01, 18.79it/s, Loss=0.42187500][A[A


Epoch 6/50:  50%|█████     | 17/34 [00:00<00:00, 23.43it/s, Loss=0.42187500][A[A


Epoch 6/50:  50%|█████     | 17/34 [00:00<00:00, 23.43it/s, Loss=0.43750000][A[A


Epoch 6/50:  50%|█████     | 17/34 [00:00<00:00, 23.43it/s, Loss=0.43164062][A[A


Epoch 6/50:  50%|█████     | 17/34 [00:00<00:00, 23.43it/s, Loss=0.42187500][A[A


Epoch 6/50:  50%|█████     | 17/34 [00:00<00:00, 23.43it/s, Loss=0.43750000][A[A


Epoch 6/50:  62%|██████▏   | 21/34 [00:00<00:00, 27.05it/s, Loss=0.43750000][A[A


Epoch 6/50:  62%|██████▏   | 21/34 [00:00<00:00, 27.05it/s, Loss=0.42187500][A[A


Epoch 6/50:  62%|██████▏   | 21/34 [00:00<00:00, 27.05it/s, Loss=0.44140625][A[A


Epoch 6/50:  62%|██████▏   | 21/34 [00:01<00:00, 27.05it/s, Loss=0.41796875][A[A


Epoch 6/50:  62%|██████▏   | 21/34 [00:01<00:00, 27.05it/s, Loss=0.42578125][A[A


Epoch 6/50:  74%|███████▎  | 25/34 [00:01<00:00, 21.34it/s, Loss=0.42578125][A[A


Epoch 6/50:  74%|███████▎  | 25/34 [00:01<00:00, 21.34it/s, Loss=0.43554688][A[A


Epoch 6/50:  74%|███████▎  | 25/34 [00:01<00:00, 21.34it/s, Loss=0.42382812][A[A


Epoch 6/50:  74%|███████▎  | 25/34 [00:01<00:00, 21.34it/s, Loss=0.42968750][A[A


Epoch 6/50:  74%|███████▎  | 25/34 [00:01<00:00, 21.34it/s, Loss=0.42578125][A[A


Epoch 6/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.86it/s, Loss=0.42578125][A[A


Epoch 6/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.86it/s, Loss=0.42187500][A[A


Epoch 6/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.86it/s, Loss=0.43750000][A[A


Epoch 6/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.86it/s, Loss=0.41015625][A[A


Epoch 6/50:  85%|████████▌ | 29/34 [00:01<00:00, 24.86it/s, Loss=0.42187500][A[A


Epoch 6/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.81it/s, Loss=0.42187500][A[A


Epoch 6/50:  97%|█████████▋| 33/34 [00:01<00:00, 27.81it/s, Loss=0.39453125][A[A


                                                                            [A[A

Epochs:  10%|█         | 5/50 [00:08<01:01,  1.37s/it, Mean Loss=0.42836627, LR=5.01e-03][A

Epochs:  12%|█▏        | 6/50 [00:08<01:00,  1.37s/it, Mean Loss=0.42836627, LR=5.01e-03][A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.42187500][A[A


Epoch 7/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.41210938][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.41210938][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.41015625][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.41406250][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.40625000][A[A


Epoch 7/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.41015625][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.69it/s, Loss=0.41015625][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.69it/s, Loss=0.41015625][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.69it/s, Loss=0.39843750][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.69it/s, Loss=0.41601562][A[A


Epoch 7/50:  18%|█▊        | 6/34 [00:00<00:01, 20.69it/s, Loss=0.41406250][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.41406250][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.41406250][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.41210938][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.41601562][A[A


Epoch 7/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.41796875][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=0.41796875][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=0.43359375][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=0.41015625][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=0.41406250][A[A


Epoch 7/50:  41%|████      | 14/34 [00:00<00:01, 19.89it/s, Loss=0.41796875][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.94it/s, Loss=0.41796875][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.94it/s, Loss=0.41015625][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.94it/s, Loss=0.40625000][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.94it/s, Loss=0.41406250][A[A


Epoch 7/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.94it/s, Loss=0.41015625][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.23it/s, Loss=0.41015625][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.23it/s, Loss=0.41015625][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.23it/s, Loss=0.40625000][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.23it/s, Loss=0.41015625][A[A


Epoch 7/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.23it/s, Loss=0.41015625][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.41015625][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.40625000][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.40039062][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.40234375][A[A


Epoch 7/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.41796875][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.41796875][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.41210938][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.40039062][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.40625000][A[A


Epoch 7/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.86it/s, Loss=0.43750000][A[A


                                                                            [A[A

Epochs:  12%|█▏        | 6/50 [00:09<01:00,  1.37s/it, Mean Loss=0.41205193, LR=4.37e-03][A

Epochs:  14%|█▍        | 7/50 [00:09<00:59,  1.38s/it, Mean Loss=0.41205193, LR=4.37e-03][A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.40234375][A[A


Epoch 8/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.39843750][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.39843750][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.39843750][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.39453125][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.40234375][A[A


Epoch 8/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.39453125][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.93it/s, Loss=0.39453125][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.93it/s, Loss=0.40625000][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.93it/s, Loss=0.39453125][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.93it/s, Loss=0.40234375][A[A


Epoch 8/50:  18%|█▊        | 6/34 [00:00<00:01, 20.93it/s, Loss=0.40234375][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.40234375][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.39453125][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.39453125][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.38671875][A[A


Epoch 8/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.39062500][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.39062500][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.40625000][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.38671875][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.40625000][A[A


Epoch 8/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.39843750][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=0.39843750][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=0.39843750][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=0.39648438][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=0.39843750][A[A


Epoch 8/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.22it/s, Loss=0.38671875][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=0.38671875][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=0.39453125][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.39062500][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.39843750][A[A


Epoch 8/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.39843750][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.39843750][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.40234375][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.38476562][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.39843750][A[A


Epoch 8/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.38281250][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=0.38281250][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=0.39843750][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=0.39062500][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=0.39062500][A[A


Epoch 8/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.78it/s, Loss=0.33203125][A[A


                                                                            [A[A

Epochs:  14%|█▍        | 7/50 [00:10<00:59,  1.38s/it, Mean Loss=0.39418658, LR=3.80e-03][A

Epochs:  16%|█▌        | 8/50 [00:10<00:57,  1.38s/it, Mean Loss=0.39418658, LR=3.80e-03][A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.38281250][A[A


Epoch 9/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.37109375][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37109375][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37500000][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37890625][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.38085938][A[A


Epoch 9/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37890625][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.16it/s, Loss=0.37890625][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.16it/s, Loss=0.39453125][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.16it/s, Loss=0.37890625][A[A


Epoch 9/50:  18%|█▊        | 6/34 [00:00<00:01, 20.16it/s, Loss=0.38671875][A[A


Epoch 9/50:  26%|██▋       | 9/34 [00:00<00:01, 18.52it/s, Loss=0.38671875][A[A


Epoch 9/50:  26%|██▋       | 9/34 [00:00<00:01, 18.52it/s, Loss=0.38867188][A[A


Epoch 9/50:  26%|██▋       | 9/34 [00:00<00:01, 18.52it/s, Loss=0.38085938][A[A


Epoch 9/50:  26%|██▋       | 9/34 [00:00<00:01, 18.52it/s, Loss=0.37500000][A[A


Epoch 9/50:  26%|██▋       | 9/34 [00:00<00:01, 18.52it/s, Loss=0.37109375][A[A


Epoch 9/50:  38%|███▊      | 13/34 [00:00<00:01, 16.67it/s, Loss=0.37109375][A[A


Epoch 9/50:  38%|███▊      | 13/34 [00:00<00:01, 16.67it/s, Loss=0.38281250][A[A


Epoch 9/50:  38%|███▊      | 13/34 [00:00<00:01, 16.67it/s, Loss=0.37695312][A[A


Epoch 9/50:  38%|███▊      | 13/34 [00:00<00:01, 16.67it/s, Loss=0.37500000][A[A


Epoch 9/50:  38%|███▊      | 13/34 [00:00<00:01, 16.67it/s, Loss=0.39062500][A[A


Epoch 9/50:  50%|█████     | 17/34 [00:00<00:00, 21.17it/s, Loss=0.39062500][A[A


Epoch 9/50:  50%|█████     | 17/34 [00:00<00:00, 21.17it/s, Loss=0.37500000][A[A


Epoch 9/50:  50%|█████     | 17/34 [00:00<00:00, 21.17it/s, Loss=0.37500000][A[A


Epoch 9/50:  50%|█████     | 17/34 [00:00<00:00, 21.17it/s, Loss=0.37109375][A[A


Epoch 9/50:  50%|█████     | 17/34 [00:01<00:00, 21.17it/s, Loss=0.38671875][A[A


Epoch 9/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.83it/s, Loss=0.38671875][A[A


Epoch 9/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.83it/s, Loss=0.37890625][A[A


Epoch 9/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.83it/s, Loss=0.37500000][A[A


Epoch 9/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.83it/s, Loss=0.37109375][A[A


Epoch 9/50:  71%|███████   | 24/34 [00:01<00:00, 19.51it/s, Loss=0.37109375][A[A


Epoch 9/50:  71%|███████   | 24/34 [00:01<00:00, 19.51it/s, Loss=0.38671875][A[A


Epoch 9/50:  71%|███████   | 24/34 [00:01<00:00, 19.51it/s, Loss=0.37695312][A[A


Epoch 9/50:  71%|███████   | 24/34 [00:01<00:00, 19.51it/s, Loss=0.38671875][A[A


Epoch 9/50:  71%|███████   | 24/34 [00:01<00:00, 19.51it/s, Loss=0.38671875][A[A


Epoch 9/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.38671875][A[A


Epoch 9/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.36523438][A[A


Epoch 9/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.36718750][A[A


Epoch 9/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.37695312][A[A


Epoch 9/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.37500000][A[A


Epoch 9/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.62it/s, Loss=0.37500000][A[A


Epoch 9/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.62it/s, Loss=0.37500000][A[A


Epoch 9/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.62it/s, Loss=0.34570312][A[A


                                                                            [A[A

Epochs:  16%|█▌        | 8/50 [00:12<00:57,  1.38s/it, Mean Loss=0.37775735, LR=3.31e-03][A

Epochs:  18%|█▊        | 9/50 [00:12<00:57,  1.41s/it, Mean Loss=0.37775735, LR=3.31e-03][A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.37500000][A[A


Epoch 10/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.37109375][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37109375][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.36718750][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.37890625][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.36914062][A[A


Epoch 10/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.36328125][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.36328125][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.36523438][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.37695312][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.37695312][A[A


Epoch 10/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.37304688][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=0.37304688][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=0.36718750][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=0.35546875][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=0.36718750][A[A


Epoch 10/50:  29%|██▉       | 10/34 [00:00<00:00, 26.80it/s, Loss=0.35937500][A[A


Epoch 10/50:  41%|████      | 14/34 [00:00<00:00, 20.06it/s, Loss=0.35937500][A[A


Epoch 10/50:  41%|████      | 14/34 [00:00<00:00, 20.06it/s, Loss=0.36718750][A[A


Epoch 10/50:  41%|████      | 14/34 [00:00<00:00, 20.06it/s, Loss=0.36328125][A[A


Epoch 10/50:  41%|████      | 14/34 [00:00<00:00, 20.06it/s, Loss=0.37500000][A[A


Epoch 10/50:  41%|████      | 14/34 [00:00<00:00, 20.06it/s, Loss=0.37695312][A[A


Epoch 10/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.37695312][A[A


Epoch 10/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.36914062][A[A


Epoch 10/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.36328125][A[A


Epoch 10/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.36914062][A[A


Epoch 10/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.36328125][A[A


Epoch 10/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.36328125][A[A


Epoch 10/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.36328125][A[A


Epoch 10/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.36718750][A[A


Epoch 10/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.35742188][A[A


Epoch 10/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.36718750][A[A


Epoch 10/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.37it/s, Loss=0.36718750][A[A


Epoch 10/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.37it/s, Loss=0.36328125][A[A


Epoch 10/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.37it/s, Loss=0.36132812][A[A


Epoch 10/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.37it/s, Loss=0.35742188][A[A


Epoch 10/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.37it/s, Loss=0.36328125][A[A


Epoch 10/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.60it/s, Loss=0.36328125][A[A


Epoch 10/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.60it/s, Loss=0.36914062][A[A


Epoch 10/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.60it/s, Loss=0.37304688][A[A


Epoch 10/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.60it/s, Loss=0.36328125][A[A


Epoch 10/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.60it/s, Loss=0.38281250][A[A


                                                                             [A[A

Epochs:  18%|█▊        | 9/50 [00:13<00:57,  1.41s/it, Mean Loss=0.36770450, LR=2.88e-03][A

Epochs:  20%|██        | 10/50 [00:13<00:56,  1.41s/it, Mean Loss=0.36770450, LR=2.88e-03][A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.36328125][A[A


Epoch 11/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.35937500][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.35937500][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.35546875][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.36718750][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.36132812][A[A


Epoch 11/50:   6%|▌         | 2/34 [00:00<00:03,  9.42it/s, Loss=0.36132812][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 21.07it/s, Loss=0.36132812][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 21.07it/s, Loss=0.36132812][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 21.07it/s, Loss=0.36132812][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 21.07it/s, Loss=0.35156250][A[A


Epoch 11/50:  18%|█▊        | 6/34 [00:00<00:01, 21.07it/s, Loss=0.35742188][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 27.06it/s, Loss=0.35742188][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 27.06it/s, Loss=0.37109375][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 27.06it/s, Loss=0.35546875][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 27.06it/s, Loss=0.35937500][A[A


Epoch 11/50:  29%|██▉       | 10/34 [00:00<00:00, 27.06it/s, Loss=0.36328125][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=0.36328125][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=0.35156250][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=0.36718750][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=0.35937500][A[A


Epoch 11/50:  41%|████      | 14/34 [00:00<00:00, 20.26it/s, Loss=0.35351562][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.38it/s, Loss=0.35351562][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.38it/s, Loss=0.35351562][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.38it/s, Loss=0.35351562][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.38it/s, Loss=0.35742188][A[A


Epoch 11/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.38it/s, Loss=0.35546875][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.35546875][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.36132812][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.35937500][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.36132812][A[A


Epoch 11/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.36132812][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.36132812][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.36328125][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.35937500][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.34960938][A[A


Epoch 11/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.47it/s, Loss=0.35546875][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.35546875][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.35156250][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.35546875][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.35351562][A[A


Epoch 11/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.42382812][A[A


                                                                             [A[A

Epochs:  20%|██        | 10/50 [00:15<00:56,  1.41s/it, Mean Loss=0.36046645, LR=2.51e-03][A

Epochs:  22%|██▏       | 11/50 [00:15<00:54,  1.40s/it, Mean Loss=0.36046645, LR=2.51e-03][A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.34765625][A[A


Epoch 12/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.35351562][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=0.35351562][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=0.35156250][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=0.35937500][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=0.36328125][A[A


Epoch 12/50:   6%|▌         | 2/34 [00:00<00:03,  9.48it/s, Loss=0.35156250][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.20it/s, Loss=0.35156250][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.20it/s, Loss=0.35546875][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.20it/s, Loss=0.36328125][A[A


Epoch 12/50:  18%|█▊        | 6/34 [00:00<00:01, 21.20it/s, Loss=0.35546875][A[A


Epoch 12/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.35546875][A[A


Epoch 12/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.35742188][A[A


Epoch 12/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.34765625][A[A


Epoch 12/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.35156250][A[A


Epoch 12/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.35156250][A[A


Epoch 12/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.34960938][A[A


Epoch 12/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.35156250][A[A


Epoch 12/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.37500000][A[A


Epoch 12/50:  44%|████▍     | 15/34 [00:00<00:01, 17.58it/s, Loss=0.37500000][A[A


Epoch 12/50:  44%|████▍     | 15/34 [00:00<00:01, 17.58it/s, Loss=0.34765625][A[A


Epoch 12/50:  44%|████▍     | 15/34 [00:00<00:01, 17.58it/s, Loss=0.34960938][A[A


Epoch 12/50:  44%|████▍     | 15/34 [00:00<00:01, 17.58it/s, Loss=0.35937500][A[A


Epoch 12/50:  44%|████▍     | 15/34 [00:00<00:01, 17.58it/s, Loss=0.34765625][A[A


Epoch 12/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.29it/s, Loss=0.34765625][A[A


Epoch 12/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.29it/s, Loss=0.34375000][A[A


Epoch 12/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.29it/s, Loss=0.36328125][A[A


Epoch 12/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.29it/s, Loss=0.35156250][A[A


Epoch 12/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.29it/s, Loss=0.35351562][A[A


Epoch 12/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.94it/s, Loss=0.35351562][A[A


Epoch 12/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.94it/s, Loss=0.34765625][A[A


Epoch 12/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.94it/s, Loss=0.33789062][A[A


Epoch 12/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.94it/s, Loss=0.35156250][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.83it/s, Loss=0.35156250][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.83it/s, Loss=0.36328125][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.83it/s, Loss=0.35937500][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.83it/s, Loss=0.35351562][A[A


Epoch 12/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.83it/s, Loss=0.34375000][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.64it/s, Loss=0.34375000][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.64it/s, Loss=0.35546875][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.64it/s, Loss=0.35156250][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.64it/s, Loss=0.35546875][A[A


Epoch 12/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.64it/s, Loss=0.32226562][A[A


                                                                             [A[A

Epochs:  22%|██▏       | 11/50 [00:16<00:54,  1.40s/it, Mean Loss=0.35271140, LR=2.19e-03][A

Epochs:  24%|██▍       | 12/50 [00:16<00:54,  1.42s/it, Mean Loss=0.35271140, LR=2.19e-03][A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.36328125][A[A


Epoch 13/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.34375000][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.34375000][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.33398438][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.34375000][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.35156250][A[A


Epoch 13/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.33203125][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=0.33203125][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=0.35546875][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=0.34375000][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=0.35156250][A[A


Epoch 13/50:  18%|█▊        | 6/34 [00:00<00:01, 21.19it/s, Loss=0.34960938][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.23it/s, Loss=0.34960938][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.23it/s, Loss=0.35156250][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.23it/s, Loss=0.34375000][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.23it/s, Loss=0.34375000][A[A


Epoch 13/50:  29%|██▉       | 10/34 [00:00<00:00, 27.23it/s, Loss=0.35351562][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.35351562][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.35351562][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.34765625][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.35156250][A[A


Epoch 13/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.33984375][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.33984375][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.34765625][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.33203125][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.35156250][A[A


Epoch 13/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.33984375][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=0.33984375][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.50it/s, Loss=0.35156250][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=0.34375000][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=0.35937500][A[A


Epoch 13/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.50it/s, Loss=0.34375000][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.34375000][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.35156250][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.34375000][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.35156250][A[A


Epoch 13/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.34375000][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.51it/s, Loss=0.34375000][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.51it/s, Loss=0.34765625][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.51it/s, Loss=0.33593750][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.51it/s, Loss=0.34765625][A[A


Epoch 13/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.51it/s, Loss=0.39453125][A[A


                                                                             [A[A

Epochs:  24%|██▍       | 12/50 [00:18<00:54,  1.42s/it, Mean Loss=0.34823070, LR=1.91e-03][A

Epochs:  26%|██▌       | 13/50 [00:18<00:52,  1.41s/it, Mean Loss=0.34823070, LR=1.91e-03][A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.35156250][A[A


Epoch 14/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33984375][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.33984375][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.34179688][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.34765625][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.34375000][A[A


Epoch 14/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.34765625][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 20.96it/s, Loss=0.34765625][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 20.96it/s, Loss=0.34765625][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 20.96it/s, Loss=0.33203125][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 20.96it/s, Loss=0.33984375][A[A


Epoch 14/50:  18%|█▊        | 6/34 [00:00<00:01, 20.96it/s, Loss=0.34570312][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.34570312][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.34375000][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.34375000][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.34375000][A[A


Epoch 14/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.34375000][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.34375000][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.33593750][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.34375000][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.35156250][A[A


Epoch 14/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.33593750][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.33593750][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.34375000][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.33203125][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.33789062][A[A


Epoch 14/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.34179688][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.66it/s, Loss=0.34179688][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.66it/s, Loss=0.33593750][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.32421875][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.33398438][A[A


Epoch 14/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.34570312][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.45it/s, Loss=0.34570312][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.45it/s, Loss=0.33984375][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.45it/s, Loss=0.32812500][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.45it/s, Loss=0.33984375][A[A


Epoch 14/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.45it/s, Loss=0.33984375][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=0.33984375][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=0.34765625][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=0.33398438][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=0.34765625][A[A


Epoch 14/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.71it/s, Loss=0.39843750][A[A


                                                                             [A[A

Epochs:  26%|██▌       | 13/50 [00:19<00:52,  1.41s/it, Mean Loss=0.34265855, LR=1.66e-03][A

Epochs:  28%|██▊       | 14/50 [00:19<00:50,  1.40s/it, Mean Loss=0.34265855, LR=1.66e-03][A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33789062][A[A


Epoch 15/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.34375000][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.34375000][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.33203125][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.35156250][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.33984375][A[A


Epoch 15/50:   6%|▌         | 2/34 [00:00<00:03,  9.33it/s, Loss=0.33593750][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=0.33593750][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=0.33789062][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=0.34375000][A[A


Epoch 15/50:  18%|█▊        | 6/34 [00:00<00:01, 20.87it/s, Loss=0.33203125][A[A


Epoch 15/50:  26%|██▋       | 9/34 [00:00<00:01, 19.54it/s, Loss=0.33203125][A[A


Epoch 15/50:  26%|██▋       | 9/34 [00:00<00:01, 19.54it/s, Loss=0.33593750][A[A


Epoch 15/50:  26%|██▋       | 9/34 [00:00<00:01, 19.54it/s, Loss=0.33984375][A[A


Epoch 15/50:  26%|██▋       | 9/34 [00:00<00:01, 19.54it/s, Loss=0.33984375][A[A


Epoch 15/50:  26%|██▋       | 9/34 [00:00<00:01, 19.54it/s, Loss=0.34179688][A[A


Epoch 15/50:  38%|███▊      | 13/34 [00:00<00:01, 16.63it/s, Loss=0.34179688][A[A


Epoch 15/50:  38%|███▊      | 13/34 [00:00<00:01, 16.63it/s, Loss=0.33203125][A[A


Epoch 15/50:  38%|███▊      | 13/34 [00:00<00:01, 16.63it/s, Loss=0.34179688][A[A


Epoch 15/50:  38%|███▊      | 13/34 [00:00<00:01, 16.63it/s, Loss=0.33593750][A[A


Epoch 15/50:  38%|███▊      | 13/34 [00:00<00:01, 16.63it/s, Loss=0.33984375][A[A


Epoch 15/50:  50%|█████     | 17/34 [00:00<00:00, 21.13it/s, Loss=0.33984375][A[A


Epoch 15/50:  50%|█████     | 17/34 [00:00<00:00, 21.13it/s, Loss=0.33007812][A[A


Epoch 15/50:  50%|█████     | 17/34 [00:00<00:00, 21.13it/s, Loss=0.33593750][A[A


Epoch 15/50:  50%|█████     | 17/34 [00:00<00:00, 21.13it/s, Loss=0.33203125][A[A


Epoch 15/50:  50%|█████     | 17/34 [00:00<00:00, 21.13it/s, Loss=0.33789062][A[A


Epoch 15/50:  62%|██████▏   | 21/34 [00:00<00:00, 24.99it/s, Loss=0.33789062][A[A


Epoch 15/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.99it/s, Loss=0.33593750][A[A


Epoch 15/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.99it/s, Loss=0.34179688][A[A


Epoch 15/50:  62%|██████▏   | 21/34 [00:01<00:00, 24.99it/s, Loss=0.33007812][A[A


Epoch 15/50:  71%|███████   | 24/34 [00:01<00:00, 19.56it/s, Loss=0.33007812][A[A


Epoch 15/50:  71%|███████   | 24/34 [00:01<00:00, 19.56it/s, Loss=0.33789062][A[A


Epoch 15/50:  71%|███████   | 24/34 [00:01<00:00, 19.56it/s, Loss=0.32812500][A[A


Epoch 15/50:  71%|███████   | 24/34 [00:01<00:00, 19.56it/s, Loss=0.33203125][A[A


Epoch 15/50:  71%|███████   | 24/34 [00:01<00:00, 19.56it/s, Loss=0.33593750][A[A


Epoch 15/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.28it/s, Loss=0.33593750][A[A


Epoch 15/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.28it/s, Loss=0.33007812][A[A


Epoch 15/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.28it/s, Loss=0.33398438][A[A


Epoch 15/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.28it/s, Loss=0.32226562][A[A


Epoch 15/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.28it/s, Loss=0.33203125][A[A


Epoch 15/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.46it/s, Loss=0.33203125][A[A


Epoch 15/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.46it/s, Loss=0.32812500][A[A


Epoch 15/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.46it/s, Loss=0.31835938][A[A


                                                                             [A[A

Epochs:  28%|██▊       | 14/50 [00:20<00:50,  1.40s/it, Mean Loss=0.33542050, LR=1.45e-03][A

Epochs:  30%|███       | 15/50 [00:20<00:49,  1.43s/it, Mean Loss=0.33542050, LR=1.45e-03][A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33593750][A[A


Epoch 16/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33007812][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.33007812][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.33203125][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.33398438][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.32812500][A[A


Epoch 16/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.33398438][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.33398438][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.33398438][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.33398438][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.33984375][A[A


Epoch 16/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.34960938][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.34960938][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.33984375][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.32812500][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.33984375][A[A


Epoch 16/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.33203125][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=0.33203125][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=0.33203125][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=0.32812500][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=0.33789062][A[A


Epoch 16/50:  41%|████      | 14/34 [00:00<00:00, 20.31it/s, Loss=0.33203125][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.48it/s, Loss=0.33203125][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.48it/s, Loss=0.33203125][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.48it/s, Loss=0.31640625][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.48it/s, Loss=0.33007812][A[A


Epoch 16/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.48it/s, Loss=0.33398438][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.66it/s, Loss=0.33398438][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.66it/s, Loss=0.33593750][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.33593750][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.32617188][A[A


Epoch 16/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.66it/s, Loss=0.33398438][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=0.33398438][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=0.32421875][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=0.33203125][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=0.33398438][A[A


Epoch 16/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.60it/s, Loss=0.31250000][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.31250000][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.33203125][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.33593750][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.32812500][A[A


Epoch 16/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.33984375][A[A


                                                                             [A[A

Epochs:  30%|███       | 15/50 [00:22<00:49,  1.43s/it, Mean Loss=0.33249081, LR=1.26e-03][A

Epochs:  32%|███▏      | 16/50 [00:22<00:48,  1.41s/it, Mean Loss=0.33249081, LR=1.26e-03][A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33203125][A[A


Epoch 17/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31640625][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.31640625][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.31640625][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.33203125][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.32421875][A[A


Epoch 17/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.33593750][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.33593750][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.32421875][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.33007812][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.32421875][A[A


Epoch 17/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.32421875][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.32421875][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.32812500][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.32812500][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.33593750][A[A


Epoch 17/50:  29%|██▉       | 10/34 [00:00<00:00, 26.77it/s, Loss=0.33593750][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=0.33593750][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=0.33984375][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=0.32421875][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=0.34179688][A[A


Epoch 17/50:  41%|████      | 14/34 [00:00<00:00, 20.29it/s, Loss=0.33593750][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=0.33593750][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=0.33398438][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=0.33398438][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=0.32226562][A[A


Epoch 17/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.39it/s, Loss=0.32421875][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=0.32421875][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=0.33203125][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.32031250][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.33203125][A[A


Epoch 17/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.32812500][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=0.32812500][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=0.31835938][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=0.32617188][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=0.31445312][A[A


Epoch 17/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.62it/s, Loss=0.33007812][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=0.33007812][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=0.32617188][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=0.32421875][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=0.33007812][A[A


Epoch 17/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.92it/s, Loss=0.29492188][A[A


                                                                             [A[A

Epochs:  32%|███▏      | 16/50 [00:23<00:48,  1.41s/it, Mean Loss=0.32709099, LR=1.10e-03][A

Epochs:  34%|███▍      | 17/50 [00:23<00:46,  1.40s/it, Mean Loss=0.32709099, LR=1.10e-03][A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.32031250][A[A


Epoch 18/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.33203125][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.12it/s, Loss=0.33203125][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.12it/s, Loss=0.32421875][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.12it/s, Loss=0.32812500][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.12it/s, Loss=0.32421875][A[A


Epoch 18/50:   6%|▌         | 2/34 [00:00<00:03,  9.12it/s, Loss=0.31835938][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.31835938][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.32031250][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.32421875][A[A


Epoch 18/50:  18%|█▊        | 6/34 [00:00<00:01, 20.60it/s, Loss=0.32421875][A[A


Epoch 18/50:  26%|██▋       | 9/34 [00:00<00:01, 19.63it/s, Loss=0.32421875][A[A


Epoch 18/50:  26%|██▋       | 9/34 [00:00<00:01, 19.63it/s, Loss=0.32812500][A[A


Epoch 18/50:  26%|██▋       | 9/34 [00:00<00:01, 19.63it/s, Loss=0.32226562][A[A


Epoch 18/50:  26%|██▋       | 9/34 [00:00<00:01, 19.63it/s, Loss=0.31640625][A[A


Epoch 18/50:  35%|███▌      | 12/34 [00:00<00:00, 22.10it/s, Loss=0.31640625][A[A


Epoch 18/50:  35%|███▌      | 12/34 [00:00<00:00, 22.10it/s, Loss=0.32226562][A[A


Epoch 18/50:  35%|███▌      | 12/34 [00:00<00:00, 22.10it/s, Loss=0.32421875][A[A


Epoch 18/50:  35%|███▌      | 12/34 [00:00<00:00, 22.10it/s, Loss=0.32226562][A[A


Epoch 18/50:  44%|████▍     | 15/34 [00:00<00:01, 17.53it/s, Loss=0.32226562][A[A


Epoch 18/50:  44%|████▍     | 15/34 [00:00<00:01, 17.53it/s, Loss=0.32031250][A[A


Epoch 18/50:  44%|████▍     | 15/34 [00:00<00:01, 17.53it/s, Loss=0.32226562][A[A


Epoch 18/50:  44%|████▍     | 15/34 [00:00<00:01, 17.53it/s, Loss=0.32421875][A[A


Epoch 18/50:  44%|████▍     | 15/34 [00:00<00:01, 17.53it/s, Loss=0.32421875][A[A


Epoch 18/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.27it/s, Loss=0.32421875][A[A


Epoch 18/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.27it/s, Loss=0.32617188][A[A


Epoch 18/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.27it/s, Loss=0.33203125][A[A


Epoch 18/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.27it/s, Loss=0.32031250][A[A


Epoch 18/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.27it/s, Loss=0.31835938][A[A


Epoch 18/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.99it/s, Loss=0.31835938][A[A


Epoch 18/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.99it/s, Loss=0.32617188][A[A


Epoch 18/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.99it/s, Loss=0.32617188][A[A


Epoch 18/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.99it/s, Loss=0.32812500][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.99it/s, Loss=0.32812500][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.99it/s, Loss=0.32031250][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.99it/s, Loss=0.32617188][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.99it/s, Loss=0.32421875][A[A


Epoch 18/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.99it/s, Loss=0.32812500][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.59it/s, Loss=0.32812500][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.59it/s, Loss=0.31054688][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.59it/s, Loss=0.33007812][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.59it/s, Loss=0.33203125][A[A


Epoch 18/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.59it/s, Loss=0.24804688][A[A


                                                                             [A[A

Epochs:  34%|███▍      | 17/50 [00:25<00:46,  1.40s/it, Mean Loss=0.32174862, LR=9.55e-04][A

Epochs:  36%|███▌      | 18/50 [00:25<00:45,  1.43s/it, Mean Loss=0.32174862, LR=9.55e-04][A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.32226562][A[A


Epoch 19/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.32421875][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.32421875][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31640625][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.32031250][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31640625][A[A


Epoch 19/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31640625][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.46it/s, Loss=0.31640625][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.46it/s, Loss=0.32031250][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.46it/s, Loss=0.32031250][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.46it/s, Loss=0.32617188][A[A


Epoch 19/50:  18%|█▊        | 6/34 [00:00<00:01, 20.46it/s, Loss=0.31835938][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31835938][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.32226562][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.32031250][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.32031250][A[A


Epoch 19/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31640625][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.05it/s, Loss=0.31640625][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.05it/s, Loss=0.32031250][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.05it/s, Loss=0.32031250][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.05it/s, Loss=0.31445312][A[A


Epoch 19/50:  41%|████      | 14/34 [00:00<00:00, 20.05it/s, Loss=0.32031250][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=0.32031250][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=0.32421875][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=0.31640625][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=0.32031250][A[A


Epoch 19/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.28it/s, Loss=0.32031250][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.32031250][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.33203125][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.31835938][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.31054688][A[A


Epoch 19/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.32421875][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.32421875][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31835938][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.32031250][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.33007812][A[A


Epoch 19/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31250000][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.31250000][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.32617188][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.32421875][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.32421875][A[A


Epoch 19/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.38476562][A[A


                                                                             [A[A

Epochs:  36%|███▌      | 18/50 [00:26<00:45,  1.43s/it, Mean Loss=0.32243796, LR=8.32e-04][A

Epochs:  38%|███▊      | 19/50 [00:26<00:43,  1.41s/it, Mean Loss=0.32243796, LR=8.32e-04][A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.32812500][A[A


Epoch 20/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31835938][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.16it/s, Loss=0.31835938][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.16it/s, Loss=0.31835938][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.16it/s, Loss=0.32031250][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.16it/s, Loss=0.32031250][A[A


Epoch 20/50:   6%|▌         | 2/34 [00:00<00:03,  9.16it/s, Loss=0.31445312][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=0.31445312][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=0.31445312][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=0.32031250][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=0.31445312][A[A


Epoch 20/50:  18%|█▊        | 6/34 [00:00<00:01, 20.58it/s, Loss=0.31835938][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31835938][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31054688][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31640625][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31250000][A[A


Epoch 20/50:  29%|██▉       | 10/34 [00:00<00:00, 26.51it/s, Loss=0.31640625][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.31640625][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.31250000][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.31250000][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.32031250][A[A


Epoch 20/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.31445312][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31445312][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31640625][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31445312][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31835938][A[A


Epoch 20/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.32421875][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=0.32421875][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.65it/s, Loss=0.31250000][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.31445312][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.31835938][A[A


Epoch 20/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.65it/s, Loss=0.32031250][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.70it/s, Loss=0.32031250][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.70it/s, Loss=0.31640625][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.70it/s, Loss=0.32812500][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.70it/s, Loss=0.31054688][A[A


Epoch 20/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.70it/s, Loss=0.32421875][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.32421875][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.31640625][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.33593750][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.31835938][A[A


Epoch 20/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.91it/s, Loss=0.32421875][A[A


                                                                             [A[A

Epochs:  38%|███▊      | 19/50 [00:27<00:43,  1.41s/it, Mean Loss=0.31812960, LR=7.24e-04][A

Epochs:  40%|████      | 20/50 [00:27<00:42,  1.40s/it, Mean Loss=0.31812960, LR=7.24e-04][A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31054688][A[A


Epoch 21/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.32421875][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.32421875][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.31250000][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.31835938][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.31640625][A[A


Epoch 21/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.32421875][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.32421875][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.31445312][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.30859375][A[A


Epoch 21/50:  18%|█▊        | 6/34 [00:00<00:01, 20.77it/s, Loss=0.31445312][A[A


Epoch 21/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.31445312][A[A


Epoch 21/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.31835938][A[A


Epoch 21/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.31445312][A[A


Epoch 21/50:  26%|██▋       | 9/34 [00:00<00:01, 19.52it/s, Loss=0.31640625][A[A


Epoch 21/50:  35%|███▌      | 12/34 [00:00<00:00, 22.19it/s, Loss=0.31640625][A[A


Epoch 21/50:  35%|███▌      | 12/34 [00:00<00:00, 22.19it/s, Loss=0.32226562][A[A


Epoch 21/50:  35%|███▌      | 12/34 [00:00<00:00, 22.19it/s, Loss=0.30078125][A[A


Epoch 21/50:  35%|███▌      | 12/34 [00:00<00:00, 22.19it/s, Loss=0.32031250][A[A


Epoch 21/50:  44%|████▍     | 15/34 [00:00<00:01, 17.44it/s, Loss=0.32031250][A[A


Epoch 21/50:  44%|████▍     | 15/34 [00:00<00:01, 17.44it/s, Loss=0.31250000][A[A


Epoch 21/50:  44%|████▍     | 15/34 [00:00<00:01, 17.44it/s, Loss=0.32226562][A[A


Epoch 21/50:  44%|████▍     | 15/34 [00:00<00:01, 17.44it/s, Loss=0.31250000][A[A


Epoch 21/50:  44%|████▍     | 15/34 [00:00<00:01, 17.44it/s, Loss=0.31250000][A[A


Epoch 21/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.21it/s, Loss=0.31250000][A[A


Epoch 21/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.21it/s, Loss=0.31054688][A[A


Epoch 21/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.21it/s, Loss=0.31250000][A[A


Epoch 21/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.21it/s, Loss=0.31640625][A[A


Epoch 21/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.21it/s, Loss=0.32226562][A[A


Epoch 21/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.06it/s, Loss=0.32226562][A[A


Epoch 21/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.06it/s, Loss=0.30859375][A[A


Epoch 21/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.06it/s, Loss=0.31640625][A[A


Epoch 21/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.06it/s, Loss=0.32031250][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.06it/s, Loss=0.32031250][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.06it/s, Loss=0.31835938][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.06it/s, Loss=0.31054688][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.06it/s, Loss=0.31835938][A[A


Epoch 21/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.06it/s, Loss=0.31835938][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.78it/s, Loss=0.31835938][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.78it/s, Loss=0.32226562][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.78it/s, Loss=0.31835938][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.78it/s, Loss=0.31640625][A[A


Epoch 21/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.78it/s, Loss=0.31640625][A[A


                                                                             [A[A

Epochs:  40%|████      | 20/50 [00:29<00:42,  1.40s/it, Mean Loss=0.31594669, LR=6.31e-04][A

Epochs:  42%|████▏     | 21/50 [00:29<00:41,  1.43s/it, Mean Loss=0.31594669, LR=6.31e-04][A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31640625][A[A


Epoch 22/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31054688][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.31054688][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.31640625][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.31640625][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.31054688][A[A


Epoch 22/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.32226562][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 20.71it/s, Loss=0.32226562][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 20.71it/s, Loss=0.31640625][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 20.71it/s, Loss=0.31640625][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 20.71it/s, Loss=0.31054688][A[A


Epoch 22/50:  18%|█▊        | 6/34 [00:00<00:01, 20.71it/s, Loss=0.30859375][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.30859375][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.31250000][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.30468750][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.31054688][A[A


Epoch 22/50:  29%|██▉       | 10/34 [00:00<00:00, 26.58it/s, Loss=0.31445312][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=0.31445312][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=0.31835938][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=0.30468750][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=0.30468750][A[A


Epoch 22/50:  41%|████      | 14/34 [00:00<00:01, 19.96it/s, Loss=0.32031250][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.07it/s, Loss=0.32031250][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.07it/s, Loss=0.31640625][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.07it/s, Loss=0.31250000][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.07it/s, Loss=0.31640625][A[A


Epoch 22/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.07it/s, Loss=0.30664062][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=0.30664062][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.39it/s, Loss=0.30664062][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.31250000][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.30664062][A[A


Epoch 22/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.39it/s, Loss=0.31250000][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31250000][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31640625][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31445312][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31250000][A[A


Epoch 22/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31250000][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.31250000][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.31835938][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.31054688][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.32031250][A[A


Epoch 22/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.93it/s, Loss=0.30468750][A[A


                                                                             [A[A

Epochs:  42%|████▏     | 21/50 [00:30<00:41,  1.43s/it, Mean Loss=0.31278722, LR=5.50e-04][A

Epochs:  44%|████▍     | 22/50 [00:30<00:39,  1.41s/it, Mean Loss=0.31278722, LR=5.50e-04][A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31054688][A[A


Epoch 23/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31250000][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.31250000][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.30859375][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.31250000][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.31250000][A[A


Epoch 23/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.32031250][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.32031250][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.30468750][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.30664062][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.31250000][A[A


Epoch 23/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.30664062][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=0.30664062][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=0.31250000][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=0.31250000][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=0.30468750][A[A


Epoch 23/50:  29%|██▉       | 10/34 [00:00<00:00, 26.56it/s, Loss=0.30859375][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.30859375][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.31054688][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.31250000][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.31445312][A[A


Epoch 23/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.31835938][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.06it/s, Loss=0.31835938][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.06it/s, Loss=0.31054688][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.06it/s, Loss=0.30468750][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.06it/s, Loss=0.31445312][A[A


Epoch 23/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.06it/s, Loss=0.31835938][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=0.31835938][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=0.32031250][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.31640625][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.31250000][A[A


Epoch 23/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.30468750][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=0.30468750][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=0.31250000][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=0.31250000][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=0.31445312][A[A


Epoch 23/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.63it/s, Loss=0.31054688][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.31054688][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.30468750][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.30078125][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.30859375][A[A


Epoch 23/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.97it/s, Loss=0.38867188][A[A


                                                                             [A[A

Epochs:  44%|████▍     | 22/50 [00:32<00:39,  1.41s/it, Mean Loss=0.31341912, LR=4.79e-04][A

Epochs:  46%|████▌     | 23/50 [00:32<00:37,  1.40s/it, Mean Loss=0.31341912, LR=4.79e-04][A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 24/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31054688][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31054688][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.30859375][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31250000][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31250000][A[A


Epoch 24/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31640625][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.75it/s, Loss=0.31640625][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.75it/s, Loss=0.30859375][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.75it/s, Loss=0.31250000][A[A


Epoch 24/50:  18%|█▊        | 6/34 [00:00<00:01, 20.75it/s, Loss=0.31054688][A[A


Epoch 24/50:  26%|██▋       | 9/34 [00:00<00:01, 19.42it/s, Loss=0.31054688][A[A


Epoch 24/50:  26%|██▋       | 9/34 [00:00<00:01, 19.42it/s, Loss=0.32031250][A[A


Epoch 24/50:  26%|██▋       | 9/34 [00:00<00:01, 19.42it/s, Loss=0.31640625][A[A


Epoch 24/50:  26%|██▋       | 9/34 [00:00<00:01, 19.42it/s, Loss=0.30078125][A[A


Epoch 24/50:  35%|███▌      | 12/34 [00:00<00:00, 22.05it/s, Loss=0.30078125][A[A


Epoch 24/50:  35%|███▌      | 12/34 [00:00<00:00, 22.05it/s, Loss=0.31250000][A[A


Epoch 24/50:  35%|███▌      | 12/34 [00:00<00:00, 22.05it/s, Loss=0.31250000][A[A


Epoch 24/50:  35%|███▌      | 12/34 [00:00<00:00, 22.05it/s, Loss=0.30468750][A[A


Epoch 24/50:  44%|████▍     | 15/34 [00:00<00:01, 17.24it/s, Loss=0.30468750][A[A


Epoch 24/50:  44%|████▍     | 15/34 [00:00<00:01, 17.24it/s, Loss=0.30468750][A[A


Epoch 24/50:  44%|████▍     | 15/34 [00:00<00:01, 17.24it/s, Loss=0.31250000][A[A


Epoch 24/50:  44%|████▍     | 15/34 [00:00<00:01, 17.24it/s, Loss=0.30078125][A[A


Epoch 24/50:  44%|████▍     | 15/34 [00:00<00:01, 17.24it/s, Loss=0.31250000][A[A


Epoch 24/50:  56%|█████▌    | 19/34 [00:00<00:00, 21.92it/s, Loss=0.31250000][A[A


Epoch 24/50:  56%|█████▌    | 19/34 [00:00<00:00, 21.92it/s, Loss=0.30664062][A[A


Epoch 24/50:  56%|█████▌    | 19/34 [00:01<00:00, 21.92it/s, Loss=0.31250000][A[A


Epoch 24/50:  56%|█████▌    | 19/34 [00:01<00:00, 21.92it/s, Loss=0.30859375][A[A


Epoch 24/50:  56%|█████▌    | 19/34 [00:01<00:00, 21.92it/s, Loss=0.31250000][A[A


Epoch 24/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.58it/s, Loss=0.31250000][A[A


Epoch 24/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.58it/s, Loss=0.31640625][A[A


Epoch 24/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.58it/s, Loss=0.30273438][A[A


Epoch 24/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.58it/s, Loss=0.30468750][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.92it/s, Loss=0.30468750][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.92it/s, Loss=0.32031250][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.92it/s, Loss=0.31640625][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.92it/s, Loss=0.30468750][A[A


Epoch 24/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.92it/s, Loss=0.30664062][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.81it/s, Loss=0.30664062][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.81it/s, Loss=0.30859375][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.81it/s, Loss=0.30859375][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.81it/s, Loss=0.31445312][A[A


Epoch 24/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.81it/s, Loss=0.24218750][A[A


                                                                             [A[A

Epochs:  46%|████▌     | 23/50 [00:33<00:37,  1.40s/it, Mean Loss=0.30836397, LR=4.17e-04][A

Epochs:  48%|████▊     | 24/50 [00:33<00:37,  1.43s/it, Mean Loss=0.30836397, LR=4.17e-04][A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 25/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30468750][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=0.30468750][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=0.31250000][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=0.31054688][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=0.30859375][A[A


Epoch 25/50:   6%|▌         | 2/34 [00:00<00:03,  9.18it/s, Loss=0.29492188][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.76it/s, Loss=0.29492188][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.76it/s, Loss=0.31054688][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.76it/s, Loss=0.30078125][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.76it/s, Loss=0.31445312][A[A


Epoch 25/50:  18%|█▊        | 6/34 [00:00<00:01, 20.76it/s, Loss=0.31250000][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 26.68it/s, Loss=0.31250000][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 26.68it/s, Loss=0.30859375][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 26.68it/s, Loss=0.30273438][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 26.68it/s, Loss=0.31445312][A[A


Epoch 25/50:  29%|██▉       | 10/34 [00:00<00:00, 26.68it/s, Loss=0.30859375][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:01, 19.97it/s, Loss=0.30859375][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:01, 19.97it/s, Loss=0.29882812][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:01, 19.97it/s, Loss=0.31445312][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:01, 19.97it/s, Loss=0.30664062][A[A


Epoch 25/50:  41%|████      | 14/34 [00:00<00:01, 19.97it/s, Loss=0.30664062][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30664062][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30468750][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.31054688][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.31640625][A[A


Epoch 25/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30468750][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=0.30468750][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.36it/s, Loss=0.30468750][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.29687500][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.30859375][A[A


Epoch 25/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.36it/s, Loss=0.31835938][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31835938][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.30273438][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.30468750][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.31250000][A[A


Epoch 25/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.29882812][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.29882812][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.30859375][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.31640625][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.31640625][A[A


Epoch 25/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.38671875][A[A


                                                                             [A[A

Epochs:  48%|████▊     | 24/50 [00:35<00:37,  1.43s/it, Mean Loss=0.31031710, LR=3.63e-04][A

Epochs:  50%|█████     | 25/50 [00:35<00:35,  1.42s/it, Mean Loss=0.31031710, LR=3.63e-04][A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31250000][A[A


Epoch 26/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.30078125][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.30468750][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.31250000][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.30468750][A[A


Epoch 26/50:   6%|▌         | 2/34 [00:00<00:03,  9.24it/s, Loss=0.30859375][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30859375][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30468750][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30859375][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30468750][A[A


Epoch 26/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.31445312][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.31445312][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.32421875][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30468750][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30273438][A[A


Epoch 26/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30859375][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=0.30859375][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=0.30664062][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=0.29687500][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=0.31054688][A[A


Epoch 26/50:  41%|████      | 14/34 [00:00<00:00, 20.01it/s, Loss=0.30273438][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30273438][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30468750][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30859375][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.31250000][A[A


Epoch 26/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.04it/s, Loss=0.30859375][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.30859375][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.31054688][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.31835938][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.30664062][A[A


Epoch 26/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.30664062][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=0.30664062][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=0.30078125][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=0.30468750][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=0.30078125][A[A


Epoch 26/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.48it/s, Loss=0.30273438][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.88it/s, Loss=0.30273438][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.88it/s, Loss=0.30273438][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.88it/s, Loss=0.31250000][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.88it/s, Loss=0.30273438][A[A


Epoch 26/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.88it/s, Loss=0.33203125][A[A


                                                                             [A[A

Epochs:  50%|█████     | 25/50 [00:36<00:35,  1.42s/it, Mean Loss=0.30790441, LR=3.16e-04][A

Epochs:  52%|█████▏    | 26/50 [00:36<00:33,  1.41s/it, Mean Loss=0.30790441, LR=3.16e-04][A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30664062][A[A


Epoch 27/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31250000][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.43it/s, Loss=0.31250000][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.43it/s, Loss=0.31054688][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.43it/s, Loss=0.30468750][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.43it/s, Loss=0.30468750][A[A


Epoch 27/50:   6%|▌         | 2/34 [00:00<00:03,  9.43it/s, Loss=0.30078125][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.26it/s, Loss=0.30078125][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.26it/s, Loss=0.30664062][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.26it/s, Loss=0.30664062][A[A


Epoch 27/50:  18%|█▊        | 6/34 [00:00<00:01, 20.26it/s, Loss=0.29492188][A[A


Epoch 27/50:  26%|██▋       | 9/34 [00:00<00:01, 19.99it/s, Loss=0.29492188][A[A


Epoch 27/50:  26%|██▋       | 9/34 [00:00<00:01, 19.99it/s, Loss=0.31250000][A[A


Epoch 27/50:  26%|██▋       | 9/34 [00:00<00:01, 19.99it/s, Loss=0.30078125][A[A


Epoch 27/50:  26%|██▋       | 9/34 [00:00<00:01, 19.99it/s, Loss=0.30468750][A[A


Epoch 27/50:  35%|███▌      | 12/34 [00:00<00:00, 22.50it/s, Loss=0.30468750][A[A


Epoch 27/50:  35%|███▌      | 12/34 [00:00<00:00, 22.50it/s, Loss=0.31250000][A[A


Epoch 27/50:  35%|███▌      | 12/34 [00:00<00:00, 22.50it/s, Loss=0.31250000][A[A


Epoch 27/50:  35%|███▌      | 12/34 [00:00<00:00, 22.50it/s, Loss=0.29687500][A[A


Epoch 27/50:  44%|████▍     | 15/34 [00:00<00:01, 17.41it/s, Loss=0.29687500][A[A


Epoch 27/50:  44%|████▍     | 15/34 [00:00<00:01, 17.41it/s, Loss=0.29687500][A[A


Epoch 27/50:  44%|████▍     | 15/34 [00:00<00:01, 17.41it/s, Loss=0.30859375][A[A


Epoch 27/50:  44%|████▍     | 15/34 [00:00<00:01, 17.41it/s, Loss=0.30078125][A[A


Epoch 27/50:  44%|████▍     | 15/34 [00:00<00:01, 17.41it/s, Loss=0.30078125][A[A


Epoch 27/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.30078125][A[A


Epoch 27/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.31250000][A[A


Epoch 27/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.29687500][A[A


Epoch 27/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.07it/s, Loss=0.30859375][A[A


Epoch 27/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.07it/s, Loss=0.30664062][A[A


Epoch 27/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.69it/s, Loss=0.30664062][A[A


Epoch 27/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.69it/s, Loss=0.31640625][A[A


Epoch 27/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.69it/s, Loss=0.30273438][A[A


Epoch 27/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.69it/s, Loss=0.30859375][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.84it/s, Loss=0.30859375][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.84it/s, Loss=0.30859375][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.84it/s, Loss=0.31250000][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.84it/s, Loss=0.30468750][A[A


Epoch 27/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.84it/s, Loss=0.29882812][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.65it/s, Loss=0.29882812][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.65it/s, Loss=0.30468750][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.65it/s, Loss=0.29882812][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.65it/s, Loss=0.31250000][A[A


Epoch 27/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.65it/s, Loss=0.30078125][A[A


                                                                             [A[A

Epochs:  52%|█████▏    | 26/50 [00:37<00:33,  1.41s/it, Mean Loss=0.30554917, LR=2.75e-04][A

Epochs:  54%|█████▍    | 27/50 [00:37<00:32,  1.43s/it, Mean Loss=0.30554917, LR=2.75e-04][A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31250000][A[A


Epoch 28/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31054688][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.31054688][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.29687500][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.31445312][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.29492188][A[A


Epoch 28/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30468750][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30468750][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.31054688][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30664062][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30078125][A[A


Epoch 28/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.31054688][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.84it/s, Loss=0.31054688][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.84it/s, Loss=0.30859375][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.84it/s, Loss=0.30664062][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.84it/s, Loss=0.30468750][A[A


Epoch 28/50:  29%|██▉       | 10/34 [00:00<00:00, 26.84it/s, Loss=0.29687500][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29687500][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29882812][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.30468750][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.30468750][A[A


Epoch 28/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29296875][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.29296875][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.30859375][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.31250000][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.30468750][A[A


Epoch 28/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.13it/s, Loss=0.29687500][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.29687500][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.30it/s, Loss=0.29492188][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.31054688][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.30078125][A[A


Epoch 28/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.30it/s, Loss=0.30664062][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.30664062][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.30468750][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.30078125][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.30273438][A[A


Epoch 28/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.44it/s, Loss=0.30859375][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=0.30859375][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=0.30273438][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=0.30468750][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=0.30273438][A[A


Epoch 28/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.81it/s, Loss=0.25390625][A[A


                                                                             [A[A

Epochs:  54%|█████▍    | 27/50 [00:39<00:32,  1.43s/it, Mean Loss=0.30284926, LR=2.40e-04][A

Epochs:  56%|█████▌    | 28/50 [00:39<00:31,  1.42s/it, Mean Loss=0.30284926, LR=2.40e-04][A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 29/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30859375][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30468750][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.31054688][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30664062][A[A


Epoch 29/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30078125][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 21.00it/s, Loss=0.30078125][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 21.00it/s, Loss=0.30078125][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 21.00it/s, Loss=0.29687500][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 21.00it/s, Loss=0.30468750][A[A


Epoch 29/50:  18%|█▊        | 6/34 [00:00<00:01, 21.00it/s, Loss=0.29882812][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.89it/s, Loss=0.29882812][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.89it/s, Loss=0.30468750][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.89it/s, Loss=0.30468750][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.89it/s, Loss=0.30078125][A[A


Epoch 29/50:  29%|██▉       | 10/34 [00:00<00:00, 26.89it/s, Loss=0.30664062][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.30664062][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.31054688][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.31250000][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.29687500][A[A


Epoch 29/50:  41%|████      | 14/34 [00:00<00:00, 20.14it/s, Loss=0.31250000][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31250000][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.29492188][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.31640625][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.30664062][A[A


Epoch 29/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.29it/s, Loss=0.29101562][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=0.29101562][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=0.30078125][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.30468750][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.29882812][A[A


Epoch 29/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.31250000][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=0.31250000][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=0.29687500][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=0.29882812][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=0.31250000][A[A


Epoch 29/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.41it/s, Loss=0.29101562][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.76it/s, Loss=0.29101562][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.76it/s, Loss=0.30078125][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.76it/s, Loss=0.29687500][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.76it/s, Loss=0.30078125][A[A


Epoch 29/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.76it/s, Loss=0.36132812][A[A


                                                                             [A[A

Epochs:  56%|█████▌    | 28/50 [00:40<00:31,  1.42s/it, Mean Loss=0.30514706, LR=2.09e-04][A

Epochs:  58%|█████▊    | 29/50 [00:40<00:29,  1.40s/it, Mean Loss=0.30514706, LR=2.09e-04][A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30468750][A[A


Epoch 30/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30078125][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30078125][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30273438][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30273438][A[A


Epoch 30/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30468750][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 21.08it/s, Loss=0.30468750][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 21.08it/s, Loss=0.29882812][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 21.08it/s, Loss=0.30078125][A[A


Epoch 30/50:  18%|█▊        | 6/34 [00:00<00:01, 21.08it/s, Loss=0.31250000][A[A


Epoch 30/50:  26%|██▋       | 9/34 [00:00<00:01, 19.12it/s, Loss=0.31250000][A[A


Epoch 30/50:  26%|██▋       | 9/34 [00:00<00:01, 19.12it/s, Loss=0.30078125][A[A


Epoch 30/50:  26%|██▋       | 9/34 [00:00<00:01, 19.12it/s, Loss=0.30078125][A[A


Epoch 30/50:  26%|██▋       | 9/34 [00:00<00:01, 19.12it/s, Loss=0.29687500][A[A


Epoch 30/50:  26%|██▋       | 9/34 [00:00<00:01, 19.12it/s, Loss=0.30859375][A[A


Epoch 30/50:  38%|███▊      | 13/34 [00:00<00:01, 16.68it/s, Loss=0.30859375][A[A


Epoch 30/50:  38%|███▊      | 13/34 [00:00<00:01, 16.68it/s, Loss=0.30468750][A[A


Epoch 30/50:  38%|███▊      | 13/34 [00:00<00:01, 16.68it/s, Loss=0.30078125][A[A


Epoch 30/50:  38%|███▊      | 13/34 [00:00<00:01, 16.68it/s, Loss=0.30468750][A[A


Epoch 30/50:  38%|███▊      | 13/34 [00:00<00:01, 16.68it/s, Loss=0.30468750][A[A


Epoch 30/50:  50%|█████     | 17/34 [00:00<00:00, 21.28it/s, Loss=0.30468750][A[A


Epoch 30/50:  50%|█████     | 17/34 [00:00<00:00, 21.28it/s, Loss=0.30078125][A[A


Epoch 30/50:  50%|█████     | 17/34 [00:00<00:00, 21.28it/s, Loss=0.30078125][A[A


Epoch 30/50:  50%|█████     | 17/34 [00:00<00:00, 21.28it/s, Loss=0.31054688][A[A


Epoch 30/50:  50%|█████     | 17/34 [00:00<00:00, 21.28it/s, Loss=0.30468750][A[A


Epoch 30/50:  62%|██████▏   | 21/34 [00:00<00:00, 25.02it/s, Loss=0.30468750][A[A


Epoch 30/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.02it/s, Loss=0.29687500][A[A


Epoch 30/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.02it/s, Loss=0.30468750][A[A


Epoch 30/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.02it/s, Loss=0.30664062][A[A


Epoch 30/50:  71%|███████   | 24/34 [00:01<00:00, 19.43it/s, Loss=0.30664062][A[A


Epoch 30/50:  71%|███████   | 24/34 [00:01<00:00, 19.43it/s, Loss=0.30078125][A[A


Epoch 30/50:  71%|███████   | 24/34 [00:01<00:00, 19.43it/s, Loss=0.30859375][A[A


Epoch 30/50:  71%|███████   | 24/34 [00:01<00:00, 19.43it/s, Loss=0.30078125][A[A


Epoch 30/50:  71%|███████   | 24/34 [00:01<00:00, 19.43it/s, Loss=0.29882812][A[A


Epoch 30/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.17it/s, Loss=0.29882812][A[A


Epoch 30/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.17it/s, Loss=0.29882812][A[A


Epoch 30/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.17it/s, Loss=0.29687500][A[A


Epoch 30/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.17it/s, Loss=0.31054688][A[A


Epoch 30/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.17it/s, Loss=0.29687500][A[A


Epoch 30/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.41it/s, Loss=0.29687500][A[A


Epoch 30/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.41it/s, Loss=0.30273438][A[A


Epoch 30/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.41it/s, Loss=0.23242188][A[A


                                                                             [A[A

Epochs:  58%|█████▊    | 29/50 [00:42<00:29,  1.40s/it, Mean Loss=0.30066636, LR=1.82e-04][A

Epochs:  60%|██████    | 30/50 [00:42<00:28,  1.43s/it, Mean Loss=0.30066636, LR=1.82e-04][A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31250000][A[A


Epoch 31/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29687500][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.29687500][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30468750][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.29687500][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.30468750][A[A


Epoch 31/50:   6%|▌         | 2/34 [00:00<00:03,  9.41it/s, Loss=0.31054688][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 21.09it/s, Loss=0.31054688][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 21.09it/s, Loss=0.30078125][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 21.09it/s, Loss=0.30273438][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 21.09it/s, Loss=0.30273438][A[A


Epoch 31/50:  18%|█▊        | 6/34 [00:00<00:01, 21.09it/s, Loss=0.30078125][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=0.30078125][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=0.29296875][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=0.30859375][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=0.30468750][A[A


Epoch 31/50:  29%|██▉       | 10/34 [00:00<00:00, 27.03it/s, Loss=0.29687500][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.16it/s, Loss=0.29687500][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.16it/s, Loss=0.30859375][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.16it/s, Loss=0.29687500][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.16it/s, Loss=0.30664062][A[A


Epoch 31/50:  41%|████      | 14/34 [00:00<00:00, 20.16it/s, Loss=0.31054688][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.31054688][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.29296875][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.30468750][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.29687500][A[A


Epoch 31/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.29882812][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.54it/s, Loss=0.29882812][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.54it/s, Loss=0.30468750][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=0.30468750][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=0.29882812][A[A


Epoch 31/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.54it/s, Loss=0.29492188][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.38it/s, Loss=0.29492188][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.38it/s, Loss=0.29296875][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.38it/s, Loss=0.30468750][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.38it/s, Loss=0.30468750][A[A


Epoch 31/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.38it/s, Loss=0.29492188][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.66it/s, Loss=0.29492188][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.66it/s, Loss=0.30273438][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.66it/s, Loss=0.30078125][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.66it/s, Loss=0.30273438][A[A


Epoch 31/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.66it/s, Loss=0.32031250][A[A


                                                                             [A[A

Epochs:  60%|██████    | 30/50 [00:43<00:28,  1.43s/it, Mean Loss=0.30233226, LR=1.58e-04][A

Epochs:  62%|██████▏   | 31/50 [00:43<00:26,  1.41s/it, Mean Loss=0.30233226, LR=1.58e-04][A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30664062][A[A


Epoch 32/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29492188][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.32it/s, Loss=0.29492188][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.32it/s, Loss=0.29687500][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.32it/s, Loss=0.30468750][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.32it/s, Loss=0.29101562][A[A


Epoch 32/50:   6%|▌         | 2/34 [00:00<00:03,  9.32it/s, Loss=0.29296875][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.98it/s, Loss=0.29296875][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.98it/s, Loss=0.30273438][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.98it/s, Loss=0.29687500][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.98it/s, Loss=0.30859375][A[A


Epoch 32/50:  18%|█▊        | 6/34 [00:00<00:01, 20.98it/s, Loss=0.30078125][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=0.30078125][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=0.30078125][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=0.30078125][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=0.31054688][A[A


Epoch 32/50:  29%|██▉       | 10/34 [00:00<00:00, 27.05it/s, Loss=0.30078125][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30078125][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30859375][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.29687500][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30664062][A[A


Epoch 32/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30273438][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.30273438][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.30078125][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.30468750][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.30273438][A[A


Epoch 32/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.31it/s, Loss=0.29296875][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=0.29296875][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.60it/s, Loss=0.30078125][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=0.30664062][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=0.29882812][A[A


Epoch 32/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.60it/s, Loss=0.29492188][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=0.29492188][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=0.30859375][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=0.31250000][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=0.29882812][A[A


Epoch 32/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.53it/s, Loss=0.30078125][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=0.30078125][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=0.29882812][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=0.30664062][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=0.30273438][A[A


Epoch 32/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.77it/s, Loss=0.30859375][A[A


                                                                             [A[A

Epochs:  62%|██████▏   | 31/50 [00:44<00:26,  1.41s/it, Mean Loss=0.30187270, LR=1.38e-04][A

Epochs:  64%|██████▍   | 32/50 [00:44<00:25,  1.40s/it, Mean Loss=0.30187270, LR=1.38e-04][A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29687500][A[A


Epoch 33/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.30859375][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.29687500][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.30078125][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.30273438][A[A


Epoch 33/50:   6%|▌         | 2/34 [00:00<00:03,  9.30it/s, Loss=0.29687500][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 20.03it/s, Loss=0.29687500][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 20.03it/s, Loss=0.30859375][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 20.03it/s, Loss=0.30859375][A[A


Epoch 33/50:  18%|█▊        | 6/34 [00:00<00:01, 20.03it/s, Loss=0.30273438][A[A


Epoch 33/50:  26%|██▋       | 9/34 [00:00<00:01, 20.01it/s, Loss=0.30273438][A[A


Epoch 33/50:  26%|██▋       | 9/34 [00:00<00:01, 20.01it/s, Loss=0.29687500][A[A


Epoch 33/50:  26%|██▋       | 9/34 [00:00<00:01, 20.01it/s, Loss=0.29882812][A[A


Epoch 33/50:  26%|██▋       | 9/34 [00:00<00:01, 20.01it/s, Loss=0.29296875][A[A


Epoch 33/50:  26%|██▋       | 9/34 [00:00<00:01, 20.01it/s, Loss=0.30078125][A[A


Epoch 33/50:  38%|███▊      | 13/34 [00:00<00:01, 16.78it/s, Loss=0.30078125][A[A


Epoch 33/50:  38%|███▊      | 13/34 [00:00<00:01, 16.78it/s, Loss=0.30078125][A[A


Epoch 33/50:  38%|███▊      | 13/34 [00:00<00:01, 16.78it/s, Loss=0.31250000][A[A


Epoch 33/50:  38%|███▊      | 13/34 [00:00<00:01, 16.78it/s, Loss=0.30078125][A[A


Epoch 33/50:  38%|███▊      | 13/34 [00:00<00:01, 16.78it/s, Loss=0.29882812][A[A


Epoch 33/50:  50%|█████     | 17/34 [00:00<00:00, 21.32it/s, Loss=0.29882812][A[A


Epoch 33/50:  50%|█████     | 17/34 [00:00<00:00, 21.32it/s, Loss=0.29687500][A[A


Epoch 33/50:  50%|█████     | 17/34 [00:00<00:00, 21.32it/s, Loss=0.30078125][A[A


Epoch 33/50:  50%|█████     | 17/34 [00:00<00:00, 21.32it/s, Loss=0.30078125][A[A


Epoch 33/50:  50%|█████     | 17/34 [00:00<00:00, 21.32it/s, Loss=0.30078125][A[A


Epoch 33/50:  62%|██████▏   | 21/34 [00:00<00:00, 25.11it/s, Loss=0.30078125][A[A


Epoch 33/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.11it/s, Loss=0.30664062][A[A


Epoch 33/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.11it/s, Loss=0.30468750][A[A


Epoch 33/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.11it/s, Loss=0.29687500][A[A


Epoch 33/50:  71%|███████   | 24/34 [00:01<00:00, 19.65it/s, Loss=0.29687500][A[A


Epoch 33/50:  71%|███████   | 24/34 [00:01<00:00, 19.65it/s, Loss=0.30078125][A[A


Epoch 33/50:  71%|███████   | 24/34 [00:01<00:00, 19.65it/s, Loss=0.30078125][A[A


Epoch 33/50:  71%|███████   | 24/34 [00:01<00:00, 19.65it/s, Loss=0.29687500][A[A


Epoch 33/50:  71%|███████   | 24/34 [00:01<00:00, 19.65it/s, Loss=0.29296875][A[A


Epoch 33/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.29296875][A[A


Epoch 33/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.30078125][A[A


Epoch 33/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.29296875][A[A


Epoch 33/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.30273438][A[A


Epoch 33/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.32it/s, Loss=0.30468750][A[A


Epoch 33/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.44it/s, Loss=0.30468750][A[A


Epoch 33/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.44it/s, Loss=0.30468750][A[A


Epoch 33/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.44it/s, Loss=0.30078125][A[A


                                                                             [A[A

Epochs:  64%|██████▍   | 32/50 [00:46<00:25,  1.40s/it, Mean Loss=0.30089614, LR=1.20e-04][A

Epochs:  66%|██████▌   | 33/50 [00:46<00:24,  1.43s/it, Mean Loss=0.30089614, LR=1.20e-04][A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29492188][A[A


Epoch 34/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30078125][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30273438][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30078125][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30859375][A[A


Epoch 34/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30859375][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 20.83it/s, Loss=0.30859375][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 20.83it/s, Loss=0.29492188][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 20.83it/s, Loss=0.31054688][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 20.83it/s, Loss=0.29882812][A[A


Epoch 34/50:  18%|█▊        | 6/34 [00:00<00:01, 20.83it/s, Loss=0.30859375][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 26.86it/s, Loss=0.30859375][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 26.86it/s, Loss=0.30664062][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 26.86it/s, Loss=0.30078125][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 26.86it/s, Loss=0.30859375][A[A


Epoch 34/50:  29%|██▉       | 10/34 [00:00<00:00, 26.86it/s, Loss=0.29882812][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=0.29882812][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=0.28906250][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=0.30273438][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=0.29296875][A[A


Epoch 34/50:  41%|████      | 14/34 [00:00<00:00, 20.34it/s, Loss=0.30468750][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.46it/s, Loss=0.30468750][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.46it/s, Loss=0.30078125][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.46it/s, Loss=0.29492188][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.46it/s, Loss=0.30078125][A[A


Epoch 34/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.46it/s, Loss=0.28906250][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.62it/s, Loss=0.28906250][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.62it/s, Loss=0.30468750][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.62it/s, Loss=0.30078125][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.62it/s, Loss=0.30273438][A[A


Epoch 34/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.62it/s, Loss=0.29882812][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=0.29882812][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=0.30078125][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=0.29492188][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=0.30664062][A[A


Epoch 34/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.56it/s, Loss=0.30273438][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.30273438][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.28906250][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.29101562][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.30468750][A[A


Epoch 34/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.32421875][A[A


                                                                             [A[A

Epochs:  66%|██████▌   | 33/50 [00:47<00:24,  1.43s/it, Mean Loss=0.30118336, LR=1.05e-04][A

Epochs:  68%|██████▊   | 34/50 [00:47<00:22,  1.41s/it, Mean Loss=0.30118336, LR=1.05e-04][A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 35/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30273438][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30273438][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30664062][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.29687500][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30273438][A[A


Epoch 35/50:   6%|▌         | 2/34 [00:00<00:03,  9.26it/s, Loss=0.30078125][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.30078125][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.30078125][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.29101562][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.29492188][A[A


Epoch 35/50:  18%|█▊        | 6/34 [00:00<00:01, 20.78it/s, Loss=0.29101562][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29101562][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29492188][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29687500][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29296875][A[A


Epoch 35/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.28906250][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.28906250][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30273438][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30468750][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.29687500][A[A


Epoch 35/50:  41%|████      | 14/34 [00:00<00:00, 20.27it/s, Loss=0.30468750][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=0.30468750][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=0.30078125][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=0.30078125][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=0.29882812][A[A


Epoch 35/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.37it/s, Loss=0.29687500][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=0.29687500][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.56it/s, Loss=0.29882812][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=0.29882812][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=0.30859375][A[A


Epoch 35/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.56it/s, Loss=0.30273438][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.30273438][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.30273438][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.29687500][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.30664062][A[A


Epoch 35/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.57it/s, Loss=0.30664062][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.30664062][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.30664062][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.30078125][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.30468750][A[A


Epoch 35/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.83it/s, Loss=0.33789062][A[A


                                                                             [A[A

Epochs:  68%|██████▊   | 34/50 [00:49<00:22,  1.41s/it, Mean Loss=0.30141314, LR=9.12e-05][A

Epochs:  70%|███████   | 35/50 [00:49<00:21,  1.40s/it, Mean Loss=0.30141314, LR=9.12e-05][A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30664062][A[A


Epoch 36/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30273438][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.30273438][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29687500][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29296875][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.30468750][A[A


Epoch 36/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.30078125][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 19.75it/s, Loss=0.30078125][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 19.75it/s, Loss=0.28906250][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 19.75it/s, Loss=0.30468750][A[A


Epoch 36/50:  18%|█▊        | 6/34 [00:00<00:01, 19.75it/s, Loss=0.30273438][A[A


Epoch 36/50:  26%|██▋       | 9/34 [00:00<00:01, 19.98it/s, Loss=0.30273438][A[A


Epoch 36/50:  26%|██▋       | 9/34 [00:00<00:01, 19.98it/s, Loss=0.30078125][A[A


Epoch 36/50:  26%|██▋       | 9/34 [00:00<00:01, 19.98it/s, Loss=0.30273438][A[A


Epoch 36/50:  26%|██▋       | 9/34 [00:00<00:01, 19.98it/s, Loss=0.31054688][A[A


Epoch 36/50:  35%|███▌      | 12/34 [00:00<00:00, 22.35it/s, Loss=0.31054688][A[A


Epoch 36/50:  35%|███▌      | 12/34 [00:00<00:00, 22.35it/s, Loss=0.30078125][A[A


Epoch 36/50:  35%|███▌      | 12/34 [00:00<00:00, 22.35it/s, Loss=0.30273438][A[A


Epoch 36/50:  35%|███▌      | 12/34 [00:00<00:00, 22.35it/s, Loss=0.30468750][A[A


Epoch 36/50:  44%|████▍     | 15/34 [00:00<00:01, 17.64it/s, Loss=0.30468750][A[A


Epoch 36/50:  44%|████▍     | 15/34 [00:00<00:01, 17.64it/s, Loss=0.29296875][A[A


Epoch 36/50:  44%|████▍     | 15/34 [00:00<00:01, 17.64it/s, Loss=0.30468750][A[A


Epoch 36/50:  44%|████▍     | 15/34 [00:00<00:01, 17.64it/s, Loss=0.28515625][A[A


Epoch 36/50:  44%|████▍     | 15/34 [00:00<00:01, 17.64it/s, Loss=0.29492188][A[A


Epoch 36/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.41it/s, Loss=0.29492188][A[A


Epoch 36/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.41it/s, Loss=0.29882812][A[A


Epoch 36/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.41it/s, Loss=0.29492188][A[A


Epoch 36/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.41it/s, Loss=0.30664062][A[A


Epoch 36/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.41it/s, Loss=0.29687500][A[A


Epoch 36/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.18it/s, Loss=0.29687500][A[A


Epoch 36/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.18it/s, Loss=0.29492188][A[A


Epoch 36/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.18it/s, Loss=0.30859375][A[A


Epoch 36/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.18it/s, Loss=0.30078125][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.30078125][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.30859375][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.29492188][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.30078125][A[A


Epoch 36/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.30078125][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.80it/s, Loss=0.30078125][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.80it/s, Loss=0.29101562][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.80it/s, Loss=0.29492188][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.80it/s, Loss=0.30078125][A[A


Epoch 36/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.80it/s, Loss=0.31250000][A[A


                                                                             [A[A

Epochs:  70%|███████   | 35/50 [00:50<00:21,  1.40s/it, Mean Loss=0.30020680, LR=7.94e-05][A

Epochs:  72%|███████▏  | 36/50 [00:50<00:19,  1.43s/it, Mean Loss=0.30020680, LR=7.94e-05][A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30468750][A[A


Epoch 37/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30468750][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.30468750][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.30078125][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.29492188][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.30664062][A[A


Epoch 37/50:   6%|▌         | 2/34 [00:00<00:03,  9.19it/s, Loss=0.31835938][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=0.31835938][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=0.29492188][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=0.29882812][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=0.29296875][A[A


Epoch 37/50:  18%|█▊        | 6/34 [00:00<00:01, 20.61it/s, Loss=0.30078125][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.30078125][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.29101562][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.30468750][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.29492188][A[A


Epoch 37/50:  29%|██▉       | 10/34 [00:00<00:00, 26.63it/s, Loss=0.30468750][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.30468750][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29687500][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29882812][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29296875][A[A


Epoch 37/50:  41%|████      | 14/34 [00:00<00:00, 20.09it/s, Loss=0.29687500][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.29687500][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.30078125][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.30468750][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.29882812][A[A


Epoch 37/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.34it/s, Loss=0.29687500][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.29687500][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.30859375][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.30468750][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.30078125][A[A


Epoch 37/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.29296875][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.51it/s, Loss=0.29296875][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.51it/s, Loss=0.29687500][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.51it/s, Loss=0.30468750][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.51it/s, Loss=0.29296875][A[A


Epoch 37/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.51it/s, Loss=0.30078125][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=0.30078125][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=0.29687500][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=0.29882812][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=0.29296875][A[A


Epoch 37/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.85it/s, Loss=0.35156250][A[A


                                                                             [A[A

Epochs:  72%|███████▏  | 36/50 [00:52<00:19,  1.43s/it, Mean Loss=0.30124081, LR=6.92e-05][A

Epochs:  74%|███████▍  | 37/50 [00:52<00:18,  1.41s/it, Mean Loss=0.30124081, LR=6.92e-05][A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29687500][A[A


Epoch 38/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.30859375][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29687500][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29687500][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29882812][A[A


Epoch 38/50:   6%|▌         | 2/34 [00:00<00:03,  9.20it/s, Loss=0.29296875][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.54it/s, Loss=0.29296875][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.54it/s, Loss=0.30078125][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.54it/s, Loss=0.29687500][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.54it/s, Loss=0.30468750][A[A


Epoch 38/50:  18%|█▊        | 6/34 [00:00<00:01, 20.54it/s, Loss=0.29882812][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=0.29882812][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=0.29296875][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=0.29492188][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=0.29296875][A[A


Epoch 38/50:  29%|██▉       | 10/34 [00:00<00:00, 26.50it/s, Loss=0.31054688][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=0.31054688][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=0.30859375][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=0.29296875][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=0.31250000][A[A


Epoch 38/50:  41%|████      | 14/34 [00:00<00:00, 20.11it/s, Loss=0.30273438][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30273438][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30078125][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30273438][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30078125][A[A


Epoch 38/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.29492188][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=0.29492188][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.58it/s, Loss=0.29492188][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.29687500][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.29687500][A[A


Epoch 38/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.58it/s, Loss=0.29687500][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.29687500][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.30468750][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.30078125][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.29296875][A[A


Epoch 38/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.43it/s, Loss=0.28906250][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.64it/s, Loss=0.28906250][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.64it/s, Loss=0.29687500][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.64it/s, Loss=0.30859375][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.64it/s, Loss=0.30078125][A[A


Epoch 38/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.64it/s, Loss=0.22460938][A[A


                                                                             [A[A

Epochs:  74%|███████▍  | 37/50 [00:53<00:18,  1.41s/it, Mean Loss=0.29716222, LR=6.03e-05][A

Epochs:  76%|███████▌  | 38/50 [00:53<00:16,  1.41s/it, Mean Loss=0.29716222, LR=6.03e-05][A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 39/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29296875][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.35it/s, Loss=0.29296875][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.35it/s, Loss=0.30468750][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.35it/s, Loss=0.29296875][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.35it/s, Loss=0.29492188][A[A


Epoch 39/50:   6%|▌         | 2/34 [00:00<00:03,  9.35it/s, Loss=0.30859375][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.30859375][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.30078125][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.30468750][A[A


Epoch 39/50:  18%|█▊        | 6/34 [00:00<00:01, 20.73it/s, Loss=0.31250000][A[A


Epoch 39/50:  26%|██▋       | 9/34 [00:00<00:01, 19.51it/s, Loss=0.31250000][A[A


Epoch 39/50:  26%|██▋       | 9/34 [00:00<00:01, 19.51it/s, Loss=0.30859375][A[A


Epoch 39/50:  26%|██▋       | 9/34 [00:00<00:01, 19.51it/s, Loss=0.29687500][A[A


Epoch 39/50:  26%|██▋       | 9/34 [00:00<00:01, 19.51it/s, Loss=0.29492188][A[A


Epoch 39/50:  35%|███▌      | 12/34 [00:00<00:00, 22.16it/s, Loss=0.29492188][A[A


Epoch 39/50:  35%|███▌      | 12/34 [00:00<00:00, 22.16it/s, Loss=0.30078125][A[A


Epoch 39/50:  35%|███▌      | 12/34 [00:00<00:00, 22.16it/s, Loss=0.29296875][A[A


Epoch 39/50:  35%|███▌      | 12/34 [00:00<00:00, 22.16it/s, Loss=0.29687500][A[A


Epoch 39/50:  44%|████▍     | 15/34 [00:00<00:01, 17.49it/s, Loss=0.29687500][A[A


Epoch 39/50:  44%|████▍     | 15/34 [00:00<00:01, 17.49it/s, Loss=0.30078125][A[A


Epoch 39/50:  44%|████▍     | 15/34 [00:00<00:01, 17.49it/s, Loss=0.30468750][A[A


Epoch 39/50:  44%|████▍     | 15/34 [00:00<00:01, 17.49it/s, Loss=0.29296875][A[A


Epoch 39/50:  44%|████▍     | 15/34 [00:00<00:01, 17.49it/s, Loss=0.29882812][A[A


Epoch 39/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.28it/s, Loss=0.29882812][A[A


Epoch 39/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.28it/s, Loss=0.30078125][A[A


Epoch 39/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.28it/s, Loss=0.29296875][A[A


Epoch 39/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.28it/s, Loss=0.29492188][A[A


Epoch 39/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.28it/s, Loss=0.30468750][A[A


Epoch 39/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.13it/s, Loss=0.30468750][A[A


Epoch 39/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.13it/s, Loss=0.29492188][A[A


Epoch 39/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.13it/s, Loss=0.30078125][A[A


Epoch 39/50:  68%|██████▊   | 23/34 [00:01<00:00, 26.13it/s, Loss=0.29687500][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.08it/s, Loss=0.29687500][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.08it/s, Loss=0.29687500][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.08it/s, Loss=0.29882812][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.08it/s, Loss=0.28906250][A[A


Epoch 39/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.08it/s, Loss=0.31250000][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.77it/s, Loss=0.31250000][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.77it/s, Loss=0.29882812][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.77it/s, Loss=0.28906250][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.77it/s, Loss=0.29296875][A[A


Epoch 39/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.77it/s, Loss=0.55078125][A[A


                                                                             [A[A

Epochs:  76%|███████▌  | 38/50 [00:54<00:16,  1.41s/it, Mean Loss=0.30629596, LR=5.25e-05][A

Epochs:  78%|███████▊  | 39/50 [00:54<00:15,  1.43s/it, Mean Loss=0.30629596, LR=5.25e-05][A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29296875][A[A


Epoch 40/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.30078125][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.29492188][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.31250000][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.29882812][A[A


Epoch 40/50:   6%|▌         | 2/34 [00:00<00:03,  9.29it/s, Loss=0.29492188][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.29492188][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.29687500][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30078125][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30078125][A[A


Epoch 40/50:  18%|█▊        | 6/34 [00:00<00:01, 20.67it/s, Loss=0.30273438][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.52it/s, Loss=0.30273438][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.52it/s, Loss=0.30078125][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.52it/s, Loss=0.28710938][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.52it/s, Loss=0.29296875][A[A


Epoch 40/50:  29%|██▉       | 10/34 [00:00<00:00, 26.52it/s, Loss=0.29296875][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.29296875][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.29687500][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.30078125][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.30468750][A[A


Epoch 40/50:  41%|████      | 14/34 [00:00<00:01, 19.99it/s, Loss=0.30468750][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.18it/s, Loss=0.30468750][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.18it/s, Loss=0.29687500][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.18it/s, Loss=0.30078125][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.18it/s, Loss=0.29492188][A[A


Epoch 40/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.18it/s, Loss=0.30273438][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=0.30273438][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.47it/s, Loss=0.29687500][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=0.29296875][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=0.30078125][A[A


Epoch 40/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.47it/s, Loss=0.30273438][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.30273438][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.30468750][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.30468750][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.29296875][A[A


Epoch 40/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.71it/s, Loss=0.28515625][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.05it/s, Loss=0.28515625][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.05it/s, Loss=0.31250000][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.05it/s, Loss=0.29101562][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.05it/s, Loss=0.30078125][A[A


Epoch 40/50:  88%|████████▊ | 30/34 [00:01<00:00, 25.05it/s, Loss=0.33984375][A[A


                                                                             [A[A

Epochs:  78%|███████▊  | 39/50 [00:56<00:15,  1.43s/it, Mean Loss=0.29991958, LR=4.57e-05][A

Epochs:  80%|████████  | 40/50 [00:56<00:14,  1.41s/it, Mean Loss=0.29991958, LR=4.57e-05][A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 41/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.30859375][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.30078125][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.30468750][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.30273438][A[A


Epoch 41/50:   6%|▌         | 2/34 [00:00<00:03,  9.31it/s, Loss=0.29296875][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.29296875][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.30273438][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.29492188][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.30078125][A[A


Epoch 41/50:  18%|█▊        | 6/34 [00:00<00:01, 20.85it/s, Loss=0.31250000][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=0.31250000][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=0.29296875][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=0.29687500][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=0.30273438][A[A


Epoch 41/50:  29%|██▉       | 10/34 [00:00<00:00, 26.70it/s, Loss=0.28710938][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=0.28710938][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=0.29687500][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=0.28906250][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=0.30078125][A[A


Epoch 41/50:  41%|████      | 14/34 [00:00<00:01, 19.98it/s, Loss=0.30078125][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.15it/s, Loss=0.30078125][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.15it/s, Loss=0.28906250][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.15it/s, Loss=0.30273438][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.15it/s, Loss=0.30273438][A[A


Epoch 41/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.15it/s, Loss=0.28906250][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=0.28906250][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.44it/s, Loss=0.29687500][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.29296875][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.29492188][A[A


Epoch 41/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.44it/s, Loss=0.29882812][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=0.29882812][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=0.29882812][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=0.30468750][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=0.31054688][A[A


Epoch 41/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.66it/s, Loss=0.29687500][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.99it/s, Loss=0.29687500][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.99it/s, Loss=0.29687500][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.99it/s, Loss=0.28710938][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.99it/s, Loss=0.30078125][A[A


Epoch 41/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.99it/s, Loss=0.23242188][A[A


                                                                             [A[A

Epochs:  80%|████████  | 40/50 [00:57<00:14,  1.41s/it, Mean Loss=0.29653033, LR=3.98e-05][A

Epochs:  82%|████████▏ | 41/50 [00:57<00:12,  1.40s/it, Mean Loss=0.29653033, LR=3.98e-05][A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.28906250][A[A


Epoch 42/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.34it/s, Loss=0.30078125][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.34it/s, Loss=0.29296875][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.34it/s, Loss=0.30859375][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.34it/s, Loss=0.30273438][A[A


Epoch 42/50:   6%|▌         | 2/34 [00:00<00:03,  9.34it/s, Loss=0.30273438][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.82it/s, Loss=0.30273438][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.82it/s, Loss=0.29296875][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.82it/s, Loss=0.30468750][A[A


Epoch 42/50:  18%|█▊        | 6/34 [00:00<00:01, 20.82it/s, Loss=0.30078125][A[A


Epoch 42/50:  26%|██▋       | 9/34 [00:00<00:01, 19.87it/s, Loss=0.30078125][A[A


Epoch 42/50:  26%|██▋       | 9/34 [00:00<00:01, 19.87it/s, Loss=0.29687500][A[A


Epoch 42/50:  26%|██▋       | 9/34 [00:00<00:01, 19.87it/s, Loss=0.29296875][A[A


Epoch 42/50:  26%|██▋       | 9/34 [00:00<00:01, 19.87it/s, Loss=0.30859375][A[A


Epoch 42/50:  35%|███▌      | 12/34 [00:00<00:00, 22.14it/s, Loss=0.30859375][A[A


Epoch 42/50:  35%|███▌      | 12/34 [00:00<00:00, 22.14it/s, Loss=0.29101562][A[A


Epoch 42/50:  35%|███▌      | 12/34 [00:00<00:00, 22.14it/s, Loss=0.29101562][A[A


Epoch 42/50:  35%|███▌      | 12/34 [00:00<00:00, 22.14it/s, Loss=0.29687500][A[A


Epoch 42/50:  44%|████▍     | 15/34 [00:00<00:01, 17.40it/s, Loss=0.29687500][A[A


Epoch 42/50:  44%|████▍     | 15/34 [00:00<00:01, 17.40it/s, Loss=0.29687500][A[A


Epoch 42/50:  44%|████▍     | 15/34 [00:00<00:01, 17.40it/s, Loss=0.29101562][A[A


Epoch 42/50:  44%|████▍     | 15/34 [00:00<00:01, 17.40it/s, Loss=0.29882812][A[A


Epoch 42/50:  44%|████▍     | 15/34 [00:00<00:01, 17.40it/s, Loss=0.30468750][A[A


Epoch 42/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.03it/s, Loss=0.30468750][A[A


Epoch 42/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.03it/s, Loss=0.29296875][A[A


Epoch 42/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.03it/s, Loss=0.30273438][A[A


Epoch 42/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.03it/s, Loss=0.29296875][A[A


Epoch 42/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.03it/s, Loss=0.30078125][A[A


Epoch 42/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.81it/s, Loss=0.30078125][A[A


Epoch 42/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.81it/s, Loss=0.29687500][A[A


Epoch 42/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.81it/s, Loss=0.30664062][A[A


Epoch 42/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.81it/s, Loss=0.30273438][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.30273438][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.29101562][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.29492188][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.29296875][A[A


Epoch 42/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.04it/s, Loss=0.29492188][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.85it/s, Loss=0.29492188][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.85it/s, Loss=0.30273438][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.85it/s, Loss=0.29882812][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.85it/s, Loss=0.30468750][A[A


Epoch 42/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.85it/s, Loss=0.31835938][A[A


                                                                             [A[A

Epochs:  82%|████████▏ | 41/50 [00:59<00:12,  1.40s/it, Mean Loss=0.29877068, LR=3.47e-05][A

Epochs:  84%|████████▍ | 42/50 [00:59<00:11,  1.43s/it, Mean Loss=0.29877068, LR=3.47e-05][A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.28906250][A[A


Epoch 43/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30078125][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.30078125][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.30468750][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.31054688][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.29492188][A[A


Epoch 43/50:   6%|▌         | 2/34 [00:00<00:03,  9.15it/s, Loss=0.28710938][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.65it/s, Loss=0.28710938][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.65it/s, Loss=0.29296875][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.65it/s, Loss=0.29882812][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.65it/s, Loss=0.29687500][A[A


Epoch 43/50:  18%|█▊        | 6/34 [00:00<00:01, 20.65it/s, Loss=0.30078125][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=0.30078125][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=0.30273438][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=0.30078125][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=0.30078125][A[A


Epoch 43/50:  29%|██▉       | 10/34 [00:00<00:00, 26.67it/s, Loss=0.30078125][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.92it/s, Loss=0.30078125][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.92it/s, Loss=0.29687500][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.92it/s, Loss=0.29687500][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.92it/s, Loss=0.29101562][A[A


Epoch 43/50:  41%|████      | 14/34 [00:00<00:01, 19.92it/s, Loss=0.30273438][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.30273438][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.30078125][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.30273438][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.29296875][A[A


Epoch 43/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.29882812][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.29it/s, Loss=0.29882812][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.29it/s, Loss=0.29687500][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=0.29882812][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=0.29882812][A[A


Epoch 43/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.29it/s, Loss=0.30078125][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.30078125][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.29296875][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.29296875][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.28906250][A[A


Epoch 43/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.55it/s, Loss=0.30078125][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.30078125][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.29492188][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.30078125][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.30468750][A[A


Epoch 43/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.95it/s, Loss=0.41210938][A[A


                                                                             [A[A

Epochs:  84%|████████▍ | 42/50 [01:00<00:11,  1.43s/it, Mean Loss=0.30141314, LR=3.02e-05][A

Epochs:  86%|████████▌ | 43/50 [01:00<00:09,  1.41s/it, Mean Loss=0.30141314, LR=3.02e-05][A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 44/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.39it/s, Loss=0.29882812][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.39it/s, Loss=0.28906250][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.39it/s, Loss=0.29101562][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.39it/s, Loss=0.29101562][A[A


Epoch 44/50:   6%|▌         | 2/34 [00:00<00:03,  9.39it/s, Loss=0.29687500][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.29687500][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.29687500][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.30078125][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.30468750][A[A


Epoch 44/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.30078125][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30078125][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30078125][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.30273438][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29296875][A[A


Epoch 44/50:  29%|██▉       | 10/34 [00:00<00:00, 26.75it/s, Loss=0.29687500][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.29687500][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.30468750][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.30273438][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.29101562][A[A


Epoch 44/50:  41%|████      | 14/34 [00:00<00:01, 19.88it/s, Loss=0.30664062][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.30664062][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.29687500][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.29687500][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.30468750][A[A


Epoch 44/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.98it/s, Loss=0.29687500][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.13it/s, Loss=0.29687500][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.13it/s, Loss=0.29492188][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.13it/s, Loss=0.29687500][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.13it/s, Loss=0.29687500][A[A


Epoch 44/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.13it/s, Loss=0.29296875][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.29it/s, Loss=0.29296875][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.29it/s, Loss=0.30078125][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.29it/s, Loss=0.29101562][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.29it/s, Loss=0.30859375][A[A


Epoch 44/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.29it/s, Loss=0.29492188][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.72it/s, Loss=0.29492188][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.72it/s, Loss=0.29882812][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.72it/s, Loss=0.30664062][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.72it/s, Loss=0.29296875][A[A


Epoch 44/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.72it/s, Loss=0.24609375][A[A


                                                                             [A[A

Epochs:  86%|████████▌ | 43/50 [01:01<00:09,  1.41s/it, Mean Loss=0.29658778, LR=2.63e-05][A

Epochs:  88%|████████▊ | 44/50 [01:01<00:08,  1.41s/it, Mean Loss=0.29658778, LR=2.63e-05][A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.28320312][A[A


Epoch 45/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29296875][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.29296875][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.29882812][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.30078125][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.31054688][A[A


Epoch 45/50:   6%|▌         | 2/34 [00:00<00:03,  9.47it/s, Loss=0.29882812][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 21.06it/s, Loss=0.29882812][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 21.06it/s, Loss=0.29101562][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 21.06it/s, Loss=0.29882812][A[A


Epoch 45/50:  18%|█▊        | 6/34 [00:00<00:01, 21.06it/s, Loss=0.30273438][A[A


Epoch 45/50:  26%|██▋       | 9/34 [00:00<00:01, 19.49it/s, Loss=0.30273438][A[A


Epoch 45/50:  26%|██▋       | 9/34 [00:00<00:01, 19.49it/s, Loss=0.30664062][A[A


Epoch 45/50:  26%|██▋       | 9/34 [00:00<00:01, 19.49it/s, Loss=0.30273438][A[A


Epoch 45/50:  26%|██▋       | 9/34 [00:00<00:01, 19.49it/s, Loss=0.29687500][A[A


Epoch 45/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.29687500][A[A


Epoch 45/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.29882812][A[A


Epoch 45/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.30078125][A[A


Epoch 45/50:  35%|███▌      | 12/34 [00:00<00:00, 22.47it/s, Loss=0.30468750][A[A


Epoch 45/50:  44%|████▍     | 15/34 [00:00<00:01, 17.45it/s, Loss=0.30468750][A[A


Epoch 45/50:  44%|████▍     | 15/34 [00:00<00:01, 17.45it/s, Loss=0.30468750][A[A


Epoch 45/50:  44%|████▍     | 15/34 [00:00<00:01, 17.45it/s, Loss=0.30664062][A[A


Epoch 45/50:  44%|████▍     | 15/34 [00:00<00:01, 17.45it/s, Loss=0.29101562][A[A


Epoch 45/50:  44%|████▍     | 15/34 [00:00<00:01, 17.45it/s, Loss=0.29882812][A[A


Epoch 45/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.29882812][A[A


Epoch 45/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.29296875][A[A


Epoch 45/50:  56%|█████▌    | 19/34 [00:00<00:00, 22.07it/s, Loss=0.29687500][A[A


Epoch 45/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.07it/s, Loss=0.30078125][A[A


Epoch 45/50:  56%|█████▌    | 19/34 [00:01<00:00, 22.07it/s, Loss=0.30078125][A[A


Epoch 45/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.70it/s, Loss=0.30078125][A[A


Epoch 45/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.70it/s, Loss=0.29101562][A[A


Epoch 45/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.70it/s, Loss=0.30078125][A[A


Epoch 45/50:  68%|██████▊   | 23/34 [00:01<00:00, 25.70it/s, Loss=0.28906250][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.87it/s, Loss=0.28906250][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.87it/s, Loss=0.29492188][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.87it/s, Loss=0.29882812][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.87it/s, Loss=0.30468750][A[A


Epoch 45/50:  76%|███████▋  | 26/34 [00:01<00:00, 19.87it/s, Loss=0.29687500][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.68it/s, Loss=0.29687500][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.68it/s, Loss=0.29101562][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.68it/s, Loss=0.29101562][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.68it/s, Loss=0.29687500][A[A


Epoch 45/50:  88%|████████▊ | 30/34 [00:01<00:00, 23.68it/s, Loss=0.41406250][A[A


                                                                             [A[A

Epochs:  88%|████████▊ | 44/50 [01:03<00:08,  1.41s/it, Mean Loss=0.30147059, LR=2.29e-05][A

Epochs:  90%|█████████ | 45/50 [01:03<00:07,  1.44s/it, Mean Loss=0.30147059, LR=2.29e-05][A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29296875][A[A


Epoch 46/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.28906250][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=0.28906250][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=0.29687500][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=0.30468750][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=0.29687500][A[A


Epoch 46/50:   6%|▌         | 2/34 [00:00<00:03,  9.50it/s, Loss=0.30664062][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30664062][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.29882812][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30078125][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.29687500][A[A


Epoch 46/50:  18%|█▊        | 6/34 [00:00<00:01, 21.05it/s, Loss=0.30078125][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.30078125][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.29101562][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.28710938][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.29296875][A[A


Epoch 46/50:  29%|██▉       | 10/34 [00:00<00:00, 26.71it/s, Loss=0.29296875][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.29296875][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.30468750][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.29296875][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.30273438][A[A


Epoch 46/50:  41%|████      | 14/34 [00:00<00:00, 20.07it/s, Loss=0.30664062][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=0.30664062][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=0.28710938][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=0.29882812][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=0.30468750][A[A


Epoch 46/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.19it/s, Loss=0.28515625][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=0.28515625][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.41it/s, Loss=0.30273438][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=0.29687500][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=0.30078125][A[A


Epoch 46/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.41it/s, Loss=0.30273438][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=0.30273438][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=0.30078125][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=0.29492188][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=0.30078125][A[A


Epoch 46/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.46it/s, Loss=0.30273438][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=0.30273438][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=0.30078125][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=0.29882812][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=0.30664062][A[A


Epoch 46/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s, Loss=0.34960938][A[A


                                                                             [A[A

Epochs:  90%|█████████ | 45/50 [01:04<00:07,  1.44s/it, Mean Loss=0.29968980, LR=2.00e-05][A

Epochs:  92%|█████████▏| 46/50 [01:04<00:05,  1.42s/it, Mean Loss=0.29968980, LR=2.00e-05][A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 47/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30664062][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30664062][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30468750][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.30078125][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.29296875][A[A


Epoch 47/50:   6%|▌         | 2/34 [00:00<00:03,  9.46it/s, Loss=0.29687500][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.12it/s, Loss=0.29687500][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.12it/s, Loss=0.30468750][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.12it/s, Loss=0.29296875][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.12it/s, Loss=0.30078125][A[A


Epoch 47/50:  18%|█▊        | 6/34 [00:00<00:01, 21.12it/s, Loss=0.29296875][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.29296875][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.30468750][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.29296875][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.30273438][A[A


Epoch 47/50:  29%|██▉       | 10/34 [00:00<00:00, 27.02it/s, Loss=0.29687500][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:01, 19.77it/s, Loss=0.29687500][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:01, 19.77it/s, Loss=0.29492188][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:01, 19.77it/s, Loss=0.30468750][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:01, 19.77it/s, Loss=0.29296875][A[A


Epoch 47/50:  41%|████      | 14/34 [00:00<00:01, 19.77it/s, Loss=0.30468750][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=0.30468750][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=0.29296875][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=0.30078125][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=0.29687500][A[A


Epoch 47/50:  53%|█████▎    | 18/34 [00:00<00:00, 23.93it/s, Loss=0.29882812][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.16it/s, Loss=0.29882812][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.16it/s, Loss=0.29687500][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.16it/s, Loss=0.30078125][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.16it/s, Loss=0.28906250][A[A


Epoch 47/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.16it/s, Loss=0.29101562][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.87it/s, Loss=0.29101562][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.87it/s, Loss=0.29101562][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.87it/s, Loss=0.29296875][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.87it/s, Loss=0.28515625][A[A


Epoch 47/50:  76%|███████▋  | 26/34 [00:01<00:00, 20.87it/s, Loss=0.30468750][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.27it/s, Loss=0.30468750][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.27it/s, Loss=0.29882812][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.27it/s, Loss=0.28710938][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.27it/s, Loss=0.30468750][A[A


Epoch 47/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.27it/s, Loss=0.33593750][A[A


                                                                             [A[A

Epochs:  92%|█████████▏| 46/50 [01:06<00:05,  1.42s/it, Mean Loss=0.29865579, LR=1.74e-05][A

Epochs:  94%|█████████▍| 47/50 [01:06<00:04,  1.41s/it, Mean Loss=0.29865579, LR=1.74e-05][A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30468750][A[A


Epoch 48/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29882812][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.49it/s, Loss=0.29882812][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.49it/s, Loss=0.30468750][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.49it/s, Loss=0.29296875][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.49it/s, Loss=0.28906250][A[A


Epoch 48/50:   6%|▌         | 2/34 [00:00<00:03,  9.49it/s, Loss=0.30273438][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 20.27it/s, Loss=0.30273438][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 20.27it/s, Loss=0.30664062][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 20.27it/s, Loss=0.29687500][A[A


Epoch 48/50:  18%|█▊        | 6/34 [00:00<00:01, 20.27it/s, Loss=0.29296875][A[A


Epoch 48/50:  26%|██▋       | 9/34 [00:00<00:01, 18.96it/s, Loss=0.29296875][A[A


Epoch 48/50:  26%|██▋       | 9/34 [00:00<00:01, 18.96it/s, Loss=0.29101562][A[A


Epoch 48/50:  26%|██▋       | 9/34 [00:00<00:01, 18.96it/s, Loss=0.30078125][A[A


Epoch 48/50:  26%|██▋       | 9/34 [00:00<00:01, 18.96it/s, Loss=0.29296875][A[A


Epoch 48/50:  26%|██▋       | 9/34 [00:00<00:01, 18.96it/s, Loss=0.29882812][A[A


Epoch 48/50:  38%|███▊      | 13/34 [00:00<00:01, 16.84it/s, Loss=0.29882812][A[A


Epoch 48/50:  38%|███▊      | 13/34 [00:00<00:01, 16.84it/s, Loss=0.31250000][A[A


Epoch 48/50:  38%|███▊      | 13/34 [00:00<00:01, 16.84it/s, Loss=0.29882812][A[A


Epoch 48/50:  38%|███▊      | 13/34 [00:00<00:01, 16.84it/s, Loss=0.28515625][A[A


Epoch 48/50:  38%|███▊      | 13/34 [00:00<00:01, 16.84it/s, Loss=0.30078125][A[A


Epoch 48/50:  50%|█████     | 17/34 [00:00<00:00, 21.42it/s, Loss=0.30078125][A[A


Epoch 48/50:  50%|█████     | 17/34 [00:00<00:00, 21.42it/s, Loss=0.29492188][A[A


Epoch 48/50:  50%|█████     | 17/34 [00:00<00:00, 21.42it/s, Loss=0.29687500][A[A


Epoch 48/50:  50%|█████     | 17/34 [00:00<00:00, 21.42it/s, Loss=0.30078125][A[A


Epoch 48/50:  50%|█████     | 17/34 [00:00<00:00, 21.42it/s, Loss=0.29101562][A[A


Epoch 48/50:  62%|██████▏   | 21/34 [00:00<00:00, 25.06it/s, Loss=0.29101562][A[A


Epoch 48/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.06it/s, Loss=0.30468750][A[A


Epoch 48/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.06it/s, Loss=0.30078125][A[A


Epoch 48/50:  62%|██████▏   | 21/34 [00:01<00:00, 25.06it/s, Loss=0.28906250][A[A


Epoch 48/50:  71%|███████   | 24/34 [00:01<00:00, 19.49it/s, Loss=0.28906250][A[A


Epoch 48/50:  71%|███████   | 24/34 [00:01<00:00, 19.49it/s, Loss=0.29687500][A[A


Epoch 48/50:  71%|███████   | 24/34 [00:01<00:00, 19.49it/s, Loss=0.30078125][A[A


Epoch 48/50:  71%|███████   | 24/34 [00:01<00:00, 19.49it/s, Loss=0.29101562][A[A


Epoch 48/50:  71%|███████   | 24/34 [00:01<00:00, 19.49it/s, Loss=0.30078125][A[A


Epoch 48/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.26it/s, Loss=0.30078125][A[A


Epoch 48/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.26it/s, Loss=0.29296875][A[A


Epoch 48/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.26it/s, Loss=0.29882812][A[A


Epoch 48/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.26it/s, Loss=0.30078125][A[A


Epoch 48/50:  82%|████████▏ | 28/34 [00:01<00:00, 23.26it/s, Loss=0.30468750][A[A


Epoch 48/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.49it/s, Loss=0.30468750][A[A


Epoch 48/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.49it/s, Loss=0.28906250][A[A


Epoch 48/50:  94%|█████████▍| 32/34 [00:01<00:00, 26.49it/s, Loss=0.34960938][A[A


                                                                             [A[A

Epochs:  94%|█████████▍| 47/50 [01:07<00:04,  1.41s/it, Mean Loss=0.29923024, LR=1.51e-05][A

Epochs:  96%|█████████▌| 48/50 [01:07<00:02,  1.43s/it, Mean Loss=0.29923024, LR=1.51e-05][A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.29492188][A[A


Epoch 49/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30664062][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.30664062][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.30468750][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.30468750][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.30468750][A[A


Epoch 49/50:   6%|▌         | 2/34 [00:00<00:03,  9.36it/s, Loss=0.30078125][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.30078125][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.29492188][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.30468750][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.29687500][A[A


Epoch 49/50:  18%|█▊        | 6/34 [00:00<00:01, 20.72it/s, Loss=0.29687500][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=0.29687500][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=0.30468750][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=0.29687500][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=0.29296875][A[A


Epoch 49/50:  29%|██▉       | 10/34 [00:00<00:00, 26.83it/s, Loss=0.29492188][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.03it/s, Loss=0.29492188][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.03it/s, Loss=0.29101562][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.03it/s, Loss=0.29296875][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.03it/s, Loss=0.29101562][A[A


Epoch 49/50:  41%|████      | 14/34 [00:00<00:00, 20.03it/s, Loss=0.28906250][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.28906250][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30078125][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30078125][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30273438][A[A


Epoch 49/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.24it/s, Loss=0.30468750][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.48it/s, Loss=0.30468750][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.48it/s, Loss=0.29296875][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.48it/s, Loss=0.30078125][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.48it/s, Loss=0.29296875][A[A


Epoch 49/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.48it/s, Loss=0.30078125][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.40it/s, Loss=0.30078125][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.40it/s, Loss=0.28906250][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.40it/s, Loss=0.30078125][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.40it/s, Loss=0.30078125][A[A


Epoch 49/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.40it/s, Loss=0.28515625][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.65it/s, Loss=0.28515625][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.65it/s, Loss=0.29882812][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.65it/s, Loss=0.29296875][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.65it/s, Loss=0.30273438][A[A


Epoch 49/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.65it/s, Loss=0.27343750][A[A


                                                                             [A[A

Epochs:  96%|█████████▌| 48/50 [01:09<00:02,  1.43s/it, Mean Loss=0.29716222, LR=1.32e-05][A

Epochs:  98%|█████████▊| 49/50 [01:09<00:01,  1.42s/it, Mean Loss=0.29716222, LR=1.32e-05][A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s][A[A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.30859375][A[A


Epoch 50/50:   0%|          | 0/34 [00:00<?, ?it/s, Loss=0.31640625][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=0.31640625][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=0.29296875][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=0.29687500][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=0.30468750][A[A


Epoch 50/50:   6%|▌         | 2/34 [00:00<00:03,  9.38it/s, Loss=0.29296875][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.29296875][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.29687500][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.29687500][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.29687500][A[A


Epoch 50/50:  18%|█▊        | 6/34 [00:00<00:01, 21.04it/s, Loss=0.30078125][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=0.30078125][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=0.28906250][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=0.28515625][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=0.29687500][A[A


Epoch 50/50:  29%|██▉       | 10/34 [00:00<00:00, 27.09it/s, Loss=0.28710938][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.28710938][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.29296875][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.30078125][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.29296875][A[A


Epoch 50/50:  41%|████      | 14/34 [00:00<00:00, 20.24it/s, Loss=0.29882812][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.29882812][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.29687500][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.30273438][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.29296875][A[A


Epoch 50/50:  53%|█████▎    | 18/34 [00:00<00:00, 24.33it/s, Loss=0.30078125][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.30078125][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:00<00:00, 27.63it/s, Loss=0.29882812][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.30078125][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.28906250][A[A


Epoch 50/50:  65%|██████▍   | 22/34 [00:01<00:00, 27.63it/s, Loss=0.30468750][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=0.30468750][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=0.28906250][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=0.29687500][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=0.30468750][A[A


Epoch 50/50:  76%|███████▋  | 26/34 [00:01<00:00, 21.50it/s, Loss=0.29296875][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.29296875][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.30078125][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.30859375][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.29296875][A[A


Epoch 50/50:  88%|████████▊ | 30/34 [00:01<00:00, 24.80it/s, Loss=0.31250000][A[A


                                                                             [A[A

Epochs:  98%|█████████▊| 49/50 [01:10<00:01,  1.42s/it, Mean Loss=0.29802390, LR=1.15e-05][A

Epochs: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it, Mean Loss=0.29802390, LR=1.15e-05][A
Epochs: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it, Mean Loss=0.29802390, LR=1.15e-05]


training k-probes:   0%|          | 0/260 [00:00<?, ?it/s][A

training k-probes:   0%|          | 1/260 [00:00<00:54,  4.79it/s][A

training k-probes:   1%|          | 2/260 [00:00<01:09,  3.71it/s][A

training k-probes:   1%|          | 3/260 [00:00<00:59,  4.29it/s][A

training k-probes:   2%|▏         | 4/260 [00:00<00:54,  4.67it/s][A

training k-probes:   2%|▏         | 5/260 [00:01<00:53,  4.74it/s][A

training k-probes:   2%|▏         | 6/260 [00:01<00:54,  4.70it/s][A

training k-probes:   3%|▎         | 7/260 [00:01<00:51,  4.95it/s][A

training k-probes:   3%|▎         | 8/260 [00:01<00:49,  5.07it/s][A

training k-probes:   3%|▎         | 9/260 [00:01<00:49,  5.11it/s][A

training k-probes:   4%|▍         | 10/260 [00:02<00:50,  4.98it/s][A

training k-probes:   4%|▍         | 11/260 [00:02<00:52,  4.74it/s][A

training k-probes:   5%|▍         | 12/260 [00:02<00:51,  4.79it/s][A

training k-probes:   5%|▌         | 13/260 [00:02<00:49,  5.00it/s][A

training k-probes:   5%|▌         | 14/260 [00:02<00:48,  5.03it/s][A

training k-probes:   6%|▌         | 15/260 [00:03<00:48,  5.10it/s][A

training k-probes:   6%|▌         | 16/260 [00:03<00:47,  5.13it/s][A

training k-probes:   7%|▋         | 17/260 [00:03<00:48,  4.98it/s][A

training k-probes:   7%|▋         | 18/260 [00:03<00:51,  4.73it/s][A

training k-probes:   7%|▋         | 19/260 [00:03<00:48,  4.93it/s][A

training k-probes:   8%|▊         | 20/260 [00:04<00:50,  4.79it/s][A

training k-probes:   8%|▊         | 21/260 [00:04<00:48,  4.91it/s][A

training k-probes:   8%|▊         | 22/260 [00:04<00:51,  4.65it/s][A

training k-probes:   9%|▉         | 23/260 [00:04<00:55,  4.29it/s][A

training k-probes:   9%|▉         | 24/260 [00:05<00:53,  4.45it/s][A

training k-probes:  10%|▉         | 25/260 [00:05<00:50,  4.62it/s][A

training k-probes:  10%|█         | 26/260 [00:05<00:48,  4.85it/s][A

training k-probes:  10%|█         | 27/260 [00:05<00:48,  4.79it/s][A

training k-probes:  11%|█         | 28/260 [00:05<00:48,  4.78it/s][A

training k-probes:  11%|█         | 29/260 [00:06<00:47,  4.85it/s][A

training k-probes:  12%|█▏        | 30/260 [00:06<00:46,  4.98it/s][A

training k-probes:  12%|█▏        | 31/260 [00:06<00:46,  4.89it/s][A

training k-probes:  12%|█▏        | 32/260 [00:06<00:47,  4.80it/s][A

training k-probes:  13%|█▎        | 33/260 [00:06<00:45,  4.94it/s][A

training k-probes:  13%|█▎        | 34/260 [00:07<00:44,  5.05it/s][A

training k-probes:  13%|█▎        | 35/260 [00:07<00:44,  5.06it/s][A

training k-probes:  14%|█▍        | 36/260 [00:07<00:45,  4.95it/s][A

training k-probes:  14%|█▍        | 37/260 [00:07<00:47,  4.74it/s][A

training k-probes:  15%|█▍        | 38/260 [00:07<00:46,  4.78it/s][A

training k-probes:  15%|█▌        | 39/260 [00:08<00:44,  4.92it/s][A

training k-probes:  15%|█▌        | 40/260 [00:08<00:44,  4.94it/s][A

training k-probes:  16%|█▌        | 41/260 [00:08<00:43,  5.08it/s][A

training k-probes:  16%|█▌        | 42/260 [00:08<00:43,  5.01it/s][A

training k-probes:  17%|█▋        | 43/260 [00:09<00:51,  4.21it/s][A

training k-probes:  17%|█▋        | 44/260 [00:09<00:51,  4.23it/s][A

training k-probes:  17%|█▋        | 45/260 [00:09<00:48,  4.48it/s][A

training k-probes:  18%|█▊        | 46/260 [00:09<00:47,  4.47it/s][A

training k-probes:  18%|█▊        | 47/260 [00:09<00:45,  4.63it/s][A

training k-probes:  18%|█▊        | 48/260 [00:10<00:47,  4.51it/s][A

training k-probes:  19%|█▉        | 49/260 [00:10<00:45,  4.63it/s][A

training k-probes:  19%|█▉        | 50/260 [00:10<00:44,  4.68it/s][A

training k-probes:  20%|█▉        | 51/260 [00:10<00:44,  4.74it/s][A

training k-probes:  20%|██        | 52/260 [00:10<00:42,  4.94it/s][A

training k-probes:  20%|██        | 53/260 [00:11<00:42,  4.82it/s][A

training k-probes:  21%|██        | 54/260 [00:11<00:43,  4.73it/s][A

training k-probes:  21%|██        | 55/260 [00:11<00:42,  4.80it/s][A

training k-probes:  22%|██▏       | 56/260 [00:11<00:41,  4.96it/s][A

training k-probes:  22%|██▏       | 57/260 [00:11<00:41,  4.89it/s][A

training k-probes:  22%|██▏       | 58/260 [00:12<00:41,  4.81it/s][A

training k-probes:  23%|██▎       | 59/260 [00:12<00:41,  4.89it/s][A

training k-probes:  23%|██▎       | 60/260 [00:12<00:40,  5.00it/s][A

training k-probes:  23%|██▎       | 61/260 [00:12<00:40,  4.95it/s][A

training k-probes:  24%|██▍       | 62/260 [00:12<00:40,  4.84it/s][A

training k-probes:  24%|██▍       | 63/260 [00:13<00:47,  4.12it/s][A

training k-probes:  25%|██▍       | 64/260 [00:13<00:45,  4.29it/s][A

training k-probes:  25%|██▌       | 65/260 [00:13<00:43,  4.46it/s][A

training k-probes:  25%|██▌       | 66/260 [00:13<00:42,  4.57it/s][A

training k-probes:  26%|██▌       | 67/260 [00:14<00:40,  4.76it/s][A

training k-probes:  26%|██▌       | 68/260 [00:14<00:40,  4.71it/s][A

training k-probes:  27%|██▋       | 69/260 [00:14<00:41,  4.65it/s][A

training k-probes:  27%|██▋       | 70/260 [00:14<00:41,  4.54it/s][A

training k-probes:  27%|██▋       | 71/260 [00:14<00:39,  4.73it/s][A

training k-probes:  28%|██▊       | 72/260 [00:15<00:40,  4.59it/s][A

training k-probes:  28%|██▊       | 73/260 [00:15<00:39,  4.70it/s][A

training k-probes:  28%|██▊       | 74/260 [00:15<00:40,  4.58it/s][A

training k-probes:  29%|██▉       | 75/260 [00:15<00:39,  4.66it/s][A

training k-probes:  29%|██▉       | 76/260 [00:16<00:39,  4.62it/s][A

training k-probes:  30%|██▉       | 77/260 [00:16<00:39,  4.59it/s][A

training k-probes:  30%|███       | 78/260 [00:16<00:38,  4.78it/s][A

training k-probes:  30%|███       | 79/260 [00:16<00:38,  4.68it/s][A

training k-probes:  31%|███       | 80/260 [00:16<00:38,  4.64it/s][A

training k-probes:  31%|███       | 81/260 [00:17<00:38,  4.64it/s][A

training k-probes:  32%|███▏      | 82/260 [00:17<00:38,  4.66it/s][A

training k-probes:  32%|███▏      | 83/260 [00:17<00:50,  3.52it/s][A

training k-probes:  32%|███▏      | 84/260 [00:17<00:47,  3.73it/s][A

training k-probes:  33%|███▎      | 85/260 [00:18<00:44,  3.97it/s][A

training k-probes:  33%|███▎      | 86/260 [00:18<00:40,  4.27it/s][A

training k-probes:  33%|███▎      | 87/260 [00:18<00:40,  4.25it/s][A

training k-probes:  34%|███▍      | 88/260 [00:18<00:40,  4.29it/s][A

training k-probes:  34%|███▍      | 89/260 [00:19<00:39,  4.32it/s][A

training k-probes:  35%|███▍      | 90/260 [00:19<00:38,  4.43it/s][A

training k-probes:  35%|███▌      | 91/260 [00:19<00:37,  4.54it/s][A

training k-probes:  35%|███▌      | 92/260 [00:19<00:36,  4.62it/s][A

training k-probes:  36%|███▌      | 93/260 [00:19<00:34,  4.81it/s][A

training k-probes:  36%|███▌      | 94/260 [00:20<00:34,  4.75it/s][A

training k-probes:  37%|███▋      | 95/260 [00:20<00:35,  4.63it/s][A

training k-probes:  37%|███▋      | 96/260 [00:20<00:36,  4.48it/s][A

training k-probes:  37%|███▋      | 97/260 [00:20<00:35,  4.65it/s][A

training k-probes:  38%|███▊      | 98/260 [00:21<00:36,  4.48it/s][A

training k-probes:  38%|███▊      | 99/260 [00:21<00:35,  4.57it/s][A

training k-probes:  38%|███▊      | 100/260 [00:21<00:35,  4.45it/s][A

training k-probes:  39%|███▉      | 101/260 [00:21<00:34,  4.58it/s][A

training k-probes:  39%|███▉      | 102/260 [00:21<00:38,  4.07it/s][A

training k-probes:  40%|███▉      | 103/260 [00:22<00:37,  4.19it/s][A

training k-probes:  40%|████      | 104/260 [00:22<00:35,  4.45it/s][A

training k-probes:  40%|████      | 105/260 [00:22<00:35,  4.38it/s][A

training k-probes:  41%|████      | 106/260 [00:22<00:35,  4.38it/s][A

training k-probes:  41%|████      | 107/260 [00:23<00:35,  4.35it/s][A

training k-probes:  42%|████▏     | 108/260 [00:23<00:33,  4.50it/s][A

training k-probes:  42%|████▏     | 109/260 [00:23<00:34,  4.39it/s][A

training k-probes:  42%|████▏     | 110/260 [00:23<00:34,  4.32it/s][A

training k-probes:  43%|████▎     | 111/260 [00:24<00:33,  4.45it/s][A

training k-probes:  43%|████▎     | 112/260 [00:24<00:32,  4.60it/s][A

training k-probes:  43%|████▎     | 113/260 [00:24<00:33,  4.42it/s][A

training k-probes:  44%|████▍     | 114/260 [00:24<00:33,  4.38it/s][A

training k-probes:  44%|████▍     | 115/260 [00:24<00:34,  4.18it/s][A

training k-probes:  45%|████▍     | 116/260 [00:25<00:33,  4.27it/s][A

training k-probes:  45%|████▌     | 117/260 [00:25<00:32,  4.40it/s][A

training k-probes:  45%|████▌     | 118/260 [00:25<00:31,  4.44it/s][A

training k-probes:  46%|████▌     | 119/260 [00:25<00:30,  4.57it/s][A

training k-probes:  46%|████▌     | 120/260 [00:26<00:34,  4.11it/s][A

training k-probes:  47%|████▋     | 121/260 [00:26<00:35,  3.97it/s][A

training k-probes:  47%|████▋     | 122/260 [00:26<00:35,  3.93it/s][A

training k-probes:  47%|████▋     | 123/260 [00:26<00:32,  4.19it/s][A

training k-probes:  48%|████▊     | 124/260 [00:27<00:32,  4.14it/s][A

training k-probes:  48%|████▊     | 125/260 [00:27<00:31,  4.26it/s][A

training k-probes:  48%|████▊     | 126/260 [00:27<00:32,  4.17it/s][A

training k-probes:  49%|████▉     | 127/260 [00:27<00:30,  4.33it/s][A

training k-probes:  49%|████▉     | 128/260 [00:28<00:32,  4.11it/s][A

training k-probes:  50%|████▉     | 129/260 [00:28<00:31,  4.10it/s][A

training k-probes:  50%|█████     | 130/260 [00:28<00:30,  4.29it/s][A

training k-probes:  50%|█████     | 131/260 [00:28<00:30,  4.26it/s][A

training k-probes:  51%|█████     | 132/260 [00:28<00:30,  4.26it/s][A

training k-probes:  51%|█████     | 133/260 [00:29<00:29,  4.23it/s][A

training k-probes:  52%|█████▏    | 134/260 [00:29<00:28,  4.39it/s][A

training k-probes:  52%|█████▏    | 135/260 [00:29<00:29,  4.31it/s][A

training k-probes:  52%|█████▏    | 136/260 [00:29<00:29,  4.24it/s][A

training k-probes:  53%|█████▎    | 137/260 [00:30<00:28,  4.35it/s][A

training k-probes:  53%|█████▎    | 138/260 [00:30<00:30,  3.97it/s][A

training k-probes:  53%|█████▎    | 139/260 [00:30<00:30,  3.97it/s][A

training k-probes:  54%|█████▍    | 140/260 [00:30<00:29,  4.03it/s][A

training k-probes:  54%|█████▍    | 141/260 [00:31<00:29,  3.97it/s][A

training k-probes:  55%|█████▍    | 142/260 [00:31<00:28,  4.08it/s][A

training k-probes:  55%|█████▌    | 143/260 [00:31<00:27,  4.28it/s][A

training k-probes:  55%|█████▌    | 144/260 [00:31<00:26,  4.32it/s][A

training k-probes:  56%|█████▌    | 145/260 [00:32<00:25,  4.52it/s][A

training k-probes:  56%|█████▌    | 146/260 [00:32<00:25,  4.50it/s][A

training k-probes:  57%|█████▋    | 147/260 [00:32<00:26,  4.27it/s][A

training k-probes:  57%|█████▋    | 148/260 [00:32<00:27,  4.11it/s][A

training k-probes:  57%|█████▋    | 149/260 [00:32<00:25,  4.29it/s][A

training k-probes:  58%|█████▊    | 150/260 [00:33<00:26,  4.15it/s][A

training k-probes:  58%|█████▊    | 151/260 [00:33<00:25,  4.24it/s][A

training k-probes:  58%|█████▊    | 152/260 [00:33<00:25,  4.29it/s][A

training k-probes:  59%|█████▉    | 153/260 [00:33<00:24,  4.38it/s][A

training k-probes:  59%|█████▉    | 154/260 [00:34<00:26,  4.03it/s][A

training k-probes:  60%|█████▉    | 155/260 [00:34<00:26,  4.00it/s][A

training k-probes:  60%|██████    | 156/260 [00:34<00:31,  3.34it/s][A

training k-probes:  60%|██████    | 157/260 [00:35<00:29,  3.55it/s][A

training k-probes:  61%|██████    | 158/260 [00:35<00:27,  3.72it/s][A

training k-probes:  61%|██████    | 159/260 [00:35<00:26,  3.79it/s][A

training k-probes:  62%|██████▏   | 160/260 [00:35<00:24,  4.03it/s][A

training k-probes:  62%|██████▏   | 161/260 [00:36<00:24,  4.03it/s][A

training k-probes:  62%|██████▏   | 162/260 [00:36<00:24,  3.98it/s][A

training k-probes:  63%|██████▎   | 163/260 [00:36<00:23,  4.15it/s][A

training k-probes:  63%|██████▎   | 164/260 [00:36<00:22,  4.36it/s][A

training k-probes:  63%|██████▎   | 165/260 [00:37<00:22,  4.16it/s][A

training k-probes:  64%|██████▍   | 166/260 [00:37<00:23,  4.08it/s][A

training k-probes:  64%|██████▍   | 167/260 [00:37<00:23,  3.97it/s][A

training k-probes:  65%|██████▍   | 168/260 [00:37<00:22,  4.09it/s][A

training k-probes:  65%|██████▌   | 169/260 [00:37<00:21,  4.23it/s][A

training k-probes:  65%|██████▌   | 170/260 [00:38<00:21,  4.27it/s][A

training k-probes:  66%|██████▌   | 171/260 [00:38<00:20,  4.41it/s][A

training k-probes:  66%|██████▌   | 172/260 [00:38<00:19,  4.43it/s][A

training k-probes:  67%|██████▋   | 173/260 [00:39<00:23,  3.77it/s][A

training k-probes:  67%|██████▋   | 174/260 [00:39<00:22,  3.76it/s][A

training k-probes:  67%|██████▋   | 175/260 [00:39<00:21,  3.98it/s][A

training k-probes:  68%|██████▊   | 176/260 [00:39<00:21,  3.96it/s][A

training k-probes:  68%|██████▊   | 177/260 [00:39<00:20,  4.06it/s][A

training k-probes:  68%|██████▊   | 178/260 [00:40<00:20,  4.05it/s][A

training k-probes:  69%|██████▉   | 179/260 [00:40<00:19,  4.23it/s][A

training k-probes:  69%|██████▉   | 180/260 [00:40<00:20,  3.96it/s][A

training k-probes:  70%|██████▉   | 181/260 [00:40<00:19,  3.99it/s][A

training k-probes:  70%|███████   | 182/260 [00:41<00:21,  3.62it/s][A

training k-probes:  70%|███████   | 183/260 [00:41<00:20,  3.72it/s][A

training k-probes:  71%|███████   | 184/260 [00:41<00:19,  3.85it/s][A

training k-probes:  71%|███████   | 185/260 [00:42<00:19,  3.91it/s][A

training k-probes:  72%|███████▏  | 186/260 [00:42<00:17,  4.13it/s][A

training k-probes:  72%|███████▏  | 187/260 [00:42<00:17,  4.09it/s][A

training k-probes:  72%|███████▏  | 188/260 [00:42<00:17,  4.04it/s][A

training k-probes:  73%|███████▎  | 189/260 [00:42<00:17,  4.12it/s][A

training k-probes:  73%|███████▎  | 190/260 [00:43<00:17,  3.95it/s][A

training k-probes:  73%|███████▎  | 191/260 [00:43<00:17,  3.91it/s][A

training k-probes:  74%|███████▍  | 192/260 [00:43<00:17,  3.89it/s][A

training k-probes:  74%|███████▍  | 193/260 [00:44<00:17,  3.94it/s][A

training k-probes:  75%|███████▍  | 194/260 [00:44<00:16,  4.08it/s][A

training k-probes:  75%|███████▌  | 195/260 [00:44<00:15,  4.23it/s][A

training k-probes:  75%|███████▌  | 196/260 [00:44<00:14,  4.30it/s][A

training k-probes:  76%|███████▌  | 197/260 [00:44<00:14,  4.46it/s][A

training k-probes:  76%|███████▌  | 198/260 [00:45<00:13,  4.49it/s][A

training k-probes:  77%|███████▋  | 199/260 [00:45<00:14,  4.33it/s][A

training k-probes:  77%|███████▋  | 200/260 [00:45<00:14,  4.24it/s][A

training k-probes:  77%|███████▋  | 201/260 [00:45<00:13,  4.39it/s][A

training k-probes:  78%|███████▊  | 202/260 [00:46<00:13,  4.26it/s][A

training k-probes:  78%|███████▊  | 203/260 [00:46<00:13,  4.34it/s][A

training k-probes:  78%|███████▊  | 204/260 [00:46<00:13,  4.26it/s][A

training k-probes:  79%|███████▉  | 205/260 [00:46<00:12,  4.35it/s][A

training k-probes:  79%|███████▉  | 206/260 [00:47<00:13,  4.07it/s][A

training k-probes:  80%|███████▉  | 207/260 [00:47<00:14,  3.55it/s][A

training k-probes:  80%|████████  | 208/260 [00:47<00:15,  3.35it/s][A

training k-probes:  80%|████████  | 209/260 [00:48<00:14,  3.53it/s][A

training k-probes:  81%|████████  | 210/260 [00:48<00:13,  3.64it/s][A

training k-probes:  81%|████████  | 211/260 [00:48<00:13,  3.66it/s][A

training k-probes:  82%|████████▏ | 212/260 [00:48<00:12,  3.89it/s][A

training k-probes:  82%|████████▏ | 213/260 [00:49<00:12,  3.84it/s][A

training k-probes:  82%|████████▏ | 214/260 [00:49<00:12,  3.80it/s][A

training k-probes:  83%|████████▎ | 215/260 [00:49<00:11,  3.97it/s][A

training k-probes:  83%|████████▎ | 216/260 [00:49<00:10,  4.16it/s][A

training k-probes:  83%|████████▎ | 217/260 [00:49<00:10,  4.02it/s][A

training k-probes:  84%|████████▍ | 218/260 [00:50<00:10,  3.88it/s][A

training k-probes:  84%|████████▍ | 219/260 [00:50<00:10,  3.83it/s][A

training k-probes:  85%|████████▍ | 220/260 [00:50<00:10,  3.94it/s][A

training k-probes:  85%|████████▌ | 221/260 [00:51<00:09,  4.05it/s][A

training k-probes:  85%|████████▌ | 222/260 [00:51<00:09,  4.13it/s][A

training k-probes:  86%|████████▌ | 223/260 [00:51<00:08,  4.23it/s][A

training k-probes:  86%|████████▌ | 224/260 [00:51<00:09,  3.86it/s][A

training k-probes:  87%|████████▋ | 225/260 [00:52<00:09,  3.81it/s][A

training k-probes:  87%|████████▋ | 226/260 [00:52<00:08,  3.83it/s][A

training k-probes:  87%|████████▋ | 227/260 [00:52<00:08,  4.00it/s][A

training k-probes:  88%|████████▊ | 228/260 [00:52<00:08,  3.94it/s][A

training k-probes:  88%|████████▊ | 229/260 [00:53<00:07,  3.96it/s][A

training k-probes:  88%|████████▊ | 230/260 [00:53<00:07,  3.96it/s][A

training k-probes:  89%|████████▉ | 231/260 [00:53<00:07,  4.06it/s][A

training k-probes:  89%|████████▉ | 232/260 [00:53<00:07,  3.78it/s][A

training k-probes:  90%|████████▉ | 233/260 [00:54<00:07,  3.70it/s][A

training k-probes:  90%|█████████ | 234/260 [00:54<00:07,  3.41it/s][A

training k-probes:  90%|█████████ | 235/260 [00:54<00:07,  3.56it/s][A

training k-probes:  91%|█████████ | 236/260 [00:54<00:06,  3.59it/s][A

training k-probes:  91%|█████████ | 237/260 [00:55<00:06,  3.58it/s][A

training k-probes:  92%|█████████▏| 238/260 [00:55<00:05,  3.82it/s][A

training k-probes:  92%|█████████▏| 239/260 [00:55<00:06,  3.41it/s][A

training k-probes:  92%|█████████▏| 240/260 [00:56<00:05,  3.39it/s][A

training k-probes:  93%|█████████▎| 241/260 [00:56<00:05,  3.65it/s][A

training k-probes:  93%|█████████▎| 242/260 [00:56<00:04,  3.91it/s][A

training k-probes:  93%|█████████▎| 243/260 [00:56<00:04,  3.73it/s][A

training k-probes:  94%|█████████▍| 244/260 [00:57<00:04,  3.67it/s][A

training k-probes:  94%|█████████▍| 245/260 [00:57<00:04,  3.54it/s][A

training k-probes:  95%|█████████▍| 246/260 [00:57<00:03,  3.72it/s][A

training k-probes:  95%|█████████▌| 247/260 [00:57<00:03,  3.89it/s][A

training k-probes:  95%|█████████▌| 248/260 [00:58<00:02,  4.02it/s][A

training k-probes:  96%|█████████▌| 249/260 [00:58<00:02,  4.19it/s][A

training k-probes:  96%|█████████▌| 250/260 [00:58<00:02,  4.24it/s][A

training k-probes:  97%|█████████▋| 251/260 [00:58<00:02,  3.94it/s][A

training k-probes:  97%|█████████▋| 252/260 [00:59<00:02,  3.93it/s][A

training k-probes:  97%|█████████▋| 253/260 [00:59<00:01,  4.09it/s][A

training k-probes:  98%|█████████▊| 254/260 [00:59<00:01,  3.96it/s][A

training k-probes:  98%|█████████▊| 255/260 [00:59<00:01,  3.98it/s][A

training k-probes:  98%|█████████▊| 256/260 [01:00<00:01,  3.43it/s][A

training k-probes:  99%|█████████▉| 257/260 [01:00<00:00,  3.62it/s][A

training k-probes:  99%|█████████▉| 258/260 [01:00<00:00,  3.43it/s][A

training k-probes: 100%|█████████▉| 259/260 [01:01<00:00,  3.41it/s][A

training k-probes: 100%|██████████| 260/260 [01:01<00:00,  2.83it/s][A
training k-probes: 100%|██████████| 260/260 [01:01<00:00,  4.22it/s]


  0%|          | 0/33804 [00:00<?, ?it/s][A

  0%|          | 9/33804 [00:00<06:34, 85.70it/s][A

  0%|          | 27/33804 [00:00<04:05, 137.68it/s][A

  0%|          | 45/33804 [00:00<03:39, 153.79it/s][A

  0%|          | 61/33804 [00:00<03:44, 150.14it/s][A

  0%|          | 78/33804 [00:00<03:35, 156.72it/s][A

  0%|          | 96/33804 [00:00<03:28, 161.85it/s][A

  0%|          | 113/33804 [00:00<04:06, 136.83it/s][A

  0%|          | 128/33804 [00:00<04:23, 127.90it/s][A

  0%|          | 146/33804 [00:01<04:00, 140.13it/s][A

  0%|          | 164/33804 [00:01<03:46, 148.61it/s][A

  1%|          | 182/33804 [00:01<03:36, 155.10it/s][A

  1%|          | 199/33804 [00:01<03:31, 158.91it/s][A

  1%|          | 217/33804 [00:01<03:26, 162.39it/s][A

  1%|          | 235/33804 [00:01<03:23, 164.77it/s][A

  1%|          | 253/33804 [00:01<03:21, 166.52it/s][A

  1%|          | 271/33804 [00:01<03:18, 168.53it/s][A

  1%|          | 289/33804 [00:01<03:17, 169.64it/s][A

  1%|          | 307/33804 [00:01<03:16, 170.79it/s][A

  1%|          | 325/33804 [00:02<03:16, 170.47it/s][A

  1%|          | 343/33804 [00:02<03:16, 170.62it/s][A

  1%|          | 361/33804 [00:02<03:16, 170.61it/s][A

  1%|          | 379/33804 [00:02<03:15, 170.82it/s][A

  1%|          | 397/33804 [00:02<03:15, 170.72it/s][A

  1%|          | 415/33804 [00:02<03:16, 170.17it/s][A

  1%|▏         | 433/33804 [00:02<03:15, 170.37it/s][A

  1%|▏         | 451/33804 [00:02<03:16, 169.97it/s][A

  1%|▏         | 468/33804 [00:02<03:16, 169.67it/s][A

  1%|▏         | 485/33804 [00:03<05:04, 109.40it/s][A

  1%|▏         | 503/33804 [00:03<04:30, 123.11it/s][A

  2%|▏         | 521/33804 [00:03<04:07, 134.60it/s][A

  2%|▏         | 539/33804 [00:03<03:50, 144.26it/s][A

  2%|▏         | 557/33804 [00:03<03:39, 151.30it/s][A

  2%|▏         | 574/33804 [00:03<03:32, 156.21it/s][A

  2%|▏         | 591/33804 [00:03<03:27, 159.96it/s][A

  2%|▏         | 609/33804 [00:03<03:23, 162.93it/s][A

  2%|▏         | 627/33804 [00:04<03:20, 165.10it/s][A

  2%|▏         | 644/33804 [00:04<03:33, 155.28it/s][A

  2%|▏         | 660/33804 [00:04<03:48, 144.78it/s][A

  2%|▏         | 677/33804 [00:04<03:39, 151.18it/s][A

  2%|▏         | 695/33804 [00:04<03:31, 156.79it/s][A

  2%|▏         | 712/33804 [00:04<03:26, 159.96it/s][A

  2%|▏         | 730/33804 [00:04<03:22, 163.27it/s][A

  2%|▏         | 747/33804 [00:04<03:20, 164.95it/s][A

  2%|▏         | 764/33804 [00:04<03:18, 166.20it/s][A

  2%|▏         | 781/33804 [00:05<04:04, 135.20it/s][A

  2%|▏         | 796/33804 [00:05<04:08, 132.75it/s][A

  2%|▏         | 813/33804 [00:05<03:54, 140.71it/s][A

  2%|▏         | 828/33804 [00:05<03:53, 141.18it/s][A

  3%|▎         | 846/33804 [00:05<03:40, 149.55it/s][A

  3%|▎         | 863/33804 [00:05<03:32, 155.04it/s][A

  3%|▎         | 880/33804 [00:05<03:26, 159.06it/s][A

  3%|▎         | 897/33804 [00:05<03:23, 161.90it/s][A

  3%|▎         | 915/33804 [00:05<03:20, 164.42it/s][A

  3%|▎         | 932/33804 [00:06<03:18, 165.68it/s][A

  3%|▎         | 950/33804 [00:06<03:16, 167.20it/s][A

  3%|▎         | 967/33804 [00:06<03:15, 167.74it/s][A

  3%|▎         | 984/33804 [00:06<03:15, 168.28it/s][A

  3%|▎         | 1002/33804 [00:06<03:13, 169.10it/s][A

  3%|▎         | 1020/33804 [00:06<03:13, 169.70it/s][A

  3%|▎         | 1038/33804 [00:06<03:12, 170.57it/s][A

  3%|▎         | 1056/33804 [00:06<03:11, 170.61it/s][A

  3%|▎         | 1074/33804 [00:06<03:11, 170.76it/s][A

  3%|▎         | 1092/33804 [00:06<03:12, 170.23it/s][A

  3%|▎         | 1110/33804 [00:07<03:11, 170.57it/s][A

  3%|▎         | 1128/33804 [00:07<03:12, 169.96it/s][A

  3%|▎         | 1145/33804 [00:07<03:12, 169.67it/s][A

  3%|▎         | 1162/33804 [00:07<03:12, 169.70it/s][A

  3%|▎         | 1179/33804 [00:07<05:18, 102.49it/s][A

  4%|▎         | 1197/33804 [00:07<04:39, 116.79it/s][A

  4%|▎         | 1214/33804 [00:07<04:13, 128.41it/s][A

  4%|▎         | 1232/33804 [00:08<03:54, 138.96it/s][A

  4%|▎         | 1249/33804 [00:08<03:41, 146.74it/s][A

  4%|▎         | 1267/33804 [00:08<03:31, 153.57it/s][A

  4%|▍         | 1284/33804 [00:08<03:25, 158.02it/s][A

  4%|▍         | 1302/33804 [00:08<03:20, 161.72it/s][A

  4%|▍         | 1319/33804 [00:08<03:18, 163.78it/s][A

  4%|▍         | 1336/33804 [00:08<03:16, 165.33it/s][A

  4%|▍         | 1354/33804 [00:08<03:14, 166.90it/s][A

  4%|▍         | 1371/33804 [00:08<03:13, 167.49it/s][A

  4%|▍         | 1388/33804 [00:08<03:12, 168.09it/s][A

  4%|▍         | 1405/33804 [00:09<03:12, 168.00it/s][A

  4%|▍         | 1422/33804 [00:09<03:12, 168.18it/s][A

  4%|▍         | 1439/33804 [00:09<03:42, 145.57it/s][A

  4%|▍         | 1455/33804 [00:09<04:04, 132.19it/s][A

  4%|▍         | 1472/33804 [00:09<03:48, 141.63it/s][A

  4%|▍         | 1490/33804 [00:09<03:35, 149.61it/s][A

  4%|▍         | 1507/33804 [00:09<03:28, 155.13it/s][A

  5%|▍         | 1525/33804 [00:09<03:22, 159.13it/s][A

  5%|▍         | 1543/33804 [00:09<03:18, 162.51it/s][A

  5%|▍         | 1560/33804 [00:10<03:16, 164.44it/s][A

  5%|▍         | 1578/33804 [00:10<03:13, 166.42it/s][A

  5%|▍         | 1595/33804 [00:10<03:12, 167.14it/s][A

  5%|▍         | 1613/33804 [00:10<03:11, 168.29it/s][A

  5%|▍         | 1630/33804 [00:10<03:12, 167.25it/s][A

  5%|▍         | 1647/33804 [00:10<03:13, 166.43it/s][A

  5%|▍         | 1664/33804 [00:10<03:13, 165.87it/s][A

  5%|▍         | 1681/33804 [00:10<03:12, 166.93it/s][A

  5%|▌         | 1698/33804 [00:10<03:12, 167.11it/s][A

  5%|▌         | 1715/33804 [00:10<03:11, 167.64it/s][A

  5%|▌         | 1732/33804 [00:11<03:10, 168.32it/s][A

  5%|▌         | 1749/33804 [00:11<03:10, 168.60it/s][A

  5%|▌         | 1767/33804 [00:11<03:09, 169.25it/s][A

  5%|▌         | 1784/33804 [00:11<03:08, 169.47it/s][A

  5%|▌         | 1802/33804 [00:11<03:08, 169.90it/s][A

  5%|▌         | 1819/33804 [00:11<03:08, 169.78it/s][A

  5%|▌         | 1837/33804 [00:11<03:08, 170.03it/s][A

  5%|▌         | 1855/33804 [00:11<03:08, 169.83it/s][A

  6%|▌         | 1873/33804 [00:11<03:07, 170.17it/s][A

  6%|▌         | 1891/33804 [00:12<03:07, 169.91it/s][A

  6%|▌         | 1908/33804 [00:12<03:08, 169.16it/s][A

  6%|▌         | 1926/33804 [00:12<03:07, 169.71it/s][A

  6%|▌         | 1943/33804 [00:12<03:08, 169.09it/s][A

  6%|▌         | 1961/33804 [00:12<03:07, 169.53it/s][A

  6%|▌         | 1978/33804 [00:12<03:08, 169.26it/s][A

  6%|▌         | 1996/33804 [00:12<03:07, 169.72it/s][A

  6%|▌         | 2013/33804 [00:12<03:07, 169.62it/s][A

  6%|▌         | 2031/33804 [00:12<03:06, 169.98it/s][A

  6%|▌         | 2048/33804 [00:13<05:34, 95.03it/s] [A

  6%|▌         | 2065/33804 [00:13<04:50, 109.12it/s][A

  6%|▌         | 2082/33804 [00:13<04:20, 121.92it/s][A

  6%|▌         | 2098/33804 [00:13<04:34, 115.56it/s][A

  6%|▌         | 2112/33804 [00:13<04:40, 112.80it/s][A

  6%|▋         | 2129/33804 [00:13<04:11, 125.95it/s][A

  6%|▋         | 2146/33804 [00:13<03:51, 136.64it/s][A

  6%|▋         | 2163/33804 [00:14<03:38, 144.98it/s][A

  6%|▋         | 2180/33804 [00:14<03:28, 151.70it/s][A

  6%|▋         | 2197/33804 [00:14<03:21, 156.73it/s][A

  7%|▋         | 2215/33804 [00:14<03:16, 160.79it/s][A

  7%|▋         | 2232/33804 [00:14<03:13, 163.32it/s][A

  7%|▋         | 2250/33804 [00:14<03:10, 165.56it/s][A

  7%|▋         | 2267/33804 [00:14<03:09, 166.54it/s][A

  7%|▋         | 2284/33804 [00:14<03:08, 167.54it/s][A

  7%|▋         | 2301/33804 [00:14<03:07, 168.05it/s][A

  7%|▋         | 2318/33804 [00:14<03:06, 168.53it/s][A

  7%|▋         | 2335/33804 [00:15<03:06, 168.75it/s][A

  7%|▋         | 2352/33804 [00:15<03:06, 168.38it/s][A

  7%|▋         | 2370/33804 [00:15<03:05, 169.05it/s][A

  7%|▋         | 2387/33804 [00:15<03:06, 168.69it/s][A

  7%|▋         | 2405/33804 [00:15<03:05, 169.25it/s][A

  7%|▋         | 2422/33804 [00:15<03:05, 168.95it/s][A

  7%|▋         | 2440/33804 [00:15<03:05, 169.50it/s][A

  7%|▋         | 2457/33804 [00:15<03:04, 169.53it/s][A

  7%|▋         | 2475/33804 [00:15<03:04, 170.08it/s][A

  7%|▋         | 2493/33804 [00:15<03:04, 169.81it/s][A

  7%|▋         | 2510/33804 [00:16<03:04, 169.75it/s][A

  7%|▋         | 2527/33804 [00:16<03:04, 169.67it/s][A

  8%|▊         | 2544/33804 [00:16<03:04, 169.56it/s][A

  8%|▊         | 2562/33804 [00:16<03:03, 169.92it/s][A

  8%|▊         | 2579/33804 [00:16<03:03, 169.72it/s][A

  8%|▊         | 2596/33804 [00:16<03:04, 169.38it/s][A

  8%|▊         | 2613/33804 [00:16<03:04, 169.23it/s][A

  8%|▊         | 2631/33804 [00:16<03:03, 169.69it/s][A

  8%|▊         | 2648/33804 [00:16<03:04, 169.20it/s][A

  8%|▊         | 2666/33804 [00:16<03:03, 169.62it/s][A

  8%|▊         | 2683/33804 [00:17<03:03, 169.49it/s][A

  8%|▊         | 2700/33804 [00:17<03:03, 169.47it/s][A

  8%|▊         | 2718/33804 [00:17<03:02, 169.97it/s][A

  8%|▊         | 2735/33804 [00:17<03:02, 169.92it/s][A

  8%|▊         | 2753/33804 [00:17<03:02, 170.43it/s][A

  8%|▊         | 2771/33804 [00:17<03:02, 170.18it/s][A

  8%|▊         | 2789/33804 [00:17<03:02, 170.31it/s][A

  8%|▊         | 2807/33804 [00:17<03:47, 136.35it/s][A

  8%|▊         | 2822/33804 [00:17<03:49, 135.09it/s][A

  8%|▊         | 2839/33804 [00:18<03:39, 141.26it/s][A

  8%|▊         | 2856/33804 [00:18<03:30, 147.01it/s][A

  8%|▊         | 2873/33804 [00:18<03:22, 152.87it/s][A

  9%|▊         | 2890/33804 [00:18<03:16, 157.31it/s][A

  9%|▊         | 2908/33804 [00:18<03:11, 161.11it/s][A

  9%|▊         | 2925/33804 [00:18<03:08, 163.42it/s][A

  9%|▊         | 2943/33804 [00:18<03:06, 165.88it/s][A

  9%|▉         | 2960/33804 [00:18<03:04, 167.06it/s][A

  9%|▉         | 2978/33804 [00:18<03:03, 168.32it/s][A

  9%|▉         | 2995/33804 [00:19<03:03, 167.94it/s][A

  9%|▉         | 3013/33804 [00:19<03:02, 168.76it/s][A

  9%|▉         | 3030/33804 [00:19<03:02, 168.91it/s][A

  9%|▉         | 3048/33804 [00:19<03:01, 169.51it/s][A

  9%|▉         | 3065/33804 [00:19<03:01, 169.52it/s][A

  9%|▉         | 3082/33804 [00:19<03:01, 169.39it/s][A

  9%|▉         | 3099/33804 [00:19<03:01, 169.46it/s][A

  9%|▉         | 3116/33804 [00:20<05:52, 87.10it/s] [A

  9%|▉         | 3133/33804 [00:20<05:01, 101.87it/s][A

  9%|▉         | 3150/33804 [00:20<04:25, 115.67it/s][A

  9%|▉         | 3168/33804 [00:20<03:58, 128.64it/s][A

  9%|▉         | 3185/33804 [00:20<03:41, 138.39it/s][A

  9%|▉         | 3203/33804 [00:20<03:28, 147.00it/s][A

 10%|▉         | 3220/33804 [00:20<03:25, 148.71it/s][A

 10%|▉         | 3237/33804 [00:20<03:23, 149.85it/s][A

 10%|▉         | 3254/33804 [00:20<03:18, 154.13it/s][A

 10%|▉         | 3271/33804 [00:21<03:16, 155.55it/s][A

 10%|▉         | 3288/33804 [00:21<03:12, 158.73it/s][A

 10%|▉         | 3305/33804 [00:21<03:08, 161.47it/s][A

 10%|▉         | 3322/33804 [00:21<03:06, 163.84it/s][A

 10%|▉         | 3339/33804 [00:21<03:04, 165.36it/s][A

 10%|▉         | 3357/33804 [00:21<03:02, 166.95it/s][A

 10%|▉         | 3374/33804 [00:21<03:01, 167.72it/s][A

 10%|█         | 3392/33804 [00:21<03:00, 168.87it/s][A

 10%|█         | 3409/33804 [00:21<02:59, 169.13it/s][A

 10%|█         | 3427/33804 [00:21<02:59, 169.61it/s][A

 10%|█         | 3444/33804 [00:22<03:32, 142.97it/s][A

 10%|█         | 3459/33804 [00:22<03:49, 131.98it/s][A

 10%|█         | 3476/33804 [00:22<03:34, 141.31it/s][A

 10%|█         | 3493/33804 [00:22<03:23, 148.88it/s][A

 10%|█         | 3510/33804 [00:22<03:16, 154.25it/s][A

 10%|█         | 3527/33804 [00:22<03:11, 158.07it/s][A

 10%|█         | 3545/33804 [00:22<03:07, 161.69it/s][A

 11%|█         | 3562/33804 [00:22<03:05, 163.37it/s][A

 11%|█         | 3579/33804 [00:22<03:03, 165.16it/s][A

 11%|█         | 3596/33804 [00:23<03:01, 166.15it/s][A

 11%|█         | 3614/33804 [00:23<03:00, 167.54it/s][A

 11%|█         | 3631/33804 [00:23<02:59, 168.22it/s][A

 11%|█         | 3649/33804 [00:23<02:58, 169.12it/s][A

 11%|█         | 3666/33804 [00:23<02:58, 169.14it/s][A

 11%|█         | 3684/33804 [00:23<02:57, 169.72it/s][A

 11%|█         | 3701/33804 [00:23<02:57, 169.40it/s][A

 11%|█         | 3718/33804 [00:23<02:57, 169.41it/s][A

 11%|█         | 3736/33804 [00:23<02:57, 169.82it/s][A

 11%|█         | 3753/33804 [00:23<02:57, 169.33it/s][A

 11%|█         | 3770/33804 [00:24<02:57, 169.15it/s][A

 11%|█         | 3787/33804 [00:24<02:57, 168.86it/s][A

 11%|█▏        | 3804/33804 [00:24<02:57, 169.15it/s][A

 11%|█▏        | 3821/33804 [00:24<02:57, 168.94it/s][A

 11%|█▏        | 3839/33804 [00:24<02:56, 169.49it/s][A

 11%|█▏        | 3856/33804 [00:24<02:56, 169.30it/s][A

 11%|█▏        | 3874/33804 [00:24<02:56, 169.57it/s][A

 12%|█▏        | 3891/33804 [00:24<02:56, 169.58it/s][A

 12%|█▏        | 3908/33804 [00:24<02:56, 169.55it/s][A

 12%|█▏        | 3926/33804 [00:24<02:55, 170.14it/s][A

 12%|█▏        | 3944/33804 [00:25<02:55, 169.79it/s][A

 12%|█▏        | 3962/33804 [00:25<02:55, 169.90it/s][A

 12%|█▏        | 3979/33804 [00:25<02:56, 169.40it/s][A

 12%|█▏        | 3997/33804 [00:25<02:55, 169.83it/s][A

 12%|█▏        | 4014/33804 [00:25<02:57, 167.98it/s][A

 12%|█▏        | 4031/33804 [00:25<02:56, 168.40it/s][A

 12%|█▏        | 4048/33804 [00:25<02:56, 168.11it/s][A

 12%|█▏        | 4065/33804 [00:25<02:56, 168.06it/s][A

 12%|█▏        | 4083/33804 [00:25<02:56, 168.78it/s][A

 12%|█▏        | 4100/33804 [00:26<02:56, 168.75it/s][A

 12%|█▏        | 4118/33804 [00:26<02:55, 169.24it/s][A

 12%|█▏        | 4135/33804 [00:26<02:55, 169.30it/s][A

 12%|█▏        | 4152/33804 [00:26<03:39, 135.16it/s][A

 12%|█▏        | 4167/33804 [00:26<03:37, 136.06it/s][A

 12%|█▏        | 4184/33804 [00:26<03:24, 144.87it/s][A

 12%|█▏        | 4201/33804 [00:26<03:15, 151.22it/s][A

 12%|█▏        | 4219/33804 [00:26<03:08, 156.87it/s][A

 13%|█▎        | 4236/33804 [00:26<03:04, 160.29it/s][A

 13%|█▎        | 4254/33804 [00:27<03:00, 163.43it/s][A

 13%|█▎        | 4271/33804 [00:27<02:59, 164.68it/s][A

 13%|█▎        | 4288/33804 [00:27<02:57, 165.83it/s][A

 13%|█▎        | 4305/33804 [00:27<02:56, 166.93it/s][A

 13%|█▎        | 4322/33804 [00:27<02:55, 167.68it/s][A

 13%|█▎        | 4340/33804 [00:27<02:54, 168.51it/s][A

 13%|█▎        | 4357/33804 [00:27<02:54, 168.73it/s][A

 13%|█▎        | 4375/33804 [00:27<02:53, 169.22it/s][A

 13%|█▎        | 4392/33804 [00:27<02:54, 169.00it/s][A

 13%|█▎        | 4409/33804 [00:27<02:54, 168.77it/s][A

 13%|█▎        | 4426/33804 [00:28<02:53, 169.01it/s][A

 13%|█▎        | 4443/33804 [00:28<02:53, 169.23it/s][A

 13%|█▎        | 4460/33804 [00:28<02:53, 169.40it/s][A

 13%|█▎        | 4477/33804 [00:28<02:53, 169.38it/s][A

 13%|█▎        | 4494/33804 [00:28<06:18, 77.46it/s] [A

 13%|█▎        | 4511/33804 [00:28<05:16, 92.49it/s][A

 13%|█▎        | 4529/33804 [00:29<04:31, 107.86it/s][A

 13%|█▎        | 4546/33804 [00:29<04:02, 120.83it/s][A

 13%|█▎        | 4563/33804 [00:29<03:42, 131.61it/s][A

 14%|█▎        | 4580/33804 [00:29<03:27, 141.00it/s][A

 14%|█▎        | 4598/33804 [00:29<03:16, 148.96it/s][A

 14%|█▎        | 4615/33804 [00:29<03:09, 154.36it/s][A

 14%|█▎        | 4633/33804 [00:29<03:03, 159.01it/s][A

 14%|█▍        | 4650/33804 [00:29<03:00, 161.60it/s][A

 14%|█▍        | 4668/33804 [00:29<02:57, 164.23it/s][A

 14%|█▍        | 4685/33804 [00:29<02:55, 165.66it/s][A

 14%|█▍        | 4702/33804 [00:30<02:55, 166.24it/s][A

 14%|█▍        | 4719/33804 [00:30<02:53, 167.22it/s][A

 14%|█▍        | 4736/33804 [00:30<02:53, 167.53it/s][A

 14%|█▍        | 4754/33804 [00:30<02:52, 168.39it/s][A

 14%|█▍        | 4771/33804 [00:30<02:52, 168.45it/s][A

 14%|█▍        | 4788/33804 [00:30<04:37, 104.74it/s][A

 14%|█▍        | 4802/33804 [00:30<04:27, 108.56it/s][A

 14%|█▍        | 4820/33804 [00:31<03:55, 122.88it/s][A

 14%|█▍        | 4837/33804 [00:31<03:36, 133.82it/s][A

 14%|█▍        | 4854/33804 [00:31<03:22, 142.88it/s][A

 14%|█▍        | 4871/33804 [00:31<03:12, 149.99it/s][A

 14%|█▍        | 4889/33804 [00:31<03:05, 155.98it/s][A

 15%|█▍        | 4906/33804 [00:31<03:01, 159.57it/s][A

 15%|█▍        | 4923/33804 [00:31<02:57, 162.26it/s][A

 15%|█▍        | 4941/33804 [00:31<02:55, 164.77it/s][A

 15%|█▍        | 4958/33804 [00:31<02:54, 165.72it/s][A

 15%|█▍        | 4976/33804 [00:31<02:52, 167.12it/s][A

 15%|█▍        | 4993/33804 [00:32<02:51, 167.72it/s][A

 15%|█▍        | 5011/33804 [00:32<02:50, 168.48it/s][A

 15%|█▍        | 5028/33804 [00:32<02:50, 168.70it/s][A

 15%|█▍        | 5046/33804 [00:32<02:49, 169.50it/s][A

 15%|█▍        | 5064/33804 [00:32<02:49, 169.67it/s][A

 15%|█▌        | 5081/33804 [00:32<02:49, 169.53it/s][A

 15%|█▌        | 5099/33804 [00:32<02:49, 169.71it/s][A

 15%|█▌        | 5116/33804 [00:32<02:49, 169.40it/s][A

 15%|█▌        | 5134/33804 [00:32<02:48, 169.69it/s][A

 15%|█▌        | 5151/33804 [00:32<02:49, 169.11it/s][A

 15%|█▌        | 5168/33804 [00:33<02:49, 168.61it/s][A

 15%|█▌        | 5185/33804 [00:33<02:50, 168.19it/s][A

 15%|█▌        | 5202/33804 [00:33<02:49, 168.61it/s][A

 15%|█▌        | 5219/33804 [00:33<02:49, 168.42it/s][A

 15%|█▌        | 5236/33804 [00:33<02:49, 168.86it/s][A

 16%|█▌        | 5253/33804 [00:33<02:49, 168.88it/s][A

 16%|█▌        | 5271/33804 [00:33<02:48, 169.09it/s][A

 16%|█▌        | 5289/33804 [00:33<02:48, 169.64it/s][A

 16%|█▌        | 5307/33804 [00:33<02:47, 169.88it/s][A

 16%|█▌        | 5324/33804 [00:33<02:47, 169.81it/s][A

 16%|█▌        | 5341/33804 [00:34<02:48, 169.28it/s][A

 16%|█▌        | 5358/33804 [00:34<02:48, 169.12it/s][A

 16%|█▌        | 5375/33804 [00:34<02:48, 169.08it/s][A

 16%|█▌        | 5392/33804 [00:34<02:47, 169.30it/s][A

 16%|█▌        | 5409/33804 [00:34<02:48, 169.01it/s][A

 16%|█▌        | 5426/33804 [00:34<02:47, 169.10it/s][A

 16%|█▌        | 5443/33804 [00:34<02:48, 168.52it/s][A

 16%|█▌        | 5460/33804 [00:34<02:48, 168.69it/s][A

 16%|█▌        | 5477/33804 [00:34<02:47, 168.61it/s][A

 16%|█▋        | 5494/33804 [00:35<03:31, 134.12it/s][A

 16%|█▋        | 5509/33804 [00:35<03:31, 133.85it/s][A

 16%|█▋        | 5526/33804 [00:35<03:17, 142.90it/s][A

 16%|█▋        | 5543/33804 [00:35<03:08, 150.06it/s][A

 16%|█▋        | 5560/33804 [00:35<03:02, 154.59it/s][A

 17%|█▋        | 5578/33804 [00:35<02:57, 159.10it/s][A

 17%|█▋        | 5595/33804 [00:35<02:54, 161.52it/s][A

 17%|█▋        | 5613/33804 [00:35<02:51, 164.11it/s][A

 17%|█▋        | 5630/33804 [00:35<02:50, 165.33it/s][A

 17%|█▋        | 5647/33804 [00:36<02:49, 166.07it/s][A

 17%|█▋        | 5664/33804 [00:36<02:48, 166.68it/s][A

 17%|█▋        | 5681/33804 [00:36<02:48, 167.39it/s][A

 17%|█▋        | 5698/33804 [00:36<02:47, 167.40it/s][A

 17%|█▋        | 5715/33804 [00:36<02:47, 167.67it/s][A

 17%|█▋        | 5732/33804 [00:36<02:46, 168.32it/s][A

 17%|█▋        | 5749/33804 [00:36<02:46, 168.46it/s][A

 17%|█▋        | 5766/33804 [00:36<02:46, 168.05it/s][A

 17%|█▋        | 5783/33804 [00:36<02:46, 168.16it/s][A

 17%|█▋        | 5800/33804 [00:36<02:46, 168.63it/s][A

 17%|█▋        | 5817/33804 [00:37<02:46, 167.94it/s][A

 17%|█▋        | 5834/33804 [00:37<02:46, 168.42it/s][A

 17%|█▋        | 5851/33804 [00:37<02:46, 168.18it/s][A

 17%|█▋        | 5869/33804 [00:37<02:45, 168.78it/s][A

 17%|█▋        | 5886/33804 [00:37<02:45, 168.65it/s][A

 17%|█▋        | 5904/33804 [00:37<02:44, 169.32it/s][A

 18%|█▊        | 5921/33804 [00:37<02:45, 168.63it/s][A

 18%|█▊        | 5938/33804 [00:37<02:45, 168.44it/s][A

 18%|█▊        | 5955/33804 [00:37<02:45, 168.61it/s][A

 18%|█▊        | 5972/33804 [00:37<02:45, 168.38it/s][A

 18%|█▊        | 5989/33804 [00:38<02:44, 168.68it/s][A

 18%|█▊        | 6006/33804 [00:38<02:44, 168.70it/s][A

 18%|█▊        | 6023/33804 [00:38<02:44, 169.06it/s][A

 18%|█▊        | 6040/33804 [00:38<02:44, 169.07it/s][A

 18%|█▊        | 6057/33804 [00:38<02:43, 169.29it/s][A

 18%|█▊        | 6074/33804 [00:38<02:43, 169.15it/s][A

 18%|█▊        | 6091/33804 [00:38<02:43, 169.14it/s][A

 18%|█▊        | 6108/33804 [00:38<02:43, 168.89it/s][A

 18%|█▊        | 6125/33804 [00:38<02:43, 169.18it/s][A

 18%|█▊        | 6142/33804 [00:38<02:43, 168.99it/s][A

 18%|█▊        | 6159/33804 [00:39<02:44, 168.37it/s][A

 18%|█▊        | 6176/33804 [00:39<07:41, 59.89it/s] [A

 18%|█▊        | 6193/33804 [00:39<06:11, 74.24it/s][A

 18%|█▊        | 6210/33804 [00:39<05:08, 89.32it/s][A

 18%|█▊        | 6227/33804 [00:40<04:25, 103.94it/s][A

 18%|█▊        | 6244/33804 [00:40<03:54, 117.41it/s][A

 19%|█▊        | 6261/33804 [00:40<03:33, 129.18it/s][A

 19%|█▊        | 6279/33804 [00:40<03:16, 139.72it/s][A

 19%|█▊        | 6296/33804 [00:40<03:08, 146.30it/s][A

 19%|█▊        | 6313/33804 [00:40<03:00, 152.25it/s][A

 19%|█▊        | 6330/33804 [00:40<02:55, 156.50it/s][A

 19%|█▉        | 6347/33804 [00:40<02:51, 159.97it/s][A

 19%|█▉        | 6364/33804 [00:40<02:49, 162.16it/s][A

 19%|█▉        | 6381/33804 [00:40<02:47, 164.04it/s][A

 19%|█▉        | 6398/33804 [00:41<02:45, 165.41it/s][A

 19%|█▉        | 6415/33804 [00:41<02:44, 166.30it/s][A

 19%|█▉        | 6432/33804 [00:41<02:43, 167.24it/s][A

 19%|█▉        | 6449/33804 [00:41<02:43, 167.79it/s][A

 19%|█▉        | 6466/33804 [00:41<02:42, 168.30it/s][A

 19%|█▉        | 6483/33804 [00:41<02:42, 168.26it/s][A

 19%|█▉        | 6500/33804 [00:41<02:41, 168.59it/s][A

 19%|█▉        | 6517/33804 [00:41<02:41, 168.58it/s][A

 19%|█▉        | 6534/33804 [00:41<02:41, 168.98it/s][A

 19%|█▉        | 6551/33804 [00:41<02:41, 168.44it/s][A

 19%|█▉        | 6568/33804 [00:42<02:41, 168.59it/s][A

 19%|█▉        | 6585/33804 [00:42<02:41, 168.26it/s][A

 20%|█▉        | 6602/33804 [00:42<02:41, 168.38it/s][A

 20%|█▉        | 6619/33804 [00:42<02:41, 168.45it/s][A

 20%|█▉        | 6636/33804 [00:42<02:41, 168.37it/s][A

 20%|█▉        | 6654/33804 [00:42<02:40, 168.98it/s][A

 20%|█▉        | 6671/33804 [00:42<02:40, 168.83it/s][A

 20%|█▉        | 6689/33804 [00:42<02:40, 169.29it/s][A

 20%|█▉        | 6706/33804 [00:42<02:40, 169.01it/s][A

 20%|█▉        | 6723/33804 [00:43<02:40, 168.51it/s][A

 20%|█▉        | 6740/33804 [00:43<02:40, 168.47it/s][A

 20%|█▉        | 6757/33804 [00:43<02:40, 168.73it/s][A

 20%|██        | 6774/33804 [00:43<02:40, 168.93it/s][A

 20%|██        | 6791/33804 [00:43<02:39, 169.15it/s][A

 20%|██        | 6808/33804 [00:43<03:19, 135.07it/s][A

 20%|██        | 6823/33804 [00:43<03:23, 132.61it/s][A

 20%|██        | 6840/33804 [00:43<03:10, 141.61it/s][A

 20%|██        | 6857/33804 [00:43<03:00, 148.98it/s][A

 20%|██        | 6874/33804 [00:44<02:54, 153.92it/s][A

 20%|██        | 6891/33804 [00:44<02:51, 157.09it/s][A

 20%|██        | 6908/33804 [00:44<02:47, 160.69it/s][A

 20%|██        | 6925/33804 [00:44<02:45, 162.89it/s][A

 21%|██        | 6942/33804 [00:44<02:43, 164.28it/s][A

 21%|██        | 6959/33804 [00:44<02:42, 165.67it/s][A

 21%|██        | 6976/33804 [00:44<02:40, 166.74it/s][A

 21%|██        | 6993/33804 [00:44<02:40, 166.77it/s][A

 21%|██        | 7010/33804 [00:44<02:39, 167.72it/s][A

 21%|██        | 7027/33804 [00:44<02:39, 167.86it/s][A

 21%|██        | 7044/33804 [00:45<02:39, 168.23it/s][A

 21%|██        | 7061/33804 [00:45<02:39, 167.92it/s][A

 21%|██        | 7079/33804 [00:45<02:38, 168.64it/s][A

 21%|██        | 7096/33804 [00:45<02:39, 167.97it/s][A

 21%|██        | 7113/33804 [00:45<02:38, 168.18it/s][A

 21%|██        | 7130/33804 [00:45<02:38, 168.41it/s][A

 21%|██        | 7147/33804 [00:45<02:38, 168.51it/s][A

 21%|██        | 7164/33804 [00:45<02:37, 168.90it/s][A

 21%|██        | 7181/33804 [00:45<02:37, 168.95it/s][A

 21%|██▏       | 7198/33804 [00:45<02:37, 169.22it/s][A

 21%|██▏       | 7215/33804 [00:46<02:37, 169.04it/s][A

 21%|██▏       | 7232/33804 [00:46<02:37, 169.12it/s][A

 21%|██▏       | 7249/33804 [00:46<02:37, 168.95it/s][A

 21%|██▏       | 7266/33804 [00:46<02:36, 169.19it/s][A

 22%|██▏       | 7283/33804 [00:46<02:36, 168.93it/s][A

 22%|██▏       | 7300/33804 [00:46<02:37, 168.64it/s][A

 22%|██▏       | 7317/33804 [00:46<02:37, 168.14it/s][A

 22%|██▏       | 7334/33804 [00:46<02:37, 167.91it/s][A

 22%|██▏       | 7351/33804 [00:46<02:36, 168.49it/s][A

 22%|██▏       | 7368/33804 [00:46<02:37, 168.37it/s][A

 22%|██▏       | 7385/33804 [00:47<02:36, 168.78it/s][A

 22%|██▏       | 7402/33804 [00:47<02:36, 168.63it/s][A

 22%|██▏       | 7420/33804 [00:47<02:35, 169.14it/s][A

 22%|██▏       | 7437/33804 [00:47<02:36, 168.98it/s][A

 22%|██▏       | 7454/33804 [00:47<02:35, 169.23it/s][A

 22%|██▏       | 7471/33804 [00:47<02:35, 168.89it/s][A

 22%|██▏       | 7488/33804 [00:47<02:35, 168.78it/s][A

 22%|██▏       | 7505/33804 [00:47<03:10, 137.80it/s][A

 22%|██▏       | 7520/33804 [00:47<03:18, 132.43it/s][A

 22%|██▏       | 7537/33804 [00:48<03:05, 141.47it/s][A

 22%|██▏       | 7554/33804 [00:48<02:56, 148.88it/s][A

 22%|██▏       | 7571/33804 [00:48<02:50, 153.81it/s][A

 22%|██▏       | 7588/33804 [00:48<02:46, 157.79it/s][A

 22%|██▏       | 7605/33804 [00:48<02:42, 161.17it/s][A

 23%|██▎       | 7622/33804 [00:48<02:40, 163.31it/s][A

 23%|██▎       | 7640/33804 [00:48<02:38, 165.40it/s][A

 23%|██▎       | 7657/33804 [00:48<02:37, 166.41it/s][A

 23%|██▎       | 7674/33804 [00:48<02:36, 167.45it/s][A

 23%|██▎       | 7691/33804 [00:48<02:35, 167.75it/s][A

 23%|██▎       | 7708/33804 [00:49<02:35, 168.02it/s][A

 23%|██▎       | 7725/33804 [00:49<02:35, 167.93it/s][A

 23%|██▎       | 7743/33804 [00:49<02:34, 168.62it/s][A

 23%|██▎       | 7760/33804 [00:49<02:34, 168.48it/s][A

 23%|██▎       | 7777/33804 [00:49<02:34, 168.57it/s][A

 23%|██▎       | 7794/33804 [00:49<02:34, 168.51it/s][A

 23%|██▎       | 7811/33804 [00:49<02:34, 168.48it/s][A

 23%|██▎       | 7828/33804 [00:49<02:33, 168.75it/s][A

 23%|██▎       | 7845/33804 [00:49<02:33, 168.70it/s][A

 23%|██▎       | 7862/33804 [00:49<02:33, 168.74it/s][A

 23%|██▎       | 7879/33804 [00:50<02:33, 168.79it/s][A

 23%|██▎       | 7896/33804 [00:50<02:33, 169.03it/s][A

 23%|██▎       | 7913/33804 [00:50<02:33, 169.11it/s][A

 23%|██▎       | 7930/33804 [00:50<02:32, 169.23it/s][A

 24%|██▎       | 7947/33804 [00:50<02:33, 168.98it/s][A

 24%|██▎       | 7964/33804 [00:50<02:32, 169.20it/s][A

 24%|██▎       | 7981/33804 [00:50<02:32, 168.86it/s][A

 24%|██▎       | 7998/33804 [00:50<02:32, 168.81it/s][A

 24%|██▎       | 8016/33804 [00:50<02:32, 169.23it/s][A

 24%|██▍       | 8033/33804 [00:50<02:32, 168.87it/s][A

 24%|██▍       | 8050/33804 [00:51<02:32, 169.02it/s][A

 24%|██▍       | 8067/33804 [00:51<02:32, 168.36it/s][A

 24%|██▍       | 8084/33804 [00:51<02:32, 168.83it/s][A

 24%|██▍       | 8101/33804 [00:51<02:33, 167.99it/s][A

 24%|██▍       | 8119/33804 [00:51<02:32, 168.72it/s][A

 24%|██▍       | 8136/33804 [00:51<02:32, 168.67it/s][A

 24%|██▍       | 8154/33804 [00:51<02:31, 169.10it/s][A

 24%|██▍       | 8171/33804 [00:51<02:31, 169.11it/s][A

 24%|██▍       | 8189/33804 [00:51<02:31, 169.04it/s][A

 24%|██▍       | 8206/33804 [00:52<03:00, 141.67it/s][A

 24%|██▍       | 8221/33804 [00:52<03:13, 131.90it/s][A

 24%|██▍       | 8238/33804 [00:52<03:00, 141.42it/s][A

 24%|██▍       | 8255/33804 [00:53<07:24, 57.44it/s] [A

 24%|██▍       | 8272/33804 [00:53<05:55, 71.75it/s][A

 25%|██▍       | 8289/33804 [00:53<04:55, 86.45it/s][A

 25%|██▍       | 8307/33804 [00:53<04:09, 102.20it/s][A

 25%|██▍       | 8324/33804 [00:53<03:40, 115.82it/s][A

 25%|██▍       | 8342/33804 [00:53<03:17, 128.62it/s][A

 25%|██▍       | 8359/33804 [00:53<03:03, 138.30it/s][A

 25%|██▍       | 8377/33804 [00:53<02:53, 146.95it/s][A

 25%|██▍       | 8394/33804 [00:53<02:46, 152.54it/s][A

 25%|██▍       | 8411/33804 [00:53<02:41, 157.28it/s][A

 25%|██▍       | 8428/33804 [00:54<02:38, 159.73it/s][A

 25%|██▍       | 8445/33804 [00:54<02:36, 161.84it/s][A

 25%|██▌       | 8462/33804 [00:54<02:34, 163.92it/s][A

 25%|██▌       | 8479/33804 [00:54<02:33, 165.37it/s][A

 25%|██▌       | 8496/33804 [00:54<02:31, 166.68it/s][A

 25%|██▌       | 8513/33804 [00:54<02:31, 166.74it/s][A

 25%|██▌       | 8531/33804 [00:54<02:30, 167.78it/s][A

 25%|██▌       | 8548/33804 [00:54<02:30, 168.22it/s][A

 25%|██▌       | 8566/33804 [00:54<02:29, 168.85it/s][A

 25%|██▌       | 8583/33804 [00:54<02:29, 168.29it/s][A

 25%|██▌       | 8601/33804 [00:55<02:29, 168.86it/s][A

 25%|██▌       | 8618/33804 [00:55<02:29, 168.23it/s][A

 26%|██▌       | 8635/33804 [00:55<02:29, 168.44it/s][A

 26%|██▌       | 8652/33804 [00:55<02:28, 168.89it/s][A

 26%|██▌       | 8669/33804 [00:55<02:28, 168.84it/s][A

 26%|██▌       | 8686/33804 [00:55<02:28, 168.88it/s][A

 26%|██▌       | 8703/33804 [00:55<02:28, 168.50it/s][A

 26%|██▌       | 8720/33804 [00:55<02:28, 168.75it/s][A

 26%|██▌       | 8737/33804 [00:55<02:28, 168.70it/s][A

 26%|██▌       | 8754/33804 [00:55<02:28, 169.00it/s][A

 26%|██▌       | 8771/33804 [00:56<02:28, 169.04it/s][A

 26%|██▌       | 8789/33804 [00:56<02:27, 169.51it/s][A

 26%|██▌       | 8806/33804 [00:56<03:57, 105.06it/s][A

 26%|██▌       | 8820/33804 [00:56<03:51, 108.13it/s][A

 26%|██▌       | 8837/33804 [00:56<03:25, 121.55it/s][A

 26%|██▌       | 8854/33804 [00:56<03:07, 133.01it/s][A

 26%|██▌       | 8871/33804 [00:56<02:55, 142.09it/s][A

 26%|██▋       | 8888/33804 [00:57<02:47, 148.81it/s][A

 26%|██▋       | 8905/33804 [00:57<02:41, 154.31it/s][A

 26%|██▋       | 8922/33804 [00:57<02:37, 158.05it/s][A

 26%|██▋       | 8939/33804 [00:57<02:34, 161.36it/s][A

 26%|██▋       | 8956/33804 [00:57<02:32, 163.34it/s][A

 27%|██▋       | 8973/33804 [00:57<02:30, 165.17it/s][A

 27%|██▋       | 8990/33804 [00:57<02:29, 166.24it/s][A

 27%|██▋       | 9007/33804 [00:57<02:28, 167.19it/s][A

 27%|██▋       | 9024/33804 [00:57<02:27, 167.72it/s][A

 27%|██▋       | 9041/33804 [00:57<02:27, 168.36it/s][A

 27%|██▋       | 9058/33804 [00:58<02:27, 168.14it/s][A

 27%|██▋       | 9075/33804 [00:58<02:26, 168.32it/s][A

 27%|██▋       | 9092/33804 [00:58<02:26, 168.50it/s][A

 27%|██▋       | 9109/33804 [00:58<02:26, 168.93it/s][A

 27%|██▋       | 9126/33804 [00:58<02:26, 168.57it/s][A

 27%|██▋       | 9143/33804 [00:58<02:26, 168.25it/s][A

 27%|██▋       | 9160/33804 [00:58<02:26, 168.65it/s][A

 27%|██▋       | 9177/33804 [00:58<02:26, 168.26it/s][A

 27%|██▋       | 9194/33804 [00:58<02:26, 168.02it/s][A

 27%|██▋       | 9211/33804 [00:58<02:26, 168.00it/s][A

 27%|██▋       | 9229/33804 [00:59<02:25, 168.50it/s][A

 27%|██▋       | 9246/33804 [00:59<02:25, 168.39it/s][A

 27%|██▋       | 9264/33804 [00:59<02:25, 168.93it/s][A

 27%|██▋       | 9281/33804 [00:59<02:25, 169.06it/s][A

 28%|██▊       | 9299/33804 [00:59<02:24, 169.60it/s][A

 28%|██▊       | 9316/33804 [00:59<02:24, 169.35it/s][A

 28%|██▊       | 9333/33804 [00:59<02:24, 169.28it/s][A

 28%|██▊       | 9350/33804 [00:59<02:24, 169.44it/s][A

 28%|██▊       | 9367/33804 [00:59<02:24, 168.91it/s][A

 28%|██▊       | 9384/33804 [00:59<02:24, 168.72it/s][A

 28%|██▊       | 9401/33804 [01:00<02:25, 168.18it/s][A

 28%|██▊       | 9418/33804 [01:00<02:24, 168.26it/s][A

 28%|██▊       | 9435/33804 [01:00<02:25, 167.88it/s][A

 28%|██▊       | 9452/33804 [01:00<02:24, 168.44it/s][A

 28%|██▊       | 9469/33804 [01:00<02:24, 168.59it/s][A

 28%|██▊       | 9486/33804 [01:00<02:23, 168.90it/s][A

 28%|██▊       | 9503/33804 [01:00<02:47, 145.06it/s][A

 28%|██▊       | 9519/33804 [01:00<03:02, 133.40it/s][A

 28%|██▊       | 9536/33804 [01:00<02:50, 142.45it/s][A

 28%|██▊       | 9553/33804 [01:01<02:42, 149.46it/s][A

 28%|██▊       | 9570/33804 [01:01<02:36, 155.09it/s][A

 28%|██▊       | 9587/33804 [01:01<02:32, 158.99it/s][A

 28%|██▊       | 9604/33804 [01:01<02:29, 162.10it/s][A

 28%|██▊       | 9621/33804 [01:01<02:27, 163.92it/s][A

 29%|██▊       | 9638/33804 [01:01<02:26, 165.30it/s][A

 29%|██▊       | 9655/33804 [01:01<02:25, 166.12it/s][A

 29%|██▊       | 9672/33804 [01:01<02:24, 167.15it/s][A

 29%|██▊       | 9689/33804 [01:01<02:24, 167.41it/s][A

 29%|██▊       | 9706/33804 [01:01<02:23, 168.07it/s][A

 29%|██▉       | 9723/33804 [01:02<02:23, 168.34it/s][A

 29%|██▉       | 9740/33804 [01:02<02:22, 168.81it/s][A

 29%|██▉       | 9757/33804 [01:02<02:22, 168.85it/s][A

 29%|██▉       | 9774/33804 [01:02<02:22, 169.05it/s][A

 29%|██▉       | 9791/33804 [01:02<02:23, 167.58it/s][A

 29%|██▉       | 9808/33804 [01:02<02:23, 167.36it/s][A

 29%|██▉       | 9825/33804 [01:02<02:22, 167.91it/s][A

 29%|██▉       | 9842/33804 [01:02<02:22, 167.71it/s][A

 29%|██▉       | 9859/33804 [01:02<02:22, 168.16it/s][A

 29%|██▉       | 9876/33804 [01:02<02:22, 167.77it/s][A

 29%|██▉       | 9893/33804 [01:03<02:22, 168.07it/s][A

 29%|██▉       | 9910/33804 [01:03<02:22, 167.28it/s][A

 29%|██▉       | 9927/33804 [01:03<02:22, 167.81it/s][A

 29%|██▉       | 9944/33804 [01:03<02:22, 167.34it/s][A

 29%|██▉       | 9961/33804 [01:03<02:21, 168.12it/s][A

 30%|██▉       | 9978/33804 [01:03<02:21, 168.18it/s][A

 30%|██▉       | 9996/33804 [01:03<02:21, 168.72it/s][A

 30%|██▉       | 10013/33804 [01:03<02:21, 168.71it/s][A

 30%|██▉       | 10030/33804 [01:03<02:21, 168.49it/s][A

 30%|██▉       | 10047/33804 [01:03<02:20, 168.86it/s][A

 30%|██▉       | 10064/33804 [01:04<02:20, 168.43it/s][A

 30%|██▉       | 10081/33804 [01:04<02:20, 168.88it/s][A

 30%|██▉       | 10098/33804 [01:04<02:20, 168.55it/s][A

 30%|██▉       | 10116/33804 [01:04<02:20, 169.04it/s][A

 30%|██▉       | 10133/33804 [01:04<02:20, 168.55it/s][A

 30%|███       | 10150/33804 [01:04<02:20, 168.64it/s][A

 30%|███       | 10167/33804 [01:04<02:20, 168.61it/s][A

 30%|███       | 10184/33804 [01:04<02:19, 168.80it/s][A

 30%|███       | 10201/33804 [01:04<02:28, 158.96it/s][A

 30%|███       | 10218/33804 [01:05<03:01, 130.08it/s][A

 30%|███       | 10235/33804 [01:05<02:48, 139.47it/s][A

 30%|███       | 10252/33804 [01:05<02:40, 146.60it/s][A

 30%|███       | 10269/33804 [01:05<02:33, 152.83it/s][A

 30%|███       | 10286/33804 [01:05<02:29, 156.96it/s][A

 30%|███       | 10303/33804 [01:05<02:26, 160.40it/s][A

 31%|███       | 10320/33804 [01:05<02:24, 162.78it/s][A

 31%|███       | 10337/33804 [01:05<02:22, 164.76it/s][A

 31%|███       | 10354/33804 [01:05<02:21, 165.41it/s][A

 31%|███       | 10371/33804 [01:06<02:20, 166.46it/s][A

 31%|███       | 10388/33804 [01:06<02:20, 166.51it/s][A

 31%|███       | 10405/33804 [01:06<02:20, 166.23it/s][A

 31%|███       | 10422/33804 [01:06<02:20, 166.65it/s][A

 31%|███       | 10439/33804 [01:06<02:19, 167.34it/s][A

 31%|███       | 10456/33804 [01:06<02:19, 167.81it/s][A

 31%|███       | 10473/33804 [01:06<02:18, 168.05it/s][A

 31%|███       | 10490/33804 [01:06<02:18, 168.51it/s][A

 31%|███       | 10507/33804 [01:06<02:18, 168.47it/s][A

 31%|███       | 10524/33804 [01:06<02:17, 168.90it/s][A

 31%|███       | 10541/33804 [01:07<02:17, 168.69it/s][A

 31%|███       | 10558/33804 [01:07<02:17, 168.79it/s][A

 31%|███▏      | 10575/33804 [01:07<02:17, 168.57it/s][A

 31%|███▏      | 10592/33804 [01:07<02:17, 168.91it/s][A

 31%|███▏      | 10609/33804 [01:07<02:17, 168.81it/s][A

 31%|███▏      | 10626/33804 [01:07<02:17, 168.76it/s][A

 31%|███▏      | 10643/33804 [01:07<02:17, 168.47it/s][A

 32%|███▏      | 10660/33804 [01:07<02:17, 168.81it/s][A

 32%|███▏      | 10677/33804 [01:07<02:17, 168.62it/s][A

 32%|███▏      | 10694/33804 [01:07<02:17, 168.33it/s][A

 32%|███▏      | 10711/33804 [01:08<02:16, 168.80it/s][A

 32%|███▏      | 10728/33804 [01:08<02:16, 168.75it/s][A

 32%|███▏      | 10746/33804 [01:08<02:16, 169.15it/s][A

 32%|███▏      | 10763/33804 [01:08<02:16, 168.71it/s][A

 32%|███▏      | 10780/33804 [01:08<02:16, 168.98it/s][A

 32%|███▏      | 10797/33804 [01:08<02:16, 168.84it/s][A

 32%|███▏      | 10814/33804 [01:08<02:16, 168.95it/s][A

 32%|███▏      | 10831/33804 [01:08<02:15, 168.99it/s][A

 32%|███▏      | 10848/33804 [01:08<02:15, 169.25it/s][A

 32%|███▏      | 10865/33804 [01:08<02:15, 168.94it/s][A

 32%|███▏      | 10882/33804 [01:09<02:15, 168.82it/s][A

 32%|███▏      | 10899/33804 [01:10<08:09, 46.83it/s] [A

 32%|███▏      | 10916/33804 [01:10<06:22, 59.78it/s][A

 32%|███▏      | 10933/33804 [01:10<05:08, 74.11it/s][A

 32%|███▏      | 10950/33804 [01:10<04:16, 89.03it/s][A

 32%|███▏      | 10967/33804 [01:10<03:39, 103.83it/s][A

 32%|███▏      | 10984/33804 [01:10<03:14, 117.18it/s][A

 33%|███▎      | 11001/33804 [01:10<02:56, 128.88it/s][A

 33%|███▎      | 11018/33804 [01:10<02:44, 138.83it/s][A

 33%|███▎      | 11036/33804 [01:10<02:34, 147.29it/s][A

 33%|███▎      | 11053/33804 [01:10<02:28, 153.01it/s][A

 33%|███▎      | 11070/33804 [01:11<02:24, 157.63it/s][A

 33%|███▎      | 11087/33804 [01:11<02:21, 160.89it/s][A

 33%|███▎      | 11105/33804 [01:11<02:18, 163.72it/s][A

 33%|███▎      | 11122/33804 [01:11<02:17, 164.72it/s][A

 33%|███▎      | 11140/33804 [01:11<02:16, 166.02it/s][A

 33%|███▎      | 11157/33804 [01:11<02:15, 167.12it/s][A

 33%|███▎      | 11174/33804 [01:11<02:14, 167.67it/s][A

 33%|███▎      | 11191/33804 [01:11<02:14, 168.32it/s][A

 33%|███▎      | 11208/33804 [01:11<02:14, 167.85it/s][A

 33%|███▎      | 11225/33804 [01:11<02:14, 168.23it/s][A

 33%|███▎      | 11242/33804 [01:12<02:14, 167.60it/s][A

 33%|███▎      | 11259/33804 [01:12<02:14, 168.03it/s][A

 33%|███▎      | 11276/33804 [01:12<02:14, 168.05it/s][A

 33%|███▎      | 11293/33804 [01:12<02:13, 168.52it/s][A

 33%|███▎      | 11310/33804 [01:12<02:13, 168.08it/s][A

 34%|███▎      | 11327/33804 [01:12<02:13, 168.63it/s][A

 34%|███▎      | 11344/33804 [01:12<02:13, 168.78it/s][A

 34%|███▎      | 11362/33804 [01:12<02:12, 168.96it/s][A

 34%|███▎      | 11379/33804 [01:12<02:12, 169.15it/s][A

 34%|███▎      | 11396/33804 [01:12<02:12, 168.86it/s][A

 34%|███▍      | 11413/33804 [01:13<02:12, 168.95it/s][A

 34%|███▍      | 11430/33804 [01:13<02:12, 168.64it/s][A

 34%|███▍      | 11448/33804 [01:13<02:12, 169.04it/s][A

 34%|███▍      | 11465/33804 [01:13<02:12, 168.95it/s][A

 34%|███▍      | 11482/33804 [01:13<02:46, 134.44it/s][A

 34%|███▍      | 11497/33804 [01:13<02:49, 131.64it/s][A

 34%|███▍      | 11514/33804 [01:13<02:37, 141.20it/s][A

 34%|███▍      | 11531/33804 [01:13<02:30, 148.17it/s][A

 34%|███▍      | 11548/33804 [01:13<02:24, 154.00it/s][A

 34%|███▍      | 11565/33804 [01:14<02:20, 157.83it/s][A

 34%|███▍      | 11582/33804 [01:14<02:17, 161.25it/s][A

 34%|███▍      | 11599/33804 [01:14<02:15, 163.43it/s][A

 34%|███▍      | 11616/33804 [01:14<02:14, 164.84it/s][A

 34%|███▍      | 11633/33804 [01:14<02:13, 166.06it/s][A

 34%|███▍      | 11650/33804 [01:14<02:12, 166.84it/s][A

 35%|███▍      | 11667/33804 [01:14<02:12, 167.68it/s][A

 35%|███▍      | 11684/33804 [01:14<02:11, 167.83it/s][A

 35%|███▍      | 11701/33804 [01:14<02:11, 168.25it/s][A

 35%|███▍      | 11718/33804 [01:15<02:11, 167.94it/s][A

 35%|███▍      | 11735/33804 [01:15<02:11, 168.33it/s][A

 35%|███▍      | 11752/33804 [01:15<02:10, 168.57it/s][A

 35%|███▍      | 11769/33804 [01:15<02:10, 168.76it/s][A

 35%|███▍      | 11786/33804 [01:15<02:10, 168.98it/s][A

 35%|███▍      | 11804/33804 [01:15<02:10, 169.20it/s][A

 35%|███▍      | 11821/33804 [01:15<02:09, 169.29it/s][A

 35%|███▌      | 11838/33804 [01:15<02:09, 169.44it/s][A

 35%|███▌      | 11856/33804 [01:15<02:09, 169.73it/s][A

 35%|███▌      | 11873/33804 [01:15<02:09, 169.31it/s][A

 35%|███▌      | 11890/33804 [01:16<02:09, 169.38it/s][A

 35%|███▌      | 11907/33804 [01:16<02:09, 169.27it/s][A

 35%|███▌      | 11925/33804 [01:16<02:09, 169.60it/s][A

 35%|███▌      | 11942/33804 [01:16<02:09, 169.05it/s][A

 35%|███▌      | 11959/33804 [01:16<02:09, 169.32it/s][A

 35%|███▌      | 11976/33804 [01:16<02:09, 168.30it/s][A

 35%|███▌      | 11993/33804 [01:16<02:09, 168.52it/s][A

 36%|███▌      | 12010/33804 [01:16<02:09, 168.57it/s][A

 36%|███▌      | 12027/33804 [01:16<02:09, 168.44it/s][A

 36%|███▌      | 12044/33804 [01:16<02:08, 168.89it/s][A

 36%|███▌      | 12061/33804 [01:17<02:08, 169.14it/s][A

 36%|███▌      | 12078/33804 [01:17<02:09, 168.02it/s][A

 36%|███▌      | 12095/33804 [01:17<02:09, 167.80it/s][A

 36%|███▌      | 12112/33804 [01:17<02:09, 167.41it/s][A

 36%|███▌      | 12129/33804 [01:17<02:09, 167.46it/s][A

 36%|███▌      | 12146/33804 [01:17<02:09, 167.54it/s][A

 36%|███▌      | 12163/33804 [01:17<02:09, 167.58it/s][A

 36%|███▌      | 12180/33804 [01:17<03:31, 102.42it/s][A

 36%|███▌      | 12194/33804 [01:18<03:24, 105.55it/s][A

 36%|███▌      | 12211/33804 [01:18<03:00, 119.31it/s][A

 36%|███▌      | 12228/33804 [01:18<02:45, 130.40it/s][A

 36%|███▌      | 12245/33804 [01:18<02:33, 140.09it/s][A

 36%|███▋      | 12262/33804 [01:18<02:26, 147.29it/s][A

 36%|███▋      | 12279/33804 [01:18<02:20, 153.22it/s][A

 36%|███▋      | 12296/33804 [01:18<02:16, 157.31it/s][A

 36%|███▋      | 12313/33804 [01:18<02:14, 159.80it/s][A

 36%|███▋      | 12330/33804 [01:18<02:12, 162.37it/s][A

 37%|███▋      | 12347/33804 [01:18<02:11, 163.76it/s][A

 37%|███▋      | 12364/33804 [01:19<02:09, 165.29it/s][A

 37%|███▋      | 12381/33804 [01:19<02:08, 166.29it/s][A

 37%|███▋      | 12399/33804 [01:19<02:07, 167.52it/s][A

 37%|███▋      | 12416/33804 [01:19<02:07, 167.78it/s][A

 37%|███▋      | 12433/33804 [01:19<02:07, 168.27it/s][A

 37%|███▋      | 12450/33804 [01:19<02:06, 168.52it/s][A

 37%|███▋      | 12468/33804 [01:19<02:06, 169.03it/s][A

 37%|███▋      | 12485/33804 [01:19<02:06, 168.71it/s][A

 37%|███▋      | 12502/33804 [01:19<02:06, 168.78it/s][A

 37%|███▋      | 12519/33804 [01:20<02:05, 169.08it/s][A

 37%|███▋      | 12536/33804 [01:20<02:05, 169.08it/s][A

 37%|███▋      | 12554/33804 [01:20<02:05, 169.45it/s][A

 37%|███▋      | 12571/33804 [01:20<02:05, 169.35it/s][A

 37%|███▋      | 12588/33804 [01:20<02:05, 169.39it/s][A

 37%|███▋      | 12605/33804 [01:20<02:05, 169.04it/s][A

 37%|███▋      | 12623/33804 [01:20<02:05, 169.40it/s][A

 37%|███▋      | 12640/33804 [01:20<02:05, 169.13it/s][A

 37%|███▋      | 12657/33804 [01:20<02:04, 169.25it/s][A

 37%|███▋      | 12674/33804 [01:20<02:05, 168.82it/s][A

 38%|███▊      | 12691/33804 [01:21<02:06, 167.33it/s][A

 38%|███▊      | 12708/33804 [01:21<02:07, 165.82it/s][A

 38%|███▊      | 12725/33804 [01:21<02:07, 165.76it/s][A

 38%|███▊      | 12742/33804 [01:21<02:07, 165.76it/s][A

 38%|███▊      | 12759/33804 [01:21<02:07, 165.61it/s][A

 38%|███▊      | 12776/33804 [01:21<02:06, 165.74it/s][A

 38%|███▊      | 12793/33804 [01:21<02:06, 165.76it/s][A

 38%|███▊      | 12810/33804 [01:21<02:06, 166.01it/s][A

 38%|███▊      | 12827/33804 [01:21<02:06, 165.85it/s][A

 38%|███▊      | 12844/33804 [01:21<02:06, 166.07it/s][A

 38%|███▊      | 12861/33804 [01:22<02:06, 166.00it/s][A

 38%|███▊      | 12878/33804 [01:22<02:37, 132.52it/s][A

 38%|███▊      | 12893/33804 [01:22<02:39, 130.73it/s][A

 38%|███▊      | 12910/33804 [01:22<02:29, 139.79it/s][A

 38%|███▊      | 12927/33804 [01:22<02:22, 146.50it/s][A

 38%|███▊      | 12944/33804 [01:22<02:17, 151.99it/s][A

 38%|███▊      | 12961/33804 [01:22<02:13, 156.13it/s][A

 38%|███▊      | 12978/33804 [01:22<02:10, 159.90it/s][A

 38%|███▊      | 12995/33804 [01:22<02:07, 162.77it/s][A

 38%|███▊      | 13012/33804 [01:23<02:06, 164.57it/s][A

 39%|███▊      | 13030/33804 [01:23<02:04, 166.32it/s][A

 39%|███▊      | 13047/33804 [01:23<02:04, 167.19it/s][A

 39%|███▊      | 13064/33804 [01:23<02:03, 167.84it/s][A

 39%|███▊      | 13081/33804 [01:23<02:03, 167.99it/s][A

 39%|███▊      | 13099/33804 [01:23<02:02, 168.72it/s][A

 39%|███▉      | 13116/33804 [01:23<02:02, 168.71it/s][A

 39%|███▉      | 13133/33804 [01:23<02:02, 168.63it/s][A

 39%|███▉      | 13150/33804 [01:23<02:02, 168.52it/s][A

 39%|███▉      | 13167/33804 [01:23<02:02, 168.28it/s][A

 39%|███▉      | 13184/33804 [01:24<02:02, 168.44it/s][A

 39%|███▉      | 13201/33804 [01:24<02:02, 168.55it/s][A

 39%|███▉      | 13219/33804 [01:24<02:01, 169.23it/s][A

 39%|███▉      | 13236/33804 [01:24<02:01, 169.12it/s][A

 39%|███▉      | 13254/33804 [01:24<02:01, 169.54it/s][A

 39%|███▉      | 13271/33804 [01:24<02:01, 169.58it/s][A

 39%|███▉      | 13288/33804 [01:24<02:01, 168.36it/s][A

 39%|███▉      | 13305/33804 [01:24<02:02, 167.00it/s][A

 39%|███▉      | 13322/33804 [01:24<02:03, 166.25it/s][A

 39%|███▉      | 13339/33804 [01:25<02:03, 165.32it/s][A

 40%|███▉      | 13356/33804 [01:25<02:04, 164.87it/s][A

 40%|███▉      | 13373/33804 [01:25<02:04, 164.37it/s][A

 40%|███▉      | 13390/33804 [01:25<02:04, 163.71it/s][A

 40%|███▉      | 13407/33804 [01:25<02:04, 163.89it/s][A

 40%|███▉      | 13424/33804 [01:25<02:04, 163.78it/s][A

 40%|███▉      | 13441/33804 [01:25<02:04, 164.08it/s][A

 40%|███▉      | 13458/33804 [01:25<02:03, 164.12it/s][A

 40%|███▉      | 13475/33804 [01:25<02:03, 164.37it/s][A

 40%|███▉      | 13492/33804 [01:25<02:03, 164.22it/s][A

 40%|███▉      | 13509/33804 [01:26<02:03, 164.63it/s][A

 40%|████      | 13526/33804 [01:26<02:03, 164.50it/s][A

 40%|████      | 13543/33804 [01:26<02:03, 164.61it/s][A

 40%|████      | 13560/33804 [01:26<02:11, 154.02it/s][A

 40%|████      | 13576/33804 [01:26<02:40, 125.88it/s][A

 40%|████      | 13593/33804 [01:26<02:28, 136.15it/s][A

 40%|████      | 13610/33804 [01:26<02:19, 144.77it/s][A

 40%|████      | 13627/33804 [01:26<02:13, 150.77it/s][A

 40%|████      | 13644/33804 [01:26<02:09, 155.26it/s][A

 40%|████      | 13661/33804 [01:27<02:06, 159.29it/s][A

 40%|████      | 13678/33804 [01:27<02:04, 161.70it/s][A

 41%|████      | 13695/33804 [01:27<02:02, 164.02it/s][A

 41%|████      | 13712/33804 [01:27<02:01, 165.43it/s][A

 41%|████      | 13729/33804 [01:27<02:00, 166.71it/s][A

 41%|████      | 13746/33804 [01:27<01:59, 167.41it/s][A

 41%|████      | 13763/33804 [01:27<01:59, 168.01it/s][A

 41%|████      | 13780/33804 [01:27<01:59, 167.73it/s][A

 41%|████      | 13797/33804 [01:27<01:58, 168.25it/s][A

 41%|████      | 13814/33804 [01:27<01:58, 168.34it/s][A

 41%|████      | 13831/33804 [01:28<01:58, 168.62it/s][A

 41%|████      | 13848/33804 [01:28<01:58, 168.44it/s][A

 41%|████      | 13865/33804 [01:28<01:58, 168.07it/s][A

 41%|████      | 13882/33804 [01:28<01:58, 168.37it/s][A

 41%|████      | 13899/33804 [01:28<01:59, 167.10it/s][A

 41%|████      | 13916/33804 [01:28<01:58, 167.34it/s][A

 41%|████      | 13933/33804 [01:28<01:58, 167.18it/s][A

 41%|████▏     | 13950/33804 [01:28<01:58, 167.31it/s][A

 41%|████▏     | 13967/33804 [01:28<01:58, 167.11it/s][A

 41%|████▏     | 13984/33804 [01:29<01:58, 167.40it/s][A

 41%|████▏     | 14001/33804 [01:29<01:58, 166.79it/s][A

 41%|████▏     | 14018/33804 [01:29<01:58, 166.92it/s][A

 42%|████▏     | 14035/33804 [01:29<01:58, 166.23it/s][A

 42%|████▏     | 14052/33804 [01:29<01:58, 166.54it/s][A

 42%|████▏     | 14069/33804 [01:29<01:58, 166.23it/s][A

 42%|████▏     | 14086/33804 [01:29<01:58, 165.70it/s][A

 42%|████▏     | 14103/33804 [01:29<01:58, 166.00it/s][A

 42%|████▏     | 14120/33804 [01:29<01:58, 165.72it/s][A

 42%|████▏     | 14137/33804 [01:29<01:58, 165.85it/s][A

 42%|████▏     | 14154/33804 [01:30<01:58, 165.85it/s][A

 42%|████▏     | 14171/33804 [01:30<01:58, 165.78it/s][A

 42%|████▏     | 14188/33804 [01:31<08:05, 40.37it/s] [A

 42%|████▏     | 14205/33804 [01:31<06:14, 52.30it/s][A

 42%|████▏     | 14222/33804 [01:31<04:57, 65.86it/s][A

 42%|████▏     | 14240/33804 [01:31<04:00, 81.43it/s][A

 42%|████▏     | 14257/33804 [01:31<03:23, 96.11it/s][A

 42%|████▏     | 14275/33804 [01:31<02:55, 111.06it/s][A

 42%|████▏     | 14292/33804 [01:31<02:38, 123.48it/s][A

 42%|████▏     | 14310/33804 [01:32<02:24, 134.91it/s][A

 42%|████▏     | 14327/33804 [01:32<02:16, 143.21it/s][A

 42%|████▏     | 14344/33804 [01:32<02:09, 149.98it/s][A

 42%|████▏     | 14361/33804 [01:32<02:05, 155.19it/s][A

 43%|████▎     | 14378/33804 [01:32<02:02, 159.06it/s][A

 43%|████▎     | 14395/33804 [01:32<01:59, 162.05it/s][A

 43%|████▎     | 14412/33804 [01:32<01:58, 163.87it/s][A

 43%|████▎     | 14429/33804 [01:32<01:57, 165.44it/s][A

 43%|████▎     | 14446/33804 [01:32<01:56, 165.98it/s][A

 43%|████▎     | 14463/33804 [01:32<01:55, 166.85it/s][A

 43%|████▎     | 14480/33804 [01:33<01:55, 167.42it/s][A

 43%|████▎     | 14497/33804 [01:33<01:55, 167.39it/s][A

 43%|████▎     | 14514/33804 [01:33<01:54, 167.83it/s][A

 43%|████▎     | 14532/33804 [01:33<01:54, 168.25it/s][A

 43%|████▎     | 14550/33804 [01:33<01:54, 168.89it/s][A

 43%|████▎     | 14567/33804 [01:33<01:54, 168.48it/s][A

 43%|████▎     | 14585/33804 [01:33<01:53, 169.17it/s][A

 43%|████▎     | 14602/33804 [01:33<01:53, 168.75it/s][A

 43%|████▎     | 14620/33804 [01:33<01:53, 169.23it/s][A

 43%|████▎     | 14637/33804 [01:33<01:53, 169.23it/s][A

 43%|████▎     | 14654/33804 [01:34<01:53, 169.14it/s][A

 43%|████▎     | 14671/33804 [01:34<01:53, 168.66it/s][A

 43%|████▎     | 14688/33804 [01:34<01:53, 169.00it/s][A

 44%|████▎     | 14705/33804 [01:34<01:53, 168.22it/s][A

 44%|████▎     | 14722/33804 [01:34<01:53, 168.40it/s][A

 44%|████▎     | 14739/33804 [01:34<01:53, 168.70it/s][A

 44%|████▎     | 14756/33804 [01:34<01:52, 168.78it/s][A

 44%|████▎     | 14773/33804 [01:34<01:52, 169.12it/s][A

 44%|████▍     | 14790/33804 [01:34<01:52, 169.06it/s][A

 44%|████▍     | 14807/33804 [01:35<02:27, 129.23it/s][A

 44%|████▍     | 14824/33804 [01:35<02:17, 138.49it/s][A

 44%|████▍     | 14841/33804 [01:35<02:09, 146.39it/s][A

 44%|████▍     | 14858/33804 [01:35<02:04, 152.31it/s][A

 44%|████▍     | 14875/33804 [01:35<02:01, 156.02it/s][A

 44%|████▍     | 14892/33804 [01:35<01:59, 158.78it/s][A

 44%|████▍     | 14909/33804 [01:35<01:56, 161.62it/s][A

 44%|████▍     | 14926/33804 [01:35<01:55, 162.97it/s][A

 44%|████▍     | 14943/33804 [01:35<01:54, 164.61it/s][A

 44%|████▍     | 14960/33804 [01:35<01:53, 165.64it/s][A

 44%|████▍     | 14977/33804 [01:36<01:53, 166.55it/s][A

 44%|████▍     | 14994/33804 [01:36<01:52, 167.35it/s][A

 44%|████▍     | 15011/33804 [01:36<01:52, 167.68it/s][A

 44%|████▍     | 15029/33804 [01:36<01:51, 168.46it/s][A

 45%|████▍     | 15046/33804 [01:36<01:51, 168.56it/s][A

 45%|████▍     | 15063/33804 [01:36<01:50, 168.91it/s][A

 45%|████▍     | 15080/33804 [01:36<01:50, 169.04it/s][A

 45%|████▍     | 15097/33804 [01:36<01:50, 168.93it/s][A

 45%|████▍     | 15114/33804 [01:36<01:50, 169.06it/s][A

 45%|████▍     | 15132/33804 [01:36<01:50, 169.47it/s][A

 45%|████▍     | 15149/33804 [01:37<01:50, 169.07it/s][A

 45%|████▍     | 15166/33804 [01:37<01:50, 168.69it/s][A

 45%|████▍     | 15183/33804 [01:37<01:50, 169.02it/s][A

 45%|████▍     | 15200/33804 [01:37<01:50, 168.62it/s][A

 45%|████▌     | 15218/33804 [01:37<01:49, 169.16it/s][A

 45%|████▌     | 15235/33804 [01:37<01:49, 168.97it/s][A

 45%|████▌     | 15252/33804 [01:37<01:50, 168.03it/s][A

 45%|████▌     | 15269/33804 [01:37<01:50, 168.23it/s][A

 45%|████▌     | 15287/33804 [01:37<01:49, 168.89it/s][A

 45%|████▌     | 15304/33804 [01:38<01:49, 168.76it/s][A

 45%|████▌     | 15321/33804 [01:38<01:49, 169.04it/s][A

 45%|████▌     | 15338/33804 [01:38<01:49, 168.66it/s][A

 45%|████▌     | 15356/33804 [01:38<01:49, 168.72it/s][A

 45%|████▌     | 15373/33804 [01:38<01:49, 169.09it/s][A

 46%|████▌     | 15390/33804 [01:38<01:48, 168.99it/s][A

 46%|████▌     | 15407/33804 [01:38<01:48, 169.13it/s][A

 46%|████▌     | 15424/33804 [01:38<01:48, 168.82it/s][A

 46%|████▌     | 15441/33804 [01:38<01:48, 169.04it/s][A

 46%|████▌     | 15458/33804 [01:38<01:48, 168.91it/s][A

 46%|████▌     | 15475/33804 [01:39<01:48, 169.08it/s][A

 46%|████▌     | 15492/33804 [01:39<01:54, 160.51it/s][A

 46%|████▌     | 15509/33804 [01:39<02:19, 131.00it/s][A

 46%|████▌     | 15526/33804 [01:39<02:10, 140.33it/s][A

 46%|████▌     | 15543/33804 [01:39<02:03, 148.00it/s][A

 46%|████▌     | 15560/33804 [01:39<01:58, 153.56it/s][A

 46%|████▌     | 15577/33804 [01:39<01:55, 157.99it/s][A

 46%|████▌     | 15594/33804 [01:39<01:53, 161.04it/s][A

 46%|████▌     | 15611/33804 [01:39<01:51, 163.18it/s][A

 46%|████▌     | 15628/33804 [01:40<01:50, 165.01it/s][A

 46%|████▋     | 15645/33804 [01:40<01:50, 164.36it/s][A

 46%|████▋     | 15662/33804 [01:40<01:49, 165.24it/s][A

 46%|████▋     | 15679/33804 [01:40<01:49, 166.25it/s][A

 46%|████▋     | 15696/33804 [01:40<01:48, 167.12it/s][A

 46%|████▋     | 15713/33804 [01:40<01:47, 167.52it/s][A

 47%|████▋     | 15731/33804 [01:40<01:47, 168.39it/s][A

 47%|████▋     | 15748/33804 [01:40<01:47, 168.70it/s][A

 47%|████▋     | 15766/33804 [01:40<01:46, 169.22it/s][A

 47%|████▋     | 15783/33804 [01:40<01:46, 169.05it/s][A

 47%|████▋     | 15800/33804 [01:41<01:46, 168.73it/s][A

 47%|████▋     | 15817/33804 [01:41<01:46, 168.97it/s][A

 47%|████▋     | 15834/33804 [01:41<01:46, 168.18it/s][A

 47%|████▋     | 15852/33804 [01:41<01:46, 168.83it/s][A

 47%|████▋     | 15869/33804 [01:41<01:46, 168.53it/s][A

 47%|████▋     | 15886/33804 [01:41<01:46, 168.30it/s][A

 47%|████▋     | 15903/33804 [01:41<01:46, 168.07it/s][A

 47%|████▋     | 15920/33804 [01:41<01:46, 168.40it/s][A

 47%|████▋     | 15937/33804 [01:41<01:46, 167.90it/s][A

 47%|████▋     | 15955/33804 [01:41<01:45, 168.60it/s][A

 47%|████▋     | 15972/33804 [01:42<01:45, 168.54it/s][A

 47%|████▋     | 15989/33804 [01:42<01:45, 168.92it/s][A

 47%|████▋     | 16006/33804 [01:42<01:45, 168.87it/s][A

 47%|████▋     | 16023/33804 [01:42<01:45, 168.98it/s][A

 47%|████▋     | 16040/33804 [01:42<01:45, 167.89it/s][A

 48%|████▊     | 16057/33804 [01:42<01:45, 168.34it/s][A

 48%|████▊     | 16075/33804 [01:42<01:44, 168.88it/s][A

 48%|████▊     | 16092/33804 [01:42<01:44, 169.08it/s][A

 48%|████▊     | 16109/33804 [01:42<01:44, 169.26it/s][A

 48%|████▊     | 16126/33804 [01:42<01:44, 168.97it/s][A

 48%|████▊     | 16143/33804 [01:43<01:44, 169.02it/s][A

 48%|████▊     | 16160/33804 [01:43<01:44, 168.46it/s][A

 48%|████▊     | 16178/33804 [01:43<01:44, 168.96it/s][A

 48%|████▊     | 16195/33804 [01:43<02:00, 146.00it/s][A

 48%|████▊     | 16211/33804 [01:43<02:54, 100.70it/s][A

 48%|████▊     | 16228/33804 [01:43<02:33, 114.75it/s][A

 48%|████▊     | 16245/33804 [01:43<02:18, 126.88it/s][A

 48%|████▊     | 16262/33804 [01:44<02:07, 137.20it/s][A

 48%|████▊     | 16279/33804 [01:44<02:00, 144.91it/s][A

 48%|████▊     | 16296/33804 [01:44<01:55, 151.53it/s][A

 48%|████▊     | 16313/33804 [01:44<01:51, 156.35it/s][A

 48%|████▊     | 16330/33804 [01:44<01:49, 160.08it/s][A

 48%|████▊     | 16347/33804 [01:44<01:47, 162.34it/s][A

 48%|████▊     | 16364/33804 [01:44<01:45, 164.54it/s][A

 48%|████▊     | 16381/33804 [01:44<01:45, 165.12it/s][A

 49%|████▊     | 16399/33804 [01:44<01:44, 166.66it/s][A

 49%|████▊     | 16416/33804 [01:44<01:44, 166.69it/s][A

 49%|████▊     | 16433/33804 [01:45<01:43, 167.13it/s][A

 49%|████▊     | 16450/33804 [01:45<01:43, 167.91it/s][A

 49%|████▊     | 16467/33804 [01:45<01:43, 168.13it/s][A

 49%|████▉     | 16485/33804 [01:45<01:42, 168.91it/s][A

 49%|████▉     | 16502/33804 [01:45<01:42, 168.90it/s][A

 49%|████▉     | 16519/33804 [01:45<01:42, 169.06it/s][A

 49%|████▉     | 16536/33804 [01:45<01:42, 168.64it/s][A

 49%|████▉     | 16554/33804 [01:45<01:41, 169.30it/s][A

 49%|████▉     | 16571/33804 [01:45<01:41, 169.18it/s][A

 49%|████▉     | 16588/33804 [01:45<01:41, 169.34it/s][A

 49%|████▉     | 16605/33804 [01:46<01:41, 169.00it/s][A

 49%|████▉     | 16622/33804 [01:46<01:41, 168.93it/s][A

 49%|████▉     | 16639/33804 [01:46<01:41, 168.51it/s][A

 49%|████▉     | 16656/33804 [01:46<01:41, 168.45it/s][A

 49%|████▉     | 16673/33804 [01:46<01:41, 168.88it/s][A

 49%|████▉     | 16690/33804 [01:46<01:41, 168.28it/s][A

 49%|████▉     | 16708/33804 [01:46<01:41, 168.81it/s][A

 49%|████▉     | 16725/33804 [01:46<01:41, 168.83it/s][A

 50%|████▉     | 16743/33804 [01:46<01:40, 169.24it/s][A

 50%|████▉     | 16760/33804 [01:46<01:41, 168.47it/s][A

 50%|████▉     | 16777/33804 [01:47<01:40, 168.86it/s][A

 50%|████▉     | 16794/33804 [01:47<01:40, 168.73it/s][A

 50%|████▉     | 16811/33804 [01:47<01:41, 168.06it/s][A

 50%|████▉     | 16828/33804 [01:47<01:40, 168.23it/s][A

 50%|████▉     | 16845/33804 [01:47<01:40, 168.18it/s][A

 50%|████▉     | 16862/33804 [01:47<01:40, 167.99it/s][A

 50%|████▉     | 16879/33804 [01:47<01:40, 167.90it/s][A

 50%|████▉     | 16896/33804 [01:47<02:01, 139.02it/s][A

 50%|█████     | 16911/33804 [01:47<02:09, 130.71it/s][A

 50%|█████     | 16928/33804 [01:48<02:00, 140.60it/s][A

 50%|█████     | 16945/33804 [01:48<01:53, 148.09it/s][A

 50%|█████     | 16962/33804 [01:48<01:49, 154.08it/s][A

 50%|█████     | 16979/33804 [01:48<01:46, 158.33it/s][A

 50%|█████     | 16996/33804 [01:48<01:44, 161.50it/s][A

 50%|█████     | 17013/33804 [01:48<01:42, 163.76it/s][A

 50%|█████     | 17030/33804 [01:48<01:41, 165.51it/s][A

 50%|█████     | 17047/33804 [01:48<01:40, 166.59it/s][A

 50%|█████     | 17064/33804 [01:48<01:40, 167.39it/s][A

 51%|█████     | 17081/33804 [01:49<01:39, 167.82it/s][A

 51%|█████     | 17098/33804 [01:49<01:39, 167.23it/s][A

 51%|█████     | 17115/33804 [01:49<01:39, 167.97it/s][A

 51%|█████     | 17132/33804 [01:49<01:39, 168.31it/s][A

 51%|█████     | 17150/33804 [01:49<01:38, 168.93it/s][A

 51%|█████     | 17167/33804 [01:49<01:38, 168.90it/s][A

 51%|█████     | 17184/33804 [01:49<01:38, 168.64it/s][A

 51%|█████     | 17201/33804 [01:49<01:38, 168.63it/s][A

 51%|█████     | 17219/33804 [01:49<01:38, 169.12it/s][A

 51%|█████     | 17236/33804 [01:49<01:38, 168.81it/s][A

 51%|█████     | 17254/33804 [01:50<01:37, 169.23it/s][A

 51%|█████     | 17271/33804 [01:50<01:38, 168.36it/s][A

 51%|█████     | 17288/33804 [01:50<01:38, 168.52it/s][A

 51%|█████     | 17305/33804 [01:50<01:37, 168.83it/s][A

 51%|█████     | 17322/33804 [01:50<01:37, 168.85it/s][A

 51%|█████▏    | 17339/33804 [01:50<01:37, 168.72it/s][A

 51%|█████▏    | 17356/33804 [01:50<01:37, 168.64it/s][A

 51%|█████▏    | 17373/33804 [01:50<01:37, 168.89it/s][A

 51%|█████▏    | 17390/33804 [01:50<01:37, 168.76it/s][A

 51%|█████▏    | 17408/33804 [01:50<01:36, 169.23it/s][A

 52%|█████▏    | 17425/33804 [01:51<01:36, 169.19it/s][A

 52%|█████▏    | 17442/33804 [01:51<01:36, 169.36it/s][A

 52%|█████▏    | 17459/33804 [01:51<01:36, 169.31it/s][A

 52%|█████▏    | 17477/33804 [01:51<01:36, 169.59it/s][A

 52%|█████▏    | 17494/33804 [01:51<01:36, 169.48it/s][A

 52%|█████▏    | 17511/33804 [01:51<01:36, 169.44it/s][A

 52%|█████▏    | 17529/33804 [01:51<01:35, 169.60it/s][A

 52%|█████▏    | 17546/33804 [01:51<01:36, 169.22it/s][A

 52%|█████▏    | 17563/33804 [01:51<01:36, 168.78it/s][A

 52%|█████▏    | 17580/33804 [01:51<01:36, 168.58it/s][A

 52%|█████▏    | 17597/33804 [01:52<01:54, 141.00it/s][A

 52%|█████▏    | 17612/33804 [01:52<02:04, 130.56it/s][A

 52%|█████▏    | 17629/33804 [01:52<01:55, 140.39it/s][A

 52%|█████▏    | 17646/33804 [01:52<01:49, 147.35it/s][A

 52%|█████▏    | 17663/33804 [01:52<01:45, 153.55it/s][A

 52%|█████▏    | 17680/33804 [01:52<01:42, 157.84it/s][A

 52%|█████▏    | 17698/33804 [01:52<01:39, 161.51it/s][A

 52%|█████▏    | 17715/33804 [01:52<01:38, 163.82it/s][A

 52%|█████▏    | 17732/33804 [01:52<01:37, 164.96it/s][A

 53%|█████▎    | 17749/33804 [01:53<01:36, 166.36it/s][A

 53%|█████▎    | 17766/33804 [01:53<01:36, 166.68it/s][A

 53%|█████▎    | 17783/33804 [01:53<01:35, 167.64it/s][A

 53%|█████▎    | 17800/33804 [01:53<01:35, 167.72it/s][A

 53%|█████▎    | 17817/33804 [01:53<01:35, 168.02it/s][A

 53%|█████▎    | 17834/33804 [01:53<01:35, 167.76it/s][A

 53%|█████▎    | 17851/33804 [01:53<01:34, 168.22it/s][A

 53%|█████▎    | 17868/33804 [01:53<01:34, 168.37it/s][A

 53%|█████▎    | 17885/33804 [01:53<01:34, 168.34it/s][A

 53%|█████▎    | 17902/33804 [01:53<01:34, 168.41it/s][A

 53%|█████▎    | 17919/33804 [01:54<01:34, 168.76it/s][A

 53%|█████▎    | 17936/33804 [01:54<01:34, 168.77it/s][A

 53%|█████▎    | 17953/33804 [01:54<01:33, 168.88it/s][A

 53%|█████▎    | 17970/33804 [01:54<01:33, 168.58it/s][A

 53%|█████▎    | 17987/33804 [01:54<01:33, 168.59it/s][A

 53%|█████▎    | 18004/33804 [01:54<01:33, 168.64it/s][A

 53%|█████▎    | 18021/33804 [01:54<01:33, 168.84it/s][A

 53%|█████▎    | 18038/33804 [01:54<01:33, 169.09it/s][A

 53%|█████▎    | 18055/33804 [01:54<01:33, 169.07it/s][A

 53%|█████▎    | 18072/33804 [01:54<01:33, 169.05it/s][A

 54%|█████▎    | 18089/33804 [01:55<01:33, 168.46it/s][A

 54%|█████▎    | 18106/33804 [01:55<01:33, 168.63it/s][A

 54%|█████▎    | 18123/33804 [01:55<01:32, 168.73it/s][A

 54%|█████▎    | 18140/33804 [01:55<01:32, 169.07it/s][A

 54%|█████▎    | 18157/33804 [01:55<01:32, 169.12it/s][A

 54%|█████▍    | 18174/33804 [01:55<01:32, 169.27it/s][A

 54%|█████▍    | 18191/33804 [01:55<01:32, 169.30it/s][A

 54%|█████▍    | 18208/33804 [01:55<01:32, 168.67it/s][A

 54%|█████▍    | 18226/33804 [01:55<01:32, 169.22it/s][A

 54%|█████▍    | 18243/33804 [01:55<01:32, 168.72it/s][A

 54%|█████▍    | 18260/33804 [01:56<01:31, 169.02it/s][A

 54%|█████▍    | 18277/33804 [01:56<01:31, 169.01it/s][A

 54%|█████▍    | 18294/33804 [01:56<01:37, 159.57it/s][A

 54%|█████▍    | 18311/33804 [01:57<07:24, 34.85it/s] [A

 54%|█████▍    | 18328/33804 [01:57<05:38, 45.71it/s][A

 54%|█████▍    | 18345/33804 [01:57<04:24, 58.47it/s][A

 54%|█████▍    | 18362/33804 [01:58<03:32, 72.71it/s][A

 54%|█████▍    | 18379/33804 [01:58<02:56, 87.50it/s][A

 54%|█████▍    | 18397/33804 [01:58<02:29, 103.11it/s][A

 54%|█████▍    | 18414/33804 [01:58<02:12, 116.49it/s][A

 55%|█████▍    | 18431/33804 [01:58<01:59, 128.36it/s][A

 55%|█████▍    | 18449/33804 [01:58<01:50, 138.93it/s][A

 55%|█████▍    | 18466/33804 [01:58<01:44, 146.59it/s][A

 55%|█████▍    | 18484/33804 [01:58<01:39, 153.24it/s][A

 55%|█████▍    | 18501/33804 [01:58<01:37, 156.75it/s][A

 55%|█████▍    | 18519/33804 [01:58<01:35, 160.61it/s][A

 55%|█████▍    | 18536/33804 [01:59<01:33, 162.66it/s][A

 55%|█████▍    | 18553/33804 [01:59<01:32, 164.60it/s][A

 55%|█████▍    | 18570/33804 [01:59<01:31, 165.68it/s][A

 55%|█████▍    | 18587/33804 [01:59<01:31, 166.50it/s][A

 55%|█████▌    | 18604/33804 [01:59<01:31, 166.38it/s][A

 55%|█████▌    | 18621/33804 [01:59<01:30, 167.39it/s][A

 55%|█████▌    | 18638/33804 [01:59<01:30, 167.47it/s][A

 55%|█████▌    | 18655/33804 [01:59<01:30, 167.74it/s][A

 55%|█████▌    | 18672/33804 [01:59<01:29, 168.37it/s][A

 55%|█████▌    | 18689/33804 [01:59<01:29, 168.58it/s][A

 55%|█████▌    | 18706/33804 [02:00<01:29, 168.84it/s][A

 55%|█████▌    | 18723/33804 [02:00<01:29, 168.64it/s][A

 55%|█████▌    | 18740/33804 [02:00<01:29, 168.97it/s][A

 55%|█████▌    | 18757/33804 [02:00<01:29, 169.05it/s][A

 56%|█████▌    | 18774/33804 [02:00<01:28, 169.22it/s][A

 56%|█████▌    | 18791/33804 [02:00<01:28, 168.83it/s][A

 56%|█████▌    | 18808/33804 [02:00<01:55, 130.04it/s][A

 56%|█████▌    | 18824/33804 [02:00<01:49, 136.23it/s][A

 56%|█████▌    | 18841/33804 [02:00<01:43, 144.74it/s][A

 56%|█████▌    | 18858/33804 [02:01<01:39, 150.55it/s][A

 56%|█████▌    | 18875/33804 [02:01<01:36, 155.05it/s][A

 56%|█████▌    | 18892/33804 [02:01<01:33, 158.80it/s][A

 56%|█████▌    | 18909/33804 [02:01<01:32, 161.54it/s][A

 56%|█████▌    | 18926/33804 [02:01<01:30, 163.88it/s][A

 56%|█████▌    | 18943/33804 [02:01<01:29, 165.41it/s][A

 56%|█████▌    | 18961/33804 [02:01<01:28, 166.94it/s][A

 56%|█████▌    | 18978/33804 [02:01<01:28, 167.66it/s][A

 56%|█████▌    | 18995/33804 [02:01<01:28, 168.18it/s][A

 56%|█████▌    | 19012/33804 [02:01<01:27, 168.46it/s][A

 56%|█████▋    | 19029/33804 [02:02<01:27, 168.65it/s][A

 56%|█████▋    | 19046/33804 [02:02<01:27, 168.72it/s][A

 56%|█████▋    | 19063/33804 [02:02<01:27, 169.06it/s][A

 56%|█████▋    | 19080/33804 [02:02<01:27, 169.02it/s][A

 56%|█████▋    | 19097/33804 [02:02<01:27, 168.75it/s][A

 57%|█████▋    | 19114/33804 [02:02<01:26, 168.98it/s][A

 57%|█████▋    | 19131/33804 [02:02<01:27, 168.59it/s][A

 57%|█████▋    | 19149/33804 [02:02<01:26, 169.06it/s][A

 57%|█████▋    | 19166/33804 [02:02<01:26, 168.88it/s][A

 57%|█████▋    | 19183/33804 [02:02<01:26, 169.08it/s][A

 57%|█████▋    | 19200/33804 [02:03<01:26, 169.11it/s][A

 57%|█████▋    | 19218/33804 [02:03<01:26, 169.50it/s][A

 57%|█████▋    | 19235/33804 [02:03<01:26, 168.80it/s][A

 57%|█████▋    | 19252/33804 [02:03<01:26, 169.15it/s][A

 57%|█████▋    | 19269/33804 [02:03<01:26, 168.67it/s][A

 57%|█████▋    | 19287/33804 [02:03<01:25, 168.85it/s][A

 57%|█████▋    | 19305/33804 [02:03<01:25, 169.23it/s][A

 57%|█████▋    | 19322/33804 [02:03<01:25, 169.11it/s][A

 57%|█████▋    | 19339/33804 [02:03<01:25, 168.92it/s][A

 57%|█████▋    | 19356/33804 [02:04<01:25, 168.77it/s][A

 57%|█████▋    | 19373/33804 [02:04<01:25, 168.60it/s][A

 57%|█████▋    | 19390/33804 [02:04<01:25, 168.58it/s][A

 57%|█████▋    | 19407/33804 [02:04<01:25, 168.91it/s][A

 57%|█████▋    | 19424/33804 [02:04<01:25, 169.06it/s][A

 58%|█████▊    | 19442/33804 [02:04<01:24, 169.40it/s][A

 58%|█████▊    | 19459/33804 [02:04<01:24, 169.23it/s][A

 58%|█████▊    | 19476/33804 [02:04<01:24, 169.37it/s][A

 58%|█████▊    | 19493/33804 [02:04<01:24, 169.26it/s][A

 58%|█████▊    | 19510/33804 [02:05<01:50, 129.22it/s][A

 58%|█████▊    | 19526/33804 [02:05<01:44, 135.98it/s][A

 58%|█████▊    | 19543/33804 [02:05<01:38, 144.06it/s][A

 58%|█████▊    | 19560/33804 [02:05<01:34, 150.71it/s][A

 58%|█████▊    | 19577/33804 [02:05<01:31, 155.54it/s][A

 58%|█████▊    | 19594/33804 [02:05<01:29, 158.98it/s][A

 58%|█████▊    | 19611/33804 [02:05<01:27, 161.73it/s][A

 58%|█████▊    | 19628/33804 [02:05<01:26, 163.92it/s][A

 58%|█████▊    | 19645/33804 [02:05<01:25, 165.27it/s][A

 58%|█████▊    | 19663/33804 [02:05<01:24, 166.83it/s][A

 58%|█████▊    | 19680/33804 [02:06<01:24, 167.48it/s][A

 58%|█████▊    | 19697/33804 [02:06<01:23, 168.22it/s][A

 58%|█████▊    | 19714/33804 [02:06<01:23, 168.31it/s][A

 58%|█████▊    | 19731/33804 [02:06<01:23, 168.36it/s][A

 58%|█████▊    | 19749/33804 [02:06<01:23, 168.86it/s][A

 58%|█████▊    | 19766/33804 [02:06<01:23, 168.57it/s][A

 59%|█████▊    | 19784/33804 [02:06<01:22, 169.09it/s][A

 59%|█████▊    | 19801/33804 [02:06<01:22, 168.87it/s][A

 59%|█████▊    | 19819/33804 [02:06<01:22, 169.42it/s][A

 59%|█████▊    | 19836/33804 [02:06<01:22, 168.83it/s][A

 59%|█████▊    | 19853/33804 [02:07<01:22, 169.04it/s][A

 59%|█████▉    | 19870/33804 [02:07<01:22, 168.29it/s][A

 59%|█████▉    | 19887/33804 [02:07<01:22, 168.78it/s][A

 59%|█████▉    | 19904/33804 [02:07<01:22, 168.76it/s][A

 59%|█████▉    | 19921/33804 [02:07<01:22, 168.87it/s][A

 59%|█████▉    | 19939/33804 [02:07<01:21, 169.23it/s][A

 59%|█████▉    | 19956/33804 [02:07<01:21, 169.30it/s][A

 59%|█████▉    | 19973/33804 [02:07<01:21, 169.38it/s][A

 59%|█████▉    | 19990/33804 [02:07<01:21, 169.35it/s][A

 59%|█████▉    | 20007/33804 [02:07<01:21, 169.44it/s][A

 59%|█████▉    | 20024/33804 [02:08<01:21, 168.93it/s][A

 59%|█████▉    | 20041/33804 [02:08<01:21, 169.13it/s][A

 59%|█████▉    | 20058/33804 [02:08<01:21, 169.04it/s][A

 59%|█████▉    | 20075/33804 [02:08<01:21, 169.10it/s][A

 59%|█████▉    | 20092/33804 [02:08<01:21, 168.95it/s][A

 59%|█████▉    | 20109/33804 [02:08<01:20, 169.16it/s][A

 60%|█████▉    | 20126/33804 [02:08<01:20, 169.18it/s][A

 60%|█████▉    | 20143/33804 [02:08<01:20, 168.94it/s][A

 60%|█████▉    | 20161/33804 [02:08<01:20, 169.36it/s][A

 60%|█████▉    | 20178/33804 [02:08<01:20, 169.28it/s][A

 60%|█████▉    | 20195/33804 [02:09<01:20, 169.40it/s][A

 60%|█████▉    | 20212/33804 [02:09<02:15, 100.05it/s][A

 60%|█████▉    | 20228/33804 [02:09<02:01, 111.51it/s][A

 60%|█████▉    | 20245/33804 [02:09<01:49, 124.18it/s][A

 60%|█████▉    | 20262/33804 [02:09<01:40, 135.05it/s][A

 60%|█████▉    | 20279/33804 [02:09<01:34, 143.33it/s][A

 60%|██████    | 20296/33804 [02:09<01:29, 150.32it/s][A

 60%|██████    | 20313/33804 [02:10<01:27, 154.85it/s][A

 60%|██████    | 20330/33804 [02:10<01:24, 158.70it/s][A

 60%|██████    | 20347/33804 [02:10<01:23, 161.36it/s][A

 60%|██████    | 20364/33804 [02:10<01:22, 163.45it/s][A

 60%|██████    | 20382/33804 [02:10<01:21, 165.52it/s][A

 60%|██████    | 20399/33804 [02:10<01:20, 165.53it/s][A

 60%|██████    | 20416/33804 [02:10<01:20, 166.61it/s][A

 60%|██████    | 20433/33804 [02:10<01:20, 167.12it/s][A

 60%|██████    | 20450/33804 [02:10<01:19, 167.94it/s][A

 61%|██████    | 20467/33804 [02:10<01:19, 168.09it/s][A

 61%|██████    | 20485/33804 [02:11<01:18, 168.77it/s][A

 61%|██████    | 20502/33804 [02:11<01:18, 168.43it/s][A

 61%|██████    | 20519/33804 [02:11<01:18, 168.43it/s][A

 61%|██████    | 20536/33804 [02:11<01:18, 168.25it/s][A

 61%|██████    | 20553/33804 [02:11<01:18, 168.37it/s][A

 61%|██████    | 20570/33804 [02:11<01:18, 168.09it/s][A

 61%|██████    | 20587/33804 [02:11<01:18, 167.81it/s][A

 61%|██████    | 20604/33804 [02:11<01:18, 168.37it/s][A

 61%|██████    | 20621/33804 [02:11<01:18, 168.53it/s][A

 61%|██████    | 20639/33804 [02:11<01:17, 169.07it/s][A

 61%|██████    | 20656/33804 [02:12<01:17, 169.14it/s][A

 61%|██████    | 20673/33804 [02:12<01:17, 169.32it/s][A

 61%|██████    | 20690/33804 [02:12<01:17, 168.84it/s][A

 61%|██████▏   | 20708/33804 [02:12<01:17, 169.28it/s][A

 61%|██████▏   | 20725/33804 [02:12<01:17, 168.98it/s][A

 61%|██████▏   | 20743/33804 [02:12<01:17, 169.32it/s][A

 61%|██████▏   | 20760/33804 [02:12<01:17, 169.10it/s][A

 61%|██████▏   | 20777/33804 [02:12<01:17, 167.80it/s][A

 62%|██████▏   | 20794/33804 [02:12<01:17, 167.99it/s][A

 62%|██████▏   | 20811/33804 [02:12<01:17, 167.53it/s][A

 62%|██████▏   | 20828/33804 [02:13<01:17, 168.05it/s][A

 62%|██████▏   | 20845/33804 [02:13<01:17, 167.96it/s][A

 62%|██████▏   | 20862/33804 [02:13<01:16, 168.36it/s][A

 62%|██████▏   | 20879/33804 [02:13<01:16, 168.30it/s][A

 62%|██████▏   | 20897/33804 [02:13<01:16, 168.86it/s][A

 62%|██████▏   | 20914/33804 [02:13<01:39, 129.77it/s][A

 62%|██████▏   | 20931/33804 [02:13<01:32, 139.38it/s][A

 62%|██████▏   | 20948/33804 [02:13<01:27, 146.80it/s][A

 62%|██████▏   | 20966/33804 [02:13<01:23, 152.99it/s][A

 62%|██████▏   | 20983/33804 [02:14<01:21, 157.29it/s][A

 62%|██████▏   | 21000/33804 [02:14<01:19, 160.48it/s][A

 62%|██████▏   | 21017/33804 [02:14<01:18, 163.16it/s][A

 62%|██████▏   | 21034/33804 [02:14<01:17, 164.78it/s][A

 62%|██████▏   | 21051/33804 [02:14<01:16, 165.92it/s][A

 62%|██████▏   | 21068/33804 [02:14<01:16, 166.69it/s][A

 62%|██████▏   | 21085/33804 [02:14<01:16, 167.07it/s][A

 62%|██████▏   | 21102/33804 [02:14<01:15, 167.63it/s][A

 62%|██████▏   | 21120/33804 [02:14<01:15, 168.44it/s][A

 63%|██████▎   | 21137/33804 [02:15<01:15, 168.64it/s][A

 63%|██████▎   | 21155/33804 [02:15<01:14, 169.00it/s][A

 63%|██████▎   | 21172/33804 [02:15<01:15, 167.90it/s][A

 63%|██████▎   | 21189/33804 [02:15<01:15, 167.91it/s][A

 63%|██████▎   | 21206/33804 [02:15<01:14, 168.49it/s][A

 63%|██████▎   | 21223/33804 [02:15<01:14, 168.43it/s][A

 63%|██████▎   | 21240/33804 [02:15<01:14, 168.60it/s][A

 63%|██████▎   | 21257/33804 [02:15<01:14, 168.47it/s][A

 63%|██████▎   | 21274/33804 [02:15<01:14, 168.55it/s][A

 63%|██████▎   | 21291/33804 [02:15<01:14, 168.33it/s][A

 63%|██████▎   | 21308/33804 [02:16<01:14, 168.71it/s][A

 63%|██████▎   | 21325/33804 [02:16<01:14, 168.53it/s][A

 63%|██████▎   | 21342/33804 [02:16<01:13, 168.96it/s][A

 63%|██████▎   | 21359/33804 [02:16<01:13, 168.81it/s][A

 63%|██████▎   | 21376/33804 [02:16<01:13, 169.13it/s][A

 63%|██████▎   | 21393/33804 [02:16<01:13, 168.43it/s][A

 63%|██████▎   | 21410/33804 [02:16<01:13, 168.07it/s][A

 63%|██████▎   | 21428/33804 [02:16<01:13, 168.75it/s][A

 63%|██████▎   | 21445/33804 [02:16<01:13, 168.38it/s][A

 63%|██████▎   | 21463/33804 [02:16<01:12, 169.08it/s][A

 64%|██████▎   | 21480/33804 [02:17<01:13, 168.74it/s][A

 64%|██████▎   | 21497/33804 [02:17<01:12, 168.61it/s][A

 64%|██████▎   | 21514/33804 [02:17<01:13, 168.35it/s][A

 64%|██████▎   | 21531/33804 [02:17<01:12, 168.64it/s][A

 64%|██████▎   | 21548/33804 [02:17<01:13, 167.45it/s][A

 64%|██████▍   | 21565/33804 [02:17<01:13, 167.63it/s][A

 64%|██████▍   | 21582/33804 [02:17<01:12, 167.67it/s][A

 64%|██████▍   | 21599/33804 [02:17<01:15, 162.24it/s][A

 64%|██████▍   | 21616/33804 [02:17<01:35, 128.06it/s][A

 64%|██████▍   | 21633/33804 [02:18<01:28, 137.80it/s][A

 64%|██████▍   | 21650/33804 [02:18<01:23, 145.73it/s][A

 64%|██████▍   | 21667/33804 [02:18<01:19, 151.79it/s][A

 64%|██████▍   | 21684/33804 [02:18<01:17, 156.59it/s][A

 64%|██████▍   | 21701/33804 [02:18<01:15, 159.62it/s][A

 64%|██████▍   | 21718/33804 [02:18<01:14, 162.45it/s][A

 64%|██████▍   | 21735/33804 [02:18<01:13, 164.13it/s][A

 64%|██████▍   | 21752/33804 [02:18<01:12, 165.75it/s][A

 64%|██████▍   | 21769/33804 [02:18<01:12, 166.35it/s][A

 64%|██████▍   | 21786/33804 [02:18<01:11, 166.95it/s][A

 64%|██████▍   | 21803/33804 [02:19<01:11, 167.10it/s][A

 65%|██████▍   | 21820/33804 [02:19<01:11, 167.60it/s][A

 65%|██████▍   | 21837/33804 [02:19<01:11, 167.82it/s][A

 65%|██████▍   | 21854/33804 [02:19<01:11, 168.06it/s][A

 65%|██████▍   | 21871/33804 [02:19<01:10, 168.64it/s][A

 65%|██████▍   | 21888/33804 [02:19<01:10, 168.70it/s][A

 65%|██████▍   | 21905/33804 [02:19<01:10, 168.98it/s][A

 65%|██████▍   | 21922/33804 [02:19<01:10, 167.77it/s][A

 65%|██████▍   | 21939/33804 [02:19<01:10, 168.27it/s][A

 65%|██████▍   | 21956/33804 [02:19<01:10, 168.44it/s][A

 65%|██████▌   | 21973/33804 [02:20<01:10, 168.66it/s][A

 65%|██████▌   | 21990/33804 [02:20<01:10, 168.25it/s][A

 65%|██████▌   | 22007/33804 [02:20<01:10, 168.27it/s][A

 65%|██████▌   | 22024/33804 [02:20<01:10, 168.24it/s][A

 65%|██████▌   | 22042/33804 [02:20<01:09, 168.80it/s][A

 65%|██████▌   | 22059/33804 [02:20<01:09, 168.45it/s][A

 65%|██████▌   | 22076/33804 [02:20<01:09, 167.97it/s][A

 65%|██████▌   | 22093/33804 [02:20<01:09, 168.54it/s][A

 65%|██████▌   | 22110/33804 [02:20<01:09, 168.78it/s][A

 65%|██████▌   | 22127/33804 [02:20<01:09, 169.14it/s][A

 66%|██████▌   | 22144/33804 [02:21<01:09, 168.83it/s][A

 66%|██████▌   | 22161/33804 [02:21<01:08, 169.02it/s][A

 66%|██████▌   | 22178/33804 [02:21<01:09, 168.24it/s][A

 66%|██████▌   | 22195/33804 [02:21<01:08, 168.71it/s][A

 66%|██████▌   | 22212/33804 [02:21<01:08, 168.51it/s][A

 66%|██████▌   | 22229/33804 [02:21<01:08, 168.88it/s][A

 66%|██████▌   | 22246/33804 [02:21<01:08, 168.42it/s][A

 66%|██████▌   | 22263/33804 [02:21<01:08, 168.81it/s][A

 66%|██████▌   | 22280/33804 [02:21<01:08, 168.32it/s][A

 66%|██████▌   | 22297/33804 [02:22<01:08, 168.37it/s][A

 66%|██████▌   | 22314/33804 [02:22<01:30, 126.74it/s][A

 66%|██████▌   | 22331/33804 [02:22<01:23, 136.75it/s][A

 66%|██████▌   | 22348/33804 [02:22<01:18, 145.16it/s][A

 66%|██████▌   | 22365/33804 [02:22<01:15, 150.85it/s][A

 66%|██████▌   | 22382/33804 [02:22<01:13, 156.01it/s][A

 66%|██████▋   | 22399/33804 [02:22<01:11, 159.38it/s][A

 66%|██████▋   | 22416/33804 [02:22<01:10, 161.93it/s][A

 66%|██████▋   | 22433/33804 [02:22<01:09, 163.94it/s][A

 66%|██████▋   | 22450/33804 [02:23<01:08, 165.40it/s][A

 66%|██████▋   | 22467/33804 [02:23<01:08, 165.74it/s][A

 67%|██████▋   | 22484/33804 [02:23<01:07, 166.78it/s][A

 67%|██████▋   | 22501/33804 [02:23<01:07, 167.09it/s][A

 67%|██████▋   | 22518/33804 [02:23<01:07, 167.25it/s][A

 67%|██████▋   | 22535/33804 [02:23<01:07, 167.48it/s][A

 67%|██████▋   | 22552/33804 [02:23<01:07, 167.71it/s][A

 67%|██████▋   | 22569/33804 [02:23<01:06, 168.27it/s][A

 67%|██████▋   | 22586/33804 [02:23<01:06, 168.43it/s][A

 67%|██████▋   | 22604/33804 [02:23<01:06, 168.95it/s][A

 67%|██████▋   | 22621/33804 [02:24<01:06, 169.00it/s][A

 67%|██████▋   | 22638/33804 [02:24<01:06, 168.34it/s][A

 67%|██████▋   | 22655/33804 [02:24<01:06, 168.43it/s][A

 67%|██████▋   | 22672/33804 [02:24<01:06, 168.60it/s][A

 67%|██████▋   | 22689/33804 [02:24<01:05, 168.64it/s][A

 67%|██████▋   | 22706/33804 [02:24<01:06, 167.92it/s][A

 67%|██████▋   | 22723/33804 [02:24<01:05, 168.12it/s][A

 67%|██████▋   | 22740/33804 [02:24<01:05, 167.83it/s][A

 67%|██████▋   | 22757/33804 [02:24<01:05, 168.14it/s][A

 67%|██████▋   | 22774/33804 [02:24<01:05, 168.09it/s][A

 67%|██████▋   | 22792/33804 [02:25<01:05, 168.77it/s][A

 67%|██████▋   | 22809/33804 [02:25<01:05, 168.21it/s][A

 68%|██████▊   | 22826/33804 [02:25<01:05, 168.69it/s][A

 68%|██████▊   | 22843/33804 [02:25<01:05, 168.54it/s][A

 68%|██████▊   | 22861/33804 [02:25<01:04, 169.16it/s][A

 68%|██████▊   | 22878/33804 [02:25<01:04, 168.90it/s][A

 68%|██████▊   | 22895/33804 [02:25<01:04, 169.08it/s][A

 68%|██████▊   | 22912/33804 [02:25<01:04, 168.87it/s][A

 68%|██████▊   | 22929/33804 [02:25<01:04, 169.02it/s][A

 68%|██████▊   | 22946/33804 [02:25<01:04, 168.96it/s][A

 68%|██████▊   | 22963/33804 [02:26<01:04, 168.79it/s][A

 68%|██████▊   | 22980/33804 [02:26<01:04, 168.83it/s][A

 68%|██████▊   | 22997/33804 [02:26<01:06, 162.68it/s][A

 68%|██████▊   | 23014/33804 [02:26<01:24, 127.43it/s][A

 68%|██████▊   | 23031/33804 [02:26<01:18, 137.31it/s][A

 68%|██████▊   | 23049/33804 [02:26<01:13, 146.08it/s][A

 68%|██████▊   | 23066/33804 [02:26<01:10, 152.06it/s][A

 68%|██████▊   | 23083/33804 [02:26<01:08, 156.18it/s][A

 68%|██████▊   | 23100/33804 [02:26<01:07, 159.67it/s][A

 68%|██████▊   | 23117/33804 [02:27<01:05, 162.23it/s][A

 68%|██████▊   | 23134/33804 [02:27<01:05, 164.09it/s][A

 68%|██████▊   | 23151/33804 [02:27<01:04, 165.30it/s][A

 69%|██████▊   | 23168/33804 [02:27<01:04, 166.14it/s][A

 69%|██████▊   | 23185/33804 [02:27<01:03, 166.68it/s][A

 69%|██████▊   | 23202/33804 [02:27<01:03, 167.37it/s][A

 69%|██████▊   | 23219/33804 [02:27<01:03, 167.34it/s][A

 69%|██████▊   | 23236/33804 [02:27<01:03, 167.38it/s][A

 69%|██████▉   | 23253/33804 [02:27<01:03, 167.25it/s][A

 69%|██████▉   | 23270/33804 [02:28<01:02, 167.77it/s][A

 69%|██████▉   | 23287/33804 [02:28<01:02, 167.68it/s][A

 69%|██████▉   | 23304/33804 [02:28<01:02, 168.26it/s][A

 69%|██████▉   | 23321/33804 [02:28<01:02, 168.58it/s][A

 69%|██████▉   | 23338/33804 [02:28<01:01, 168.94it/s][A

 69%|██████▉   | 23355/33804 [02:28<01:01, 168.94it/s][A

 69%|██████▉   | 23372/33804 [02:28<01:01, 168.67it/s][A

 69%|██████▉   | 23389/33804 [02:28<01:01, 168.71it/s][A

 69%|██████▉   | 23406/33804 [02:28<01:01, 168.58it/s][A

 69%|██████▉   | 23424/33804 [02:28<01:01, 169.16it/s][A

 69%|██████▉   | 23441/33804 [02:30<06:02, 28.59it/s] [A

 69%|██████▉   | 23454/33804 [02:30<04:53, 35.26it/s][A

 69%|██████▉   | 23471/33804 [02:30<03:40, 46.85it/s][A

 69%|██████▉   | 23488/33804 [02:30<02:50, 60.33it/s][A

 70%|██████▉   | 23505/33804 [02:31<02:17, 75.07it/s][A

 70%|██████▉   | 23522/33804 [02:31<01:53, 90.35it/s][A

 70%|██████▉   | 23539/33804 [02:31<01:37, 105.23it/s][A

 70%|██████▉   | 23557/33804 [02:31<01:25, 119.58it/s][A

 70%|██████▉   | 23574/33804 [02:31<01:18, 131.09it/s][A

 70%|██████▉   | 23591/33804 [02:31<01:12, 139.94it/s][A

 70%|██████▉   | 23608/33804 [02:31<01:09, 147.33it/s][A

 70%|██████▉   | 23625/33804 [02:31<01:06, 153.34it/s][A

 70%|██████▉   | 23642/33804 [02:31<01:04, 157.02it/s][A

 70%|██████▉   | 23659/33804 [02:32<01:03, 160.59it/s][A

 70%|███████   | 23676/33804 [02:32<01:02, 162.53it/s][A

 70%|███████   | 23693/33804 [02:32<01:01, 164.05it/s][A

 70%|███████   | 23710/33804 [02:32<01:00, 165.51it/s][A

 70%|███████   | 23727/33804 [02:32<01:00, 165.86it/s][A

 70%|███████   | 23744/33804 [02:32<01:00, 166.81it/s][A

 70%|███████   | 23761/33804 [02:32<01:00, 167.16it/s][A

 70%|███████   | 23778/33804 [02:32<00:59, 167.93it/s][A

 70%|███████   | 23795/33804 [02:32<00:59, 167.96it/s][A

 70%|███████   | 23812/33804 [02:32<00:59, 168.45it/s][A

 70%|███████   | 23829/33804 [02:33<00:59, 168.38it/s][A

 71%|███████   | 23846/33804 [02:33<00:59, 168.09it/s][A

 71%|███████   | 23863/33804 [02:33<00:59, 168.26it/s][A

 71%|███████   | 23880/33804 [02:33<00:58, 168.41it/s][A

 71%|███████   | 23897/33804 [02:33<00:58, 168.40it/s][A

 71%|███████   | 23914/33804 [02:33<00:58, 168.74it/s][A

 71%|███████   | 23931/33804 [02:33<00:58, 168.44it/s][A

 71%|███████   | 23948/33804 [02:33<00:58, 167.96it/s][A

 71%|███████   | 23965/33804 [02:33<00:58, 168.30it/s][A

 71%|███████   | 23982/33804 [02:33<00:58, 166.79it/s][A

 71%|███████   | 23999/33804 [02:34<00:58, 167.37it/s][A

 71%|███████   | 24016/33804 [02:34<00:58, 166.34it/s][A

 71%|███████   | 24033/33804 [02:34<00:58, 166.97it/s][A

 71%|███████   | 24050/33804 [02:34<00:58, 167.12it/s][A

 71%|███████   | 24067/33804 [02:34<00:58, 167.53it/s][A

 71%|███████   | 24084/33804 [02:34<00:58, 167.37it/s][A

 71%|███████▏  | 24101/33804 [02:34<00:57, 167.64it/s][A

 71%|███████▏  | 24118/33804 [02:34<00:57, 167.50it/s][A

 71%|███████▏  | 24135/33804 [02:34<01:07, 143.07it/s][A

 71%|███████▏  | 24150/33804 [02:35<01:14, 130.20it/s][A

 71%|███████▏  | 24167/33804 [02:35<01:08, 139.74it/s][A

 72%|███████▏  | 24184/33804 [02:35<01:05, 146.03it/s][A

 72%|███████▏  | 24201/33804 [02:35<01:03, 151.25it/s][A

 72%|███████▏  | 24218/33804 [02:35<01:01, 155.84it/s][A

 72%|███████▏  | 24235/33804 [02:35<01:00, 158.93it/s][A

 72%|███████▏  | 24252/33804 [02:35<00:59, 161.00it/s][A

 72%|███████▏  | 24269/33804 [02:35<00:58, 162.42it/s][A

 72%|███████▏  | 24286/33804 [02:35<00:57, 164.23it/s][A

 72%|███████▏  | 24303/33804 [02:35<00:57, 165.38it/s][A

 72%|███████▏  | 24320/33804 [02:36<00:57, 166.37it/s][A

 72%|███████▏  | 24337/33804 [02:36<00:56, 166.20it/s][A

 72%|███████▏  | 24354/33804 [02:36<00:56, 167.06it/s][A

 72%|███████▏  | 24371/33804 [02:36<00:57, 165.40it/s][A

 72%|███████▏  | 24388/33804 [02:36<00:56, 165.94it/s][A

 72%|███████▏  | 24405/33804 [02:36<00:56, 165.90it/s][A

 72%|███████▏  | 24422/33804 [02:36<00:56, 165.80it/s][A

 72%|███████▏  | 24439/33804 [02:36<00:56, 165.87it/s][A

 72%|███████▏  | 24456/33804 [02:36<00:56, 165.61it/s][A

 72%|███████▏  | 24473/33804 [02:36<00:56, 165.59it/s][A

 72%|███████▏  | 24490/33804 [02:37<00:56, 165.57it/s][A

 72%|███████▏  | 24507/33804 [02:37<00:55, 166.06it/s][A

 73%|███████▎  | 24524/33804 [02:37<00:55, 166.27it/s][A

 73%|███████▎  | 24541/33804 [02:37<00:55, 166.78it/s][A

 73%|███████▎  | 24558/33804 [02:37<00:55, 166.82it/s][A

 73%|███████▎  | 24575/33804 [02:37<00:55, 167.15it/s][A

 73%|███████▎  | 24592/33804 [02:37<00:55, 166.96it/s][A

 73%|███████▎  | 24609/33804 [02:37<00:54, 167.38it/s][A

 73%|███████▎  | 24626/33804 [02:37<00:54, 167.19it/s][A

 73%|███████▎  | 24643/33804 [02:38<00:54, 167.12it/s][A

 73%|███████▎  | 24660/33804 [02:38<00:54, 167.29it/s][A

 73%|███████▎  | 24677/33804 [02:38<00:54, 166.91it/s][A

 73%|███████▎  | 24694/33804 [02:38<00:54, 166.93it/s][A

 73%|███████▎  | 24711/33804 [02:38<00:54, 166.84it/s][A

 73%|███████▎  | 24728/33804 [02:38<00:54, 167.34it/s][A

 73%|███████▎  | 24745/33804 [02:38<00:54, 165.84it/s][A

 73%|███████▎  | 24762/33804 [02:38<00:54, 165.86it/s][A

 73%|███████▎  | 24779/33804 [02:38<00:54, 165.70it/s][A

 73%|███████▎  | 24796/33804 [02:38<00:54, 165.89it/s][A

 73%|███████▎  | 24813/33804 [02:39<00:54, 165.67it/s][A

 73%|███████▎  | 24830/33804 [02:39<01:08, 131.38it/s][A

 73%|███████▎  | 24845/33804 [02:39<01:09, 128.80it/s][A

 74%|███████▎  | 24862/33804 [02:39<01:04, 138.37it/s][A

 74%|███████▎  | 24879/33804 [02:39<01:01, 145.34it/s][A

 74%|███████▎  | 24896/33804 [02:39<00:58, 151.18it/s][A

 74%|███████▎  | 24913/33804 [02:39<00:57, 154.78it/s][A

 74%|███████▎  | 24930/33804 [02:39<00:56, 157.74it/s][A

 74%|███████▍  | 24947/33804 [02:39<00:55, 160.39it/s][A

 74%|███████▍  | 24964/33804 [02:40<00:54, 161.83it/s][A

 74%|███████▍  | 24981/33804 [02:40<00:54, 163.23it/s][A

 74%|███████▍  | 24998/33804 [02:40<00:53, 163.90it/s][A

 74%|███████▍  | 25015/33804 [02:40<00:53, 164.98it/s][A

 74%|███████▍  | 25032/33804 [02:40<00:52, 165.56it/s][A

 74%|███████▍  | 25049/33804 [02:40<00:52, 165.76it/s][A

 74%|███████▍  | 25066/33804 [02:40<00:52, 165.56it/s][A

 74%|███████▍  | 25083/33804 [02:40<00:52, 165.94it/s][A

 74%|███████▍  | 25100/33804 [02:40<00:52, 165.98it/s][A

 74%|███████▍  | 25117/33804 [02:40<00:52, 165.08it/s][A

 74%|███████▍  | 25134/33804 [02:41<00:52, 164.73it/s][A

 74%|███████▍  | 25151/33804 [02:41<00:52, 163.69it/s][A

 74%|███████▍  | 25168/33804 [02:41<00:52, 163.69it/s][A

 75%|███████▍  | 25185/33804 [02:41<00:52, 163.53it/s][A

 75%|███████▍  | 25202/33804 [02:41<00:52, 163.53it/s][A

 75%|███████▍  | 25219/33804 [02:41<00:52, 163.67it/s][A

 75%|███████▍  | 25236/33804 [02:41<00:52, 164.13it/s][A

 75%|███████▍  | 25253/33804 [02:41<00:52, 163.81it/s][A

 75%|███████▍  | 25270/33804 [02:41<00:51, 164.23it/s][A

 75%|███████▍  | 25287/33804 [02:42<00:51, 164.87it/s][A

 75%|███████▍  | 25304/33804 [02:42<00:51, 165.58it/s][A

 75%|███████▍  | 25321/33804 [02:42<00:51, 165.21it/s][A

 75%|███████▍  | 25338/33804 [02:42<00:51, 165.67it/s][A

 75%|███████▌  | 25355/33804 [02:42<00:50, 165.72it/s][A

 75%|███████▌  | 25372/33804 [02:42<00:50, 165.68it/s][A

 75%|███████▌  | 25389/33804 [02:42<00:50, 165.87it/s][A

 75%|███████▌  | 25406/33804 [02:42<00:50, 165.66it/s][A

 75%|███████▌  | 25423/33804 [02:42<00:50, 165.74it/s][A

 75%|███████▌  | 25440/33804 [02:42<00:50, 165.61it/s][A

 75%|███████▌  | 25457/33804 [02:43<00:50, 166.01it/s][A

 75%|███████▌  | 25474/33804 [02:43<00:50, 165.92it/s][A

 75%|███████▌  | 25491/33804 [02:43<00:50, 165.32it/s][A

 75%|███████▌  | 25508/33804 [02:43<00:52, 157.33it/s][A

 76%|███████▌  | 25524/33804 [02:43<01:04, 127.90it/s][A

 76%|███████▌  | 25541/33804 [02:43<01:00, 136.91it/s][A

 76%|███████▌  | 25558/33804 [02:43<00:57, 144.60it/s][A

 76%|███████▌  | 25575/33804 [02:43<00:54, 149.79it/s][A

 76%|███████▌  | 25592/33804 [02:43<00:53, 154.38it/s][A

 76%|███████▌  | 25609/33804 [02:44<00:52, 157.26it/s][A

 76%|███████▌  | 25626/33804 [02:44<00:51, 159.18it/s][A

 76%|███████▌  | 25643/33804 [02:44<00:50, 160.74it/s][A

 76%|███████▌  | 25660/33804 [02:44<00:50, 160.57it/s][A

 76%|███████▌  | 25677/33804 [02:44<00:50, 161.06it/s][A

 76%|███████▌  | 25694/33804 [02:44<00:49, 162.68it/s][A

 76%|███████▌  | 25711/33804 [02:44<00:49, 164.00it/s][A

 76%|███████▌  | 25728/33804 [02:44<00:49, 164.75it/s][A

 76%|███████▌  | 25745/33804 [02:44<00:48, 165.97it/s][A

 76%|███████▌  | 25762/33804 [02:44<00:48, 166.26it/s][A

 76%|███████▋  | 25779/33804 [02:45<00:48, 166.82it/s][A

 76%|███████▋  | 25796/33804 [02:45<00:48, 166.65it/s][A

 76%|███████▋  | 25813/33804 [02:45<00:47, 166.70it/s][A

 76%|███████▋  | 25830/33804 [02:45<00:47, 166.37it/s][A

 76%|███████▋  | 25847/33804 [02:45<00:47, 166.66it/s][A

 77%|███████▋  | 25864/33804 [02:45<00:47, 166.82it/s][A

 77%|███████▋  | 25881/33804 [02:45<00:47, 166.60it/s][A

 77%|███████▋  | 25898/33804 [02:45<00:47, 166.92it/s][A

 77%|███████▋  | 25915/33804 [02:45<00:47, 166.32it/s][A

 77%|███████▋  | 25932/33804 [02:46<00:47, 166.99it/s][A

 77%|███████▋  | 25949/33804 [02:46<00:47, 166.80it/s][A

 77%|███████▋  | 25966/33804 [02:46<00:46, 167.07it/s][A

 77%|███████▋  | 25983/33804 [02:46<00:46, 166.93it/s][A

 77%|███████▋  | 26000/33804 [02:46<00:47, 165.84it/s][A

 77%|███████▋  | 26017/33804 [02:46<00:47, 165.39it/s][A

 77%|███████▋  | 26034/33804 [02:46<00:46, 165.95it/s][A

 77%|███████▋  | 26051/33804 [02:46<00:46, 166.59it/s][A

 77%|███████▋  | 26068/33804 [02:46<00:46, 167.50it/s][A

 77%|███████▋  | 26085/33804 [02:46<00:46, 167.70it/s][A

 77%|███████▋  | 26102/33804 [02:47<00:45, 168.08it/s][A

 77%|███████▋  | 26119/33804 [02:47<00:45, 168.33it/s][A

 77%|███████▋  | 26136/33804 [02:47<00:45, 167.86it/s][A

 77%|███████▋  | 26153/33804 [02:47<00:45, 168.14it/s][A

 77%|███████▋  | 26170/33804 [02:47<00:45, 168.10it/s][A

 77%|███████▋  | 26187/33804 [02:47<00:45, 168.55it/s][A

 78%|███████▊  | 26204/33804 [02:47<00:48, 155.74it/s][A

 78%|███████▊  | 26220/33804 [02:47<00:58, 130.03it/s][A

 78%|███████▊  | 26237/33804 [02:47<00:54, 139.86it/s][A

 78%|███████▊  | 26254/33804 [02:48<00:51, 147.57it/s][A

 78%|███████▊  | 26271/33804 [02:48<00:49, 152.49it/s][A

 78%|███████▊  | 26288/33804 [02:48<00:47, 156.73it/s][A

 78%|███████▊  | 26305/33804 [02:48<00:46, 160.21it/s][A

 78%|███████▊  | 26322/33804 [02:48<00:46, 162.22it/s][A

 78%|███████▊  | 26340/33804 [02:48<00:45, 164.69it/s][A

 78%|███████▊  | 26357/33804 [02:48<00:44, 165.76it/s][A

 78%|███████▊  | 26375/33804 [02:48<00:44, 167.06it/s][A

 78%|███████▊  | 26392/33804 [02:48<00:44, 166.84it/s][A

 78%|███████▊  | 26410/33804 [02:48<00:44, 167.79it/s][A

 78%|███████▊  | 26427/33804 [02:49<00:43, 167.89it/s][A

 78%|███████▊  | 26444/33804 [02:49<00:43, 168.31it/s][A

 78%|███████▊  | 26461/33804 [02:49<00:43, 168.46it/s][A

 78%|███████▊  | 26479/33804 [02:49<00:43, 168.99it/s][A

 78%|███████▊  | 26496/33804 [02:49<00:43, 169.03it/s][A

 78%|███████▊  | 26513/33804 [02:49<00:43, 169.14it/s][A

 78%|███████▊  | 26530/33804 [02:49<00:43, 168.81it/s][A

 79%|███████▊  | 26547/33804 [02:49<00:43, 167.86it/s][A

 79%|███████▊  | 26564/33804 [02:49<00:43, 168.20it/s][A

 79%|███████▊  | 26581/33804 [02:49<00:42, 168.47it/s][A

 79%|███████▊  | 26599/33804 [02:50<00:42, 169.10it/s][A

 79%|███████▊  | 26616/33804 [02:50<00:42, 168.80it/s][A

 79%|███████▉  | 26633/33804 [02:50<00:42, 168.92it/s][A

 79%|███████▉  | 26650/33804 [02:50<00:42, 168.89it/s][A

 79%|███████▉  | 26667/33804 [02:50<00:42, 168.77it/s][A

 79%|███████▉  | 26684/33804 [02:50<00:42, 168.63it/s][A

 79%|███████▉  | 26701/33804 [02:50<00:42, 168.65it/s][A

 79%|███████▉  | 26718/33804 [02:50<00:41, 168.79it/s][A

 79%|███████▉  | 26735/33804 [02:50<00:41, 168.84it/s][A

 79%|███████▉  | 26753/33804 [02:50<00:41, 169.43it/s][A

 79%|███████▉  | 26770/33804 [02:51<00:41, 169.28it/s][A

 79%|███████▉  | 26787/33804 [02:51<00:41, 168.94it/s][A

 79%|███████▉  | 26804/33804 [02:51<00:41, 168.85it/s][A

 79%|███████▉  | 26821/33804 [02:51<00:41, 168.17it/s][A

 79%|███████▉  | 26838/33804 [02:51<00:41, 168.48it/s][A

 79%|███████▉  | 26856/33804 [02:51<00:41, 168.94it/s][A

 79%|███████▉  | 26873/33804 [02:51<00:41, 168.75it/s][A

 80%|███████▉  | 26890/33804 [02:51<00:40, 168.89it/s][A

 80%|███████▉  | 26907/33804 [02:51<00:50, 136.57it/s][A

 80%|███████▉  | 26922/33804 [02:52<00:52, 132.22it/s][A

 80%|███████▉  | 26939/33804 [02:52<00:48, 141.01it/s][A

 80%|███████▉  | 26956/33804 [02:52<00:46, 148.39it/s][A

 80%|███████▉  | 26974/33804 [02:52<00:44, 154.65it/s][A

 80%|███████▉  | 26991/33804 [02:52<00:42, 158.67it/s][A

 80%|███████▉  | 27008/33804 [02:52<00:42, 161.75it/s][A

 80%|███████▉  | 27025/33804 [02:52<00:41, 163.63it/s][A

 80%|███████▉  | 27043/33804 [02:52<00:40, 165.59it/s][A

 80%|████████  | 27060/33804 [02:52<00:40, 166.46it/s][A

 80%|████████  | 27077/33804 [02:53<00:40, 167.42it/s][A

 80%|████████  | 27094/33804 [02:53<00:40, 166.50it/s][A

 80%|████████  | 27111/33804 [02:53<00:40, 166.40it/s][A

 80%|████████  | 27128/33804 [02:53<00:40, 166.74it/s][A

 80%|████████  | 27145/33804 [02:53<00:39, 167.44it/s][A

 80%|████████  | 27162/33804 [02:53<00:39, 167.51it/s][A

 80%|████████  | 27179/33804 [02:53<00:39, 167.85it/s][A

 80%|████████  | 27196/33804 [02:53<00:39, 168.32it/s][A

 81%|████████  | 27213/33804 [02:53<00:39, 168.47it/s][A

 81%|████████  | 27231/33804 [02:53<00:38, 169.08it/s][A

 81%|████████  | 27248/33804 [02:54<00:38, 169.21it/s][A

 81%|████████  | 27265/33804 [02:54<00:38, 169.13it/s][A

 81%|████████  | 27282/33804 [02:54<00:38, 168.58it/s][A

 81%|████████  | 27300/33804 [02:54<00:38, 169.10it/s][A

 81%|████████  | 27317/33804 [02:54<00:38, 168.74it/s][A

 81%|████████  | 27335/33804 [02:54<00:38, 169.25it/s][A

 81%|████████  | 27352/33804 [02:54<00:38, 169.09it/s][A

 81%|████████  | 27369/33804 [02:54<00:38, 168.06it/s][A

 81%|████████  | 27386/33804 [02:54<00:38, 168.55it/s][A

 81%|████████  | 27403/33804 [02:54<00:38, 168.35it/s][A

 81%|████████  | 27420/33804 [02:55<00:37, 168.71it/s][A

 81%|████████  | 27437/33804 [02:55<00:37, 168.61it/s][A

 81%|████████  | 27454/33804 [02:55<00:37, 168.92it/s][A

 81%|████████▏ | 27471/33804 [02:55<00:37, 168.87it/s][A

 81%|████████▏ | 27489/33804 [02:55<00:37, 169.50it/s][A

 81%|████████▏ | 27506/33804 [02:55<00:37, 169.55it/s][A

 81%|████████▏ | 27524/33804 [02:55<00:37, 169.70it/s][A

 81%|████████▏ | 27541/33804 [02:55<00:36, 169.55it/s][A

 82%|████████▏ | 27558/33804 [02:55<00:36, 169.31it/s][A

 82%|████████▏ | 27575/33804 [02:55<00:36, 169.46it/s][A

 82%|████████▏ | 27592/33804 [02:56<00:37, 166.33it/s][A

 82%|████████▏ | 27609/33804 [02:56<00:45, 136.31it/s][A

 82%|████████▏ | 27624/33804 [02:56<00:47, 131.38it/s][A

 82%|████████▏ | 27641/33804 [02:56<00:43, 140.87it/s][A

 82%|████████▏ | 27658/33804 [02:56<00:41, 147.92it/s][A

 82%|████████▏ | 27675/33804 [02:56<00:39, 153.81it/s][A

 82%|████████▏ | 27692/33804 [02:56<00:38, 158.05it/s][A

 82%|████████▏ | 27710/33804 [02:56<00:37, 161.69it/s][A

 82%|████████▏ | 27727/33804 [02:57<00:37, 163.83it/s][A

 82%|████████▏ | 27745/33804 [02:57<00:36, 165.82it/s][A

 82%|████████▏ | 27762/33804 [02:57<00:36, 166.66it/s][A

 82%|████████▏ | 27779/33804 [02:57<00:35, 167.58it/s][A

 82%|████████▏ | 27796/33804 [02:57<00:35, 167.70it/s][A

 82%|████████▏ | 27813/33804 [02:57<00:35, 168.23it/s][A

 82%|████████▏ | 27830/33804 [02:57<00:35, 168.74it/s][A

 82%|████████▏ | 27847/33804 [02:57<00:35, 168.70it/s][A

 82%|████████▏ | 27864/33804 [02:57<00:35, 168.53it/s][A

 82%|████████▏ | 27881/33804 [02:57<00:35, 167.92it/s][A

 83%|████████▎ | 27898/33804 [02:58<00:35, 168.34it/s][A

 83%|████████▎ | 27915/33804 [02:58<00:34, 168.29it/s][A

 83%|████████▎ | 27933/33804 [02:58<00:34, 168.85it/s][A

 83%|████████▎ | 27950/33804 [02:58<00:34, 168.97it/s][A

 83%|████████▎ | 27968/33804 [02:58<00:34, 169.41it/s][A

 83%|████████▎ | 27985/33804 [02:58<00:34, 169.31it/s][A

 83%|████████▎ | 28002/33804 [02:58<00:34, 169.00it/s][A

 83%|████████▎ | 28020/33804 [02:58<00:34, 169.40it/s][A

 83%|████████▎ | 28037/33804 [02:58<00:34, 169.19it/s][A

 83%|████████▎ | 28054/33804 [02:58<00:33, 169.21it/s][A

 83%|████████▎ | 28071/33804 [02:59<00:33, 168.94it/s][A

 83%|████████▎ | 28088/33804 [02:59<00:33, 168.99it/s][A

 83%|████████▎ | 28105/33804 [02:59<00:33, 168.41it/s][A

 83%|████████▎ | 28122/33804 [02:59<00:33, 168.50it/s][A

 83%|████████▎ | 28139/33804 [02:59<00:33, 168.34it/s][A

 83%|████████▎ | 28156/33804 [02:59<00:33, 167.83it/s][A

 83%|████████▎ | 28173/33804 [02:59<00:33, 168.07it/s][A

 83%|████████▎ | 28190/33804 [02:59<00:33, 168.13it/s][A

 83%|████████▎ | 28207/33804 [02:59<00:33, 168.17it/s][A

 83%|████████▎ | 28224/33804 [02:59<00:33, 168.54it/s][A

 84%|████████▎ | 28242/33804 [03:00<00:32, 169.16it/s][A

 84%|████████▎ | 28259/33804 [03:00<00:32, 169.33it/s][A

 84%|████████▎ | 28276/33804 [03:00<00:32, 169.30it/s][A

 84%|████████▎ | 28293/33804 [03:00<00:32, 169.31it/s][A

 84%|████████▎ | 28310/33804 [03:00<00:40, 134.04it/s][A

 84%|████████▍ | 28325/33804 [03:00<00:41, 133.45it/s][A

 84%|████████▍ | 28342/33804 [03:00<00:38, 142.62it/s][A

 84%|████████▍ | 28359/33804 [03:00<00:36, 148.98it/s][A

 84%|████████▍ | 28376/33804 [03:00<00:35, 154.37it/s][A

 84%|████████▍ | 28393/33804 [03:01<00:34, 157.82it/s][A

 84%|████████▍ | 28410/33804 [03:01<00:33, 160.89it/s][A

 84%|████████▍ | 28427/33804 [03:01<00:33, 162.07it/s][A

 84%|████████▍ | 28444/33804 [03:01<00:32, 164.32it/s][A

 84%|████████▍ | 28461/33804 [03:01<00:32, 165.53it/s][A

 84%|████████▍ | 28478/33804 [03:01<00:31, 166.57it/s][A

 84%|████████▍ | 28495/33804 [03:01<00:31, 167.50it/s][A

 84%|████████▍ | 28512/33804 [03:01<00:31, 167.93it/s][A

 84%|████████▍ | 28529/33804 [03:01<00:31, 168.47it/s][A

 84%|████████▍ | 28546/33804 [03:01<00:31, 168.33it/s][A

 84%|████████▍ | 28564/33804 [03:02<00:31, 168.92it/s][A

 85%|████████▍ | 28581/33804 [03:02<00:30, 168.71it/s][A

 85%|████████▍ | 28598/33804 [03:02<00:30, 168.87it/s][A

 85%|████████▍ | 28615/33804 [03:02<00:30, 168.26it/s][A

 85%|████████▍ | 28632/33804 [03:02<00:30, 168.65it/s][A

 85%|████████▍ | 28649/33804 [03:02<00:30, 167.25it/s][A

 85%|████████▍ | 28666/33804 [03:02<00:30, 167.96it/s][A

 85%|████████▍ | 28683/33804 [03:02<00:30, 167.51it/s][A

 85%|████████▍ | 28700/33804 [03:02<00:30, 167.90it/s][A

 85%|████████▍ | 28718/33804 [03:02<00:30, 168.63it/s][A

 85%|████████▌ | 28735/33804 [03:03<00:30, 168.41it/s][A

 85%|████████▌ | 28752/33804 [03:03<00:29, 168.78it/s][A

 85%|████████▌ | 28769/33804 [03:03<00:29, 168.78it/s][A

 85%|████████▌ | 28786/33804 [03:03<00:29, 168.85it/s][A

 85%|████████▌ | 28803/33804 [03:03<00:29, 168.65it/s][A

 85%|████████▌ | 28821/33804 [03:03<00:29, 169.22it/s][A

 85%|████████▌ | 28838/33804 [03:03<00:29, 168.85it/s][A

 85%|████████▌ | 28855/33804 [03:03<00:29, 169.09it/s][A

 85%|████████▌ | 28872/33804 [03:03<00:29, 168.85it/s][A

 85%|████████▌ | 28889/33804 [03:04<00:29, 168.35it/s][A

 86%|████████▌ | 28906/33804 [03:04<00:29, 168.52it/s][A

 86%|████████▌ | 28923/33804 [03:04<00:28, 168.61it/s][A

 86%|████████▌ | 28940/33804 [03:04<00:28, 169.02it/s][A

 86%|████████▌ | 28957/33804 [03:04<00:28, 168.73it/s][A

 86%|████████▌ | 28974/33804 [03:04<00:28, 168.97it/s][A

 86%|████████▌ | 28991/33804 [03:04<00:28, 168.90it/s][A

 86%|████████▌ | 29008/33804 [03:04<00:34, 137.36it/s][A

 86%|████████▌ | 29023/33804 [03:04<00:36, 131.87it/s][A

 86%|████████▌ | 29040/33804 [03:05<00:33, 141.42it/s][A

 86%|████████▌ | 29057/33804 [03:05<00:31, 148.53it/s][A

 86%|████████▌ | 29074/33804 [03:05<00:30, 154.24it/s][A

 86%|████████▌ | 29091/33804 [03:05<00:29, 157.63it/s][A

 86%|████████▌ | 29108/33804 [03:05<00:29, 160.59it/s][A

 86%|████████▌ | 29125/33804 [03:05<00:28, 162.58it/s][A

 86%|████████▌ | 29142/33804 [03:05<00:28, 163.62it/s][A

 86%|████████▋ | 29159/33804 [03:05<00:28, 165.23it/s][A

 86%|████████▋ | 29176/33804 [03:05<00:27, 165.53it/s][A

 86%|████████▋ | 29193/33804 [03:05<00:27, 166.84it/s][A

 86%|████████▋ | 29210/33804 [03:06<00:27, 167.13it/s][A

 86%|████████▋ | 29227/33804 [03:06<00:27, 167.85it/s][A

 87%|████████▋ | 29244/33804 [03:06<00:27, 168.22it/s][A

 87%|████████▋ | 29261/33804 [03:06<00:26, 168.64it/s][A

 87%|████████▋ | 29278/33804 [03:06<00:26, 168.58it/s][A

 87%|████████▋ | 29295/33804 [03:06<00:26, 168.86it/s][A

 87%|████████▋ | 29312/33804 [03:06<00:26, 168.95it/s][A

 87%|████████▋ | 29329/33804 [03:06<00:26, 169.05it/s][A

 87%|████████▋ | 29346/33804 [03:06<00:26, 169.06it/s][A

 87%|████████▋ | 29363/33804 [03:06<00:26, 168.95it/s][A

 87%|████████▋ | 29380/33804 [03:07<00:26, 168.81it/s][A

 87%|████████▋ | 29397/33804 [03:07<00:26, 168.61it/s][A

 87%|████████▋ | 29414/33804 [03:07<00:25, 168.90it/s][A

 87%|████████▋ | 29431/33804 [03:07<00:25, 168.60it/s][A

 87%|████████▋ | 29448/33804 [03:07<00:25, 168.94it/s][A

 87%|████████▋ | 29465/33804 [03:07<00:25, 168.91it/s][A

 87%|████████▋ | 29482/33804 [03:07<00:25, 168.52it/s][A

 87%|████████▋ | 29499/33804 [03:07<00:25, 168.45it/s][A

 87%|████████▋ | 29516/33804 [03:07<00:25, 168.90it/s][A

 87%|████████▋ | 29533/33804 [03:07<00:25, 168.48it/s][A

 87%|████████▋ | 29550/33804 [03:08<00:25, 168.69it/s][A

 87%|████████▋ | 29567/33804 [03:08<00:25, 168.44it/s][A

 88%|████████▊ | 29584/33804 [03:08<00:24, 168.82it/s][A

 88%|████████▊ | 29601/33804 [03:08<00:24, 168.64it/s][A

 88%|████████▊ | 29618/33804 [03:08<00:24, 168.40it/s][A

 88%|████████▊ | 29635/33804 [03:08<00:24, 168.27it/s][A

 88%|████████▊ | 29652/33804 [03:08<00:24, 168.45it/s][A

 88%|████████▊ | 29669/33804 [03:08<00:24, 168.85it/s][A

 88%|████████▊ | 29686/33804 [03:08<00:24, 168.84it/s][A

 88%|████████▊ | 29703/33804 [03:08<00:25, 161.22it/s][A

 88%|████████▊ | 29720/33804 [03:09<00:31, 130.08it/s][A

 88%|████████▊ | 29737/33804 [03:09<00:29, 139.22it/s][A

 88%|████████▊ | 29754/33804 [03:09<00:27, 146.94it/s][A

 88%|████████▊ | 29771/33804 [03:09<00:26, 153.00it/s][A

 88%|████████▊ | 29788/33804 [03:09<00:25, 157.13it/s][A

 88%|████████▊ | 29805/33804 [03:09<00:24, 160.69it/s][A

 88%|████████▊ | 29822/33804 [03:09<00:24, 162.90it/s][A

 88%|████████▊ | 29839/33804 [03:09<00:24, 164.39it/s][A

 88%|████████▊ | 29856/33804 [03:11<02:35, 25.43it/s] [A

 88%|████████▊ | 29873/33804 [03:11<01:55, 34.12it/s][A

 88%|████████▊ | 29890/33804 [03:12<01:27, 44.88it/s][A

 88%|████████▊ | 29907/33804 [03:12<01:07, 57.55it/s][A

 89%|████████▊ | 29924/33804 [03:12<00:54, 71.79it/s][A

 89%|████████▊ | 29941/33804 [03:12<00:44, 86.37it/s][A

 89%|████████▊ | 29958/33804 [03:12<00:38, 101.19it/s][A

 89%|████████▊ | 29975/33804 [03:12<00:33, 114.71it/s][A

 89%|████████▊ | 29992/33804 [03:12<00:30, 127.04it/s][A

 89%|████████▉ | 30009/33804 [03:12<00:27, 137.00it/s][A

 89%|████████▉ | 30026/33804 [03:12<00:25, 145.42it/s][A

 89%|████████▉ | 30043/33804 [03:12<00:24, 151.37it/s][A

 89%|████████▉ | 30060/33804 [03:13<00:23, 156.17it/s][A

 89%|████████▉ | 30077/33804 [03:13<00:23, 159.19it/s][A

 89%|████████▉ | 30094/33804 [03:13<00:28, 129.42it/s][A

 89%|████████▉ | 30109/33804 [03:13<00:27, 132.14it/s][A

 89%|████████▉ | 30127/33804 [03:13<00:25, 142.07it/s][A

 89%|████████▉ | 30145/33804 [03:13<00:24, 149.84it/s][A

 89%|████████▉ | 30162/33804 [03:13<00:23, 154.95it/s][A

 89%|████████▉ | 30179/33804 [03:13<00:22, 159.10it/s][A

 89%|████████▉ | 30196/33804 [03:13<00:22, 161.75it/s][A

 89%|████████▉ | 30213/33804 [03:14<00:22, 162.99it/s][A

 89%|████████▉ | 30230/33804 [03:14<00:21, 164.17it/s][A

 89%|████████▉ | 30247/33804 [03:14<00:21, 165.72it/s][A

 90%|████████▉ | 30264/33804 [03:14<00:21, 166.39it/s][A

 90%|████████▉ | 30281/33804 [03:14<00:21, 167.32it/s][A

 90%|████████▉ | 30298/33804 [03:14<00:20, 166.96it/s][A

 90%|████████▉ | 30316/33804 [03:14<00:20, 167.92it/s][A

 90%|████████▉ | 30333/33804 [03:14<00:20, 167.70it/s][A

 90%|████████▉ | 30350/33804 [03:14<00:20, 167.97it/s][A

 90%|████████▉ | 30368/33804 [03:14<00:20, 168.74it/s][A

 90%|████████▉ | 30385/33804 [03:15<00:20, 168.82it/s][A

 90%|████████▉ | 30403/33804 [03:15<00:20, 169.21it/s][A

 90%|████████▉ | 30420/33804 [03:15<00:20, 168.24it/s][A

 90%|█████████ | 30437/33804 [03:15<00:19, 168.75it/s][A

 90%|█████████ | 30454/33804 [03:15<00:19, 168.75it/s][A

 90%|█████████ | 30471/33804 [03:15<00:19, 168.88it/s][A

 90%|█████████ | 30488/33804 [03:15<00:19, 168.14it/s][A

 90%|█████████ | 30505/33804 [03:15<00:19, 168.43it/s][A

 90%|█████████ | 30522/33804 [03:15<00:19, 168.11it/s][A

 90%|█████████ | 30539/33804 [03:16<00:19, 167.86it/s][A

 90%|█████████ | 30556/33804 [03:16<00:19, 168.40it/s][A

 90%|█████████ | 30573/33804 [03:16<00:19, 168.40it/s][A

 90%|█████████ | 30590/33804 [03:16<00:19, 168.86it/s][A

 91%|█████████ | 30607/33804 [03:16<00:18, 168.85it/s][A

 91%|█████████ | 30624/33804 [03:16<00:18, 168.83it/s][A

 91%|█████████ | 30641/33804 [03:16<00:18, 168.94it/s][A

 91%|█████████ | 30659/33804 [03:16<00:18, 169.40it/s][A

 91%|█████████ | 30676/33804 [03:16<00:18, 169.01it/s][A

 91%|█████████ | 30693/33804 [03:16<00:18, 168.99it/s][A

 91%|█████████ | 30710/33804 [03:17<00:18, 169.06it/s][A

 91%|█████████ | 30727/33804 [03:17<00:18, 169.17it/s][A

 91%|█████████ | 30744/33804 [03:17<00:18, 168.68it/s][A

 91%|█████████ | 30761/33804 [03:17<00:18, 168.02it/s][A

 91%|█████████ | 30778/33804 [03:17<00:17, 168.33it/s][A

 91%|█████████ | 30795/33804 [03:17<00:23, 126.68it/s][A

 91%|█████████ | 30812/33804 [03:17<00:21, 136.31it/s][A

 91%|█████████ | 30829/33804 [03:17<00:20, 144.40it/s][A

 91%|█████████▏| 30847/33804 [03:17<00:19, 151.56it/s][A

 91%|█████████▏| 30864/33804 [03:18<00:18, 156.21it/s][A

 91%|█████████▏| 30881/33804 [03:18<00:18, 160.06it/s][A

 91%|█████████▏| 30898/33804 [03:18<00:17, 162.62it/s][A

 91%|█████████▏| 30916/33804 [03:18<00:17, 164.87it/s][A

 92%|█████████▏| 30933/33804 [03:18<00:17, 166.01it/s][A

 92%|█████████▏| 30950/33804 [03:18<00:17, 166.38it/s][A

 92%|█████████▏| 30968/33804 [03:18<00:16, 167.69it/s][A

 92%|█████████▏| 30985/33804 [03:18<00:16, 167.88it/s][A

 92%|█████████▏| 31002/33804 [03:18<00:16, 168.41it/s][A

 92%|█████████▏| 31019/33804 [03:18<00:16, 168.43it/s][A

 92%|█████████▏| 31036/33804 [03:19<00:16, 168.58it/s][A

 92%|█████████▏| 31053/33804 [03:19<00:16, 168.23it/s][A

 92%|█████████▏| 31070/33804 [03:19<00:16, 168.73it/s][A

 92%|█████████▏| 31087/33804 [03:19<00:16, 168.84it/s][A

 92%|█████████▏| 31105/33804 [03:19<00:15, 169.25it/s][A

 92%|█████████▏| 31122/33804 [03:19<00:15, 169.22it/s][A

 92%|█████████▏| 31140/33804 [03:19<00:15, 169.25it/s][A

 92%|█████████▏| 31158/33804 [03:19<00:15, 169.50it/s][A

 92%|█████████▏| 31175/33804 [03:19<00:15, 169.24it/s][A

 92%|█████████▏| 31192/33804 [03:19<00:15, 169.29it/s][A

 92%|█████████▏| 31209/33804 [03:20<00:15, 169.25it/s][A

 92%|█████████▏| 31226/33804 [03:20<00:15, 169.47it/s][A

 92%|█████████▏| 31243/33804 [03:20<00:15, 168.99it/s][A

 92%|█████████▏| 31260/33804 [03:20<00:15, 168.22it/s][A

 93%|█████████▎| 31277/33804 [03:20<00:15, 167.93it/s][A

 93%|█████████▎| 31294/33804 [03:20<00:14, 168.41it/s][A

 93%|█████████▎| 31311/33804 [03:20<00:14, 168.28it/s][A

 93%|█████████▎| 31328/33804 [03:20<00:14, 168.67it/s][A

 93%|█████████▎| 31345/33804 [03:20<00:14, 168.80it/s][A

 93%|█████████▎| 31362/33804 [03:20<00:14, 168.88it/s][A

 93%|█████████▎| 31380/33804 [03:21<00:14, 169.33it/s][A

 93%|█████████▎| 31397/33804 [03:21<00:14, 169.12it/s][A

 93%|█████████▎| 31415/33804 [03:21<00:14, 169.40it/s][A

 93%|█████████▎| 31432/33804 [03:21<00:14, 168.78it/s][A

 93%|█████████▎| 31450/33804 [03:21<00:13, 169.37it/s][A

 93%|█████████▎| 31467/33804 [03:21<00:13, 169.30it/s][A

 93%|█████████▎| 31484/33804 [03:21<00:14, 162.89it/s][A

 93%|█████████▎| 31501/33804 [03:21<00:18, 127.05it/s][A

 93%|█████████▎| 31518/33804 [03:22<00:16, 136.36it/s][A

 93%|█████████▎| 31535/33804 [03:22<00:15, 144.49it/s][A

 93%|█████████▎| 31552/33804 [03:22<00:14, 150.79it/s][A

 93%|█████████▎| 31569/33804 [03:22<00:14, 156.02it/s][A

 93%|█████████▎| 31586/33804 [03:22<00:13, 159.66it/s][A

 93%|█████████▎| 31604/33804 [03:22<00:13, 162.76it/s][A

 94%|█████████▎| 31621/33804 [03:22<00:13, 164.48it/s][A

 94%|█████████▎| 31638/33804 [03:22<00:13, 166.04it/s][A

 94%|█████████▎| 31655/33804 [03:22<00:12, 166.52it/s][A

 94%|█████████▎| 31673/33804 [03:22<00:12, 167.78it/s][A

 94%|█████████▎| 31690/33804 [03:23<00:12, 167.92it/s][A

 94%|█████████▍| 31707/33804 [03:23<00:12, 167.87it/s][A

 94%|█████████▍| 31724/33804 [03:23<00:12, 167.65it/s][A

 94%|█████████▍| 31741/33804 [03:23<00:12, 168.15it/s][A

 94%|█████████▍| 31758/33804 [03:23<00:12, 167.88it/s][A

 94%|█████████▍| 31775/33804 [03:23<00:12, 167.67it/s][A

 94%|█████████▍| 31792/33804 [03:23<00:12, 167.03it/s][A

 94%|█████████▍| 31809/33804 [03:23<00:11, 166.95it/s][A

 94%|█████████▍| 31827/33804 [03:23<00:11, 167.92it/s][A

 94%|█████████▍| 31844/33804 [03:23<00:11, 168.26it/s][A

 94%|█████████▍| 31861/33804 [03:24<00:11, 168.64it/s][A

 94%|█████████▍| 31878/33804 [03:24<00:11, 168.41it/s][A

 94%|█████████▍| 31895/33804 [03:24<00:11, 168.81it/s][A

 94%|█████████▍| 31912/33804 [03:24<00:11, 168.98it/s][A

 94%|█████████▍| 31929/33804 [03:24<00:11, 169.14it/s][A

 95%|█████████▍| 31946/33804 [03:24<00:10, 169.27it/s][A

 95%|█████████▍| 31963/33804 [03:24<00:10, 169.48it/s][A

 95%|█████████▍| 31980/33804 [03:24<00:10, 169.17it/s][A

 95%|█████████▍| 31997/33804 [03:24<00:10, 168.59it/s][A

 95%|█████████▍| 32014/33804 [03:24<00:10, 168.90it/s][A

 95%|█████████▍| 32031/33804 [03:25<00:10, 168.78it/s][A

 95%|█████████▍| 32049/33804 [03:25<00:10, 169.15it/s][A

 95%|█████████▍| 32066/33804 [03:25<00:10, 167.52it/s][A

 95%|█████████▍| 32083/33804 [03:25<00:10, 168.21it/s][A

 95%|█████████▍| 32100/33804 [03:25<00:10, 168.60it/s][A

 95%|█████████▌| 32118/33804 [03:25<00:09, 169.13it/s][A

 95%|█████████▌| 32135/33804 [03:25<00:09, 168.88it/s][A

 95%|█████████▌| 32153/33804 [03:25<00:09, 169.27it/s][A

 95%|█████████▌| 32170/33804 [03:25<00:09, 168.92it/s][A

 95%|█████████▌| 32187/33804 [03:26<00:11, 145.51it/s][A

 95%|█████████▌| 32203/33804 [03:26<00:12, 132.51it/s][A

 95%|█████████▌| 32220/33804 [03:26<00:11, 141.29it/s][A

 95%|█████████▌| 32237/33804 [03:26<00:10, 148.56it/s][A

 95%|█████████▌| 32254/33804 [03:26<00:10, 153.38it/s][A

 95%|█████████▌| 32272/33804 [03:26<00:09, 158.25it/s][A

 96%|█████████▌| 32289/33804 [03:26<00:09, 160.92it/s][A

 96%|█████████▌| 32307/33804 [03:26<00:09, 163.68it/s][A

 96%|█████████▌| 32324/33804 [03:26<00:09, 164.36it/s][A

 96%|█████████▌| 32341/33804 [03:27<00:08, 165.94it/s][A

 96%|█████████▌| 32358/33804 [03:27<00:08, 166.71it/s][A

 96%|█████████▌| 32376/33804 [03:27<00:08, 167.48it/s][A

 96%|█████████▌| 32393/33804 [03:27<00:08, 168.05it/s][A

 96%|█████████▌| 32410/33804 [03:27<00:08, 168.28it/s][A

 96%|█████████▌| 32428/33804 [03:27<00:08, 168.95it/s][A

 96%|█████████▌| 32445/33804 [03:27<00:08, 168.98it/s][A

 96%|█████████▌| 32462/33804 [03:27<00:07, 168.98it/s][A

 96%|█████████▌| 32479/33804 [03:27<00:07, 167.88it/s][A

 96%|█████████▌| 32496/33804 [03:27<00:07, 168.18it/s][A

 96%|█████████▌| 32513/33804 [03:28<00:07, 167.89it/s][A

 96%|█████████▌| 32530/33804 [03:28<00:07, 168.32it/s][A

 96%|█████████▋| 32547/33804 [03:28<00:07, 167.82it/s][A

 96%|█████████▋| 32565/33804 [03:28<00:07, 168.57it/s][A

 96%|█████████▋| 32582/33804 [03:28<00:07, 168.74it/s][A

 96%|█████████▋| 32599/33804 [03:28<00:07, 167.79it/s][A

 96%|█████████▋| 32616/33804 [03:28<00:07, 168.42it/s][A

 97%|█████████▋| 32633/33804 [03:28<00:06, 168.34it/s][A

 97%|█████████▋| 32650/33804 [03:28<00:06, 168.70it/s][A

 97%|█████████▋| 32667/33804 [03:28<00:06, 168.79it/s][A

 97%|█████████▋| 32685/33804 [03:29<00:06, 169.20it/s][A

 97%|█████████▋| 32702/33804 [03:29<00:06, 167.56it/s][A

 97%|█████████▋| 32719/33804 [03:29<00:06, 168.17it/s][A

 97%|█████████▋| 32736/33804 [03:29<00:06, 167.71it/s][A

 97%|█████████▋| 32753/33804 [03:29<00:06, 168.06it/s][A

 97%|█████████▋| 32770/33804 [03:29<00:06, 167.84it/s][A

 97%|█████████▋| 32787/33804 [03:29<00:06, 168.44it/s][A

 97%|█████████▋| 32804/33804 [03:29<00:05, 168.31it/s][A

 97%|█████████▋| 32821/33804 [03:29<00:05, 168.54it/s][A

 97%|█████████▋| 32838/33804 [03:29<00:05, 168.92it/s][A

 97%|█████████▋| 32855/33804 [03:30<00:05, 167.82it/s][A

 97%|█████████▋| 32872/33804 [03:30<00:05, 167.90it/s][A

 97%|█████████▋| 32889/33804 [03:30<00:06, 136.83it/s][A

 97%|█████████▋| 32904/33804 [03:30<00:06, 131.63it/s][A

 97%|█████████▋| 32921/33804 [03:30<00:06, 141.01it/s][A

 97%|█████████▋| 32938/33804 [03:30<00:05, 148.40it/s][A

 97%|█████████▋| 32955/33804 [03:30<00:05, 153.74it/s][A

 98%|█████████▊| 32972/33804 [03:30<00:05, 158.02it/s][A

 98%|█████████▊| 32989/33804 [03:30<00:05, 160.10it/s][A

 98%|█████████▊| 33006/33804 [03:31<00:04, 162.72it/s][A

 98%|█████████▊| 33023/33804 [03:31<00:04, 164.01it/s][A

 98%|█████████▊| 33040/33804 [03:31<00:04, 165.52it/s][A

 98%|█████████▊| 33057/33804 [03:31<00:04, 166.22it/s][A

 98%|█████████▊| 33074/33804 [03:31<00:04, 166.96it/s][A

 98%|█████████▊| 33091/33804 [03:31<00:04, 167.62it/s][A

 98%|█████████▊| 33108/33804 [03:31<00:04, 167.30it/s][A

 98%|█████████▊| 33125/33804 [03:31<00:04, 166.76it/s][A

 98%|█████████▊| 33142/33804 [03:31<00:03, 167.23it/s][A

 98%|█████████▊| 33159/33804 [03:31<00:03, 167.95it/s][A

 98%|█████████▊| 33176/33804 [03:32<00:03, 167.83it/s][A

 98%|█████████▊| 33193/33804 [03:32<00:03, 168.25it/s][A

 98%|█████████▊| 33210/33804 [03:32<00:03, 167.96it/s][A

 98%|█████████▊| 33227/33804 [03:32<00:03, 168.15it/s][A

 98%|█████████▊| 33244/33804 [03:32<00:03, 168.15it/s][A

 98%|█████████▊| 33261/33804 [03:32<00:03, 168.44it/s][A

 98%|█████████▊| 33278/33804 [03:32<00:03, 168.30it/s][A

 98%|█████████▊| 33295/33804 [03:32<00:03, 168.28it/s][A

 99%|█████████▊| 33312/33804 [03:32<00:02, 168.79it/s][A

 99%|█████████▊| 33329/33804 [03:33<00:02, 168.73it/s][A

 99%|█████████▊| 33346/33804 [03:33<00:02, 168.67it/s][A

 99%|█████████▊| 33363/33804 [03:33<00:02, 168.50it/s][A

 99%|█████████▊| 33380/33804 [03:33<00:02, 168.00it/s][A

 99%|█████████▉| 33397/33804 [03:33<00:02, 167.01it/s][A

 99%|█████████▉| 33414/33804 [03:33<00:02, 167.70it/s][A

 99%|█████████▉| 33431/33804 [03:33<00:02, 167.82it/s][A

 99%|█████████▉| 33448/33804 [03:33<00:02, 168.30it/s][A

 99%|█████████▉| 33465/33804 [03:33<00:02, 167.78it/s][A

 99%|█████████▉| 33482/33804 [03:33<00:01, 168.08it/s][A

 99%|█████████▉| 33499/33804 [03:34<00:01, 168.15it/s][A

 99%|█████████▉| 33516/33804 [03:34<00:01, 167.85it/s][A

 99%|█████████▉| 33533/33804 [03:34<00:01, 168.40it/s][A

 99%|█████████▉| 33550/33804 [03:34<00:01, 168.38it/s][A

 99%|█████████▉| 33567/33804 [03:34<00:01, 168.73it/s][A

 99%|█████████▉| 33584/33804 [03:34<00:01, 146.45it/s][A

 99%|█████████▉| 33600/33804 [03:34<00:01, 132.47it/s][A

 99%|█████████▉| 33617/33804 [03:34<00:01, 141.09it/s][A

 99%|█████████▉| 33634/33804 [03:34<00:01, 148.68it/s][A

100%|█████████▉| 33651/33804 [03:35<00:00, 153.86it/s][A

100%|█████████▉| 33668/33804 [03:35<00:00, 157.00it/s][A

100%|█████████▉| 33685/33804 [03:35<00:00, 159.80it/s][A

100%|█████████▉| 33702/33804 [03:35<00:00, 162.49it/s][A

100%|█████████▉| 33719/33804 [03:35<00:00, 163.32it/s][A

100%|█████████▉| 33736/33804 [03:35<00:00, 164.86it/s][A

100%|█████████▉| 33753/33804 [03:35<00:00, 165.43it/s][A

100%|█████████▉| 33770/33804 [03:35<00:00, 166.39it/s][A

100%|█████████▉| 33787/33804 [03:35<00:00, 167.43it/s][A

100%|██████████| 33804/33804 [03:35<00:00, 167.88it/s][A
100%|██████████| 33804/33804 [03:35<00:00, 156.54it/s]
/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/absorption/common.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  probe = torch.load(


  0%|          | 0/26 [00:00<?, ?it/s][A

  4%|▍         | 1/26 [00:01<00:48,  1.93s/it][A

  8%|▊         | 2/26 [00:02<00:32,  1.34s/it][A

 12%|█▏        | 3/26 [00:17<02:55,  7.63s/it][A

 15%|█▌        | 4/26 [00:18<01:50,  5.01s/it][A

 19%|█▉        | 5/26 [00:21<01:22,  3.94s/it][A

 23%|██▎       | 6/26 [00:21<00:58,  2.91s/it][A

 27%|██▋       | 7/26 [00:24<00:52,  2.78s/it][A

 31%|███       | 8/26 [00:29<01:05,  3.62s/it][A

 35%|███▍      | 9/26 [00:34<01:08,  4.04s/it][A

 38%|███▊      | 10/26 [00:34<00:45,  2.85s/it][A

 42%|████▏     | 11/26 [00:35<00:30,  2.03s/it][A

 46%|████▌     | 12/26 [00:36<00:23,  1.67s/it][A

 50%|█████     | 13/26 [00:44<00:47,  3.65s/it][A

 54%|█████▍    | 14/26 [00:45<00:33,  2.80s/it][A

 58%|█████▊    | 15/26 [00:51<00:43,  3.93s/it][A

 62%|██████▏   | 16/26 [01:05<01:09,  6.93s/it][A

 65%|██████▌   | 17/26 [01:05<00:44,  4.94s/it][A

 69%|██████▉   | 18/26 [01:06<00:28,  3.54s/it][A

 73%|███████▎  | 19/26 [01:26<00:59,  8.51s/it][A

 77%|███████▋  | 20/26 [01:27<00:38,  6.34s/it][A

 81%|████████  | 21/26 [01:30<00:26,  5.22s/it][A

 85%|████████▍ | 22/26 [01:30<00:14,  3.72s/it][A

 88%|████████▊ | 23/26 [01:31<00:08,  2.99s/it][A

 96%|█████████▌| 25/26 [01:31<00:01,  1.68s/it][A

100%|██████████| 26/26 [01:32<00:00,  1.37s/it][A
100%|██████████| 26/26 [01:32<00:00,  3.55s/it]

Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [08:13<00:00, 493.38s/it]
Running SAE evaluation on all selected SAEs: 100%|██████████| 1/1 [08:13<00:00, 493.38s/it]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:00<?, ?it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A

Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.36s/it][A

Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.36s/it][A

Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.44s/it][A
Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]
/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/sae_lens/training/activations_store.py:285: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.
  warnings.warn(


Reconstruction Batches:   0%|          | 0/200 [00:00<?, ?it/s][A
Reconstruction Batches:   0%|          | 0/200 [00:00<?, ?it/s]

Running SAE evaluation on all selected SAEs:   0%|          | 0/1 [00:20<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210915_adaptive_activation_weighted_orthogonal_sae/experiment.py", line 938, in <module>
    evaluate_trained_sae(
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210915_adaptive_activation_weighted_orthogonal_sae/experiment.py", line 839, in evaluate_trained_sae
    eval_runners[eval_type]()
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210915_adaptive_activation_weighted_orthogonal_sae/experiment.py", line 734, in <lambda>
    lambda: core.multiple_evals(
            ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 955, in multiple_evals
    scalar_metrics, feature_metrics = run_evals(
                                      ^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 154, in run_evals
    reconstruction_metrics = get_downstream_reconstruction_metrics(
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 333, in get_downstream_reconstruction_metrics
    for metric_name, metric_value in get_recons_loss(
                                     ^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 671, in get_recons_loss
    recons_logits, recons_ce_loss = model.run_with_hooks(
                                    ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/hook_points.py", line 455, in run_with_hooks
    return hooked_model.forward(*model_args, **model_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py", line 575, in forward
    residual = block(
               ^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py", line 183, in forward
    resid_post = self.hook_resid_post(resid_mid + mlp_out)  # [batch, pos, d_model]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1803, in inner
    hook_result = hook(self, args, result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/transformer_lens/hook_points.py", line 109, in full_hook
    return hook(module_output, hook=self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/evals/core/main.py", line 581, in standard_replacement_hook
    reconstructed_activations = sae.decode(sae.encode(activations)).to(activations.dtype)
                                           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/orthogonal/20250206_210915_adaptive_activation_weighted_orthogonal_sae/experiment.py", line 130, in encode
    self.activation_ema = self.ema_decay * self.activation_ema + (1 - self.ema_decay) * batch_activations
                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (87) must match the size of tensor b (128) at non-singleton dimension 0

Error during experiments: cannot access local variable 'results' where it is not associated with a value
Experiments failed for idea 20250206_210915_adaptive_activation_weighted_orthogonal_sae
FINISHED IDEA
Completed idea: adaptive_activation_weighted_orthogonal_sae, Success: False
All ideas evaluated.
