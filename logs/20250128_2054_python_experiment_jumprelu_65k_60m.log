Step 0: {'loss': 129055.1953125, 'mse_loss': 129055.1953125, 'sparsity_loss': 0.0, 'l0': 32760.8359375, 'dead_features': 0}
Step 100: {'loss': 123543.4609375, 'mse_loss': 7821.423828125, 'sparsity_loss': 115722.0390625, 'l0': 30446.572265625, 'dead_features': 0}
Step 200: {'loss': 211292.0625, 'mse_loss': 6468.7646484375, 'sparsity_loss': 204823.296875, 'l0': 28643.298828125, 'dead_features': 0}
Step 300: {'loss': 239880.828125, 'mse_loss': 5413.75537109375, 'sparsity_loss': 234467.078125, 'l0': 25024.91015625, 'dead_features': 0}
Step 400: {'loss': 209198.0, 'mse_loss': 3584.70068359375, 'sparsity_loss': 205613.296875, 'l0': 20298.72265625, 'dead_features': 0}
Step 500: {'loss': 148253.265625, 'mse_loss': 2909.82861328125, 'sparsity_loss': 145343.4375, 'l0': 15269.5732421875, 'dead_features': 0}
Step 600: {'loss': 126497.4296875, 'mse_loss': 4595.76318359375, 'sparsity_loss': 121901.6640625, 'l0': 12768.9423828125, 'dead_features': 0}
Step 700: {'loss': 97100.6875, 'mse_loss': 2358.55517578125, 'sparsity_loss': 94742.1328125, 'l0': 10425.6103515625, 'dead_features': 0}
Step 800: {'loss': 100035.6875, 'mse_loss': 2299.06591796875, 'sparsity_loss': 97736.625, 'l0': 9906.18359375, 'dead_features': 0}
Step 900: {'loss': 66686.09375, 'mse_loss': 2383.83203125, 'sparsity_loss': 64302.26171875, 'l0': 7580.26220703125, 'dead_features': 0}
Step 1000: {'loss': 86835.7265625, 'mse_loss': 7869.8779296875, 'sparsity_loss': 78965.8515625, 'l0': 7968.12451171875, 'dead_features': 0}
Step 1100: {'loss': 77807.0859375, 'mse_loss': 1528.4124755859375, 'sparsity_loss': 76278.671875, 'l0': 7468.1806640625, 'dead_features': 0}
Step 1200: {'loss': 90968.890625, 'mse_loss': 11054.0068359375, 'sparsity_loss': 79914.8828125, 'l0': 7319.0810546875, 'dead_features': 0}
Step 1300: {'loss': 84570.71875, 'mse_loss': 7824.8662109375, 'sparsity_loss': 76745.8515625, 'l0': 6892.27880859375, 'dead_features': 0}
Step 1400: {'loss': 73881.890625, 'mse_loss': 1387.7318115234375, 'sparsity_loss': 72494.15625, 'l0': 6456.24365234375, 'dead_features': 0}
Step 1500: {'loss': 79203.375, 'mse_loss': 1392.4891357421875, 'sparsity_loss': 77810.8828125, 'l0': 6461.982421875, 'dead_features': 1}
Step 1600: {'loss': 70092.4375, 'mse_loss': 1250.614501953125, 'sparsity_loss': 68841.8203125, 'l0': 5886.93359375, 'dead_features': 0}
Step 1700: {'loss': 58564.6796875, 'mse_loss': 3264.9677734375, 'sparsity_loss': 55299.7109375, 'l0': 5121.3125, 'dead_features': 0}
Step 1800: {'loss': 77748.7890625, 'mse_loss': 4034.809326171875, 'sparsity_loss': 73713.9765625, 'l0': 5743.78955078125, 'dead_features': 0}
Step 1900: {'loss': 79217.2890625, 'mse_loss': 1086.198974609375, 'sparsity_loss': 78131.09375, 'l0': 5755.61669921875, 'dead_features': 3}
Step 2000: {'loss': 74760.328125, 'mse_loss': 2252.9013671875, 'sparsity_loss': 72507.4296875, 'l0': 5405.4404296875, 'dead_features': 7}
Step 2100: {'loss': 59585.11328125, 'mse_loss': 2068.8203125, 'sparsity_loss': 57516.29296875, 'l0': 4816.5107421875, 'dead_features': 9}
Step 2200: {'loss': 78368.03125, 'mse_loss': 846.3218383789062, 'sparsity_loss': 77521.7109375, 'l0': 5588.5439453125, 'dead_features': 11}
Step 2300: {'loss': 74805.46875, 'mse_loss': 2155.1171875, 'sparsity_loss': 72650.3515625, 'l0': 5410.74560546875, 'dead_features': 14}
Step 2400: {'loss': 71616.4453125, 'mse_loss': 2336.290283203125, 'sparsity_loss': 69280.15625, 'l0': 5284.224609375, 'dead_features': 18}
Step 2500: {'loss': 48304.67578125, 'mse_loss': 1975.582763671875, 'sparsity_loss': 46329.09375, 'l0': 4324.8388671875, 'dead_features': 23}
Step 2600: {'loss': 59825.9609375, 'mse_loss': 813.759521484375, 'sparsity_loss': 59012.203125, 'l0': 4878.4853515625, 'dead_features': 33}
Step 2700: {'loss': 42508.28125, 'mse_loss': 2200.7646484375, 'sparsity_loss': 40307.515625, 'l0': 4035.34619140625, 'dead_features': 44}
Step 2800: {'loss': 55099.9609375, 'mse_loss': 977.9445190429688, 'sparsity_loss': 54122.015625, 'l0': 4672.82763671875, 'dead_features': 56}
Step 2900: {'loss': 51559.0859375, 'mse_loss': 4155.931640625, 'sparsity_loss': 47403.15625, 'l0': 4374.453125, 'dead_features': 79}
Step 3000: {'loss': 47244.1640625, 'mse_loss': 948.927001953125, 'sparsity_loss': 46295.23828125, 'l0': 4323.265625, 'dead_features': 77}
Step 3100: {'loss': 49212.46484375, 'mse_loss': 801.278564453125, 'sparsity_loss': 48411.1875, 'l0': 4420.50830078125, 'dead_features': 110}
Step 3200: {'loss': 37357.3828125, 'mse_loss': 1979.8516845703125, 'sparsity_loss': 35377.53125, 'l0': 3781.783203125, 'dead_features': 133}
Step 3300: {'loss': 34588.3125, 'mse_loss': 2632.5205078125, 'sparsity_loss': 31955.791015625, 'l0': 3595.236328125, 'dead_features': 139}
Step 3400: {'loss': 38149.125, 'mse_loss': 1088.3990478515625, 'sparsity_loss': 37060.7265625, 'l0': 3870.232421875, 'dead_features': 176}
Step 3500: {'loss': 42412.28515625, 'mse_loss': 888.0306396484375, 'sparsity_loss': 41524.25390625, 'l0': 4095.5, 'dead_features': 208}
Step 3600: {'loss': 34931.9375, 'mse_loss': 1887.5875244140625, 'sparsity_loss': 33044.3515625, 'l0': 3655.62109375, 'dead_features': 254}
Step 3700: {'loss': 39348.765625, 'mse_loss': 1007.8120727539062, 'sparsity_loss': 38340.953125, 'l0': 3936.1689453125, 'dead_features': 287}
Step 3800: {'loss': 38056.1640625, 'mse_loss': 926.749267578125, 'sparsity_loss': 37129.4140625, 'l0': 3873.798828125, 'dead_features': 381}
Step 3900: {'loss': 30996.12890625, 'mse_loss': 2059.790771484375, 'sparsity_loss': 28936.337890625, 'l0': 3422.13671875, 'dead_features': 452}
Step 4000: {'loss': 29929.115234375, 'mse_loss': 2118.845703125, 'sparsity_loss': 27810.26953125, 'l0': 3355.2822265625, 'dead_features': 468}
Step 4100: {'loss': 33362.0234375, 'mse_loss': 1985.50537109375, 'sparsity_loss': 31376.51953125, 'l0': 3562.68359375, 'dead_features': 516}
Step 4200: {'loss': 33141.9609375, 'mse_loss': 1828.92138671875, 'sparsity_loss': 31313.041015625, 'l0': 3559.09814453125, 'dead_features': 565}
Step 4300: {'loss': 31392.1875, 'mse_loss': 1701.33544921875, 'sparsity_loss': 29690.8515625, 'l0': 3466.20654296875, 'dead_features': 628}
Step 4400: {'loss': 29966.794921875, 'mse_loss': 2331.359619140625, 'sparsity_loss': 27635.435546875, 'l0': 3344.78173828125, 'dead_features': 675}
Step 4500: {'loss': 32257.015625, 'mse_loss': 1398.03662109375, 'sparsity_loss': 30858.978515625, 'l0': 3533.3447265625, 'dead_features': 763}
Step 4600: {'loss': 32466.505859375, 'mse_loss': 1351.8204345703125, 'sparsity_loss': 31114.685546875, 'l0': 3547.87109375, 'dead_features': 876}
Step 4700: {'loss': 29290.54296875, 'mse_loss': 1169.25048828125, 'sparsity_loss': 28121.29296875, 'l0': 3373.880859375, 'dead_features': 935}
Step 4800: {'loss': 32521.11328125, 'mse_loss': 2932.226806640625, 'sparsity_loss': 29588.88671875, 'l0': 3460.2841796875, 'dead_features': 974}
Step 4900: {'loss': 29051.587890625, 'mse_loss': 1218.991943359375, 'sparsity_loss': 27832.595703125, 'l0': 3356.62060546875, 'dead_features': 1043}
Step 5000: {'loss': 30911.388671875, 'mse_loss': 1574.99951171875, 'sparsity_loss': 29336.388671875, 'l0': 3445.57373046875, 'dead_features': 1129}
Step 5100: {'loss': 23684.265625, 'mse_loss': 1234.01904296875, 'sparsity_loss': 22450.24609375, 'l0': 3016.68115234375, 'dead_features': 1195}
Step 5200: {'loss': 29500.00390625, 'mse_loss': 2146.8193359375, 'sparsity_loss': 27353.185546875, 'l0': 3327.759765625, 'dead_features': 1271}
Step 5300: {'loss': 28889.55078125, 'mse_loss': 930.037841796875, 'sparsity_loss': 27959.513671875, 'l0': 3364.2197265625, 'dead_features': 1303}
Step 5400: {'loss': 28836.431640625, 'mse_loss': 1379.94677734375, 'sparsity_loss': 27456.484375, 'l0': 3333.99951171875, 'dead_features': 1393}
Step 5500: {'loss': 23786.486328125, 'mse_loss': 1186.546875, 'sparsity_loss': 22599.939453125, 'l0': 3026.6552734375, 'dead_features': 1472}
Step 5600: {'loss': 27709.068359375, 'mse_loss': 1719.378662109375, 'sparsity_loss': 25989.689453125, 'l0': 3244.263671875, 'dead_features': 1528}
Step 5700: {'loss': 23356.96484375, 'mse_loss': 1563.2254638671875, 'sparsity_loss': 21793.740234375, 'l0': 2972.54052734375, 'dead_features': 1584}
Step 5800: {'loss': 28156.62109375, 'mse_loss': 1697.604736328125, 'sparsity_loss': 26459.015625, 'l0': 3273.24560546875, 'dead_features': 1836}
Step 5900: {'loss': 22842.2890625, 'mse_loss': 1548.6551513671875, 'sparsity_loss': 21293.634765625, 'l0': 2938.4677734375, 'dead_features': 1776}
Step 6000: {'loss': 25450.759765625, 'mse_loss': 1434.5997314453125, 'sparsity_loss': 24016.16015625, 'l0': 3119.4296875, 'dead_features': 1802}
Step 6100: {'loss': 26404.837890625, 'mse_loss': 1257.4049072265625, 'sparsity_loss': 25147.43359375, 'l0': 3191.58837890625, 'dead_features': 1907}
Step 6200: {'loss': 25096.81640625, 'mse_loss': 1864.290771484375, 'sparsity_loss': 23232.525390625, 'l0': 3068.44384765625, 'dead_features': 1965}
Step 6300: {'loss': 25233.8046875, 'mse_loss': 1113.9454345703125, 'sparsity_loss': 24119.859375, 'l0': 3126.11376953125, 'dead_features': 1971}
Step 6400: {'loss': 29213.203125, 'mse_loss': 3568.6767578125, 'sparsity_loss': 25644.525390625, 'l0': 3222.78173828125, 'dead_features': 2065}
Step 6500: {'loss': 29931.734375, 'mse_loss': 2843.51611328125, 'sparsity_loss': 27088.21875, 'l0': 3311.69970703125, 'dead_features': 2109}
Step 6600: {'loss': 24766.796875, 'mse_loss': 910.4300537109375, 'sparsity_loss': 23856.3671875, 'l0': 3109.10107421875, 'dead_features': 2184}
Step 6700: {'loss': 21950.0390625, 'mse_loss': 1511.25439453125, 'sparsity_loss': 20438.78515625, 'l0': 2879.28564453125, 'dead_features': 2335}
Step 6800: {'loss': 24638.857421875, 'mse_loss': 1887.672119140625, 'sparsity_loss': 22751.185546875, 'l0': 3036.69921875, 'dead_features': 2353}
Step 6900: {'loss': 20895.232421875, 'mse_loss': 1228.400634765625, 'sparsity_loss': 19666.83203125, 'l0': 2824.76953125, 'dead_features': 2363}
Step 7000: {'loss': 23583.130859375, 'mse_loss': 1455.21484375, 'sparsity_loss': 22127.916015625, 'l0': 2995.0908203125, 'dead_features': 2382}
Step 7100: {'loss': 26250.037109375, 'mse_loss': 1123.429443359375, 'sparsity_loss': 25126.607421875, 'l0': 3190.27490234375, 'dead_features': 2420}
Step 7200: {'loss': 21691.916015625, 'mse_loss': 1236.98046875, 'sparsity_loss': 20454.935546875, 'l0': 2880.4150390625, 'dead_features': 2575}
Step 7300: {'loss': 24969.87109375, 'mse_loss': 1586.4508056640625, 'sparsity_loss': 23383.419921875, 'l0': 3078.32763671875, 'dead_features': 2679}
Step 7400: {'loss': 21700.4140625, 'mse_loss': 1468.614990234375, 'sparsity_loss': 20231.798828125, 'l0': 2864.7705078125, 'dead_features': 2732}
Step 7500: {'loss': 29288.599609375, 'mse_loss': 917.722412109375, 'sparsity_loss': 28370.876953125, 'l0': 3388.7314453125, 'dead_features': 2777}
Step 7600: {'loss': 24891.31640625, 'mse_loss': 814.0802001953125, 'sparsity_loss': 24077.236328125, 'l0': 3123.3681640625, 'dead_features': 2747}
Step 7700: {'loss': 25412.740234375, 'mse_loss': 1047.7109375, 'sparsity_loss': 24365.029296875, 'l0': 3141.8603515625, 'dead_features': 2823}
Step 7800: {'loss': 25953.66015625, 'mse_loss': 714.8285522460938, 'sparsity_loss': 25238.83203125, 'l0': 3197.3466796875, 'dead_features': 2855}
Step 7900: {'loss': 22700.57421875, 'mse_loss': 1838.51025390625, 'sparsity_loss': 20862.064453125, 'l0': 2908.7412109375, 'dead_features': 2959}
Step 8000: {'loss': 24252.509765625, 'mse_loss': 1480.1436767578125, 'sparsity_loss': 22772.365234375, 'l0': 3038.10302734375, 'dead_features': 3054}
Step 8100: {'loss': 20741.25390625, 'mse_loss': 1371.138916015625, 'sparsity_loss': 19370.115234375, 'l0': 2803.53125, 'dead_features': 3179}
Step 8200: {'loss': 21780.880859375, 'mse_loss': 1236.586669921875, 'sparsity_loss': 20544.294921875, 'l0': 2886.65625, 'dead_features': 3223}
Step 8300: {'loss': 28359.607421875, 'mse_loss': 881.0562744140625, 'sparsity_loss': 27478.55078125, 'l0': 3335.3310546875, 'dead_features': 3237}
Step 8400: {'loss': 28358.833984375, 'mse_loss': 997.61328125, 'sparsity_loss': 27361.220703125, 'l0': 3328.24560546875, 'dead_features': 3266}
Step 8500: {'loss': 25355.033203125, 'mse_loss': 1599.852783203125, 'sparsity_loss': 23755.1796875, 'l0': 3102.54296875, 'dead_features': 3342}
Step 8600: {'loss': 27750.298828125, 'mse_loss': 1147.104248046875, 'sparsity_loss': 26603.1953125, 'l0': 3282.09716796875, 'dead_features': 3376}
Step 8700: {'loss': 24305.294921875, 'mse_loss': 895.5625, 'sparsity_loss': 23409.732421875, 'l0': 3080.0478515625, 'dead_features': 3448}
Step 8800: {'loss': 26940.869140625, 'mse_loss': 1375.80224609375, 'sparsity_loss': 25565.06640625, 'l0': 3217.81591796875, 'dead_features': 3554}
Step 8900: {'loss': 22849.931640625, 'mse_loss': 1129.51904296875, 'sparsity_loss': 21720.412109375, 'l0': 2967.5693359375, 'dead_features': 3674}
Step 9000: {'loss': 26059.783203125, 'mse_loss': 1250.077392578125, 'sparsity_loss': 24809.705078125, 'l0': 3170.21923828125, 'dead_features': 3748}
Step 9100: {'loss': 22572.271484375, 'mse_loss': 1071.747802734375, 'sparsity_loss': 21500.5234375, 'l0': 2952.611328125, 'dead_features': 3831}
Step 9200: {'loss': 22106.984375, 'mse_loss': 1100.2880859375, 'sparsity_loss': 21006.697265625, 'l0': 2918.7373046875, 'dead_features': 3882}
Step 9300: {'loss': 26373.197265625, 'mse_loss': 1686.969482421875, 'sparsity_loss': 24686.228515625, 'l0': 3162.3701171875, 'dead_features': 3906}
Step 9400: {'loss': 27169.9453125, 'mse_loss': 903.7823486328125, 'sparsity_loss': 26266.162109375, 'l0': 3261.36767578125, 'dead_features': 4009}
Step 9500: {'loss': 22864.689453125, 'mse_loss': 1595.892822265625, 'sparsity_loss': 21268.796875, 'l0': 2936.76513671875, 'dead_features': 4030}
Step 9600: {'loss': 21825.474609375, 'mse_loss': 1499.17138671875, 'sparsity_loss': 20326.302734375, 'l0': 2871.40673828125, 'dead_features': 4053}
Step 9700: {'loss': 27784.73046875, 'mse_loss': 664.1503295898438, 'sparsity_loss': 27120.580078125, 'l0': 3313.66552734375, 'dead_features': 4125}
Step 9800: {'loss': 77854.421875, 'mse_loss': 53884.72265625, 'sparsity_loss': 23969.703125, 'l0': 3116.43017578125, 'dead_features': 4174}
Step 9900: {'loss': 27205.361328125, 'mse_loss': 949.8651123046875, 'sparsity_loss': 26255.49609375, 'l0': 3260.70947265625, 'dead_features': 4343}
Step 10000: {'loss': 18819.2734375, 'mse_loss': 1271.8541259765625, 'sparsity_loss': 17547.419921875, 'l0': 2669.33349609375, 'dead_features': 4482}
Step 10100: {'loss': 34777.2421875, 'mse_loss': 12100.2412109375, 'sparsity_loss': 22677.001953125, 'l0': 3031.77685546875, 'dead_features': 4593}
Step 10200: {'loss': 24457.458984375, 'mse_loss': 802.0037231445312, 'sparsity_loss': 23655.455078125, 'l0': 3096.06591796875, 'dead_features': 4680}
Step 10300: {'loss': 23147.193359375, 'mse_loss': 1040.142822265625, 'sparsity_loss': 22107.05078125, 'l0': 2993.68798828125, 'dead_features': 4811}
Step 10400: {'loss': 22713.0234375, 'mse_loss': 1120.17578125, 'sparsity_loss': 21592.84765625, 'l0': 2958.90087890625, 'dead_features': 4924}
Step 10500: {'loss': 29197.9375, 'mse_loss': 798.2568359375, 'sparsity_loss': 28399.681640625, 'l0': 3390.44091796875, 'dead_features': 5003}
Step 10600: {'loss': 25109.6953125, 'mse_loss': 1153.5595703125, 'sparsity_loss': 23956.134765625, 'l0': 3115.5537109375, 'dead_features': 5196}
Step 10700: {'loss': 23159.73828125, 'mse_loss': 1164.2408447265625, 'sparsity_loss': 21995.498046875, 'l0': 2986.17578125, 'dead_features': 5306}
Step 10800: {'loss': 24404.09765625, 'mse_loss': 1042.690185546875, 'sparsity_loss': 23361.408203125, 'l0': 3076.8876953125, 'dead_features': 5381}
Step 10900: {'loss': 24807.7421875, 'mse_loss': 1130.0625, 'sparsity_loss': 23677.6796875, 'l0': 3097.5107421875, 'dead_features': 5491}
Step 11000: {'loss': 27884.806640625, 'mse_loss': 567.8885498046875, 'sparsity_loss': 27316.91796875, 'l0': 3325.56591796875, 'dead_features': 5631}
Step 11100: {'loss': 21952.20703125, 'mse_loss': 992.2447509765625, 'sparsity_loss': 20959.962890625, 'l0': 2915.51123046875, 'dead_features': 5778}
Step 11200: {'loss': 22658.01171875, 'mse_loss': 1311.0477294921875, 'sparsity_loss': 21346.96484375, 'l0': 2942.1201171875, 'dead_features': 5845}
Step 11300: {'loss': 24176.49609375, 'mse_loss': 1303.9482421875, 'sparsity_loss': 22872.546875, 'l0': 3044.734375, 'dead_features': 5965}
Step 11400: {'loss': 25076.84765625, 'mse_loss': 1168.678955078125, 'sparsity_loss': 23908.16796875, 'l0': 3112.453125, 'dead_features': 6141}
Step 11500: {'loss': 21156.396484375, 'mse_loss': 1187.8985595703125, 'sparsity_loss': 19968.498046875, 'l0': 2846.19873046875, 'dead_features': 6339}
Step 11600: {'loss': 22678.2890625, 'mse_loss': 998.818115234375, 'sparsity_loss': 21679.470703125, 'l0': 2964.7900390625, 'dead_features': 6513}
Step 11700: {'loss': 26070.8359375, 'mse_loss': 656.801025390625, 'sparsity_loss': 25414.03515625, 'l0': 3208.35595703125, 'dead_features': 6622}
Step 11800: {'loss': 23747.11328125, 'mse_loss': 1356.627197265625, 'sparsity_loss': 22390.486328125, 'l0': 3012.68994140625, 'dead_features': 6706}
Step 11900: {'loss': 24452.310546875, 'mse_loss': 1153.6583251953125, 'sparsity_loss': 23298.65234375, 'l0': 3072.779296875, 'dead_features': 6757}
Step 12000: {'loss': 26237.41015625, 'mse_loss': 1385.3663330078125, 'sparsity_loss': 24852.04296875, 'l0': 3172.90625, 'dead_features': 6917}
Step 12100: {'loss': 21680.869140625, 'mse_loss': 1114.596923828125, 'sparsity_loss': 20566.271484375, 'l0': 2888.18896484375, 'dead_features': 7103}
Step 12200: {'loss': 22577.478515625, 'mse_loss': 1275.2401123046875, 'sparsity_loss': 21302.23828125, 'l0': 2939.05712890625, 'dead_features': 7297}
Step 12300: {'loss': 24988.962890625, 'mse_loss': 776.281982421875, 'sparsity_loss': 24212.681640625, 'l0': 3132.0849609375, 'dead_features': 7469}
Step 12400: {'loss': 22573.923828125, 'mse_loss': 1134.509033203125, 'sparsity_loss': 21439.4140625, 'l0': 2948.44091796875, 'dead_features': 7648}
Step 12500: {'loss': 24994.177734375, 'mse_loss': 1558.127197265625, 'sparsity_loss': 23436.05078125, 'l0': 3081.767578125, 'dead_features': 7796}
Step 12600: {'loss': 22496.4375, 'mse_loss': 1041.6895751953125, 'sparsity_loss': 21454.748046875, 'l0': 2949.48779296875, 'dead_features': 7910}
Step 12700: {'loss': 24332.982421875, 'mse_loss': 1437.980224609375, 'sparsity_loss': 22895.001953125, 'l0': 3046.21875, 'dead_features': 7998}
Step 12800: {'loss': 22902.41015625, 'mse_loss': 1184.8939208984375, 'sparsity_loss': 21717.515625, 'l0': 2967.37255859375, 'dead_features': 8139}
Step 12900: {'loss': 23411.123046875, 'mse_loss': 1230.442626953125, 'sparsity_loss': 22180.6796875, 'l0': 2998.6357421875, 'dead_features': 8232}
Step 13000: {'loss': 22767.802734375, 'mse_loss': 1050.166259765625, 'sparsity_loss': 21717.63671875, 'l0': 2967.380859375, 'dead_features': 8455}
Step 13100: {'loss': 23263.724609375, 'mse_loss': 859.2398071289062, 'sparsity_loss': 22404.484375, 'l0': 3013.62548828125, 'dead_features': 8564}
Step 13200: {'loss': 25200.048828125, 'mse_loss': 1009.1343994140625, 'sparsity_loss': 24190.9140625, 'l0': 3130.685546875, 'dead_features': 8680}
Step 13300: {'loss': 24575.5, 'mse_loss': 873.133056640625, 'sparsity_loss': 23702.3671875, 'l0': 3099.11474609375, 'dead_features': 8842}
Step 13400: {'loss': 24927.83984375, 'mse_loss': 785.7044677734375, 'sparsity_loss': 24142.134765625, 'l0': 3127.5478515625, 'dead_features': 9102}
Step 13500: {'loss': 22176.6796875, 'mse_loss': 1190.4967041015625, 'sparsity_loss': 20986.18359375, 'l0': 2917.32177734375, 'dead_features': 9232}
Step 13600: {'loss': 24605.572265625, 'mse_loss': 1275.1204833984375, 'sparsity_loss': 23330.451171875, 'l0': 3074.86181640625, 'dead_features': 9316}
Step 13700: {'loss': 22599.04296875, 'mse_loss': 1046.732421875, 'sparsity_loss': 21552.310546875, 'l0': 2956.14111328125, 'dead_features': 9417}
Step 13800: {'loss': 24863.873046875, 'mse_loss': 1015.6095581054688, 'sparsity_loss': 23848.263671875, 'l0': 3108.57666015625, 'dead_features': 9386}
Step 13900: {'loss': 25535.892578125, 'mse_loss': 1505.646728515625, 'sparsity_loss': 24030.24609375, 'l0': 3120.33837890625, 'dead_features': 9386}
Step 14000: {'loss': 23623.59765625, 'mse_loss': 933.4146728515625, 'sparsity_loss': 22690.18359375, 'l0': 3032.65234375, 'dead_features': 9560}
Step 14100: {'loss': 24252.8046875, 'mse_loss': 1404.693603515625, 'sparsity_loss': 22848.111328125, 'l0': 3043.1181640625, 'dead_features': 9776}
Step 14200: {'loss': 26274.2109375, 'mse_loss': 1122.3885498046875, 'sparsity_loss': 25151.822265625, 'l0': 3191.865234375, 'dead_features': 9928}
Step 14300: {'loss': 25152.7578125, 'mse_loss': 688.1387939453125, 'sparsity_loss': 24464.619140625, 'l0': 3148.23388671875, 'dead_features': 10092}
Step 14400: {'loss': 22589.5625, 'mse_loss': 1015.1376953125, 'sparsity_loss': 21574.42578125, 'l0': 2957.64697265625, 'dead_features': 10280}
Step 14500: {'loss': 22732.30859375, 'mse_loss': 896.0400390625, 'sparsity_loss': 21836.267578125, 'l0': 2975.419921875, 'dead_features': 10409}
Step 14600: {'loss': 21675.560546875, 'mse_loss': 996.5579833984375, 'sparsity_loss': 20679.001953125, 'l0': 2896.0390625, 'dead_features': 10516}
Step 14700: {'loss': 24989.68359375, 'mse_loss': 1244.625244140625, 'sparsity_loss': 23745.05859375, 'l0': 3101.88623046875, 'dead_features': 10572}
Step 14800: {'loss': 20922.560546875, 'mse_loss': 1054.4449462890625, 'sparsity_loss': 19868.115234375, 'l0': 2839.0859375, 'dead_features': 10741}
Step 14900: {'loss': 28244.61328125, 'mse_loss': 1221.78466796875, 'sparsity_loss': 27022.828125, 'l0': 3307.72412109375, 'dead_features': 10848}
Step 15000: {'loss': 24947.201171875, 'mse_loss': 1460.78369140625, 'sparsity_loss': 23486.41796875, 'l0': 3085.0556640625, 'dead_features': 11019}
Step 15100: {'loss': 24314.837890625, 'mse_loss': 757.2581787109375, 'sparsity_loss': 23557.580078125, 'l0': 3089.69580078125, 'dead_features': 11174}
Step 15200: {'loss': 22399.12890625, 'mse_loss': 1336.3406982421875, 'sparsity_loss': 21062.7890625, 'l0': 2922.60498046875, 'dead_features': 11320}
Step 15300: {'loss': 21831.8203125, 'mse_loss': 1146.975830078125, 'sparsity_loss': 20684.84375, 'l0': 2896.4453125, 'dead_features': 11475}
Step 15400: {'loss': 22077.341796875, 'mse_loss': 996.22998046875, 'sparsity_loss': 21081.111328125, 'l0': 2923.8671875, 'dead_features': 11567}
Step 15500: {'loss': 37838.60546875, 'mse_loss': 16868.298828125, 'sparsity_loss': 20970.306640625, 'l0': 2916.2255859375, 'dead_features': 11705}
Step 15600: {'loss': 24642.814453125, 'mse_loss': 852.6934814453125, 'sparsity_loss': 23790.12109375, 'l0': 3104.80908203125, 'dead_features': 11812}
Step 15700: {'loss': 25098.056640625, 'mse_loss': 1486.016845703125, 'sparsity_loss': 23612.0390625, 'l0': 3093.24169921875, 'dead_features': 11809}
Step 15800: {'loss': 22548.0703125, 'mse_loss': 889.204345703125, 'sparsity_loss': 21658.865234375, 'l0': 2963.39013671875, 'dead_features': 11978}
Step 15900: {'loss': 22241.22265625, 'mse_loss': 1179.544677734375, 'sparsity_loss': 21061.677734375, 'l0': 2922.5283203125, 'dead_features': 12020}
Step 16000: {'loss': 24168.2890625, 'mse_loss': 1479.887939453125, 'sparsity_loss': 22688.400390625, 'l0': 3032.53369140625, 'dead_features': 12122}
Step 16100: {'loss': 22388.12890625, 'mse_loss': 1265.6346435546875, 'sparsity_loss': 21122.494140625, 'l0': 2926.7158203125, 'dead_features': 12126}
Step 16200: {'loss': 26502.4296875, 'mse_loss': 629.93505859375, 'sparsity_loss': 25872.494140625, 'l0': 3236.98583984375, 'dead_features': 12349}
Step 16300: {'loss': 22474.1484375, 'mse_loss': 1437.21630859375, 'sparsity_loss': 21036.931640625, 'l0': 2920.82275390625, 'dead_features': 12465}
Step 16400: {'loss': 26248.3359375, 'mse_loss': 894.395751953125, 'sparsity_loss': 25353.939453125, 'l0': 3204.583984375, 'dead_features': 12595}
Step 16500: {'loss': 24273.96875, 'mse_loss': 910.3739624023438, 'sparsity_loss': 23363.595703125, 'l0': 3077.03076171875, 'dead_features': 12692}
Step 16600: {'loss': 24251.859375, 'mse_loss': 1021.3565673828125, 'sparsity_loss': 23230.501953125, 'l0': 3068.31103515625, 'dead_features': 12723}
Step 16700: {'loss': 23834.4609375, 'mse_loss': 1345.3583984375, 'sparsity_loss': 22489.1015625, 'l0': 3019.2734375, 'dead_features': 12803}
Step 16800: {'loss': 23662.26953125, 'mse_loss': 1224.577392578125, 'sparsity_loss': 22437.69140625, 'l0': 3015.84326171875, 'dead_features': 12718}
Step 16900: {'loss': 26516.158203125, 'mse_loss': 1162.442626953125, 'sparsity_loss': 25353.71484375, 'l0': 3204.56982421875, 'dead_features': 12700}
Step 17000: {'loss': 23205.3203125, 'mse_loss': 1011.1131591796875, 'sparsity_loss': 22194.20703125, 'l0': 2999.5439453125, 'dead_features': 12825}
Step 17100: {'loss': 21302.09375, 'mse_loss': 1054.6531982421875, 'sparsity_loss': 20247.44140625, 'l0': 2865.8701171875, 'dead_features': 12843}
Step 17200: {'loss': 23183.46484375, 'mse_loss': 1124.12060546875, 'sparsity_loss': 22059.34375, 'l0': 2990.4775390625, 'dead_features': 12999}
Step 17300: {'loss': 22022.833984375, 'mse_loss': 1156.939697265625, 'sparsity_loss': 20865.89453125, 'l0': 2909.00634765625, 'dead_features': 13134}
Step 17400: {'loss': 25160.87109375, 'mse_loss': 519.580078125, 'sparsity_loss': 24641.291015625, 'l0': 3159.5087890625, 'dead_features': 13323}
Step 17500: {'loss': 21858.21484375, 'mse_loss': 1232.8447265625, 'sparsity_loss': 20625.37109375, 'l0': 2892.30712890625, 'dead_features': 13295}
Step 17600: {'loss': 23212.51953125, 'mse_loss': 1253.686279296875, 'sparsity_loss': 21958.833984375, 'l0': 2983.70263671875, 'dead_features': 13371}
Step 17700: {'loss': 21260.42578125, 'mse_loss': 975.1260986328125, 'sparsity_loss': 20285.298828125, 'l0': 2868.529296875, 'dead_features': 13482}
Step 17800: {'loss': 25551.77734375, 'mse_loss': 947.454345703125, 'sparsity_loss': 24604.322265625, 'l0': 3157.15283203125, 'dead_features': 13484}
Step 17900: {'loss': 21924.3203125, 'mse_loss': 1057.367919921875, 'sparsity_loss': 20866.953125, 'l0': 2909.07958984375, 'dead_features': 13488}
Step 18000: {'loss': 20820.85546875, 'mse_loss': 1128.610595703125, 'sparsity_loss': 19692.244140625, 'l0': 2826.5810546875, 'dead_features': 13587}
Step 18100: {'loss': 22612.275390625, 'mse_loss': 1160.289794921875, 'sparsity_loss': 21451.986328125, 'l0': 2949.29931640625, 'dead_features': 13747}
Step 18200: {'loss': 26859.140625, 'mse_loss': 723.6396484375, 'sparsity_loss': 26135.501953125, 'l0': 3253.29541015625, 'dead_features': 13941}
Step 18300: {'loss': 23026.44140625, 'mse_loss': 1169.907470703125, 'sparsity_loss': 21856.533203125, 'l0': 2976.791015625, 'dead_features': 13978}
Step 18400: {'loss': 23475.890625, 'mse_loss': 1333.03271484375, 'sparsity_loss': 22142.857421875, 'l0': 2996.09521484375, 'dead_features': 13978}
Step 18500: {'loss': 23435.96484375, 'mse_loss': 1287.13525390625, 'sparsity_loss': 22148.830078125, 'l0': 2996.49658203125, 'dead_features': 13968}
Step 18600: {'loss': 26690.240234375, 'mse_loss': 1393.4437255859375, 'sparsity_loss': 25296.796875, 'l0': 3200.9931640625, 'dead_features': 13795}
Step 18700: {'loss': 25391.359375, 'mse_loss': 1052.7061767578125, 'sparsity_loss': 24338.65234375, 'l0': 3140.169921875, 'dead_features': 12988}
Step 18800: {'loss': 24838.701171875, 'mse_loss': 562.345703125, 'sparsity_loss': 24276.35546875, 'l0': 3136.17431640625, 'dead_features': 12891}
Step 18900: {'loss': 20876.015625, 'mse_loss': 1112.5482177734375, 'sparsity_loss': 19763.466796875, 'l0': 2831.65185546875, 'dead_features': 12937}
Step 19000: {'loss': 21143.66796875, 'mse_loss': 835.5963134765625, 'sparsity_loss': 20308.072265625, 'l0': 2870.1279296875, 'dead_features': 13480}
Step 19100: {'loss': 22173.48046875, 'mse_loss': 1206.347900390625, 'sparsity_loss': 20967.1328125, 'l0': 2916.00634765625, 'dead_features': 13959}
Step 19200: {'loss': 26102.771484375, 'mse_loss': 1396.4720458984375, 'sparsity_loss': 24706.298828125, 'l0': 3163.6474609375, 'dead_features': 14095}
Step 19300: {'loss': 19720.771484375, 'mse_loss': 892.0909423828125, 'sparsity_loss': 18828.6796875, 'l0': 2764.3525390625, 'dead_features': 14173}
Step 19400: {'loss': 20962.369140625, 'mse_loss': 889.4464721679688, 'sparsity_loss': 20072.921875, 'l0': 2853.57861328125, 'dead_features': 14231}
Step 19500: {'loss': 25187.54296875, 'mse_loss': 435.8558654785156, 'sparsity_loss': 24751.6875, 'l0': 3166.53369140625, 'dead_features': 14324}
Step 19600: {'loss': 20627.763671875, 'mse_loss': 922.923828125, 'sparsity_loss': 19704.83984375, 'l0': 2827.478515625, 'dead_features': 14455}
Step 19700: {'loss': 22868.0, 'mse_loss': 1135.65771484375, 'sparsity_loss': 21732.341796875, 'l0': 2968.37841796875, 'dead_features': 14445}
Step 19800: {'loss': 23316.298828125, 'mse_loss': 903.4735107421875, 'sparsity_loss': 22412.826171875, 'l0': 3014.1826171875, 'dead_features': 14412}
Step 19900: {'loss': 21904.25, 'mse_loss': 1303.4554443359375, 'sparsity_loss': 20600.794921875, 'l0': 2890.59521484375, 'dead_features': 14486}
Step 20000: {'loss': 23769.19140625, 'mse_loss': 571.408447265625, 'sparsity_loss': 23197.783203125, 'l0': 3066.16357421875, 'dead_features': 14719}
Step 20100: {'loss': 22335.677734375, 'mse_loss': 1204.613037109375, 'sparsity_loss': 21131.064453125, 'l0': 2927.3056640625, 'dead_features': 14757}
Step 20200: {'loss': 24427.20703125, 'mse_loss': 554.3036499023438, 'sparsity_loss': 23872.904296875, 'l0': 3110.171875, 'dead_features': 14858}
Step 20300: {'loss': 23212.033203125, 'mse_loss': 1288.610595703125, 'sparsity_loss': 21923.421875, 'l0': 2981.31201171875, 'dead_features': 14841}
Step 20400: {'loss': 22230.10546875, 'mse_loss': 1242.1165771484375, 'sparsity_loss': 20987.98828125, 'l0': 2917.4462890625, 'dead_features': 15065}
Step 20500: {'loss': 22124.421875, 'mse_loss': 1181.881591796875, 'sparsity_loss': 20942.541015625, 'l0': 2914.3076171875, 'dead_features': 15083}
Step 20600: {'loss': 20656.564453125, 'mse_loss': 1217.6541748046875, 'sparsity_loss': 19438.91015625, 'l0': 2808.4697265625, 'dead_features': 15228}
Step 20700: {'loss': 23378.802734375, 'mse_loss': 695.3753051757812, 'sparsity_loss': 22683.427734375, 'l0': 3032.20361328125, 'dead_features': 15267}
Step 20800: {'loss': 21425.908203125, 'mse_loss': 799.114013671875, 'sparsity_loss': 20626.794921875, 'l0': 2892.40625, 'dead_features': 15408}
Step 20900: {'loss': 23843.982421875, 'mse_loss': 409.2625427246094, 'sparsity_loss': 23434.720703125, 'l0': 3081.6806640625, 'dead_features': 15549}
Step 21000: {'loss': 21502.09765625, 'mse_loss': 1238.2344970703125, 'sparsity_loss': 20263.86328125, 'l0': 2867.02392578125, 'dead_features': 15688}
Step 21100: {'loss': 24256.2109375, 'mse_loss': 1339.692138671875, 'sparsity_loss': 22916.51953125, 'l0': 3047.640625, 'dead_features': 15877}
Step 21200: {'loss': 22864.3828125, 'mse_loss': 1400.34375, 'sparsity_loss': 21464.0390625, 'l0': 2950.1220703125, 'dead_features': 15843}
Step 21300: {'loss': 22532.79296875, 'mse_loss': 1222.3046875, 'sparsity_loss': 21310.48828125, 'l0': 2939.62255859375, 'dead_features': 15819}
Step 21400: {'loss': 19274.68359375, 'mse_loss': 1028.201904296875, 'sparsity_loss': 18246.482421875, 'l0': 2721.5908203125, 'dead_features': 15892}
Step 21500: {'loss': 20139.927734375, 'mse_loss': 835.267578125, 'sparsity_loss': 19304.66015625, 'l0': 2798.82421875, 'dead_features': 15910}
Step 21600: {'loss': 19309.72265625, 'mse_loss': 791.17578125, 'sparsity_loss': 18518.546875, 'l0': 2741.6572265625, 'dead_features': 16006}
Step 21700: {'loss': 24227.482421875, 'mse_loss': 393.9698486328125, 'sparsity_loss': 23833.51171875, 'l0': 3107.62109375, 'dead_features': 16059}
Step 21800: {'loss': 18755.615234375, 'mse_loss': 979.80126953125, 'sparsity_loss': 17775.814453125, 'l0': 2686.51953125, 'dead_features': 16095}
Step 21900: {'loss': 19918.43359375, 'mse_loss': 724.845458984375, 'sparsity_loss': 19193.587890625, 'l0': 2790.818359375, 'dead_features': 16274}
Step 22000: {'loss': 21543.892578125, 'mse_loss': 1238.759521484375, 'sparsity_loss': 20305.1328125, 'l0': 2869.92138671875, 'dead_features': 16273}
Step 22100: {'loss': 19033.587890625, 'mse_loss': 796.594482421875, 'sparsity_loss': 18236.994140625, 'l0': 2720.88818359375, 'dead_features': 16278}
Step 22200: {'loss': 22890.7890625, 'mse_loss': 760.6948852539062, 'sparsity_loss': 22130.09375, 'l0': 2995.2373046875, 'dead_features': 16325}
Step 22300: {'loss': 19483.978515625, 'mse_loss': 1010.1427001953125, 'sparsity_loss': 18473.8359375, 'l0': 2738.36962890625, 'dead_features': 16421}
Step 22400: {'loss': 19786.310546875, 'mse_loss': 1110.770751953125, 'sparsity_loss': 18675.5390625, 'l0': 2753.16943359375, 'dead_features': 16437}
Step 22500: {'loss': 18860.87890625, 'mse_loss': 854.5635986328125, 'sparsity_loss': 18006.314453125, 'l0': 2703.751953125, 'dead_features': 16524}
Step 22600: {'loss': 19584.40625, 'mse_loss': 593.0086059570312, 'sparsity_loss': 18991.3984375, 'l0': 2776.185546875, 'dead_features': 16568}
Step 22700: {'loss': 20844.70703125, 'mse_loss': 1175.4781494140625, 'sparsity_loss': 19669.228515625, 'l0': 2824.9404296875, 'dead_features': 16637}
Step 22800: {'loss': 21147.564453125, 'mse_loss': 1290.521240234375, 'sparsity_loss': 19857.04296875, 'l0': 2838.30029296875, 'dead_features': 16784}
Step 22900: {'loss': 21878.435546875, 'mse_loss': 918.4780883789062, 'sparsity_loss': 20959.95703125, 'l0': 2915.5107421875, 'dead_features': 16842}
Step 23000: {'loss': 19427.330078125, 'mse_loss': 1005.9007568359375, 'sparsity_loss': 18421.4296875, 'l0': 2734.51123046875, 'dead_features': 16938}
Step 23100: {'loss': 20114.10546875, 'mse_loss': 1044.361328125, 'sparsity_loss': 19069.744140625, 'l0': 2781.86474609375, 'dead_features': 16956}
Step 23200: {'loss': 21581.69921875, 'mse_loss': 1386.685546875, 'sparsity_loss': 20195.013671875, 'l0': 2862.18310546875, 'dead_features': 17038}
Step 23300: {'loss': 20198.07421875, 'mse_loss': 1222.999267578125, 'sparsity_loss': 18975.07421875, 'l0': 2775.0009765625, 'dead_features': 17129}
Step 23400: {'loss': 20731.185546875, 'mse_loss': 811.6328125, 'sparsity_loss': 19919.552734375, 'l0': 2842.73291015625, 'dead_features': 17138}
Step 23500: {'loss': 20863.76171875, 'mse_loss': 952.3198852539062, 'sparsity_loss': 19911.44140625, 'l0': 2842.158203125, 'dead_features': 17209}
Step 23600: {'loss': 18881.25390625, 'mse_loss': 670.1053466796875, 'sparsity_loss': 18211.1484375, 'l0': 2718.9736328125, 'dead_features': 17262}
Step 23700: {'loss': 18489.408203125, 'mse_loss': 742.7786865234375, 'sparsity_loss': 17746.62890625, 'l0': 2684.32958984375, 'dead_features': 17311}
Step 23800: {'loss': 18861.201171875, 'mse_loss': 969.954345703125, 'sparsity_loss': 17891.24609375, 'l0': 2695.1630859375, 'dead_features': 17387}
Step 23900: {'loss': 17849.908203125, 'mse_loss': 788.751708984375, 'sparsity_loss': 17061.15625, 'l0': 2632.3671875, 'dead_features': 17630}
Step 24000: {'loss': 18327.03515625, 'mse_loss': 1039.139404296875, 'sparsity_loss': 17287.896484375, 'l0': 2649.6689453125, 'dead_features': 17700}
Step 24100: {'loss': 20883.85546875, 'mse_loss': 1202.51416015625, 'sparsity_loss': 19681.341796875, 'l0': 2825.80419921875, 'dead_features': 17756}
Step 24200: {'loss': 18104.203125, 'mse_loss': 991.0189208984375, 'sparsity_loss': 17113.18359375, 'l0': 2636.34716796875, 'dead_features': 17885}
Step 24300: {'loss': 17388.94140625, 'mse_loss': 931.8509521484375, 'sparsity_loss': 16457.08984375, 'l0': 2585.70361328125, 'dead_features': 17954}
Step 24400: {'loss': 17920.23046875, 'mse_loss': 647.1218872070312, 'sparsity_loss': 17273.109375, 'l0': 2648.5439453125, 'dead_features': 18044}
Step 24500: {'loss': 18792.029296875, 'mse_loss': 664.8638916015625, 'sparsity_loss': 18127.166015625, 'l0': 2712.7431640625, 'dead_features': 18240}
Step 24600: {'loss': 18133.49609375, 'mse_loss': 749.2095947265625, 'sparsity_loss': 17384.287109375, 'l0': 2656.98974609375, 'dead_features': 18247}
Step 24700: {'loss': 18530.515625, 'mse_loss': 1096.9677734375, 'sparsity_loss': 17433.548828125, 'l0': 2660.72314453125, 'dead_features': 18329}
Step 24800: {'loss': 17215.509765625, 'mse_loss': 784.44970703125, 'sparsity_loss': 16431.060546875, 'l0': 2583.673828125, 'dead_features': 18375}
Step 24900: {'loss': 19010.1875, 'mse_loss': 1169.876953125, 'sparsity_loss': 17840.310546875, 'l0': 2691.3525390625, 'dead_features': 18245}
Step 25000: {'loss': 18865.703125, 'mse_loss': 1050.830810546875, 'sparsity_loss': 17814.873046875, 'l0': 2689.447265625, 'dead_features': 18298}
Step 25100: {'loss': 18591.443359375, 'mse_loss': 914.9952392578125, 'sparsity_loss': 17676.447265625, 'l0': 2679.05615234375, 'dead_features': 18394}
Step 25200: {'loss': 17448.212890625, 'mse_loss': 1009.984130859375, 'sparsity_loss': 16438.228515625, 'l0': 2584.23291015625, 'dead_features': 18725}
Step 25300: {'loss': 17269.37890625, 'mse_loss': 794.0452270507812, 'sparsity_loss': 16475.333984375, 'l0': 2587.12548828125, 'dead_features': 18835}
Step 25400: {'loss': 17220.314453125, 'mse_loss': 924.9374389648438, 'sparsity_loss': 16295.3779296875, 'l0': 2573.06689453125, 'dead_features': 18901}
Step 25500: {'loss': 17783.01171875, 'mse_loss': 1094.26416015625, 'sparsity_loss': 16688.748046875, 'l0': 2603.69873046875, 'dead_features': 18941}
Step 25600: {'loss': 16639.63671875, 'mse_loss': 625.2838745117188, 'sparsity_loss': 16014.3525390625, 'l0': 2550.95654296875, 'dead_features': 18978}
Step 25700: {'loss': 17347.939453125, 'mse_loss': 1002.4175415039062, 'sparsity_loss': 16345.5224609375, 'l0': 2576.9921875, 'dead_features': 19057}
Step 25800: {'loss': 16893.38671875, 'mse_loss': 750.9307250976562, 'sparsity_loss': 16142.455078125, 'l0': 2561.05908203125, 'dead_features': 19129}
Step 25900: {'loss': 19506.28515625, 'mse_loss': 453.4617614746094, 'sparsity_loss': 19052.82421875, 'l0': 2780.63916015625, 'dead_features': 19209}
Step 26000: {'loss': 16203.3037109375, 'mse_loss': 716.8916015625, 'sparsity_loss': 15486.412109375, 'l0': 2508.88818359375, 'dead_features': 19359}
Step 26100: {'loss': 18408.720703125, 'mse_loss': 1010.375244140625, 'sparsity_loss': 17398.345703125, 'l0': 2658.0556640625, 'dead_features': 19431}
Step 26200: {'loss': 17112.15234375, 'mse_loss': 1063.693115234375, 'sparsity_loss': 16048.4599609375, 'l0': 2553.650390625, 'dead_features': 19621}
Step 26300: {'loss': 18740.865234375, 'mse_loss': 1138.176513671875, 'sparsity_loss': 17602.689453125, 'l0': 2673.50244140625, 'dead_features': 19799}
Step 26400: {'loss': 16083.033203125, 'mse_loss': 650.56396484375, 'sparsity_loss': 15432.4697265625, 'l0': 2504.5498046875, 'dead_features': 19883}
Step 26500: {'loss': 16083.7265625, 'mse_loss': 765.7777099609375, 'sparsity_loss': 15317.94921875, 'l0': 2495.31396484375, 'dead_features': 19850}
Step 26600: {'loss': 15893.9169921875, 'mse_loss': 742.4819946289062, 'sparsity_loss': 15151.4345703125, 'l0': 2481.8232421875, 'dead_features': 19950}
Step 26700: {'loss': 17587.107421875, 'mse_loss': 754.7818603515625, 'sparsity_loss': 16832.326171875, 'l0': 2614.7890625, 'dead_features': 20139}
Step 26800: {'loss': 16830.7578125, 'mse_loss': 901.2237548828125, 'sparsity_loss': 15929.5341796875, 'l0': 2544.2451171875, 'dead_features': 20383}
Step 26900: {'loss': 15925.88671875, 'mse_loss': 759.0321655273438, 'sparsity_loss': 15166.8544921875, 'l0': 2483.07568359375, 'dead_features': 20478}
Step 27000: {'loss': 15972.2109375, 'mse_loss': 941.68994140625, 'sparsity_loss': 15030.5205078125, 'l0': 2471.98046875, 'dead_features': 20550}
Step 27100: {'loss': 16247.755859375, 'mse_loss': 1004.2483520507812, 'sparsity_loss': 15243.5078125, 'l0': 2489.2919921875, 'dead_features': 20645}
Step 27200: {'loss': 16850.0625, 'mse_loss': 992.8018798828125, 'sparsity_loss': 15857.259765625, 'l0': 2538.51220703125, 'dead_features': 20656}
Step 27300: {'loss': 17060.349609375, 'mse_loss': 495.1214599609375, 'sparsity_loss': 16565.228515625, 'l0': 2594.11962890625, 'dead_features': 20619}
Step 27400: {'loss': 15107.4541015625, 'mse_loss': 708.5440673828125, 'sparsity_loss': 14398.91015625, 'l0': 2419.9091796875, 'dead_features': 20310}
Step 27500: {'loss': 15653.4345703125, 'mse_loss': 881.499755859375, 'sparsity_loss': 14771.9345703125, 'l0': 2450.796875, 'dead_features': 20492}
Step 27600: {'loss': 15979.5771484375, 'mse_loss': 1060.8271484375, 'sparsity_loss': 14918.75, 'l0': 2462.8466796875, 'dead_features': 20914}
Step 27700: {'loss': 16448.81640625, 'mse_loss': 1097.787109375, 'sparsity_loss': 15351.029296875, 'l0': 2497.9853515625, 'dead_features': 21131}
Step 27800: {'loss': 15555.244140625, 'mse_loss': 568.9111328125, 'sparsity_loss': 14986.3330078125, 'l0': 2468.37353515625, 'dead_features': 21309}
Step 27900: {'loss': 15384.77734375, 'mse_loss': 561.916748046875, 'sparsity_loss': 14822.8603515625, 'l0': 2454.9833984375, 'dead_features': 20677}
Step 28000: {'loss': 14874.6201171875, 'mse_loss': 644.3610229492188, 'sparsity_loss': 14230.2587890625, 'l0': 2405.81298828125, 'dead_features': 20767}
Step 28100: {'loss': 15805.451171875, 'mse_loss': 1067.78173828125, 'sparsity_loss': 14737.669921875, 'l0': 2447.97607421875, 'dead_features': 20881}
Step 28200: {'loss': 16630.66015625, 'mse_loss': 1131.4306640625, 'sparsity_loss': 15499.2294921875, 'l0': 2509.91796875, 'dead_features': 21310}
Step 28300: {'loss': 15769.0888671875, 'mse_loss': 802.4451293945312, 'sparsity_loss': 14966.6435546875, 'l0': 2466.7646484375, 'dead_features': 21272}
Step 28400: {'loss': 15139.896484375, 'mse_loss': 840.23291015625, 'sparsity_loss': 14299.6630859375, 'l0': 2411.6240234375, 'dead_features': 21333}
Step 28500: {'loss': 15145.8466796875, 'mse_loss': 990.3477783203125, 'sparsity_loss': 14155.4990234375, 'l0': 2399.53759765625, 'dead_features': 21816}
Step 28600: {'loss': 14988.4716796875, 'mse_loss': 982.2906494140625, 'sparsity_loss': 14006.1806640625, 'l0': 2386.9541015625, 'dead_features': 22219}
Step 28700: {'loss': 15516.6484375, 'mse_loss': 1006.1583862304688, 'sparsity_loss': 14510.490234375, 'l0': 2429.18994140625, 'dead_features': 22131}
Step 28800: {'loss': 15811.6708984375, 'mse_loss': 1010.8037719726562, 'sparsity_loss': 14800.8671875, 'l0': 2453.17626953125, 'dead_features': 22095}
Step 28900: {'loss': 14614.3583984375, 'mse_loss': 883.4097900390625, 'sparsity_loss': 13730.9482421875, 'l0': 2363.58251953125, 'dead_features': 22009}
Step 29000: {'loss': 15886.251953125, 'mse_loss': 1123.33544921875, 'sparsity_loss': 14762.916015625, 'l0': 2450.0546875, 'dead_features': 22197}
Step 29100: {'loss': 13870.7890625, 'mse_loss': 667.87646484375, 'sparsity_loss': 13202.9130859375, 'l0': 2318.07861328125, 'dead_features': 22474}
Step 29200: {'loss': 14626.4267578125, 'mse_loss': 624.3564453125, 'sparsity_loss': 14002.0703125, 'l0': 2386.60693359375, 'dead_features': 22679}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/templates/sae_variants/run_0/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_0/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9490000605583191, scr_score: 0.5937502619346752
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9510000348091125, scr_score: 0.6249998835845888
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9420000314712524, scr_score: 0.4843747235133985
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9430000185966492, scr_score: 0.4999995343383553
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9460000395774841, scr_score: 0.5468748981365152
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9500000476837158, scr_score: 0.609375072759632
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9460000395774841, scr_score: 0.5468748981365152
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5870000123977661, scr_score: 0.009828028179079856
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5980000495910645, scr_score: 0.036855142283733294
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6010000109672546, scr_score: 0.04422605358149169
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6050000190734863, scr_score: 0.05405408176057155
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6170000433921814, scr_score: 0.08353816629781112
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6310000419616699, scr_score: 0.11793619170022296
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: 0.19901724111671262
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.893000066280365, scr_score: 0.39805839288494044
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8890000581741333, scr_score: 0.3592233571539762
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8670000433921814, scr_score: 0.14563094997665002
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8810000419616699, scr_score: 0.28155328569204763
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8220000267028809, scr_score: -0.2912624786392547
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.7890000343322754, scr_score: -0.6116508000622666
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.7190000414848328, scr_score: -1.2912624786392548
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6680000424385071, scr_score: 0.08022922342486603
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6930000185966492, scr_score: 0.1518623976302968
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.690000057220459, scr_score: 0.1432665191978205
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7090000510215759, scr_score: 0.19770776575133966
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7430000305175781, scr_score: 0.29512891682811726
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6480000019073486, scr_score: 0.022922513273562452
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.5840000510215759, scr_score: -0.16045844684973207
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9470000267028809, scr_score: 0.5873013770560047
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9500000476837158, scr_score: 0.6349207400434262
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9480000734329224, scr_score: 0.6031751287885597
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9470000267028809, scr_score: 0.5873013770560047
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9480000734329224, scr_score: 0.6031751287885597
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9470000267028809, scr_score: 0.5873013770560047
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9280000329017639, scr_score: 0.2857142857142857
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.5960000157356262, scr_score: 0.02025320352298121
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6010000109672546, scr_score: 0.03291141800038962
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6080000400543213, scr_score: 0.05063300880745302
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6230000257492065, scr_score: 0.08860765223967824
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6310000419616699, scr_score: 0.10886085576265946
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6410000324249268, scr_score: 0.13417728471747628
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6080000400543213, scr_score: 0.05063300880745302
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.89000004529953, scr_score: 0.3828126309672156
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8600000143051147, scr_score: 0.1484374272404358
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8840000629425049, scr_score: 0.33593777648634404
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.9040000438690186, scr_score: 0.4921876018633899
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8890000581741333, scr_score: 0.3750002328306055
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8160000443458557, scr_score: -0.19531228172130735
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8860000371932983, scr_score: 0.3515625727595642
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6650000214576721, scr_score: 0.02373891963843859
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6780000329017639, scr_score: 0.0623146198338074
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6970000267028809, scr_score: 0.11869442132381734
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6860000491142273, scr_score: 0.08605353947224599
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7050000429153442, scr_score: 0.14243334096225593
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6380000114440918, scr_score: -0.05637980149000994
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6740000247955322, scr_score: 0.050445160014588104
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8100000619888306, scr_score: 0.016393554752069144
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8010000586509705, scr_score: -0.03278678379574697
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8170000314712524, scr_score: 0.05464474822904205
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8420000672340393, scr_score: 0.1912569445100385
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8270000219345093, scr_score: 0.1092894964580841
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.843000054359436, scr_score: 0.19672135419126444
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8180000185966492, scr_score: 0.06010915791026799
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7850000262260437, scr_score: 0.37647056898581666
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.831000030040741, scr_score: 0.5568627020158754
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8300000429153442, scr_score: 0.5529411847203642
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.831000030040741, scr_score: 0.5568627020158754
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8400000333786011, scr_score: 0.5921568251627849
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8240000605583191, scr_score: 0.5294118472036429
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8790000677108765, scr_score: 0.7450981033806111
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.7950000166893005, scr_score: 0.06666642213478169
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8060000538825989, scr_score: 0.12307687134902433
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8140000104904175, scr_score: 0.16410229135546164
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8400000333786011, scr_score: 0.29743574695473746
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8210000395774841, scr_score: 0.1999998777340575
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8350000381469727, scr_score: 0.27179474482639304
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8480000495910645, scr_score: 0.338461472626031
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7460000514984131, scr_score: 0.12096785435318151
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7690000534057617, scr_score: 0.2137097878988163
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7780000567436218, scr_score: 0.2500001201706423
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7910000085830688, scr_score: 0.30241927537102686
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.8210000395774841, scr_score: 0.4233871297242084
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.8270000219345093, scr_score: 0.4475806044583308
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.8700000643730164, scr_score: 0.6209678543531815
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7820000648498535, scr_score: 0.12107628352460723
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8200000524520874, scr_score: 0.2914798230249813
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8360000252723694, scr_score: 0.3632285832884071
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8480000495910645, scr_score: 0.4170403539500374
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8650000691413879, scr_score: 0.4932736288993267
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8340000510215759, scr_score: 0.35426008848750934
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8450000286102295, scr_score: 0.4035873444632762
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7550000548362732, scr_score: 0.0714287044746317
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7820000648498535, scr_score: 0.1919644437064823
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8020000457763672, scr_score: 0.28125005820765137
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8180000185966492, scr_score: 0.35267849659016254
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8340000510215759, scr_score: 0.4241072010647942
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8490000367164612, scr_score: 0.49107141194067105
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8840000629425049, scr_score: 0.6473215034098375
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7390000224113464, scr_score: 0.042918406627421406
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7450000643730164, scr_score: 0.06866965525497992
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7500000596046448, scr_score: 0.09012885856869063
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7560000419616699, scr_score: 0.11587985138236706
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7610000371932983, scr_score: 0.13733905469607777
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7440000176429749, scr_score: 0.0643776099411321
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7910000085830688, scr_score: 0.266094274578342
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7800000309944153, scr_score: 0.09047614452250444
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7700000405311584, scr_score: 0.042857150966616867
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7820000648498535, scr_score: 0.10000011353263609
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.784000039100647, scr_score: 0.10952379871117751
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.784000039100647, scr_score: 0.10952379871117751
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8010000586509705, scr_score: 0.19047625805514054
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8100000619888306, scr_score: 0.2333334090217574

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 0: 0.9570000171661377
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 1: 0.9670000672340393
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 2: 0.9540000557899475
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 6: 0.9900000691413879
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 9: 0.9810000658035278
Num non-zero elements: 1
Test accuracy for 0: 0.705
Num non-zero elements: 1
Test accuracy for 1: 0.653
Num non-zero elements: 1
Test accuracy for 2: 0.877
Num non-zero elements: 1
Test accuracy for 6: 0.751
Num non-zero elements: 1
Test accuracy for 9: 0.929
Num non-zero elements: 2
Test accuracy for 0: 0.711
Num non-zero elements: 2
Test accuracy for 1: 0.697
Num non-zero elements: 2
Test accuracy for 2: 0.876
Num non-zero elements: 2
Test accuracy for 6: 0.913
Num non-zero elements: 2
Test accuracy for 9: 0.937
Num non-zero elements: 5
Test accuracy for 0: 0.799
Num non-zero elements: 5
Test accuracy for 1: 0.73
Num non-zero elements: 5
Test accuracy for 2: 0.892
Num non-zero elements: 5
Test accuracy for 6: 0.968
Num non-zero elements: 5
Test accuracy for 9: 0.934
Num non-zero elements: 10
Test accuracy for 0: 0.826
Num non-zero elements: 10
Test accuracy for 1: 0.813
Num non-zero elements: 10
Test accuracy for 2: 0.914
Num non-zero elements: 10
Test accuracy for 6: 0.971
Num non-zero elements: 10
Test accuracy for 9: 0.939
Num non-zero elements: 20
Test accuracy for 0: 0.872
Num non-zero elements: 20
Test accuracy for 1: 0.85
Num non-zero elements: 20
Test accuracy for 2: 0.924
Num non-zero elements: 20
Test accuracy for 6: 0.986
Num non-zero elements: 20
Test accuracy for 9: 0.954
Num non-zero elements: 50
Test accuracy for 0: 0.933
Num non-zero elements: 50
Test accuracy for 1: 0.942
Num non-zero elements: 50
Test accuracy for 2: 0.922
Num non-zero elements: 50
Test accuracy for 6: 0.99
Num non-zero elements: 50
Test accuracy for 9: 0.969
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 11: 0.9650000333786011
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 13: 0.9480000734329224
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 14: 0.9640000462532043
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 35 epochs
Test accuracy for 18: 0.9290000200271606
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 19: 0.9620000720024109
Num non-zero elements: 1
Test accuracy for 11: 0.758
Num non-zero elements: 1
Test accuracy for 13: 0.64
Num non-zero elements: 1
Test accuracy for 14: 0.796
Num non-zero elements: 1
Test accuracy for 18: 0.699
Num non-zero elements: 1
Test accuracy for 19: 0.779
Num non-zero elements: 2
Test accuracy for 11: 0.807
Num non-zero elements: 2
Test accuracy for 13: 0.676
Num non-zero elements: 2
Test accuracy for 14: 0.805
Num non-zero elements: 2
Test accuracy for 18: 0.711
Num non-zero elements: 2
Test accuracy for 19: 0.799
Num non-zero elements: 5
Test accuracy for 11: 0.818
Num non-zero elements: 5
Test accuracy for 13: 0.835
Num non-zero elements: 5
Test accuracy for 14: 0.847
Num non-zero elements: 5
Test accuracy for 18: 0.806
Num non-zero elements: 5
Test accuracy for 19: 0.805
Num non-zero elements: 10
Test accuracy for 11: 0.876
Num non-zero elements: 10
Test accuracy for 13: 0.891
Num non-zero elements: 10
Test accuracy for 14: 0.893
Num non-zero elements: 10
Test accuracy for 18: 0.82
Num non-zero elements: 10
Test accuracy for 19: 0.829
Num non-zero elements: 20
Test accuracy for 11: 0.949
Num non-zero elements: 20
Test accuracy for 13: 0.906
Num non-zero elements: 20
Test accuracy for 14: 0.913
Num non-zero elements: 20
Test accuracy for 18: 0.849
Num non-zero elements: 20
Test accuracy for 19: 0.877
Num non-zero elements: 50
Test accuracy for 11: 0.955
Num non-zero elements: 50
Test accuracy for 13: 0.917
Num non-zero elements: 50
Test accuracy for 14: 0.927
Num non-zero elements: 50
Test accuracy for 18: 0.86
Num non-zero elements: 50
Test accuracy for 19: 0.935
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 20: 0.9610000252723694
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 21: 0.9140000343322754
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 22: 0.9310000538825989
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 40 epochs
Test accuracy for 25: 0.9720000624656677
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 26: 0.89000004529953
Num non-zero elements: 1
Test accuracy for 20: 0.865
Num non-zero elements: 1
Test accuracy for 21: 0.601
Num non-zero elements: 1
Test accuracy for 22: 0.674
Num non-zero elements: 1
Test accuracy for 25: 0.906
Num non-zero elements: 1
Test accuracy for 26: 0.714
Num non-zero elements: 2
Test accuracy for 20: 0.867
Num non-zero elements: 2
Test accuracy for 21: 0.835
Num non-zero elements: 2
Test accuracy for 22: 0.74
Num non-zero elements: 2
Test accuracy for 25: 0.892
Num non-zero elements: 2
Test accuracy for 26: 0.703
Num non-zero elements: 5
Test accuracy for 20: 0.891
Num non-zero elements: 5
Test accuracy for 21: 0.839
Num non-zero elements: 5
Test accuracy for 22: 0.725
Num non-zero elements: 5
Test accuracy for 25: 0.905
Num non-zero elements: 5
Test accuracy for 26: 0.802
Num non-zero elements: 10
Test accuracy for 20: 0.937
Num non-zero elements: 10
Test accuracy for 21: 0.846
Num non-zero elements: 10
Test accuracy for 22: 0.742
Num non-zero elements: 10
Test accuracy for 25: 0.927
Num non-zero elements: 10
Test accuracy for 26: 0.815
Num non-zero elements: 20
Test accuracy for 20: 0.935
Num non-zero elements: 20
Test accuracy for 21: 0.875
Num non-zero elements: 20
Test accuracy for 22: 0.819
Num non-zero elements: 20
Test accuracy for 25: 0.937
Num non-zero elements: 20
Test accuracy for 26: 0.815
Num non-zero elements: 50
Test accuracy for 20: 0.941
Num non-zero elements: 50
Test accuracy for 21: 0.875
Num non-zero elements: 50
Test accuracy for 22: 0.856
Num non-zero elements: 50
Test accuracy for 25: 0.945
Num non-zero elements: 50
Test accuracy for 26: 0.859
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 1: 0.9480000734329224
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 36 epochs
Test accuracy for 2: 0.9410000443458557
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 44 epochs
Test accuracy for 3: 0.9280000329017639
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 5: 0.921000063419342
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 39 epochs
Test accuracy for 6: 0.8710000514984131
Num non-zero elements: 1
Test accuracy for 1: 0.839
Num non-zero elements: 1
Test accuracy for 2: 0.607
Num non-zero elements: 1
Test accuracy for 3: 0.67
Num non-zero elements: 1
Test accuracy for 5: 0.838
Num non-zero elements: 1
Test accuracy for 6: 0.601
Num non-zero elements: 2
Test accuracy for 1: 0.898
Num non-zero elements: 2
Test accuracy for 2: 0.721
Num non-zero elements: 2
Test accuracy for 3: 0.663
Num non-zero elements: 2
Test accuracy for 5: 0.837
Num non-zero elements: 2
Test accuracy for 6: 0.704
Num non-zero elements: 5
Test accuracy for 1: 0.91
Num non-zero elements: 5
Test accuracy for 2: 0.724
Num non-zero elements: 5
Test accuracy for 3: 0.736
Num non-zero elements: 5
Test accuracy for 5: 0.848
Num non-zero elements: 5
Test accuracy for 6: 0.761
Num non-zero elements: 10
Test accuracy for 1: 0.911
Num non-zero elements: 10
Test accuracy for 2: 0.75
Num non-zero elements: 10
Test accuracy for 3: 0.816
Num non-zero elements: 10
Test accuracy for 5: 0.854
Num non-zero elements: 10
Test accuracy for 6: 0.772
Num non-zero elements: 20
Test accuracy for 1: 0.924
Num non-zero elements: 20
Test accuracy for 2: 0.818
Num non-zero elements: 20
Test accuracy for 3: 0.839
Num non-zero elements: 20
Test accuracy for 5: 0.89
Num non-zero elements: 20
Test accuracy for 6: 0.792
Num non-zero elements: 50
Test accuracy for 1: 0.922
Num non-zero elements: 50
Test accuracy for 2: 0.913
Num non-zero elements: 50
Test accuracy for 3: 0.874
Num non-zero elements: 50
Test accuracy for 5: 0.9
Num non-zero elements: 50
Test accuracy for 6: 0.795
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 1.0: 0.9820000529289246
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 5.0: 0.9790000319480896
Num non-zero elements: 1
Test accuracy for 1.0: 0.866
Num non-zero elements: 1
Test accuracy for 5.0: 0.866
Num non-zero elements: 2
Test accuracy for 1.0: 0.913
Num non-zero elements: 2
Test accuracy for 5.0: 0.913
Num non-zero elements: 5
Test accuracy for 1.0: 0.933
Num non-zero elements: 5
Test accuracy for 5.0: 0.933
Num non-zero elements: 10
Test accuracy for 1.0: 0.938
Num non-zero elements: 10
Test accuracy for 5.0: 0.938
Num non-zero elements: 20
Test accuracy for 1.0: 0.949
Num non-zero elements: 20
Test accuracy for 5.0: 0.949
Num non-zero elements: 50
Test accuracy for 1.0: 0.951
Num non-zero elements: 50
Test accuracy for 5.0: 0.951
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 34 epochs
Test accuracy for C: 0.9650000333786011
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 32 epochs
Test accuracy for Python: 0.9890000224113464
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 25 epochs
Test accuracy for HTML: 0.9860000610351562
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 38 epochs
Test accuracy for Java: 0.9670000672340393
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 22 epochs
Test accuracy for PHP: 0.9540000557899475
Num non-zero elements: 1
Test accuracy for C: 0.617
Num non-zero elements: 1
Test accuracy for Python: 0.54
Num non-zero elements: 1
Test accuracy for HTML: 0.586
Num non-zero elements: 1
Test accuracy for Java: 0.64
Num non-zero elements: 1
Test accuracy for PHP: 0.615
Num non-zero elements: 2
Test accuracy for C: 0.703
Num non-zero elements: 2
Test accuracy for Python: 0.616
Num non-zero elements: 2
Test accuracy for HTML: 0.836
Num non-zero elements: 2
Test accuracy for Java: 0.672
Num non-zero elements: 2
Test accuracy for PHP: 0.66
Num non-zero elements: 5
Test accuracy for C: 0.753
Num non-zero elements: 5
Test accuracy for Python: 0.665
Num non-zero elements: 5
Test accuracy for HTML: 0.92
Num non-zero elements: 5
Test accuracy for Java: 0.713
Num non-zero elements: 5
Test accuracy for PHP: 0.685
Num non-zero elements: 10
Test accuracy for C: 0.783
Num non-zero elements: 10
Test accuracy for Python: 0.78
Num non-zero elements: 10
Test accuracy for HTML: 0.943
Num non-zero elements: 10
Test accuracy for Java: 0.796
Num non-zero elements: 10
Test accuracy for PHP: 0.733
Num non-zero elements: 20
Test accuracy for C: 0.837
Num non-zero elements: 20
Test accuracy for Python: 0.792
Num non-zero elements: 20
Test accuracy for HTML: 0.953
Num non-zero elements: 20
Test accuracy for Java: 0.846
Num non-zero elements: 20
Test accuracy for PHP: 0.782
Num non-zero elements: 50
Test accuracy for C: 0.914
Num non-zero elements: 50
Test accuracy for Python: 0.918
Num non-zero elements: 50
Test accuracy for HTML: 0.956
Num non-zero elements: 50
Test accuracy for Java: 0.909
Num non-zero elements: 50
Test accuracy for PHP: 0.933
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 0: 0.9440000653266907
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 1: 0.987000048160553
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 2: 0.9330000281333923
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 33 epochs
Test accuracy for 3: 0.9590000510215759
Num non-zero elements: 1
Test accuracy for 0: 0.723
Num non-zero elements: 1
Test accuracy for 1: 0.974
Num non-zero elements: 1
Test accuracy for 2: 0.745
Num non-zero elements: 1
Test accuracy for 3: 0.753
Num non-zero elements: 2
Test accuracy for 0: 0.762
Num non-zero elements: 2
Test accuracy for 1: 0.975
Num non-zero elements: 2
Test accuracy for 2: 0.737
Num non-zero elements: 2
Test accuracy for 3: 0.806
Num non-zero elements: 5
Test accuracy for 0: 0.891
Num non-zero elements: 5
Test accuracy for 1: 0.978
Num non-zero elements: 5
Test accuracy for 2: 0.853
Num non-zero elements: 5
Test accuracy for 3: 0.845
Num non-zero elements: 10
Test accuracy for 0: 0.908
Num non-zero elements: 10
Test accuracy for 1: 0.974
Num non-zero elements: 10
Test accuracy for 2: 0.85
Num non-zero elements: 10
Test accuracy for 3: 0.905
Num non-zero elements: 20
Test accuracy for 0: 0.919
Num non-zero elements: 20
Test accuracy for 1: 0.975
Num non-zero elements: 20
Test accuracy for 2: 0.88
Num non-zero elements: 20
Test accuracy for 3: 0.918
Num non-zero elements: 50
Test accuracy for 0: 0.916
Num non-zero elements: 50
Test accuracy for 1: 0.976
Num non-zero elements: 50
Test accuracy for 2: 0.876
Num non-zero elements: 50
Test accuracy for 3: 0.933
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 12 epochs
Test accuracy for en: 0.999000072479248
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 16 epochs
Test accuracy for fr: 1.0
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 13 epochs
Test accuracy for de: 1.0
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 14 epochs
Test accuracy for es: 0.999000072479248
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.float32
GPU probe training early stopping triggered after 16 epochs
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.999
Num non-zero elements: 1
Test accuracy for fr: 0.847
Num non-zero elements: 1
Test accuracy for de: 0.8
Num non-zero elements: 1
Test accuracy for es: 0.992
Num non-zero elements: 1
Test accuracy for nl: 0.706
Num non-zero elements: 2
Test accuracy for en: 0.999
Num non-zero elements: 2
Test accuracy for fr: 0.854
Num non-zero elements: 2
Test accuracy for de: 0.872
Num non-zero elements: 2
Test accuracy for es: 0.991
Num non-zero elements: 2
Test accuracy for nl: 0.861
Num non-zero elements: 5
Test accuracy for en: 1.0
Num non-zero elements: 5
Test accuracy for fr: 0.868
Num non-zero elements: 5
Test accuracy for de: 0.974
Num non-zero elements: 5
Test accuracy for es: 0.997
Num non-zero elements: 5
Test accuracy for nl: 0.862
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.987
Num non-zero elements: 10
Test accuracy for de: 0.979
Num non-zero elements: 10
Test accuracy for es: 0.996
Num non-zero elements: 10
Test accuracy for nl: 0.859
Num non-zero elements: 20
Test accuracy for en: 0.999
Num non-zero elements: 20
Test accuracy for fr: 0.991
Num non-zero elements: 20
Test accuracy for de: 0.989
Num non-zero elements: 20
Test accuracy for es: 0.997
Num non-zero elements: 20
Test accuracy for nl: 0.962
Num non-zero elements: 50
Test accuracy for en: 0.999
Num non-zero elements: 50
Test accuracy for fr: 0.995
Num non-zero elements: 50
Test accuracy for de: 0.996
Num non-zero elements: 50
Test accuracy for es: 0.998
Num non-zero elements: 50
Test accuracy for nl: 0.994

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.055334 0.060503 0.026297 ... 0.110983 0.10498  0.903909]
(65536,)
Retain sparsity: [0.077796 0.049172 0.035404 ... 0.125102 0.112777 0.923225]
(65536,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [10506 25842 45314  2077 56357 23027 39119 61347  2779 21104 20107 36120
 44205 21364 34103 17564 40779  9286 19471 31912 48339 36359 34559 65092
 22709 17153 28389  8491 42849 17326 50353  7897 53329 47303 11280 23881
 18455 54730 19873 32604 25749 31533 22212 62161 15985 16021 37693 38727
 64081 19944 27614 26232 54135 62951 26348 20822  1147 63697 58458 29914
 41995 52815 37793 19592  9135 17785 36803 61383 12346  2086 55085 14808
 27546 12828 57693 14060 27484 31026 25288  6378 62809 11105 55176 52073
 37081   190 42057 58038 34781 26769  1647 25831 28120  4879 15161 38806
  2924 62297 52236  5062 65524 59740 38292 43825 26854  6470 61641   230
 20669 39117 55369 19350 24176 45545 25200 60009 31765  4073 18990 63512
 43530   706 60541  9344 59973 48315  9336 16748 16835 34065  7706 41502
 58280  1096 19856 41726 22679 42491 62964 37951  7286 38142 35941  3511
  5229 37863 30113  9746 43667 21987 56116  8685 17274 48219 42589 43262
 52831 45285 49092 44118 55453 36319 63961 14767 61353 59834 47157 62300
 61637  7579 44794 34961 29889  1361 12581 42694 46048 17960 61054 64070
 39113 31454  4036 16436 32365 28740 34350 27024 15704 56320 14154  4034
 53433  7719 59048 44493 17617 45592 19731  8031 11518  9566 19830 40956
 64581 54639  7639  3239 55189 26063 52898 53800 23010 15894 64990 41124
 24731  3234 11273 55705 63924 22399  5300  7722 25968 34665 55757 61991
 59378 32074 46580 52984  9664 47173 10561 11803  7957 22593 52156 63398
 10656 51744 38395 22005 49485 29408 46217  1747 14907 14643  8511 42447
   740 60643 48739 64472 47878 35043 26078 51965  8010 30902  1837  4592
  1689 17568 55642 60098  9398 42494  2878 38463  3733 34405 49866 60759
 62948 41590 40827  2859 63109 44631  5662 22776 45075 54818 23783 16902
 61668 61516  1735 50236 29268 22175 32192 19186 33598 25504 49321 22802
 51150 17172 17134  5634 18834 10485 20371 25058 50721  1441 43925 31556
 26491 36918 15765 62863 57872 53277 51824 42701 30872  8247 56959 32030
 46027 14935 34083 28931 29847 36177 27808 33248 41597 39376 25091 31798
 36705 46804 55151 12400 33041 46696 46871  7531 27366  5375 35575 54390
 21451 30426  4855 53328  5190 19500  1683 42836 17464 50856 34971 23317
 59937 58894 48457 16425 46397 16379 37054 56669 41526 61844  4849 19225
 20948 18541 45435 33513 60649 53646  1464 27010 34449 39383 13799 38274
 59571  7274 55794 40614 14013 53247 35950  8497 16946   422 18209 53259
 33609 18662 62254 48889 55331 25781 62158 51010   594 15822 54211 45911
 37872 51514 63205 38235 33699 33167 38868 21402 14274 27136 63107 61623
 48551 63495  2888 44172 27805  6881 39395 49755 62004 51537 34056 49969
 31523 24251 38466 14621   515 15761 34015 11217  3150 14766 30440  7820
 48013 54140 37355 34965 21912 63510 31599 47162 21304 23967 16753  2403
 42613  6567  3295 23108 44270 60426 35324 61432 21509 12551 61649 42333
 34645 14104 45293 22913 57985  2025 57709  5735 57664 51285 58462 24362
  2467 58358 31068 38375 47016 43450 51146 61208 18727 18938 43391 24615
 29569 26762 20131 18794 52099 58863 13755 28813  3051 35113 27137 47129
 35317 28631 47629 41209 54863 57379 55758 14153 62251 32121 30381 42916
 58248 41010  7302  1617 19749 25255 12261 13728 22524  5335 50037 54488
  2654  4231  9570  3567 26484 24050 37779 51288 32492 60238  6128 34842
 49897 35414 14799 30143 36912 42187 18777 39909 49499 25674 19489 33780
 63440 44980 13770 45157 17635 33622 22347  5174 56714 50625 32370 61232
 28422 60712 34610 42840 48083  4655  2455 65342   893  5200  5881 39170
 52903 33159  2479 40699 43941 38827 49854 30568  2784 63251 37077 17668
 21614 24726 12404 61521 27056 22236 13986  4547 38472 28575 40412  6704
  7082 37095 16135 37680 56754 34588 43753  7005 33871 17372 35651 50546
 47283 44224 27043 44967 64181 21230 23860   701  7996 26403 14475  2486
  4731 48100 43977 22137  9583 53833 64146 40976 33487 63537 45858 12056
 56059 62347 12524 18108 42110 42224 44086 53934 12451  6597 34007  5374
 15920 57656 19880 62502 16004 36981 31484  1462 41057 26673 36304 44176
 26906  1291 33669 62645  9818 30473  3641 17202   782 12701  9196  1078
 51533  7671 46698 18545 12820 58639 63646 25189  8591 13686 44767 28679
 55150 49804 24065 49549 51061  8195 12223 16731 19219 24501 27916 22070
 65401 48976 41300  8484 20533  1640 26567  9980 50959 21471 39872  3723
 29947 17188 42636 38897 52176 47108 55298 30391 61962 46163 16651 12906
 21518 38630  8749 56464 41418  5406 63598 48110 45811 39754 29077 22222
 29776  5074 15679 58065 53938 37253 62343 62163 40719  9498 46058 57519
 41234 45885 24098 18375 19744 42009 40538 16898 53368 17467 50624 59008
  5211 62675 58629 46556 37600 35825 15722  4697  1094  3789  1832 24826
 20122 63520 59713 57460 56755 34314 54672 51393 64813 28448 56548 21182
  2649 25990 57867 15553 23720]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([10506, 25842, 45314,  2077, 56357, 23027, 39119, 61347,  2779,
       21104]), array([10506, 25842, 45314,  2077, 56357, 23027, 39119, 61347,  2779,
       21104, 20107, 36120, 44205, 21364, 34103, 17564, 40779,  9286,
       19471, 31912])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [53269 56968 12105 ... 57867 15553 23720]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([53269, 56968, 12105, 51412, 36614, 10812, 28826,  7585, 31297,
       31809]), array([53269, 56968, 12105, 51412, 36614, 10812, 28826,  7585, 31297,
       31809, 40159, 13356, 49187, 26211, 45538,  6749, 31925, 43322,
       56757, 32266])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0        1.0                     1.0  ...          25                   1.0
1        1.0                     1.0  ...         200                   1.0
2        1.0                     1.0  ...         100                   1.0
3        1.0                     1.0  ...         200                   1.0
4        1.0                     1.0  ...         200                   1.0
5        1.0                     1.0  ...          50                   1.0
6        1.0                     1.0  ...         100                   1.0
7        1.0                     1.0  ...         100                   1.0
8        1.0                     1.0  ...          50                   1.0
9        1.0                     1.0  ...          50                   1.0
10       1.0                     1.0  ...          25                   1.0
11       1.0                     1.0  ...         100                   1.0
12       1.0                     1.0  ...          25                   1.0
13       1.0                     1.0  ...          25                   1.0
14       1.0                     1.0  ...         200                   1.0
15       1.0                     1.0  ...          50                   1.0

[16 rows x 10 columns]
