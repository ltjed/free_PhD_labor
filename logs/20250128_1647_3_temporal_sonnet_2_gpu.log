Using GPUs: [0, 1]
Using Anthropic API with model claude-3-5-sonnet-20241022.
Using OpenAI API with deepseek-reasoner.
ideas
Loaded existing ideas:
[{'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}, {'Name': 'hierarchical_sae', 'Title': 'Two-Level Hierarchical Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement unified two-level SAE architecture\n2. Train on WMDP-bio and WikiText datasets\n3. Compare three intervention strategies:\n   - Base-level clamping (baseline)\n   - Concept-level clamping\n   - Multi-level coordinated clamping\n4. Evaluate using standard metrics:\n   - WMDP-bio accuracy reduction\n   - MMLU preservation (>0.99)\n5. Analyze feature interactions between levels', 'Technical_Details': 'Architecture:\n- Base level: d_sae features, initialized with Xavier uniform\n- Concept level: d_sae/4 features, initialized near zero\n- Unified forward pass: z_1 = TopK(W_1*x + b_1, k=32), z_2 = TopK(W_2*z_1 + b_2, k=8)\n\nLoss function: L = L_rec + λ_1||z_1||_1 + λ_2||z_2||_1 + λ_h||W_2||_1\nwhere λ_1=0.1, λ_2=0.2, λ_h=0.01\n\nTraining:\n- Single phase optimization with Adam(lr=3e-4)\n- Gradient scaling: 0.1× for concept level in first 1000 steps\n- Batch size 2048, context length 128\n\nIntervention:\n- Compute feature importance scores using dual-dataset approach\n- Select top-k features at each level (k_1=16, k_2=4)\n- Apply coordinated negative clamping (-2.0 base, -1.0 concept)', 'Research_Impact': 'A key challenge in selective unlearning is maintaining model stability during interventions. Current approaches often cause cascading effects when clamping features, degrading performance on unrelated tasks. This research addresses the challenge through hierarchical feature organization and coordinated multi-level interventions. By carefully controlling the interaction between base and concept features during clamping, we can achieve more stable and targeted knowledge removal.', 'Implementation_Plan': '1. Create TwoLevelSAE extending CustomSAE\n2. Add TopK activation with different k per level\n3. Implement unified training in CustomTrainer\n4. Add feature importance calculation utilities\n5. Create intervention coordination module\n6. Update evaluation pipeline for multi-level analysis', 'Interestingness_Evaluation': 'The unified training approach with coordinated interventions provides an elegant solution to the stability-effectiveness trade-off in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The unified training eliminates complexity of sequential phases; fixed hyperparameters and simple TopK operations ensure efficient implementation; single forward pass with two levels stays well within 30-minute limit on H100; clear initialization and training procedure reduces development time.', 'Feasibility': 10, 'Novelty_Evaluation': 'While hierarchical models exist, the combination of unified training, coordinated interventions, and specific application to knowledge unlearning represents a novel contribution.', 'Novelty': 9, 'Overall_Score': 9.5, 'novel': True}]
Running 2 parallel processes
