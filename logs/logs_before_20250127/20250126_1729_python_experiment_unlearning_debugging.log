Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 605.5480346679688, 'l1_loss': 6244.248046875, 'l2_loss': 355.77813720703125}
Step 200: {'loss': 429.0907287597656, 'l1_loss': 2760.68212890625, 'l2_loss': 318.6634521484375}
Step 300: {'loss': 358.4563903808594, 'l1_loss': 1215.6123046875, 'l2_loss': 309.8319091796875}
Step 400: {'loss': 333.53631591796875, 'l1_loss': 803.9303588867188, 'l2_loss': 301.37908935546875}
Step 500: {'loss': 307.1258850097656, 'l1_loss': 876.6446533203125, 'l2_loss': 272.0600891113281}
Step 600: {'loss': 285.1235656738281, 'l1_loss': 905.6268920898438, 'l2_loss': 248.89849853515625}
Step 700: {'loss': 265.24444580078125, 'l1_loss': 892.3418579101562, 'l2_loss': 229.55078125}
Step 800: {'loss': 256.65167236328125, 'l1_loss': 874.7533569335938, 'l2_loss': 221.66152954101562}
Step 900: {'loss': 246.30035400390625, 'l1_loss': 713.0364990234375, 'l2_loss': 217.77890014648438}
Step 1000: {'loss': 244.66363525390625, 'l1_loss': 747.2108154296875, 'l2_loss': 214.77520751953125}
Step 1100: {'loss': 236.99093627929688, 'l1_loss': 617.1775512695312, 'l2_loss': 212.3038330078125}
Step 1200: {'loss': 229.92526245117188, 'l1_loss': 511.72625732421875, 'l2_loss': 209.45620727539062}
Step 1300: {'loss': 231.43133544921875, 'l1_loss': 554.423828125, 'l2_loss': 209.25437927246094}
Step 1400: {'loss': 227.81143188476562, 'l1_loss': 561.0849609375, 'l2_loss': 205.3680419921875}
Step 1500: {'loss': 225.99208068847656, 'l1_loss': 511.9103088378906, 'l2_loss': 205.5156707763672}
Step 1600: {'loss': 224.94239807128906, 'l1_loss': 513.8416748046875, 'l2_loss': 204.38873291015625}
Step 1700: {'loss': 222.3531494140625, 'l1_loss': 493.55487060546875, 'l2_loss': 202.6109619140625}
Step 1800: {'loss': 222.1446990966797, 'l1_loss': 523.8516845703125, 'l2_loss': 201.1906280517578}
Step 1900: {'loss': 220.44122314453125, 'l1_loss': 507.92169189453125, 'l2_loss': 200.12435913085938}
Step 2000: {'loss': 219.18801879882812, 'l1_loss': 510.1575927734375, 'l2_loss': 198.78172302246094}
Step 2100: {'loss': 219.13693237304688, 'l1_loss': 495.57489013671875, 'l2_loss': 199.31393432617188}
Step 2200: {'loss': 214.81642150878906, 'l1_loss': 456.8924255371094, 'l2_loss': 196.5407257080078}
Step 2300: {'loss': 215.5642852783203, 'l1_loss': 514.3161010742188, 'l2_loss': 194.99163818359375}
Step 2400: {'loss': 215.71888732910156, 'l1_loss': 508.0993957519531, 'l2_loss': 195.39491271972656}
Step 2500: {'loss': 213.24505615234375, 'l1_loss': 520.8946533203125, 'l2_loss': 192.40927124023438}
Step 2600: {'loss': 213.49964904785156, 'l1_loss': 552.4005126953125, 'l2_loss': 191.40362548828125}
Step 2700: {'loss': 212.8905792236328, 'l1_loss': 523.7506103515625, 'l2_loss': 191.9405517578125}
Step 2800: {'loss': 211.572265625, 'l1_loss': 502.1622619628906, 'l2_loss': 191.48577880859375}
Step 2900: {'loss': 209.38613891601562, 'l1_loss': 486.47296142578125, 'l2_loss': 189.92721557617188}
Step 3000: {'loss': 211.12387084960938, 'l1_loss': 530.3363647460938, 'l2_loss': 189.91041564941406}
Step 3100: {'loss': 208.66983032226562, 'l1_loss': 520.5751342773438, 'l2_loss': 187.84683227539062}
Step 3200: {'loss': 208.5030975341797, 'l1_loss': 498.684326171875, 'l2_loss': 188.55572509765625}
Step 3300: {'loss': 207.44886779785156, 'l1_loss': 500.3273620605469, 'l2_loss': 187.43577575683594}
Step 3400: {'loss': 208.40890502929688, 'l1_loss': 528.6917724609375, 'l2_loss': 187.26123046875}
Step 3500: {'loss': 204.75234985351562, 'l1_loss': 493.5687255859375, 'l2_loss': 185.0095977783203}
Step 3600: {'loss': 207.27964782714844, 'l1_loss': 522.7098999023438, 'l2_loss': 186.37124633789062}
Step 3700: {'loss': 205.98080444335938, 'l1_loss': 506.7685852050781, 'l2_loss': 185.71006774902344}
Step 3800: {'loss': 204.1490478515625, 'l1_loss': 518.6546020507812, 'l2_loss': 183.40286254882812}
Step 3900: {'loss': 204.56619262695312, 'l1_loss': 510.291259765625, 'l2_loss': 184.154541015625}
Step 4000: {'loss': 203.77603149414062, 'l1_loss': 494.03515625, 'l2_loss': 184.01461791992188}
Step 4100: {'loss': 203.6402587890625, 'l1_loss': 494.53143310546875, 'l2_loss': 183.85899353027344}
Step 4200: {'loss': 203.35235595703125, 'l1_loss': 519.930908203125, 'l2_loss': 182.55511474609375}
Step 4300: {'loss': 203.46624755859375, 'l1_loss': 514.32421875, 'l2_loss': 182.89328002929688}
Step 4400: {'loss': 201.53680419921875, 'l1_loss': 502.9695739746094, 'l2_loss': 181.41802978515625}
Step 4500: {'loss': 202.3533477783203, 'l1_loss': 511.13446044921875, 'l2_loss': 181.90797424316406}
Step 4600: {'loss': 201.53875732421875, 'l1_loss': 510.26593017578125, 'l2_loss': 181.1281280517578}
Step 4700: {'loss': 200.69119262695312, 'l1_loss': 528.0531005859375, 'l2_loss': 179.56906127929688}
Step 4800: {'loss': 200.93646240234375, 'l1_loss': 514.9027099609375, 'l2_loss': 180.34036254882812}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/templates/sae_variants/run_0/final_info.json

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Sparsity calculation for google/gemma-2-2b_layer_19_sae_custom_sae is already done
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
df[wmdp-bio]:/n
0     1.0
1     1.0
2     1.0
3     1.0
4     1.0
5     1.0
6     1.0
7     1.0
8     1.0
9     1.0
10    1.0
11    1.0
12    1.0
13    1.0
14    1.0
15    1.0
Name: wmdp-bio, dtype: float64
df[all_side_effects_mcq] is:/n
0     1.0
1     1.0
2     1.0
3     1.0
4     1.0
5     1.0
6     1.0
7     1.0
8     1.0
9     1.0
10    1.0
11    1.0
12    1.0
13    1.0
14    1.0
15    1.0
Name: all_side_effects_mcq, dtype: float64
