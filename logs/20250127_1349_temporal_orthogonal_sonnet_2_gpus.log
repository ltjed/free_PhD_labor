Using GPUs: [0, 1]
Using Anthropic API with model claude-3-5-sonnet-20241022.

Generating idea 1/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea focuses on orthogonality between features, but I believe we can refine this approach by introducing a more targeted form of feature separation - nested feature hierarchies. Rather than enforcing orthogonality across all features, we can organize them into nested subsets where each level captures increasingly specific concepts while maintaining interpretability.

The key insight is that many concepts in language models naturally form hierarchies (e.g., "animal" -> "mammal" -> "dog" -> "labrador"). By explicitly modeling these relationships in the SAE architecture through nested reconstruction objectives, we can better disentangle features while maintaining semantic relationships. This builds directly on the orthogonality concept from the prototype but provides more structure.

The implementation primarily involves modifying the loss function to include multiple reconstruction targets at different feature subset sizes, similar to how the brain processes information at different levels of abstraction. This is simpler than the prototype's adaptive orthogonality as it doesn't require complex scheduling or group-specific components.

For mechanistic interpretability, nested features should improve performance because:
1. It naturally combats feature absorption by encouraging clear hierarchical relationships
2. Features at different levels can specialize for different levels of abstraction
3. The hierarchy provides an interpretable structure for understanding feature relationships

NEW IDEA JSON:
```json
{
    "Name": "nested_feature_sae",
    "Title": "Nested Feature Learning for Hierarchical Knowledge Organization in Sparse Autoencoders",
    "Experiment": "1. Implement nested reconstruction loss with feature subsets\n2. Train model on standard datasets using different subset sizes\n3. Evaluate feature interpretability across hierarchy levels\n4. Compare against baseline SAE on standard benchmarks\n5. Analyze feature activation patterns across hierarchy levels",
    "Technical_Details": "The method uses a modified loss function: L = Σ_i (w_i * L_recon_i) + λ * L_sparse where L_recon_i is the reconstruction loss using the first k_i features (k_1 < k_2 < ... < k_n = d_sae). Each reconstruction target uses the same decoder weights but different feature subsets, encouraging a natural hierarchy where early features capture broad concepts and later features add specificity. The w_i weights balance the importance of different reconstruction targets.",
    "Implementation_Plan": "1. Modify CustomSAE forward pass to track multiple reconstructions\n2. Add nested reconstruction loss computation\n3. Update training loop to handle multiple loss components\n4. Add feature subset selection logic\n5. Implement evaluation metrics for hierarchical feature analysis",
    "Interestingness_Evaluation": "The hierarchical organization of features provides a natural and interpretable way to understand how the model processes information at different levels of abstraction.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes to existing architecture, mainly focusing on loss computation and feature selection; training time remains similar to baseline as all reconstructions share same forward pass; no complex scheduling or grouping required.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While hierarchical representations exist in other contexts, applying them to SAE feature learning for improved interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical structure should directly improve feature interpretability by providing clear relationships between features at different abstraction levels, benefiting both sparse probing and core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders through nested feature learning. Our method organizes features into a natural hierarchy by training with multiple reconstruction targets using nested feature subsets. This approach encourages the model to learn features at different levels of abstraction while maintaining clear relationships between them. Unlike previous methods that treat all features equally or enforce strict orthogonality, our approach recognizes and exploits the inherent hierarchical nature of concepts in language models. We introduce a modified loss function that balances reconstruction quality across different feature subset sizes while maintaining sparsity. This work provides a framework for understanding how sparse autoencoders organize knowledge at different levels of abstraction, potentially offering new insights into the internal representations of language models."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'nested_feature_sae', 'Title': 'Nested Feature Learning for Hierarchical Knowledge Organization in Sparse Autoencoders', 'Experiment': '1. Implement nested reconstruction loss with feature subsets\n2. Train model on standard datasets using different subset sizes\n3. Evaluate feature interpretability across hierarchy levels\n4. Compare against baseline SAE on standard benchmarks\n5. Analyze feature activation patterns across hierarchy levels', 'Technical_Details': 'The method uses a modified loss function: L = Σ_i (w_i * L_recon_i) + λ * L_sparse where L_recon_i is the reconstruction loss using the first k_i features (k_1 < k_2 < ... < k_n = d_sae). Each reconstruction target uses the same decoder weights but different feature subsets, encouraging a natural hierarchy where early features capture broad concepts and later features add specificity. The w_i weights balance the importance of different reconstruction targets.', 'Implementation_Plan': '1. Modify CustomSAE forward pass to track multiple reconstructions\n2. Add nested reconstruction loss computation\n3. Update training loop to handle multiple loss components\n4. Add feature subset selection logic\n5. Implement evaluation metrics for hierarchical feature analysis', 'Interestingness_Evaluation': 'The hierarchical organization of features provides a natural and interpretable way to understand how the model processes information at different levels of abstraction.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes to existing architecture, mainly focusing on loss computation and feature selection; training time remains similar to baseline as all reconstructions share same forward pass; no complex scheduling or grouping required.', 'Feasibility': 9, 'Novelty_Evaluation': 'While hierarchical representations exist in other contexts, applying them to SAE feature learning for improved interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical structure should directly improve feature interpretability by providing clear relationships between features at different abstraction levels, benefiting both sparse probing and core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a novel approach to improving the interpretability of sparse autoencoders through nested feature learning. Our method organizes features into a natural hierarchy by training with multiple reconstruction targets using nested feature subsets. This approach encourages the model to learn features at different levels of abstraction while maintaining clear relationships between them. Unlike previous methods that treat all features equally or enforce strict orthogonality, our approach recognizes and exploits the inherent hierarchical nature of concepts in language models. We introduce a modified loss function that balances reconstruction quality across different feature subset sizes while maintaining sparsity. This work provides a framework for understanding how sparse autoencoders organize knowledge at different levels of abstraction, potentially offering new insights into the internal representations of language models.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the nested feature idea is interesting, I believe I can make it more focused and feasible while maintaining its core benefits. The previous version had too many moving parts with multiple reconstruction targets, which could complicate training and potentially impact performance.

Key improvements for this iteration:
1. Simplify to just two levels (general and specific features) rather than arbitrary nested levels
2. Use a fixed split point rather than multiple weighted reconstructions
3. More explicit connection to the original orthogonality concept from the prototype

The refined approach divides the feature space into two parts: base features that capture fundamental concepts and refinement features that add specificity. This maintains the hierarchical benefits while being much simpler to implement and tune. It's also more closely aligned with how humans understand concepts (basic categories vs. specific details).

For mechanistic interpretability, this two-level approach should be even more effective because:
1. Clear separation between basic and refined features makes interpretation more straightforward
2. The fixed split point provides a natural way to analyze feature importance
3. Simpler training dynamics should lead to more stable feature learning

NEW IDEA JSON:
```json
{
    "Name": "two_level_sae",
    "Title": "Two-Level Feature Organization for Improved Interpretability in Sparse Autoencoders",
    "Experiment": "1. Implement two-level reconstruction loss\n2. Add feature split mechanism with fixed division point\n3. Train model with different split ratios\n4. Compare feature interpretability between levels\n5. Analyze activation patterns across base and refinement features",
    "Technical_Details": "The method splits the feature space into two parts: base features (first k neurons) and refinement features (remaining d_sae-k neurons). The loss function becomes L = L_recon + λ_1 * L_sparse + λ_2 * L_base where L_base is the reconstruction loss using only base features. This encourages base features to capture fundamental concepts while refinement features add specificity. The split point k is a hyperparameter, typically set to d_sae/4.",
    "Implementation_Plan": "1. Modify CustomSAE to track base feature activations\n2. Add base reconstruction loss computation\n3. Update training loop with combined loss\n4. Add split point configuration\n5. Implement evaluation metrics for two-level analysis",
    "Interestingness_Evaluation": "The explicit separation into base and refinement features provides a clear and intuitive way to analyze model knowledge organization.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires minimal changes to existing architecture; only adds one additional loss term; training time nearly identical to baseline; simple hyperparameter tuning with just split point and loss weight.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While feature hierarchies exist in other contexts, the specific two-level organization for SAE interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The clear separation between base and refinement features should significantly improve interpretability metrics by providing a natural way to analyze feature importance and relationships.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders through a two-level feature organization scheme. Our method explicitly divides the feature space into base features that capture fundamental concepts and refinement features that add specificity. This is achieved through a modified loss function that encourages base features to learn complete but general representations while allowing refinement features to capture more specific aspects. Unlike previous approaches that treat all features equally, our method provides a natural way to analyze how knowledge is organized at different levels of abstraction. The approach introduces minimal computational overhead while providing a clear framework for understanding feature relationships and importance hierarchies in the context of mechanistic interpretability."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'two_level_sae', 'Title': 'Two-Level Feature Organization for Improved Interpretability in Sparse Autoencoders', 'Experiment': '1. Implement two-level reconstruction loss\n2. Add feature split mechanism with fixed division point\n3. Train model with different split ratios\n4. Compare feature interpretability between levels\n5. Analyze activation patterns across base and refinement features', 'Technical_Details': 'The method splits the feature space into two parts: base features (first k neurons) and refinement features (remaining d_sae-k neurons). The loss function becomes L = L_recon + λ_1 * L_sparse + λ_2 * L_base where L_base is the reconstruction loss using only base features. This encourages base features to capture fundamental concepts while refinement features add specificity. The split point k is a hyperparameter, typically set to d_sae/4.', 'Implementation_Plan': '1. Modify CustomSAE to track base feature activations\n2. Add base reconstruction loss computation\n3. Update training loop with combined loss\n4. Add split point configuration\n5. Implement evaluation metrics for two-level analysis', 'Interestingness_Evaluation': 'The explicit separation into base and refinement features provides a clear and intuitive way to analyze model knowledge organization.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires minimal changes to existing architecture; only adds one additional loss term; training time nearly identical to baseline; simple hyperparameter tuning with just split point and loss weight.', 'Feasibility': 9, 'Novelty_Evaluation': 'While feature hierarchies exist in other contexts, the specific two-level organization for SAE interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The clear separation between base and refinement features should significantly improve interpretability metrics by providing a natural way to analyze feature importance and relationships.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'We present a novel approach to improving the interpretability of sparse autoencoders through a two-level feature organization scheme. Our method explicitly divides the feature space into base features that capture fundamental concepts and refinement features that add specificity. This is achieved through a modified loss function that encourages base features to learn complete but general representations while allowing refinement features to capture more specific aspects. Unlike previous approaches that treat all features equally, our method provides a natural way to analyze how knowledge is organized at different levels of abstraction. The approach introduces minimal computational overhead while providing a clear framework for understanding feature relationships and importance hierarchies in the context of mechanistic interpretability.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After reviewing the two-level approach, I realize we can make it even more focused and effective by specifically targeting feature absorption - a key challenge in SAE interpretability. Instead of a general base/refinement split, we can explicitly design the two levels to combat feature absorption.

The key insight is that feature absorption often happens when one feature implies another (like "short" implying "starts with S"). By structuring our two levels specifically around this phenomenon, with the base level forced to learn atomic features and the refinement level learning combinations, we can directly address this challenge.

Improvements in this iteration:
1. More focused purpose for the two levels (atomic vs. composite features)
2. Clearer connection to feature absorption problem
3. Simpler implementation by removing the need for a configurable split point

For mechanistic interpretability, this absorption-focused approach should be more effective because:
1. Directly addresses a key challenge in feature interpretability
2. Creates cleaner separation between atomic and composite concepts
3. Makes feature relationships more explicit and analyzable

NEW IDEA JSON:
```json
{
    "Name": "absorption_resistant_sae",
    "Title": "Absorption-Resistant Feature Learning in Sparse Autoencoders",
    "Experiment": "1. Implement atomic-composite feature separation\n2. Add atomic feature reconstruction loss\n3. Train model with focus on atomic feature preservation\n4. Evaluate feature absorption metrics\n5. Analyze atomic vs composite feature patterns",
    "Technical_Details": "The method divides features into atomic and composite sets, with the first d_sae/3 features designated as atomic. The loss function is L = L_recon + λ_1 * L_sparse + λ_2 * L_atomic where L_atomic = ||x_atomic - D(E_atomic(x))||_2 encourages atomic features to capture basic concepts that resist absorption. E_atomic uses only atomic feature encoders, while the full encoder can learn combinations. A higher λ_2 weight (typically 0.5) ensures atomic features remain distinct.",
    "Implementation_Plan": "1. Modify CustomSAE to separate atomic features\n2. Add atomic reconstruction loss\n3. Update training loop with atomic loss term\n4. Implement atomic feature selection\n5. Add absorption resistance metrics",
    "Interestingness_Evaluation": "The direct targeting of feature absorption through atomic-composite separation provides a focused solution to a fundamental interpretability challenge.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only minor modifications to existing architecture; single additional loss term; fixed feature division removes need for tuning split point; training time comparable to baseline.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While feature absorption has been studied, explicitly structuring the architecture to resist it through atomic-composite separation is novel.",
    "Novelty": 8,
    "Expected_Research_Impact": "The explicit resistance to feature absorption should significantly improve interpretability by maintaining cleaner, more atomic features.",
    "Research_Impact": 9,
    "Overall_Score": 8.7,
    "Abstract": "We present an approach to improving sparse autoencoder interpretability by explicitly addressing the feature absorption problem. Our method divides the feature space into atomic and composite features, using a specialized loss function to ensure atomic features maintain their independence and resist absorption into composite concepts. This separation is achieved through a modified architecture that forces certain features to learn basic, non-decomposable concepts while allowing others to capture more complex combinations. Unlike previous approaches that treat feature absorption as a side effect to be minimized, our method directly structures the learning process to prevent it. The approach maintains computational efficiency while providing a framework specifically designed to improve feature interpretability through resistance to absorption."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'absorption_resistant_sae', 'Title': 'Absorption-Resistant Feature Learning in Sparse Autoencoders', 'Experiment': '1. Implement atomic-composite feature separation\n2. Add atomic feature reconstruction loss\n3. Train model with focus on atomic feature preservation\n4. Evaluate feature absorption metrics\n5. Analyze atomic vs composite feature patterns', 'Technical_Details': 'The method divides features into atomic and composite sets, with the first d_sae/3 features designated as atomic. The loss function is L = L_recon + λ_1 * L_sparse + λ_2 * L_atomic where L_atomic = ||x_atomic - D(E_atomic(x))||_2 encourages atomic features to capture basic concepts that resist absorption. E_atomic uses only atomic feature encoders, while the full encoder can learn combinations. A higher λ_2 weight (typically 0.5) ensures atomic features remain distinct.', 'Implementation_Plan': '1. Modify CustomSAE to separate atomic features\n2. Add atomic reconstruction loss\n3. Update training loop with atomic loss term\n4. Implement atomic feature selection\n5. Add absorption resistance metrics', 'Interestingness_Evaluation': 'The direct targeting of feature absorption through atomic-composite separation provides a focused solution to a fundamental interpretability challenge.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only minor modifications to existing architecture; single additional loss term; fixed feature division removes need for tuning split point; training time comparable to baseline.', 'Feasibility': 9, 'Novelty_Evaluation': 'While feature absorption has been studied, explicitly structuring the architecture to resist it through atomic-composite separation is novel.', 'Novelty': 8, 'Expected_Research_Impact': 'The explicit resistance to feature absorption should significantly improve interpretability by maintaining cleaner, more atomic features.', 'Research_Impact': 9, 'Overall_Score': 8.7, 'Abstract': 'We present an approach to improving sparse autoencoder interpretability by explicitly addressing the feature absorption problem. Our method divides the feature space into atomic and composite features, using a specialized loss function to ensure atomic features maintain their independence and resist absorption into composite concepts. This separation is achieved through a modified architecture that forces certain features to learn basic, non-decomposable concepts while allowing others to capture more complex combinations. Unlike previous approaches that treat feature absorption as a side effect to be minimized, our method directly structures the learning process to prevent it. The approach maintains computational efficiency while providing a framework specifically designed to improve feature interpretability through resistance to absorption.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, I believe the previous idea's scores were too optimistic, particularly regarding novelty and research impact. While the atomic-composite separation is interesting, it still adds complexity that might not be necessary. We can make the approach even more focused and practical by simplifying it further.

The key insight remains the same - preventing feature absorption - but we can achieve this with an even simpler mechanism: using a progressive sparsity penalty that varies based on feature index. This maintains the spirit of separating basic and complex features while eliminating the need for separate atomic reconstruction loss.

Key improvements:
1. Remove separate atomic reconstruction loss in favor of indexed sparsity penalties
2. Simpler implementation with just modification to existing L1 penalty
3. More direct connection to standard SAE training

For mechanistic interpretability, this approach should be more effective because:
1. Progressive sparsity naturally encourages separation of features
2. Simpler training dynamics lead to more stable feature learning
3. Maintains interpretability without adding architectural complexity

The previous overall score of 8.7 was too high given the added complexity. This simplified version should be more feasible while maintaining the core benefits.

NEW IDEA JSON:
```json
{
    "Name": "progressive_sparsity_sae",
    "Title": "Progressive Sparsity for Feature Absorption Resistance in Sparse Autoencoders",
    "Experiment": "1. Implement progressive sparsity penalty\n2. Train model with varying sparsity schedules\n3. Evaluate feature absorption metrics\n4. Compare feature interpretability across indices\n5. Analyze activation patterns by feature position",
    "Technical_Details": "The method uses a modified sparsity penalty: L = L_recon + Σ_i (λ_1 * (1 + αi/d_sae) * |f_i|) where f_i is the i-th feature activation and α controls the progression rate (typically 0.5). Early features receive lower sparsity penalties, encouraging them to capture basic concepts, while later features face stronger penalties, naturally preventing absorption. This creates a smooth progression of feature specificity without requiring explicit separation.",
    "Implementation_Plan": "1. Modify CustomSAE L1 penalty calculation\n2. Add progressive scaling factor\n3. Update training loop with modified loss\n4. Add progression rate parameter\n5. Implement evaluation metrics",
    "Interestingness_Evaluation": "The use of progressive sparsity provides an elegant and simple solution to feature absorption while maintaining interpretability.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only modification to existing L1 penalty calculation; no additional loss terms or architectural changes; training time identical to baseline; single new hyperparameter.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While varying penalties have been used in other contexts, applying progressive sparsity specifically to combat feature absorption is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The simple yet effective approach to preventing feature absorption should improve interpretability metrics while maintaining practical applicability.",
    "Research_Impact": 8,
    "Overall_Score": 8.1,
    "Abstract": "We present a simple yet effective approach to improving sparse autoencoder interpretability through progressive sparsity penalties. Our method modifies the standard L1 penalty to vary based on feature index, creating a natural progression from basic to complex features without requiring architectural changes. This progressive penalty encourages earlier features to capture fundamental concepts while later features face stronger regularization, naturally preventing feature absorption. Unlike previous approaches that require explicit feature separation or complex loss terms, our method achieves similar goals through a minimal modification to the standard training objective. The approach maintains the computational efficiency of traditional sparse autoencoders while providing a natural mechanism for organizing features by complexity and preventing problematic feature interactions."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'progressive_sparsity_sae', 'Title': 'Progressive Sparsity for Feature Absorption Resistance in Sparse Autoencoders', 'Experiment': '1. Implement progressive sparsity penalty\n2. Train model with varying sparsity schedules\n3. Evaluate feature absorption metrics\n4. Compare feature interpretability across indices\n5. Analyze activation patterns by feature position', 'Technical_Details': 'The method uses a modified sparsity penalty: L = L_recon + Σ_i (λ_1 * (1 + αi/d_sae) * |f_i|) where f_i is the i-th feature activation and α controls the progression rate (typically 0.5). Early features receive lower sparsity penalties, encouraging them to capture basic concepts, while later features face stronger penalties, naturally preventing absorption. This creates a smooth progression of feature specificity without requiring explicit separation.', 'Implementation_Plan': '1. Modify CustomSAE L1 penalty calculation\n2. Add progressive scaling factor\n3. Update training loop with modified loss\n4. Add progression rate parameter\n5. Implement evaluation metrics', 'Interestingness_Evaluation': 'The use of progressive sparsity provides an elegant and simple solution to feature absorption while maintaining interpretability.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only modification to existing L1 penalty calculation; no additional loss terms or architectural changes; training time identical to baseline; single new hyperparameter.', 'Feasibility': 9, 'Novelty_Evaluation': 'While varying penalties have been used in other contexts, applying progressive sparsity specifically to combat feature absorption is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The simple yet effective approach to preventing feature absorption should improve interpretability metrics while maintaining practical applicability.', 'Research_Impact': 8, 'Overall_Score': 8.1, 'Abstract': 'We present a simple yet effective approach to improving sparse autoencoder interpretability through progressive sparsity penalties. Our method modifies the standard L1 penalty to vary based on feature index, creating a natural progression from basic to complex features without requiring architectural changes. This progressive penalty encourages earlier features to capture fundamental concepts while later features face stronger regularization, naturally preventing feature absorption. Unlike previous approaches that require explicit feature separation or complex loss terms, our method achieves similar goals through a minimal modification to the standard training objective. The approach maintains the computational efficiency of traditional sparse autoencoders while providing a natural mechanism for organizing features by complexity and preventing problematic feature interactions.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The progressive sparsity idea is on the right track, but we can make it even more focused and effective. Instead of a linear progression based on feature index, which might be too rigid, we can use activation frequency to dynamically adjust sparsity penalties. This maintains the simplicity while being more adaptive to actual feature usage patterns.

Key refinements:
1. Replace index-based progression with frequency-based scaling
2. Use moving average of activation frequencies for stability
3. More direct connection to feature absorption mechanism

The key insight is that frequently activating features are more likely to absorb others, so they should face stronger sparsity penalties. This creates a natural self-balancing mechanism that prevents any feature from becoming too dominant while maintaining interpretability.

For mechanistic interpretability, this approach should be more effective because:
1. Directly targets features that show absorption-like behavior
2. Adapts to actual usage patterns rather than fixed positions
3. Maintains simplicity while being more theoretically grounded

The previous overall score of 8.1 was reasonable, but this refinement should be even more effective while maintaining the same level of simplicity.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparsity_sae",
    "Title": "Frequency-Adaptive Sparsity for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement activation frequency tracking\n2. Add frequency-based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature distribution metrics\n5. Analyze relationship between activation frequency and feature interpretability",
    "Technical_Details": "The method scales sparsity penalties based on activation frequency: L = L_recon + Σ_i (λ_1 * (1 + β*f_avg_i) * |f_i|) where f_avg_i is the exponential moving average of feature i's activation frequency and β controls adaptation strength (typically 1.0). Frequencies are updated each batch: f_avg_i = 0.99 * f_avg_i + 0.01 * (f_i != 0). This creates natural pressure against feature absorption by penalizing overused features.",
    "Implementation_Plan": "1. Add activation frequency tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with frequency updates\n4. Add adaptation rate parameter\n5. Implement frequency analysis metrics",
    "Interestingness_Evaluation": "The use of activation frequency for adaptive sparsity provides a natural and theoretically grounded approach to preventing feature absorption.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple frequency tracking and modified L1 calculation; minimal overhead from moving average updates; training time nearly identical to baseline; single new hyperparameter.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While activation statistics have been used in neural networks, applying them specifically to prevent feature absorption in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The adaptive approach should significantly improve feature distribution and interpretability by naturally preventing any feature from becoming too dominant.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present an adaptive approach to improving sparse autoencoder interpretability through frequency-based sparsity penalties. Our method modifies the standard L1 penalty to scale with each feature's activation frequency, creating a natural mechanism for preventing feature absorption without architectural changes. This adaptive penalty uses exponential moving averages of activation frequencies to dynamically adjust regularization strength, preventing features from becoming too dominant while maintaining their interpretability. Unlike previous approaches that use fixed penalties or complex architectural modifications, our method achieves similar goals through a simple, theoretically motivated adaptation mechanism. The approach maintains the computational efficiency of traditional sparse autoencoders while providing a natural way to balance feature usage and prevent problematic feature interactions."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparsity_sae', 'Title': 'Frequency-Adaptive Sparsity for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Add frequency-based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature distribution metrics\n5. Analyze relationship between activation frequency and feature interpretability', 'Technical_Details': "The method scales sparsity penalties based on activation frequency: L = L_recon + Σ_i (λ_1 * (1 + β*f_avg_i) * |f_i|) where f_avg_i is the exponential moving average of feature i's activation frequency and β controls adaptation strength (typically 1.0). Frequencies are updated each batch: f_avg_i = 0.99 * f_avg_i + 0.01 * (f_i != 0). This creates natural pressure against feature absorption by penalizing overused features.", 'Implementation_Plan': '1. Add activation frequency tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with frequency updates\n4. Add adaptation rate parameter\n5. Implement frequency analysis metrics', 'Interestingness_Evaluation': 'The use of activation frequency for adaptive sparsity provides a natural and theoretically grounded approach to preventing feature absorption.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple frequency tracking and modified L1 calculation; minimal overhead from moving average updates; training time nearly identical to baseline; single new hyperparameter.', 'Feasibility': 9, 'Novelty_Evaluation': 'While activation statistics have been used in neural networks, applying them specifically to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The adaptive approach should significantly improve feature distribution and interpretability by naturally preventing any feature from becoming too dominant.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': "We present an adaptive approach to improving sparse autoencoder interpretability through frequency-based sparsity penalties. Our method modifies the standard L1 penalty to scale with each feature's activation frequency, creating a natural mechanism for preventing feature absorption without architectural changes. This adaptive penalty uses exponential moving averages of activation frequencies to dynamically adjust regularization strength, preventing features from becoming too dominant while maintaining their interpretability. Unlike previous approaches that use fixed penalties or complex architectural modifications, our method achieves similar goals through a simple, theoretically motivated adaptation mechanism. The approach maintains the computational efficiency of traditional sparse autoencoders while providing a natural way to balance feature usage and prevent problematic feature interactions."}
Iteration 6/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The frequency-adaptive approach is strong, but I realize we can make it even more targeted and efficient. Instead of tracking general activation frequencies, we can specifically focus on co-activation patterns, which are a more direct indicator of potential feature absorption.

The core refinement is to track pairwise co-activations and increase sparsity penalties for features that frequently co-activate with many others. This provides a more precise mechanism for preventing feature absorption while maintaining the simplicity of the previous approach.

For mechanistic interpretability, this refinement should be more effective because:
1. Directly targets the feature absorption mechanism through co-activation patterns
2. Maintains computational efficiency by using simple statistics
3. Provides more interpretable results by explicitly tracking feature relationships

However, I notice that the previous overall score of 8.3 was slightly optimistic given the complexity of tracking frequencies. This more focused version should maintain similar effectiveness while being even more directly tied to the feature absorption problem.

NEW IDEA JSON:
```json
{
    "Name": "coactivation_sae",
    "Title": "Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns",
    "Technical_Details": "The method scales sparsity penalties based on co-activation patterns: L = L_recon + Σ_i (λ_1 * (1 + γ*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. γ controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j≠i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.",
    "Implementation_Plan": "1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics",
    "Interestingness_Evaluation": "The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'coactivation_sae', 'Title': 'Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns', 'Technical_Details': 'The method scales sparsity penalties based on co-activation patterns: L = L_recon + Σ_i (λ_1 * (1 + γ*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. γ controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j≠i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.', 'Implementation_Plan': '1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics', 'Interestingness_Evaluation': 'The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.', 'Feasibility': 8, 'Novelty_Evaluation': 'While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns."}
Iteration 7/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe we've reached an optimal balance with the co-activation approach. The method directly addresses feature absorption through a simple, measurable mechanism while maintaining computational efficiency. The previous iteration's score of 7.9 accurately reflects its balance of novelty, feasibility, and potential impact.

While we could potentially add more complexity (like weighted co-activation patterns or temporal tracking), this would likely detract from the method's key strengths:
1. Direct connection to feature absorption mechanism
2. Simple implementation with minimal overhead
3. Clear interpretability of results

Any further modifications would risk either:
- Increasing computational complexity without proportional benefits
- Making the approach less interpretable
- Deviating too far from the original SAE framework

The current version represents a sweet spot between effectiveness and simplicity, making it an ideal candidate for implementation and evaluation.

I am done.

```json
{
    "Name": "coactivation_sae",
    "Title": "Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns",
    "Technical_Details": "The method scales sparsity penalties based on co-activation patterns: L = L_recon + Σ_i (λ_1 * (1 + γ*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. γ controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j≠i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.",
    "Implementation_Plan": "1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics",
    "Interestingness_Evaluation": "The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'coactivation_sae', 'Title': 'Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns', 'Technical_Details': 'The method scales sparsity penalties based on co-activation patterns: L = L_recon + Σ_i (λ_1 * (1 + γ*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. γ controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j≠i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.', 'Implementation_Plan': '1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics', 'Interestingness_Evaluation': 'The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.', 'Feasibility': 8, 'Novelty_Evaluation': 'While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns."}
Idea generation converged after 7 iterations.

Generating idea 2/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints between features, but I believe we can refine this further by focusing specifically on feature absorption - a key challenge identified in the benchmark. Instead of enforcing global orthogonality, which may be too strict and hurt reconstruction, we can introduce a targeted approach that specifically prevents feature absorption while allowing beneficial feature sharing.

The key insight is to modify the SAE's loss function to penalize feature absorption patterns without constraining general feature interactions. When feature A implies feature B (like "short" implies "starts with S"), the model tends to absorb B into A for sparsity. We can detect and penalize these absorption patterns by looking at the co-activation statistics between features.

This builds naturally on the prototype's orthogonality idea but makes it more targeted and interpretable. Rather than enforcing mathematical orthogonality, we directly address the mechanistic interpretability goal of preventing feature absorption. The implementation remains simple - we just need to track co-activation statistics and add a targeted penalty term.

NEW IDEA JSON:
```json
{
    "Name": "targeted_absorption_sae",
    "Title": "Targeted Prevention of Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement co-activation tracking between SAE features\n2. Add targeted penalty term to loss function based on co-activation patterns\n3. Train on standard datasets with varying penalty strengths\n4. Evaluate feature absorption using first-letter classification task\n5. Compare interpretability metrics against baseline SAE",
    "Technical_Details": "The method extends the standard SAE loss with an absorption penalty term: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. L_absorb is computed by maintaining an exponential moving average of pairwise feature co-activation probabilities P(f_i|f_j). When P(f_i|f_j) exceeds a threshold τ (indicating potential absorption), we add a penalty proportional to the conditional probability. The penalty encourages the model to separate commonly co-occurring features into distinct representations rather than absorbing one into the other. The EMA decay rate α and threshold τ are hyperparameters controlling the sensitivity to absorption patterns.",
    "Implementation_Plan": "1. Add CoActivationTracker class to maintain EMA statistics\n2. Modify CustomSAE forward pass to track co-activations\n3. Add absorption penalty calculation to loss function\n4. Add hyperparameters for EMA decay and threshold\n5. Implement evaluation metrics for feature absorption",
    "Interestingness_Evaluation": "The targeted approach to preventing feature absorption provides a direct solution to a core interpretability challenge while maintaining the simplicity of the base SAE architecture.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "The implementation requires only simple statistics tracking and an additional loss term; co-activation computation can be done efficiently in batches; all core SAE functionality remains unchanged; experiments will complete within 30 minutes on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While feature absorption has been identified as an issue, this is the first direct attempt to prevent it through targeted loss terms based on co-activation patterns.",
    "Novelty": 8,
    "Expected_Research_Impact": "By directly targeting feature absorption, the method should significantly improve on the absorption benchmark while maintaining or improving core metrics by allowing beneficial feature interactions.",
    "Research_Impact": 8,
    "Overall_Score": 8.5,
    "Abstract": "Feature absorption in sparse autoencoders poses a significant challenge for mechanistic interpretability, where distinct concepts become entangled in individual features. We propose a targeted approach to prevent feature absorption by tracking and penalizing suspicious co-activation patterns between features. Unlike previous methods that enforce strict constraints on feature interactions, our method selectively discourages only those patterns indicative of absorption while allowing beneficial feature sharing. We introduce an efficient co-activation tracking mechanism and a targeted penalty term that can be seamlessly integrated into existing SAE architectures. The method maintains the simplicity and efficiency of standard SAEs while directly addressing a core challenge in achieving interpretable representations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'targeted_absorption_sae', 'Title': 'Targeted Prevention of Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement co-activation tracking between SAE features\n2. Add targeted penalty term to loss function based on co-activation patterns\n3. Train on standard datasets with varying penalty strengths\n4. Evaluate feature absorption using first-letter classification task\n5. Compare interpretability metrics against baseline SAE', 'Technical_Details': 'The method extends the standard SAE loss with an absorption penalty term: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. L_absorb is computed by maintaining an exponential moving average of pairwise feature co-activation probabilities P(f_i|f_j). When P(f_i|f_j) exceeds a threshold τ (indicating potential absorption), we add a penalty proportional to the conditional probability. The penalty encourages the model to separate commonly co-occurring features into distinct representations rather than absorbing one into the other. The EMA decay rate α and threshold τ are hyperparameters controlling the sensitivity to absorption patterns.', 'Implementation_Plan': '1. Add CoActivationTracker class to maintain EMA statistics\n2. Modify CustomSAE forward pass to track co-activations\n3. Add absorption penalty calculation to loss function\n4. Add hyperparameters for EMA decay and threshold\n5. Implement evaluation metrics for feature absorption', 'Interestingness_Evaluation': 'The targeted approach to preventing feature absorption provides a direct solution to a core interpretability challenge while maintaining the simplicity of the base SAE architecture.', 'Interestingness': 8, 'Feasibility_Evaluation': 'The implementation requires only simple statistics tracking and an additional loss term; co-activation computation can be done efficiently in batches; all core SAE functionality remains unchanged; experiments will complete within 30 minutes on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While feature absorption has been identified as an issue, this is the first direct attempt to prevent it through targeted loss terms based on co-activation patterns.', 'Novelty': 8, 'Expected_Research_Impact': 'By directly targeting feature absorption, the method should significantly improve on the absorption benchmark while maintaining or improving core metrics by allowing beneficial feature interactions.', 'Research_Impact': 8, 'Overall_Score': 8.5, 'Abstract': 'Feature absorption in sparse autoencoders poses a significant challenge for mechanistic interpretability, where distinct concepts become entangled in individual features. We propose a targeted approach to prevent feature absorption by tracking and penalizing suspicious co-activation patterns between features. Unlike previous methods that enforce strict constraints on feature interactions, our method selectively discourages only those patterns indicative of absorption while allowing beneficial feature sharing. We introduce an efficient co-activation tracking mechanism and a targeted penalty term that can be seamlessly integrated into existing SAE architectures. The method maintains the simplicity and efficiency of standard SAEs while directly addressing a core challenge in achieving interpretable representations.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the targeted absorption approach is promising, I realize we can make it more precise and feasible. The current co-activation tracking might be too computationally intensive across all feature pairs. Instead, we can focus specifically on hierarchical feature relationships, which are the primary source of absorption problems (like "India" => "Asia", "short" => "starts with S").

We can simplify the implementation by tracking conditional activation frequencies only for features that consistently activate together, rather than maintaining statistics for all pairs. This makes the method more computationally efficient while still targeting the core problem. We should also be more specific about the hyperparameters - setting clear ranges for the EMA decay (0.9-0.99) and co-activation threshold (0.8-0.95) based on typical absorption patterns.

The previous overall score of 8.5 was slightly optimistic given implementation complexity. While the idea is novel and interesting, we should be more conservative about feasibility. The new version simplifies the tracking mechanism and provides more concrete technical details, making it more practical while maintaining the core benefits.

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_absorption_sae",
    "Title": "Preventing Hierarchical Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement efficient hierarchical co-activation tracking\n2. Add selective absorption penalty for highly correlated features\n3. Train on standard datasets with EMA decay={0.9, 0.95, 0.99}\n4. Evaluate feature absorption using first-letter classification\n5. Analyze impact on hierarchical concept separation",
    "Technical_Details": "The method extends standard SAE training with a targeted hierarchical absorption penalty: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. L_absorb focuses only on feature pairs with high co-activation frequency (>0.8). For these pairs, we maintain an exponential moving average (EMA) of conditional activation P(f_i|f_j) with decay rate α ∈ [0.9, 0.99]. When P(f_i|f_j) > τ (typically 0.9) but P(f_j|f_i) is significantly lower, we apply a penalty proportional to P(f_i|f_j) - P(f_j|f_i) to encourage separation of hierarchical features. This selectively prevents absorption while allowing non-hierarchical feature interactions.",
    "Implementation_Plan": "1. Add HierarchicalFeatureTracker class for efficient statistics\n2. Modify CustomSAE to track high-frequency co-activations\n3. Implement selective absorption penalty calculation\n4. Add configurable EMA and threshold parameters\n5. Add evaluation code for hierarchical feature separation",
    "Interestingness_Evaluation": "The focus on hierarchical relationships provides a principled and efficient approach to preventing feature absorption while maintaining model flexibility.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation complexity is reduced by tracking only high-frequency pairs; computation fits easily within training loop; clear parameter ranges make tuning straightforward; all operations are simple and efficient.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While feature absorption has been studied, this is the first approach to specifically target hierarchical relationships in feature learning.",
    "Novelty": 7,
    "Expected_Research_Impact": "The targeted focus on hierarchical relationships should effectively prevent feature absorption while maintaining good reconstruction, directly improving interpretability benchmarks.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "Feature absorption in sparse autoencoders often occurs along hierarchical relationships, where broader concepts absorb their more specific subconcepts. We propose a targeted approach that specifically prevents hierarchical feature absorption by tracking and penalizing asymmetric co-activation patterns between features. Our method efficiently identifies potential absorption candidates through co-activation frequency and applies a selective penalty based on conditional activation asymmetry. This approach maintains the simplicity of standard SAEs while directly addressing a common failure mode in interpretable feature learning. The method introduces minimal computational overhead and provides clear hyperparameter ranges based on typical absorption patterns."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_absorption_sae', 'Title': 'Preventing Hierarchical Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement efficient hierarchical co-activation tracking\n2. Add selective absorption penalty for highly correlated features\n3. Train on standard datasets with EMA decay={0.9, 0.95, 0.99}\n4. Evaluate feature absorption using first-letter classification\n5. Analyze impact on hierarchical concept separation', 'Technical_Details': 'The method extends standard SAE training with a targeted hierarchical absorption penalty: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. L_absorb focuses only on feature pairs with high co-activation frequency (>0.8). For these pairs, we maintain an exponential moving average (EMA) of conditional activation P(f_i|f_j) with decay rate α ∈ [0.9, 0.99]. When P(f_i|f_j) > τ (typically 0.9) but P(f_j|f_i) is significantly lower, we apply a penalty proportional to P(f_i|f_j) - P(f_j|f_i) to encourage separation of hierarchical features. This selectively prevents absorption while allowing non-hierarchical feature interactions.', 'Implementation_Plan': '1. Add HierarchicalFeatureTracker class for efficient statistics\n2. Modify CustomSAE to track high-frequency co-activations\n3. Implement selective absorption penalty calculation\n4. Add configurable EMA and threshold parameters\n5. Add evaluation code for hierarchical feature separation', 'Interestingness_Evaluation': 'The focus on hierarchical relationships provides a principled and efficient approach to preventing feature absorption while maintaining model flexibility.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation complexity is reduced by tracking only high-frequency pairs; computation fits easily within training loop; clear parameter ranges make tuning straightforward; all operations are simple and efficient.', 'Feasibility': 8, 'Novelty_Evaluation': 'While feature absorption has been studied, this is the first approach to specifically target hierarchical relationships in feature learning.', 'Novelty': 7, 'Expected_Research_Impact': 'The targeted focus on hierarchical relationships should effectively prevent feature absorption while maintaining good reconstruction, directly improving interpretability benchmarks.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'Feature absorption in sparse autoencoders often occurs along hierarchical relationships, where broader concepts absorb their more specific subconcepts. We propose a targeted approach that specifically prevents hierarchical feature absorption by tracking and penalizing asymmetric co-activation patterns between features. Our method efficiently identifies potential absorption candidates through co-activation frequency and applies a selective penalty based on conditional activation asymmetry. This approach maintains the simplicity of standard SAEs while directly addressing a common failure mode in interpretable feature learning. The method introduces minimal computational overhead and provides clear hyperparameter ranges based on typical absorption patterns.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon further reflection, while the hierarchical approach is promising, we can make it even more focused and feasible. The key insight is that we don't need to track all co-activations - we can identify potential absorption candidates directly from the decoder weights. When two features have similar decoder weights (high cosine similarity) but very different activation patterns, this strongly suggests feature absorption.

This simplifies the implementation significantly - instead of maintaining running statistics, we can compute weight similarities periodically and use this to identify potential absorption pairs. The absorption penalty then only needs to consider these specific pairs during training. This makes the method more computationally efficient and theoretically grounded, as it directly connects to the model's learned representations.

The previous overall score of 7.9 was still too high given the complexity of tracking co-activation statistics. This simplified version focuses on weight similarities instead, making it more feasible while maintaining the core goal of preventing feature absorption. The theoretical connection to decoder weights also makes the method more interpretable.

NEW IDEA JSON:
```json
{
    "Name": "weight_guided_absorption_sae",
    "Title": "Weight-Guided Prevention of Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement periodic weight similarity computation\n2. Add targeted absorption penalty for similar-weight features\n3. Train with similarity thresholds={0.7, 0.8, 0.9}\n4. Evaluate feature absorption on classification tasks\n5. Analyze weight similarity patterns",
    "Technical_Details": "The method prevents feature absorption by identifying candidate pairs through decoder weight similarities: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. Every n=100 steps, we compute pairwise cosine similarities between decoder weight vectors. Feature pairs with similarity > τ (typically 0.8) are marked as absorption candidates. For these pairs, L_absorb penalizes cases where one feature's activation frequency is much higher than the other's, encouraging balanced feature use. The penalty is proportional to the activation frequency difference weighted by the cosine similarity. This efficiently targets absorption without requiring complex statistics tracking.",
    "Implementation_Plan": "1. Add weight similarity computation function\n2. Modify CustomSAE to track candidate pairs\n3. Implement simple absorption penalty\n4. Add configurable similarity threshold\n5. Add evaluation code for weight analysis",
    "Interestingness_Evaluation": "The use of decoder weight similarities provides a simple yet principled approach to identifying and preventing feature absorption.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation is very simple - just cosine similarity computation and basic statistics; periodic updates keep overhead minimal; no complex tracking needed; clear single parameter to tune.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using decoder weight similarities to guide absorption prevention is a novel approach that connects directly to the model's learned representations.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct connection to model weights and simple implementation should provide reliable improvements in feature separation while maintaining efficiency.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "Feature absorption in sparse autoencoders often manifests as features with similar decoder weights but imbalanced activation patterns. We propose a weight-guided approach to preventing feature absorption by identifying candidate pairs through decoder weight similarities. When two features have similar weight vectors but very different activation frequencies, this suggests potential absorption. Our method periodically computes weight similarities to identify these cases and applies a targeted penalty to encourage more balanced feature use. This approach provides a simple, computationally efficient way to prevent feature absorption while maintaining the core sparse autoencoder architecture. The method requires minimal hyperparameter tuning and provides clear interpretation through its connection to the model's learned representations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'weight_guided_absorption_sae', 'Title': 'Weight-Guided Prevention of Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement periodic weight similarity computation\n2. Add targeted absorption penalty for similar-weight features\n3. Train with similarity thresholds={0.7, 0.8, 0.9}\n4. Evaluate feature absorption on classification tasks\n5. Analyze weight similarity patterns', 'Technical_Details': "The method prevents feature absorption by identifying candidate pairs through decoder weight similarities: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. Every n=100 steps, we compute pairwise cosine similarities between decoder weight vectors. Feature pairs with similarity > τ (typically 0.8) are marked as absorption candidates. For these pairs, L_absorb penalizes cases where one feature's activation frequency is much higher than the other's, encouraging balanced feature use. The penalty is proportional to the activation frequency difference weighted by the cosine similarity. This efficiently targets absorption without requiring complex statistics tracking.", 'Implementation_Plan': '1. Add weight similarity computation function\n2. Modify CustomSAE to track candidate pairs\n3. Implement simple absorption penalty\n4. Add configurable similarity threshold\n5. Add evaluation code for weight analysis', 'Interestingness_Evaluation': 'The use of decoder weight similarities provides a simple yet principled approach to identifying and preventing feature absorption.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is very simple - just cosine similarity computation and basic statistics; periodic updates keep overhead minimal; no complex tracking needed; clear single parameter to tune.', 'Feasibility': 9, 'Novelty_Evaluation': "Using decoder weight similarities to guide absorption prevention is a novel approach that connects directly to the model's learned representations.", 'Novelty': 7, 'Expected_Research_Impact': 'The direct connection to model weights and simple implementation should provide reliable improvements in feature separation while maintaining efficiency.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': "Feature absorption in sparse autoencoders often manifests as features with similar decoder weights but imbalanced activation patterns. We propose a weight-guided approach to preventing feature absorption by identifying candidate pairs through decoder weight similarities. When two features have similar weight vectors but very different activation frequencies, this suggests potential absorption. Our method periodically computes weight similarities to identify these cases and applies a targeted penalty to encourage more balanced feature use. This approach provides a simple, computationally efficient way to prevent feature absorption while maintaining the core sparse autoencoder architecture. The method requires minimal hyperparameter tuning and provides clear interpretation through its connection to the model's learned representations."}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at the previous idea, while using decoder weight similarities is efficient, we can make it even more targeted and interpretable. Instead of comparing all weight vectors, we can focus specifically on the most active features in each batch - these are the ones most likely to exhibit absorption. This further reduces computational overhead while focusing on the most relevant cases.

We should also be more precise about the absorption penalty. Rather than just looking at activation frequency differences, we can use the ratio of activations when both features are present in the input. This gives a clearer signal of absorption and is easier to interpret.

The previous overall score of 8.2 was reasonable for feasibility but perhaps optimistic for novelty. The implementation becomes even simpler with this refinement, focusing only on active features, but the core idea is more incremental. The research impact should also be adjusted down slightly as we're making the method more conservative.

NEW IDEA JSON:
```json
{
    "Name": "active_feature_absorption_sae",
    "Title": "Active Feature-Guided Prevention of Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement active feature detection per batch\n2. Add similarity computation for active features\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns",
    "Technical_Details": "The method prevents feature absorption by focusing on active features: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute pairwise decoder weight cosine similarities. When similarity > τ (typically 0.8), we compute the activation ratio R = count(f_i & f_j) / count(f_j) for the feature pair. If R > ρ (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - ρ) * similarity(f_i, f_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.",
    "Implementation_Plan": "1. Add active feature identification\n2. Implement selective similarity computation\n3. Add activation ratio calculation\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns",
    "Interestingness_Evaluation": "The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation is extremely simple - just basic counting and similarity computation for active features; minimal overhead; single clear threshold parameter; fits easily within training loop.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.",
    "Novelty": 6,
    "Expected_Research_Impact": "The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.",
    "Research_Impact": 7,
    "Overall_Score": 7.7,
    "Abstract": "Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'active_feature_absorption_sae', 'Title': 'Active Feature-Guided Prevention of Absorption in Sparse Autoencoders', 'Experiment': '1. Implement active feature detection per batch\n2. Add similarity computation for active features\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns', 'Technical_Details': 'The method prevents feature absorption by focusing on active features: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute pairwise decoder weight cosine similarities. When similarity > τ (typically 0.8), we compute the activation ratio R = count(f_i & f_j) / count(f_j) for the feature pair. If R > ρ (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - ρ) * similarity(f_i, f_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.', 'Implementation_Plan': '1. Add active feature identification\n2. Implement selective similarity computation\n3. Add activation ratio calculation\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns', 'Interestingness_Evaluation': 'The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just basic counting and similarity computation for active features; minimal overhead; single clear threshold parameter; fits easily within training loop.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.', 'Novelty': 6, 'Expected_Research_Impact': 'The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.', 'Research_Impact': 7, 'Overall_Score': 7.7, 'Abstract': 'Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The active feature approach is solid, but we can make one final simplification that makes it even more practical. Instead of computing pairwise similarities for all active features, we can use a simple threshold on the dot product between normalized decoder weights. This gives us essentially the same information but is more computationally efficient and easier to interpret.

We should also be more precise about the batch statistics. Rather than using raw counts, we can maintain simple running averages of activation patterns for active features. This smooths out noise while still keeping the implementation simple.

The previous overall score of 7.7 was about right, but this final simplification increases feasibility while maintaining the core benefits. The method becomes even more practical while still effectively targeting feature absorption.

I believe this represents the optimal balance between effectiveness and simplicity, staying true to the original goal while being maximally practical.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "active_feature_absorption_sae",
    "Title": "Active Feature-Guided Prevention of Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement active feature detection per batch\n2. Add normalized weight dot product computation\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns",
    "Technical_Details": "The method prevents feature absorption by focusing on active features: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute normalized dot products between decoder weight vectors. When dot(w_i, w_j) > τ (typically 0.8), we compute the running average activation ratio R = avg(f_i & f_j) / avg(f_j) with decay 0.99. If R > ρ (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - ρ) * dot(w_i, w_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.",
    "Implementation_Plan": "1. Add active feature identification\n2. Implement normalized dot product computation\n3. Add running average activation tracking\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns",
    "Interestingness_Evaluation": "The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation is extremely simple - just basic dot products and running averages; minimal overhead; single clear threshold parameter; fits easily within training loop.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.",
    "Novelty": 6,
    "Expected_Research_Impact": "The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.",
    "Research_Impact": 7,
    "Overall_Score": 7.7,
    "Abstract": "Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'active_feature_absorption_sae', 'Title': 'Active Feature-Guided Prevention of Absorption in Sparse Autoencoders', 'Experiment': '1. Implement active feature detection per batch\n2. Add normalized weight dot product computation\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns', 'Technical_Details': 'The method prevents feature absorption by focusing on active features: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute normalized dot products between decoder weight vectors. When dot(w_i, w_j) > τ (typically 0.8), we compute the running average activation ratio R = avg(f_i & f_j) / avg(f_j) with decay 0.99. If R > ρ (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - ρ) * dot(w_i, w_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.', 'Implementation_Plan': '1. Add active feature identification\n2. Implement normalized dot product computation\n3. Add running average activation tracking\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns', 'Interestingness_Evaluation': 'The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just basic dot products and running averages; minimal overhead; single clear threshold parameter; fits easily within training loop.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.', 'Novelty': 6, 'Expected_Research_Impact': 'The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.', 'Research_Impact': 7, 'Overall_Score': 7.7, 'Abstract': 'Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations.'}
Idea generation converged after 5 iterations.

Generating idea 3/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve knowledge separation, but may be overly complex with its adaptive scheduling and feature grouping mechanisms. I propose refining this into a simpler but potentially more effective approach focused specifically on the feature absorption problem.

The key insight is that feature absorption occurs when the SAE learns to encode hierarchical relationships (e.g. "short" implies "starts with S") in a way that merges distinct concepts. Rather than enforcing strict orthogonality between all features, we can specifically target this issue by introducing a "hierarchical repulsion" loss that penalizes feature vectors based on their co-activation patterns.

The idea maintains the core motivation of the prototype (improving feature separation) but simplifies the implementation by:
1. Removing complex adaptive scheduling
2. Eliminating need for feature grouping
3. Using simple co-activation statistics rather than matrix operations
4. Focusing specifically on preventing feature absorption rather than general orthogonality

This should improve mechanistic interpretability by encouraging the SAE to learn truly distinct features rather than merging related concepts. The hierarchical repulsion loss directly addresses a key failure mode (feature absorption) while remaining computationally efficient.

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_repulsion_sae",
    "Title": "Hierarchical Repulsion Loss for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement co-activation tracking during SAE training\n2. Add hierarchical repulsion loss based on co-activation patterns\n3. Train on standard benchmarks with varying repulsion strengths\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze learned feature relationships using co-activation matrices",
    "Technical_Details": "The method adds a hierarchical repulsion loss: L = L_recon + λ_1 * L_sparse + λ_2 * L_repulsion where L_repulsion = Σ_ij (c_ij * cos_sim(w_i, w_j)) for feature vectors w_i, w_j and co-activation coefficient c_ij. c_ij is computed as a moving average of how often features i and j activate together relative to their individual activation rates. High c_ij indicates potential feature absorption and increases repulsion between those features. Implementation uses efficient batch-wise updates of co-activation statistics.",
    "Implementation_Plan": "1. Add CoActivationTracker class to maintain statistics\n2. Modify CustomSAE forward pass to track activations\n3. Add hierarchical_repulsion_loss function\n4. Update CustomTrainer loss calculation\n5. Add evaluation metrics for co-activation patterns",
    "Interestingness_Evaluation": "The approach provides a direct and interpretable way to prevent feature absorption by targeting the specific statistical patterns that indicate merged concepts.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple tracking of co-activation statistics and cosine similarity calculations; all operations are efficient and can be done batch-wise; no complex matrix operations or scheduling needed; well within 30-minute limit on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While feature interaction has been studied before, using co-activation patterns to specifically prevent feature absorption is a novel approach.",
    "Novelty": 7,
    "Expected_Research_Impact": "By directly targeting feature absorption, this approach should significantly improve performance on sparse probing tasks by learning more distinct features. The core metrics should also improve due to better feature separation leading to more interpretable representations.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders by addressing the feature absorption problem, where distinct concepts are merged into single features. Our method introduces a hierarchical repulsion loss that penalizes feature pairs based on their co-activation patterns, encouraging the model to learn truly separate representations for related but distinct concepts. The approach maintains computational efficiency by using simple batch-wise statistics while directly targeting a key failure mode in current sparse autoencoder architectures. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature relationships through co-activation matrices."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_repulsion_sae', 'Title': 'Hierarchical Repulsion Loss for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement co-activation tracking during SAE training\n2. Add hierarchical repulsion loss based on co-activation patterns\n3. Train on standard benchmarks with varying repulsion strengths\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze learned feature relationships using co-activation matrices', 'Technical_Details': 'The method adds a hierarchical repulsion loss: L = L_recon + λ_1 * L_sparse + λ_2 * L_repulsion where L_repulsion = Σ_ij (c_ij * cos_sim(w_i, w_j)) for feature vectors w_i, w_j and co-activation coefficient c_ij. c_ij is computed as a moving average of how often features i and j activate together relative to their individual activation rates. High c_ij indicates potential feature absorption and increases repulsion between those features. Implementation uses efficient batch-wise updates of co-activation statistics.', 'Implementation_Plan': '1. Add CoActivationTracker class to maintain statistics\n2. Modify CustomSAE forward pass to track activations\n3. Add hierarchical_repulsion_loss function\n4. Update CustomTrainer loss calculation\n5. Add evaluation metrics for co-activation patterns', 'Interestingness_Evaluation': 'The approach provides a direct and interpretable way to prevent feature absorption by targeting the specific statistical patterns that indicate merged concepts.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple tracking of co-activation statistics and cosine similarity calculations; all operations are efficient and can be done batch-wise; no complex matrix operations or scheduling needed; well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While feature interaction has been studied before, using co-activation patterns to specifically prevent feature absorption is a novel approach.', 'Novelty': 7, 'Expected_Research_Impact': 'By directly targeting feature absorption, this approach should significantly improve performance on sparse probing tasks by learning more distinct features. The core metrics should also improve due to better feature separation leading to more interpretable representations.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a novel approach to improving the interpretability of sparse autoencoders by addressing the feature absorption problem, where distinct concepts are merged into single features. Our method introduces a hierarchical repulsion loss that penalizes feature pairs based on their co-activation patterns, encouraging the model to learn truly separate representations for related but distinct concepts. The approach maintains computational efficiency by using simple batch-wise statistics while directly targeting a key failure mode in current sparse autoencoder architectures. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature relationships through co-activation matrices.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, I believe my previous idea could be refined further:

1. The co-activation tracking mechanism, while simpler than the original orthogonality approach, still introduces complexity that may not be necessary. 

2. The overall score of 8.3 seems too optimistic given the implementation challenges of maintaining accurate co-activation statistics during training.

3. The approach could be simplified further while maintaining its core purpose of preventing feature absorption.

I propose refining the idea to use a simpler pairwise cosine similarity penalty applied only to features that exceed a sparsity threshold within each batch. This eliminates the need for tracking co-activation statistics over time while still targeting the feature absorption problem. The key benefits are:

1. More efficient implementation (no need to maintain moving averages)
2. Clearer theoretical connection to feature absorption
3. Easier to tune (single threshold parameter)
4. More predictable training behavior

This refinement stays true to both the original prototype's goal of feature separation and my previous idea's focus on preventing feature absorption, but with improved feasibility and clarity.

NEW IDEA JSON:
```json
{
    "Name": "threshold_repulsion_sae",
    "Title": "Threshold-Based Feature Repulsion for Preventing Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement batch-wise feature activation thresholding\n2. Add pairwise cosine similarity loss for active features\n3. Train on standard benchmarks with varying threshold values\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze impact of threshold choice on feature separation",
    "Technical_Details": "The method adds a simplified repulsion loss: L = L_recon + λ_1 * L_sparse + λ_2 * L_repulsion where L_repulsion = Σ_ij (I_i * I_j * cos_sim(w_i, w_j)) for feature vectors w_i, w_j and indicator I_k = 1 if feature k\'s activation exceeds threshold τ in the current batch, 0 otherwise. This creates repulsion between frequently active features while allowing rare features to specialize independently. The threshold τ is set relative to the target sparsity level.",
    "Implementation_Plan": "1. Add threshold_indicators function to identify active features\n2. Add threshold_repulsion_loss function\n3. Modify CustomTrainer loss calculation\n4. Add threshold parameter to configuration\n5. Add evaluation metrics for feature overlap",
    "Interestingness_Evaluation": "The approach provides an elegant and theoretically sound way to prevent feature absorption while maintaining computational efficiency.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple thresholding and cosine similarity calculations within each batch; no state maintenance needed; highly efficient and easily parallelizable; well within 30-minute limit on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While similar to previous repulsion approaches, the use of activation thresholds to target feature absorption is novel.",
    "Novelty": 6,
    "Expected_Research_Impact": "The simplified approach should effectively prevent feature absorption while being easier to implement and tune, leading to improved interpretability scores on both sparse probing and core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "We present a simplified approach to improving sparse autoencoder interpretability by preventing feature absorption through threshold-based feature repulsion. Our method introduces a computationally efficient loss term that creates repulsion between frequently active features while allowing rare features to specialize independently. The approach requires only a single threshold parameter and operates entirely within each training batch, eliminating the need for complex statistical tracking. We evaluate the effectiveness of this method on standard interpretability benchmarks and analyze how different threshold values affect feature separation patterns."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Failed to generate idea: Failed to extract JSON from LLM output

Generating idea 4/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve feature separation, but I believe we can refine this further while maintaining simplicity. The core insight is that rather than enforcing strict orthogonality between all features, we can strategically enforce it only between features that tend to exhibit feature absorption. This addresses the benchmark challenge of feature absorption more directly while being computationally efficient.

The key modification is to introduce a selective orthogonality loss that only applies to feature pairs that show high correlation in their activation patterns. This is a natural refinement of the prototype's blanket orthogonality approach - instead of forcing all features to be orthogonal (which could harm reconstruction), we only separate features that are likely to suffer from absorption.

The implementation remains simple: we track feature co-activation statistics in a running buffer and apply the orthogonality loss selectively to highly correlated pairs. This requires minimal changes to the existing code structure while targeting the specific problem of feature absorption.

I expect this to improve mechanistic interpretability because:
1. It directly addresses feature absorption while preserving useful feature relationships
2. The selective approach maintains model capacity better than blanket orthogonality
3. It provides a natural way to discover and separate polysemantic features

NEW IDEA JSON:
```json
{
    "Name": "selective_orthogonal_sae",
    "Title": "Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement running co-activation statistics tracking\n2. Add selective orthogonality loss for highly correlated feature pairs\n3. Train on standard datasets with varying correlation thresholds\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze impact on feature interpretability",
    "Technical_Details": "The method extends the standard SAE loss with a selective orthogonality term: L = L_recon + λ_1 * L_sparse + λ_2 * L_selective_ortho. L_selective_ortho applies only to feature pairs (i,j) where their co-activation correlation exceeds threshold τ. Co-activation statistics are tracked using an exponential moving average with decay rate α=0.99. The orthogonality loss for selected pairs is computed as ||w_i^T w_j||^2 where w_i, w_j are the corresponding decoder columns. Implementation includes efficient sparse matrix operations for the selective loss computation.",
    "Implementation_Plan": "1. Add CoactivationTracker class for statistics\n2. Modify CustomSAE loss function to include selective orthogonality\n3. Add correlation threshold as hyperparameter\n4. Implement efficient sparse matrix operations\n5. Add evaluation metrics for feature absorption",
    "Interestingness_Evaluation": "The selective approach provides an elegant solution to feature absorption while preserving useful feature relationships.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only modest additions to existing code; running co-activation statistics add minimal overhead; selective orthogonality computation is efficient with sparse operations; all within 30-minute training limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While orthogonality constraints are known, the selective application based on co-activation statistics is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The targeted approach to preventing feature absorption while maintaining reconstruction quality should directly improve core metrics and sparse probing performance.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption while preserving useful feature relationships. Rather than enforcing blanket orthogonality between all features, our method selectively applies orthogonality constraints only to feature pairs that exhibit high co-activation correlation, indicating potential feature absorption. We track feature co-activation statistics using an efficient running average and apply the orthogonality loss selectively based on a correlation threshold. This approach aims to prevent feature absorption while maintaining the autoencoder's reconstruction capabilities and computational efficiency. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'selective_orthogonal_sae', 'Title': 'Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement running co-activation statistics tracking\n2. Add selective orthogonality loss for highly correlated feature pairs\n3. Train on standard datasets with varying correlation thresholds\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze impact on feature interpretability', 'Technical_Details': 'The method extends the standard SAE loss with a selective orthogonality term: L = L_recon + λ_1 * L_sparse + λ_2 * L_selective_ortho. L_selective_ortho applies only to feature pairs (i,j) where their co-activation correlation exceeds threshold τ. Co-activation statistics are tracked using an exponential moving average with decay rate α=0.99. The orthogonality loss for selected pairs is computed as ||w_i^T w_j||^2 where w_i, w_j are the corresponding decoder columns. Implementation includes efficient sparse matrix operations for the selective loss computation.', 'Implementation_Plan': '1. Add CoactivationTracker class for statistics\n2. Modify CustomSAE loss function to include selective orthogonality\n3. Add correlation threshold as hyperparameter\n4. Implement efficient sparse matrix operations\n5. Add evaluation metrics for feature absorption', 'Interestingness_Evaluation': 'The selective approach provides an elegant solution to feature absorption while preserving useful feature relationships.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only modest additions to existing code; running co-activation statistics add minimal overhead; selective orthogonality computation is efficient with sparse operations; all within 30-minute training limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While orthogonality constraints are known, the selective application based on co-activation statistics is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The targeted approach to preventing feature absorption while maintaining reconstruction quality should directly improve core metrics and sparse probing performance.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': "We present a selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption while preserving useful feature relationships. Rather than enforcing blanket orthogonality between all features, our method selectively applies orthogonality constraints only to feature pairs that exhibit high co-activation correlation, indicating potential feature absorption. We track feature co-activation statistics using an efficient running average and apply the orthogonality loss selectively based on a correlation threshold. This approach aims to prevent feature absorption while maintaining the autoencoder's reconstruction capabilities and computational efficiency. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics."}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the selective orthogonality approach is promising, I realize we can make it more focused and practical. The previous version relied too heavily on correlation statistics, which could be noisy and computationally intensive. Instead, we can leverage the inherent structure of the problem: feature absorption typically occurs between features that represent hierarchical concepts (e.g., "color" and "red").

We can modify the approach to use a simpler and more direct measure: the cosine similarity between feature activations on the same input. This is computationally cheaper than tracking running statistics and more directly captures the feature absorption phenomenon. We can also make the orthogonality loss adaptive - starting strict and gradually relaxing based on the model's reconstruction performance.

This refinement maintains the core insight of selective orthogonality while being more computationally efficient and theoretically grounded in the nature of feature absorption. The implementation becomes cleaner and more focused on the specific problem we're trying to solve.

The previous overall score of 8.3 was slightly optimistic given the complexity of tracking co-activation statistics. This refined version should be more feasible while maintaining the benefits.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_selective_orthogonal_sae",
    "Title": "Adaptive Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement batch-wise cosine similarity computation\n2. Add adaptive orthogonality loss with performance-based scaling\n3. Train on standard datasets with varying initial orthogonality strengths\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature interpretability improvements",
    "Technical_Details": "The method uses a simplified selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to feature pairs with cosine similarity exceeding τ in the current batch. λ_2(t) starts at λ_2_max and decreases based on reconstruction error: λ_2(t) = λ_2_max * (1 - min(1, R(t)/R_target)) where R(t) is current reconstruction error and R_target is target error. The orthogonality loss uses efficient batch computation: L_selective_ortho = ||W^T W ⊙ M||_F where M masks low-similarity pairs.",
    "Implementation_Plan": "1. Add cosine similarity computation to forward pass\n2. Modify CustomSAE loss function for adaptive selective orthogonality\n3. Add adaptive scaling based on reconstruction error\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for feature separation",
    "Interestingness_Evaluation": "The adaptive approach provides an elegant balance between feature separation and reconstruction quality, with theoretical grounding in the nature of feature absorption.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple matrix operations; cosine similarity computation is fast and batched; adaptive scaling adds minimal overhead; all operations well within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The combination of selective orthogonality with adaptive scaling based on reconstruction performance is novel and well-motivated.",
    "Novelty": 7,
    "Expected_Research_Impact": "The simplified and more focused approach should directly improve feature separation while maintaining good reconstruction, benefiting both core metrics and sparse probing.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We present an adaptive selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to feature pairs that exhibit high cosine similarity in their activations, indicating potential feature absorption. The strength of these constraints adapts dynamically based on reconstruction performance, allowing the model to balance feature separation with reconstruction quality. The approach uses efficient batch computations and requires minimal additional computational overhead. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to hierarchical feature relationships that typically lead to feature absorption."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_selective_orthogonal_sae', 'Title': 'Adaptive Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement batch-wise cosine similarity computation\n2. Add adaptive orthogonality loss with performance-based scaling\n3. Train on standard datasets with varying initial orthogonality strengths\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature interpretability improvements', 'Technical_Details': 'The method uses a simplified selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to feature pairs with cosine similarity exceeding τ in the current batch. λ_2(t) starts at λ_2_max and decreases based on reconstruction error: λ_2(t) = λ_2_max * (1 - min(1, R(t)/R_target)) where R(t) is current reconstruction error and R_target is target error. The orthogonality loss uses efficient batch computation: L_selective_ortho = ||W^T W ⊙ M||_F where M masks low-similarity pairs.', 'Implementation_Plan': '1. Add cosine similarity computation to forward pass\n2. Modify CustomSAE loss function for adaptive selective orthogonality\n3. Add adaptive scaling based on reconstruction error\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for feature separation', 'Interestingness_Evaluation': 'The adaptive approach provides an elegant balance between feature separation and reconstruction quality, with theoretical grounding in the nature of feature absorption.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple matrix operations; cosine similarity computation is fast and batched; adaptive scaling adds minimal overhead; all operations well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'The combination of selective orthogonality with adaptive scaling based on reconstruction performance is novel and well-motivated.', 'Novelty': 7, 'Expected_Research_Impact': 'The simplified and more focused approach should directly improve feature separation while maintaining good reconstruction, benefiting both core metrics and sparse probing.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'We present an adaptive selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to feature pairs that exhibit high cosine similarity in their activations, indicating potential feature absorption. The strength of these constraints adapts dynamically based on reconstruction performance, allowing the model to balance feature separation with reconstruction quality. The approach uses efficient batch computations and requires minimal additional computational overhead. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to hierarchical feature relationships that typically lead to feature absorption.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon further reflection, I realize we can simplify and focus the idea even more while potentially increasing its effectiveness. The previous version still had some complexity with the adaptive scaling mechanism that might not be necessary. Instead, we can focus purely on the most critical aspect: preventing feature absorption between semantically related features.

The key insight is that feature absorption most commonly occurs between a general feature and its specific instances during the early stages of training. For example, a "color" feature might absorb specific color features like "red" early on. We can modify our approach to apply stronger orthogonality constraints during the initial training phase when feature absorption is most likely to occur, then gradually reduce it using a simple linear schedule.

This simplification removes the need for dynamic adaptation based on reconstruction error while maintaining the core benefit. It's more theoretically sound because it aligns with how feature absorption typically develops during training, and it's more computationally efficient because it removes the need to compute and track reconstruction error for adaptation.

The previous overall score of 8.2 was still a bit high given the complexity. This simplified version should be more feasible while being equally or more effective.

NEW IDEA JSON:
```json
{
    "Name": "early_selective_orthogonal_sae",
    "Title": "Early-Phase Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement batch-wise cosine similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation",
    "Technical_Details": "The method uses a simplified time-dependent selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to feature pairs with cosine similarity exceeding τ=0.3 in the current batch. λ_2(t) decreases linearly with training steps: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is set to 20% of total training steps. The orthogonality loss uses efficient batch computation: L_selective_ortho = ||W^T W ⊙ M||_F where M masks low-similarity pairs.",
    "Implementation_Plan": "1. Add cosine similarity computation to forward pass\n2. Modify CustomSAE loss function for time-dependent selective orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for early-stage feature separation",
    "Interestingness_Evaluation": "The focus on early-stage feature separation aligns well with theoretical understanding of how feature absorption develops during training.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only basic matrix operations and simple linear scheduling; no complex tracking or adaptation needed; highly efficient computation well within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The targeted early-phase approach to preventing feature absorption is novel and well-grounded in empirical observations.",
    "Novelty": 7,
    "Expected_Research_Impact": "The focused early-phase intervention should effectively prevent feature absorption while allowing natural feature relationships to develop later, improving interpretability metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.1,
    "Abstract": "We present an early-phase selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to feature pairs that exhibit high cosine similarity in their activations, with the constraint strength decreasing linearly during training. This approach is motivated by the observation that feature absorption typically occurs during early training phases when general features may absorb more specific ones. The method uses efficient batch computations and a simple scheduling mechanism, focusing intervention when it is most critical while allowing natural feature relationships to develop in later training stages. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'early_selective_orthogonal_sae', 'Title': 'Early-Phase Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement batch-wise cosine similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation', 'Technical_Details': 'The method uses a simplified time-dependent selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to feature pairs with cosine similarity exceeding τ=0.3 in the current batch. λ_2(t) decreases linearly with training steps: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is set to 20% of total training steps. The orthogonality loss uses efficient batch computation: L_selective_ortho = ||W^T W ⊙ M||_F where M masks low-similarity pairs.', 'Implementation_Plan': '1. Add cosine similarity computation to forward pass\n2. Modify CustomSAE loss function for time-dependent selective orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for early-stage feature separation', 'Interestingness_Evaluation': 'The focus on early-stage feature separation aligns well with theoretical understanding of how feature absorption develops during training.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only basic matrix operations and simple linear scheduling; no complex tracking or adaptation needed; highly efficient computation well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'The targeted early-phase approach to preventing feature absorption is novel and well-grounded in empirical observations.', 'Novelty': 7, 'Expected_Research_Impact': 'The focused early-phase intervention should effectively prevent feature absorption while allowing natural feature relationships to develop later, improving interpretability metrics.', 'Research_Impact': 8, 'Overall_Score': 8.1, 'Abstract': 'We present an early-phase selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to feature pairs that exhibit high cosine similarity in their activations, with the constraint strength decreasing linearly during training. This approach is motivated by the observation that feature absorption typically occurs during early training phases when general features may absorb more specific ones. The method uses efficient batch computations and a simple scheduling mechanism, focusing intervention when it is most critical while allowing natural feature relationships to develop in later training stages. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at the previous version, I realize we can make one final critical simplification that makes the approach even more focused and practical. Rather than computing cosine similarities between all feature pairs in each batch, we can leverage the fact that feature absorption typically manifests through the decoder weights themselves.

The key insight is that if two features are prone to absorption, their decoder weight vectors will naturally become similar during training. This means we can apply the orthogonality constraint based on the cosine similarity of decoder weight columns rather than activation patterns. This has several advantages:
1. More computationally efficient (computed once per step rather than per batch)
2. More stable (weight similarities change more slowly than activations)
3. More directly targets the mechanism of feature absorption

The previous overall score of 8.1 was reasonable, but this refinement should improve feasibility while maintaining or improving effectiveness. The implementation becomes even simpler while staying true to the core idea of preventing early-stage feature absorption.

NEW IDEA JSON:
```json
{
    "Name": "decoder_selective_orthogonal_sae",
    "Title": "Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation",
    "Technical_Details": "The method uses a weight-based selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding τ=0.3. λ_2(t) decreases linearly: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > τ else 0. L_selective_ortho = ||W_dec^T W_dec ⊙ M||_F.",
    "Implementation_Plan": "1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity",
    "Interestingness_Evaluation": "The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.",
    "Research_Impact": 8,
    "Overall_Score": 8.1,
    "Abstract": "We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'decoder_selective_orthogonal_sae', 'Title': 'Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation', 'Technical_Details': 'The method uses a weight-based selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding τ=0.3. λ_2(t) decreases linearly: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > τ else 0. L_selective_ortho = ||W_dec^T W_dec ⊙ M||_F.', 'Implementation_Plan': '1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity', 'Interestingness_Evaluation': 'The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.', 'Research_Impact': 8, 'Overall_Score': 8.1, 'Abstract': 'We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe the current version (decoder_selective_orthogonal_sae) has reached an optimal balance of simplicity, effectiveness, and feasibility. The approach:
1. Uses only existing model components (decoder weights)
2. Has minimal computational overhead
3. Directly targets the feature absorption mechanism
4. Has a simple, theoretically motivated schedule
5. Requires minimal code changes

Any further modifications would likely either:
- Add unnecessary complexity
- Deviate from the core insight about decoder weight similarities
- Increase computational cost
- Make the implementation more difficult
- Reduce theoretical clarity

The current overall score of 8.1 accurately reflects the balance of novelty, feasibility, and potential impact. The implementation is straightforward enough for a junior PhD student to complete within a month, and the training time will easily fit within the 30-minute limit on an H100.

I am done.

```json
{
    "Name": "decoder_selective_orthogonal_sae",
    "Title": "Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation",
    "Technical_Details": "The method uses a weight-based selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding τ=0.3. λ_2(t) decreases linearly: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > τ else 0. L_selective_ortho = ||W_dec^T W_dec ⊙ M||_F.",
    "Implementation_Plan": "1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity",
    "Interestingness_Evaluation": "The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.",
    "Research_Impact": 8,
    "Overall_Score": 8.1,
    "Abstract": "We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'decoder_selective_orthogonal_sae', 'Title': 'Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation', 'Technical_Details': 'The method uses a weight-based selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding τ=0.3. λ_2(t) decreases linearly: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > τ else 0. L_selective_ortho = ||W_dec^T W_dec ⊙ M||_F.', 'Implementation_Plan': '1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity', 'Interestingness_Evaluation': 'The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.', 'Research_Impact': 8, 'Overall_Score': 8.1, 'Abstract': 'We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies.'}
Idea generation converged after 5 iterations.

Generating idea 5/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve feature separation, but may be overcomplicating things with adaptive losses and feature grouping. Instead, I propose refining this into a simpler but potentially more effective approach focusing on hierarchical feature organization through nested dictionaries.

The key insight is that we can encourage natural feature hierarchies by having multiple reconstruction objectives at different dictionary sizes. For example, with a 2304-dimensional dictionary, we could have one loss reconstructing with all features, another using only the first 1152, and another using only the first 576. This creates pressure for the most important, general features to be learned first, while more specific features come later.

This addresses feature absorption by giving the model a natural way to organize hierarchical features - rather than absorbing "starts with S" into "short", it can learn "starts with S" early in the dictionary and more specific features later. The nested structure provides an inductive bias towards learning interpretable feature hierarchies without requiring complex orthogonality constraints.

The implementation requires minimal changes to the existing architecture - just modifying the loss function to sum multiple reconstruction objectives using different sized subsets of the dictionary. This maintains the simplicity and efficiency of the original SAE while potentially improving interpretability.

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_sae",
    "Title": "Hierarchical Feature Learning in Sparse Autoencoders through Nested Dictionaries",
    "Experiment": "1. Modify SAE loss function to include multiple reconstruction objectives using nested subsets of the dictionary\n2. Train on Gemma-2-2b model using standard datasets\n3. Compare feature interpretability and absorption metrics against baseline SAE\n4. Analyze learned feature hierarchies through activation patterns\n5. Evaluate impact on core interpretability metrics",
    "Technical_Details": "The method uses a modified loss function: L = Σ_i w_i * L_recon(x, f[:d_i]) where d_i are decreasing dictionary sizes (e.g. [d, d/2, d/4]) and w_i are fixed weights. Each L_recon uses the same encoder/decoder architecture but restricts the dictionary size. Early features must participate in all reconstructions while later features only affect larger dictionaries, naturally encouraging hierarchical organization. The encoder/decoder architecture remains unchanged to maintain efficiency.",
    "Implementation_Plan": "1. Add nested dictionary sizes parameter to CustomSAE\n2. Modify forward pass to return multiple reconstructions\n3. Update loss function in CustomTrainer to sum weighted reconstruction losses\n4. Add visualization utilities for feature hierarchy analysis\n5. Implement evaluation metrics for hierarchical organization",
    "Interestingness_Evaluation": "The approach provides a natural way to learn interpretable feature hierarchies without complex constraints or losses.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes to existing architecture; all computations remain efficient as we reuse the same features for multiple reconstructions; no complex training dynamics to manage; well within 30-minute limit on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While nested representations have been explored in other contexts, applying them to SAEs for improving interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical organization should directly improve feature interpretability by reducing absorption and encouraging natural feature taxonomies, likely improving performance on both sparse probing and core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We propose a novel approach to improving interpretability in sparse autoencoders through hierarchical feature learning. By introducing multiple reconstruction objectives with nested dictionary sizes, we encourage the model to learn natural feature hierarchies where fundamental concepts are captured early in the dictionary and more specific features appear later. This provides an inductive bias towards interpretable representations without requiring complex orthogonality constraints or feature grouping mechanisms. Our method builds on standard sparse autoencoder architectures, requiring only modifications to the loss function while maintaining computational efficiency. We hypothesize this approach will reduce feature absorption and improve the interpretability of learned representations by aligning with natural conceptual hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_sae', 'Title': 'Hierarchical Feature Learning in Sparse Autoencoders through Nested Dictionaries', 'Experiment': '1. Modify SAE loss function to include multiple reconstruction objectives using nested subsets of the dictionary\n2. Train on Gemma-2-2b model using standard datasets\n3. Compare feature interpretability and absorption metrics against baseline SAE\n4. Analyze learned feature hierarchies through activation patterns\n5. Evaluate impact on core interpretability metrics', 'Technical_Details': 'The method uses a modified loss function: L = Σ_i w_i * L_recon(x, f[:d_i]) where d_i are decreasing dictionary sizes (e.g. [d, d/2, d/4]) and w_i are fixed weights. Each L_recon uses the same encoder/decoder architecture but restricts the dictionary size. Early features must participate in all reconstructions while later features only affect larger dictionaries, naturally encouraging hierarchical organization. The encoder/decoder architecture remains unchanged to maintain efficiency.', 'Implementation_Plan': '1. Add nested dictionary sizes parameter to CustomSAE\n2. Modify forward pass to return multiple reconstructions\n3. Update loss function in CustomTrainer to sum weighted reconstruction losses\n4. Add visualization utilities for feature hierarchy analysis\n5. Implement evaluation metrics for hierarchical organization', 'Interestingness_Evaluation': 'The approach provides a natural way to learn interpretable feature hierarchies without complex constraints or losses.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes to existing architecture; all computations remain efficient as we reuse the same features for multiple reconstructions; no complex training dynamics to manage; well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While nested representations have been explored in other contexts, applying them to SAEs for improving interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical organization should directly improve feature interpretability by reducing absorption and encouraging natural feature taxonomies, likely improving performance on both sparse probing and core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We propose a novel approach to improving interpretability in sparse autoencoders through hierarchical feature learning. By introducing multiple reconstruction objectives with nested dictionary sizes, we encourage the model to learn natural feature hierarchies where fundamental concepts are captured early in the dictionary and more specific features appear later. This provides an inductive bias towards interpretable representations without requiring complex orthogonality constraints or feature grouping mechanisms. Our method builds on standard sparse autoencoder architectures, requiring only modifications to the loss function while maintaining computational efficiency. We hypothesize this approach will reduce feature absorption and improve the interpretability of learned representations by aligning with natural conceptual hierarchies.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, I believe the previous idea's score of 8.3 was slightly optimistic. While the hierarchical approach is interesting, there are some areas that could be refined:

1. The weighting scheme for different dictionary sizes wasn't fully specified, which could impact feasibility
2. The relationship to feature absorption could be made more explicit
3. The implementation plan could be more detailed about evaluation metrics

I propose refining the idea by:
1. Using specific fixed ratios for dictionary sizes (1, 1/2, 1/4) and equal weighting to keep things simple
2. Adding explicit tracking of feature reuse across dictionary sizes to measure hierarchy quality
3. Focusing more directly on the feature absorption benchmark by analyzing how hierarchical organization affects absorption scores

This maintains the core insight about hierarchical feature learning while making the implementation and evaluation more concrete. The technical complexity remains low - we're still just modifying the loss function and adding some basic metrics.

The feasibility score should be slightly lower (8 instead of 9) since we need to implement additional evaluation metrics. The overall approach remains simple enough for implementation within the time constraints.

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_sae",
    "Title": "Hierarchical Feature Learning in Sparse Autoencoders through Fixed-Ratio Nested Dictionaries",
    "Experiment": "1. Implement SAE with three fixed-ratio dictionary sizes (full, half, quarter)\n2. Train on Gemma-2-2b using equal weights for reconstruction losses\n3. Track feature reuse patterns across dictionary sizes\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy quality through activation patterns",
    "Technical_Details": "The method uses three reconstruction objectives with dictionary sizes [d, d/2, d/4] and equal weights: L = (1/3) * Σ_i L_recon(x, f[:d_i]). Feature reuse is tracked by computing correlation between feature activations across different dictionary sizes. Hierarchy quality is measured by the ratio of features that maintain consistent activation patterns across reconstructions. The encoder/decoder architecture remains unchanged, with only the loss computation modified.",
    "Implementation_Plan": "1. Add DICTIONARY_RATIOS = [1.0, 0.5, 0.25] parameter to CustomSAE\n2. Modify forward pass to compute reconstructions for each ratio\n3. Update CustomTrainer with equally-weighted reconstruction losses\n4. Add feature_reuse_analysis() to track cross-dictionary correlations\n5. Implement hierarchy_quality_score() based on activation consistency",
    "Interestingness_Evaluation": "The fixed-ratio approach provides a simple but principled way to encourage hierarchical feature organization.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation complexity is moderate due to additional metrics; core changes to architecture are minimal; evaluation metrics add some overhead but still within 30-minute limit on H100.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While hierarchical representations are not new, the fixed-ratio nested approach for SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The explicit focus on measuring and reducing feature absorption through hierarchical organization should improve performance on both core metrics and absorption benchmarks.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "We propose a simple but principled approach to improving interpretability in sparse autoencoders through hierarchical feature learning with fixed-ratio nested dictionaries. Our method uses three reconstruction objectives at different dictionary sizes (full, half, and quarter) with equal weights, encouraging the model to learn natural feature hierarchies where fundamental concepts are captured in smaller dictionaries. This provides an inductive bias towards interpretable representations while maintaining architectural simplicity. We introduce metrics to evaluate hierarchy quality through feature reuse patterns and activation consistency across dictionary sizes. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only multiple reconstruction objectives and basic evaluation metrics."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_sae', 'Title': 'Hierarchical Feature Learning in Sparse Autoencoders through Fixed-Ratio Nested Dictionaries', 'Experiment': '1. Implement SAE with three fixed-ratio dictionary sizes (full, half, quarter)\n2. Train on Gemma-2-2b using equal weights for reconstruction losses\n3. Track feature reuse patterns across dictionary sizes\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy quality through activation patterns', 'Technical_Details': 'The method uses three reconstruction objectives with dictionary sizes [d, d/2, d/4] and equal weights: L = (1/3) * Σ_i L_recon(x, f[:d_i]). Feature reuse is tracked by computing correlation between feature activations across different dictionary sizes. Hierarchy quality is measured by the ratio of features that maintain consistent activation patterns across reconstructions. The encoder/decoder architecture remains unchanged, with only the loss computation modified.', 'Implementation_Plan': '1. Add DICTIONARY_RATIOS = [1.0, 0.5, 0.25] parameter to CustomSAE\n2. Modify forward pass to compute reconstructions for each ratio\n3. Update CustomTrainer with equally-weighted reconstruction losses\n4. Add feature_reuse_analysis() to track cross-dictionary correlations\n5. Implement hierarchy_quality_score() based on activation consistency', 'Interestingness_Evaluation': 'The fixed-ratio approach provides a simple but principled way to encourage hierarchical feature organization.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation complexity is moderate due to additional metrics; core changes to architecture are minimal; evaluation metrics add some overhead but still within 30-minute limit on H100.', 'Feasibility': 8, 'Novelty_Evaluation': 'While hierarchical representations are not new, the fixed-ratio nested approach for SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The explicit focus on measuring and reducing feature absorption through hierarchical organization should improve performance on both core metrics and absorption benchmarks.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'We propose a simple but principled approach to improving interpretability in sparse autoencoders through hierarchical feature learning with fixed-ratio nested dictionaries. Our method uses three reconstruction objectives at different dictionary sizes (full, half, and quarter) with equal weights, encouraging the model to learn natural feature hierarchies where fundamental concepts are captured in smaller dictionaries. This provides an inductive bias towards interpretable representations while maintaining architectural simplicity. We introduce metrics to evaluate hierarchy quality through feature reuse patterns and activation consistency across dictionary sizes. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only multiple reconstruction objectives and basic evaluation metrics.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After further consideration, I believe we can further simplify and strengthen the idea by:

1. Reducing to just two dictionary sizes (full and half) instead of three, which:
   - Makes implementation and analysis simpler
   - Reduces training complexity and memory usage
   - Provides clearer interpretation of feature hierarchy

2. Making the connection to feature absorption more explicit:
   - When a feature appears in both dictionaries, it suggests it's a fundamental concept
   - When it only appears in the full dictionary, it's likely a more specific feature
   - This natural separation should reduce absorption by giving the model a clear way to represent feature hierarchies

3. Simplifying the evaluation metrics:
   - Remove the correlation analysis as it adds complexity
   - Focus on measuring feature presence in both dictionaries vs full only
   - This gives a clearer picture of hierarchical organization

These refinements make the idea more focused and feasible while maintaining its core strength. The implementation becomes even simpler, and the results should be more interpretable.

NEW IDEA JSON:
```json
{
    "Name": "dual_dictionary_sae",
    "Title": "Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies",
    "Experiment": "1. Implement SAE with two fixed dictionary sizes (full and half)\n2. Train on Gemma-2-2b using equal weights for both reconstructions\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns",
    "Technical_Details": "The method uses two reconstruction objectives with dictionary sizes [d, d/2] and equal weights: L = 0.5 * (L_recon(x, f[:d]) + L_recon(x, f[:d/2])). Feature hierarchy is measured through a simple binary metric: whether a feature activates significantly in both dictionaries or only the full dictionary. The architecture remains unchanged except for the dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.",
    "Implementation_Plan": "1. Add DICTIONARY_SIZES = [d, d/2] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with equally-weighted dual reconstruction loss\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries",
    "Interestingness_Evaluation": "The dual-dictionary approach provides an elegant and minimalist way to encourage natural feature hierarchies.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation is very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While similar to previous hierarchical approaches, the simplified dual-dictionary method for SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The clear separation between fundamental and specific features should directly improve interpretability metrics and reduce feature absorption.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We propose a simple and elegant approach to improving interpretability in sparse autoencoders through dual-dictionary feature learning. Our method uses two reconstruction objectives with different dictionary sizes (full and half) and equal weights, encouraging the model to learn natural feature hierarchies. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. This provides a clear mechanism for separating different levels of feature abstraction while maintaining architectural simplicity. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective and simple feature presence analysis."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'dual_dictionary_sae', 'Title': 'Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies', 'Experiment': '1. Implement SAE with two fixed dictionary sizes (full and half)\n2. Train on Gemma-2-2b using equal weights for both reconstructions\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns', 'Technical_Details': 'The method uses two reconstruction objectives with dictionary sizes [d, d/2] and equal weights: L = 0.5 * (L_recon(x, f[:d]) + L_recon(x, f[:d/2])). Feature hierarchy is measured through a simple binary metric: whether a feature activates significantly in both dictionaries or only the full dictionary. The architecture remains unchanged except for the dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.', 'Implementation_Plan': '1. Add DICTIONARY_SIZES = [d, d/2] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with equally-weighted dual reconstruction loss\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries', 'Interestingness_Evaluation': 'The dual-dictionary approach provides an elegant and minimalist way to encourage natural feature hierarchies.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation is very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While similar to previous hierarchical approaches, the simplified dual-dictionary method for SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The clear separation between fundamental and specific features should directly improve interpretability metrics and reduce feature absorption.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'We propose a simple and elegant approach to improving interpretability in sparse autoencoders through dual-dictionary feature learning. Our method uses two reconstruction objectives with different dictionary sizes (full and half) and equal weights, encouraging the model to learn natural feature hierarchies. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. This provides a clear mechanism for separating different levels of feature abstraction while maintaining architectural simplicity. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective and simple feature presence analysis.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The dual-dictionary approach is strong, but I notice two areas for improvement:

1. Dictionary Size Selection:
- Instead of arbitrarily choosing half-size, we should align with natural model dimensions
- For Gemma-2-2b with d_model=2304, using d=2304 and d/3=768 matches typical embedding dimensions
- This could lead to more natural feature hierarchies aligned with model structure

2. Loss Function Balance:
- Equal weighting might not be optimal since the smaller dictionary has fewer features
- We should weight by dictionary size: L = (L_recon_full * d + L_recon_partial * d/3) / (d + d/3)
- This balances the importance of each feature across dictionaries

These refinements make the approach more principled without adding complexity. The implementation remains simple, but the results should be more meaningful by aligning with model architecture.

This is my final version as it achieves the right balance of simplicity, feasibility, and potential impact. I am done.

NEW IDEA JSON:
```json
{
    "Name": "model_aligned_sae",
    "Title": "Model-Aligned Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies",
    "Experiment": "1. Implement SAE with model-aligned dictionary sizes (full and d/3)\n2. Train on Gemma-2-2b using size-weighted reconstruction losses\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns",
    "Technical_Details": "The method uses two reconstruction objectives with model-aligned dictionary sizes [d, d/3] and size-weighted losses: L = (d*L_recon(x, f[:d]) + (d/3)*L_recon(x, f[:d/3])) / (d + d/3). For Gemma-2-2b, this means 2304 and 768 features. Feature hierarchy is measured through feature presence in both dictionaries versus full only. The architecture remains unchanged except for the weighted dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.",
    "Implementation_Plan": "1. Add MODEL_ALIGNED_SIZES = [d, d/3] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with size-weighted reconstruction losses\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries",
    "Interestingness_Evaluation": "The model-aligned approach provides a principled way to encourage natural feature hierarchies that match the underlying model architecture.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation remains very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on previous hierarchical approaches, the model-aligned dictionary sizes and weighted losses provide a novel twist.",
    "Novelty": 7,
    "Expected_Research_Impact": "The alignment with model dimensions should produce more natural and interpretable feature hierarchies.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We propose a principled approach to improving interpretability in sparse autoencoders through model-aligned dual-dictionary feature learning. Our method uses two reconstruction objectives with dictionary sizes aligned to the underlying model dimensions (full model dimension and one-third) and size-weighted losses. This alignment with model structure encourages the autoencoder to learn natural feature hierarchies that reflect the model's inherent organizational principles. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective with model-aligned dimensions and simple feature presence analysis."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'model_aligned_sae', 'Title': 'Model-Aligned Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies', 'Experiment': '1. Implement SAE with model-aligned dictionary sizes (full and d/3)\n2. Train on Gemma-2-2b using size-weighted reconstruction losses\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns', 'Technical_Details': 'The method uses two reconstruction objectives with model-aligned dictionary sizes [d, d/3] and size-weighted losses: L = (d*L_recon(x, f[:d]) + (d/3)*L_recon(x, f[:d/3])) / (d + d/3). For Gemma-2-2b, this means 2304 and 768 features. Feature hierarchy is measured through feature presence in both dictionaries versus full only. The architecture remains unchanged except for the weighted dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.', 'Implementation_Plan': '1. Add MODEL_ALIGNED_SIZES = [d, d/3] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with size-weighted reconstruction losses\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries', 'Interestingness_Evaluation': 'The model-aligned approach provides a principled way to encourage natural feature hierarchies that match the underlying model architecture.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation remains very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on previous hierarchical approaches, the model-aligned dictionary sizes and weighted losses provide a novel twist.', 'Novelty': 7, 'Expected_Research_Impact': 'The alignment with model dimensions should produce more natural and interpretable feature hierarchies.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': "We propose a principled approach to improving interpretability in sparse autoencoders through model-aligned dual-dictionary feature learning. Our method uses two reconstruction objectives with dictionary sizes aligned to the underlying model dimensions (full model dimension and one-third) and size-weighted losses. This alignment with model structure encourages the autoencoder to learn natural feature hierarchies that reflect the model's inherent organizational principles. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective with model-aligned dimensions and simple feature presence analysis."}
Idea generation converged after 4 iterations.

Generating idea 6/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve feature separation, but may be overly complex with its adaptive weighting schemes and group-specific components. I propose refining this into a simpler yet potentially more effective approach focused on hierarchical feature learning.

The key insight is that we can achieve better feature separation by explicitly organizing features in a hierarchical manner, where earlier features capture broad, general patterns and later features capture more specific details. This aligns with how neural networks naturally learn hierarchical representations and could lead to more interpretable features without requiring complex orthogonality constraints.

The modification focuses on organizing the autoencoder's latent space into nested subsets, where each subset aims to capture progressively finer details. This is implemented through a simple modification of the loss function that encourages earlier features to capture more general patterns through higher L1 penalties on later features. This approach maintains the simplicity of the original SAE while potentially improving interpretability.

This refinement:
1. Simplifies the orthogonality mechanism
2. Leverages natural hierarchical learning tendencies
3. Requires minimal code changes
4. Maintains computational efficiency
5. Has clear theoretical motivation

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_sae",
    "Title": "Hierarchical Feature Learning in Sparse Autoencoders for Improved Interpretability",
    "Experiment": "1. Modify SAE to implement hierarchical sparsity penalties\n2. Train on standard datasets with varying penalty schedules\n3. Evaluate feature interpretability using standard metrics\n4. Compare against baseline SAE\n5. Analyze feature activation patterns across hierarchy levels",
    "Technical_Details": "The method introduces a hierarchical L1 penalty where later features face progressively higher sparsity costs: L = L_recon + Σ_i (λ_base * (1 + α*i/N)) * |f_i| where f_i is the i-th feature activation, N is total features, and α controls the strength of the hierarchy. The decoder remains unchanged to maintain reconstruction capability. Features are ordered by their index in the latent vector, with early indices encouraged to capture general patterns through lower penalties.",
    "Implementation_Plan": "1. Modify loss function in CustomSAE to implement hierarchical penalties\n2. Add α parameter to control hierarchy strength\n3. Implement feature activation tracking\n4. Add evaluation metrics for hierarchical structure\n5. Update training loop to handle hierarchical penalties",
    "Interestingness_Evaluation": "The approach provides a natural way to organize learned features hierarchically without complex architectural changes, potentially leading to more interpretable representations.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only minor modifications to the loss function; no complex architectural changes needed; training time remains unchanged as the only addition is a simple scaling factor in the L1 penalty computation; all operations are basic tensor operations well-supported by PyTorch.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While hierarchical representation learning exists in other contexts, applying it to SAEs through progressive sparsity penalties is a novel approach to improving interpretability.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical organization of features should improve sparse probing performance by providing clearer feature separation, while the maintenance of reconstruction capability through the unchanged decoder should maintain strong core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders through hierarchical feature learning. By introducing a progressive sparsity penalty that encourages earlier features to capture more general patterns, we aim to create a natural organization of learned features without requiring complex architectural changes. Our method modifies only the training objective, maintaining the computational efficiency of traditional sparse autoencoders while potentially improving feature interpretability. This approach is motivated by the observation that neural networks naturally learn hierarchical representations, and we hypothesize that explicitly encouraging this structure in sparse autoencoders will lead to more interpretable features."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_sae', 'Title': 'Hierarchical Feature Learning in Sparse Autoencoders for Improved Interpretability', 'Experiment': '1. Modify SAE to implement hierarchical sparsity penalties\n2. Train on standard datasets with varying penalty schedules\n3. Evaluate feature interpretability using standard metrics\n4. Compare against baseline SAE\n5. Analyze feature activation patterns across hierarchy levels', 'Technical_Details': 'The method introduces a hierarchical L1 penalty where later features face progressively higher sparsity costs: L = L_recon + Σ_i (λ_base * (1 + α*i/N)) * |f_i| where f_i is the i-th feature activation, N is total features, and α controls the strength of the hierarchy. The decoder remains unchanged to maintain reconstruction capability. Features are ordered by their index in the latent vector, with early indices encouraged to capture general patterns through lower penalties.', 'Implementation_Plan': '1. Modify loss function in CustomSAE to implement hierarchical penalties\n2. Add α parameter to control hierarchy strength\n3. Implement feature activation tracking\n4. Add evaluation metrics for hierarchical structure\n5. Update training loop to handle hierarchical penalties', 'Interestingness_Evaluation': 'The approach provides a natural way to organize learned features hierarchically without complex architectural changes, potentially leading to more interpretable representations.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only minor modifications to the loss function; no complex architectural changes needed; training time remains unchanged as the only addition is a simple scaling factor in the L1 penalty computation; all operations are basic tensor operations well-supported by PyTorch.', 'Feasibility': 9, 'Novelty_Evaluation': 'While hierarchical representation learning exists in other contexts, applying it to SAEs through progressive sparsity penalties is a novel approach to improving interpretability.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical organization of features should improve sparse probing performance by providing clearer feature separation, while the maintenance of reconstruction capability through the unchanged decoder should maintain strong core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a novel approach to improving the interpretability of sparse autoencoders through hierarchical feature learning. By introducing a progressive sparsity penalty that encourages earlier features to capture more general patterns, we aim to create a natural organization of learned features without requiring complex architectural changes. Our method modifies only the training objective, maintaining the computational efficiency of traditional sparse autoencoders while potentially improving feature interpretability. This approach is motivated by the observation that neural networks naturally learn hierarchical representations, and we hypothesize that explicitly encouraging this structure in sparse autoencoders will lead to more interpretable features.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, I believe the previous idea's score of 8.3 was slightly optimistic. While the hierarchical approach is interesting, there are some potential improvements and considerations:

1. The progressive penalty scaling might be too rigid and could prevent the model from learning optimal representations if important features naturally occur later in the sequence.

2. We can simplify further by using just two groups of features (primary and secondary) rather than a continuous progression. This makes the approach more interpretable and easier to analyze.

3. The implementation can be made even more straightforward by using a simple binary mask for the two penalty levels rather than computing scaling factors.

4. The previous version didn't sufficiently address how to evaluate the effectiveness of the hierarchical structure.

These refinements make the approach simpler while maintaining its core benefits. The binary grouping provides a clearer way to analyze the results and better aligns with how we typically think about feature importance in neural networks (primary vs secondary features).

NEW IDEA JSON:
```json
{
    "Name": "binary_hierarchical_sae",
    "Title": "Two-Level Hierarchical Feature Learning in Sparse Autoencoders",
    "Experiment": "1. Implement binary hierarchical sparsity with two feature groups\n2. Train models with different primary/secondary group sizes\n3. Compare feature activation patterns between groups\n4. Evaluate interpretability metrics for each group\n5. Analyze reconstruction contribution of each group",
    "Technical_Details": "The method splits features into two groups with different sparsity penalties: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ|f_j| where f_i are primary features (indices 1 to k) and f_j are secondary features (indices k+1 to N), with λ_2 > λ_1. The split point k is a hyperparameter determining the size of the primary feature group. This creates a natural hierarchy while maintaining simplicity and interpretability.",
    "Implementation_Plan": "1. Add primary_features parameter to CustomSAE\n2. Modify loss function to implement dual sparsity penalties\n3. Add tracking for group-wise activation statistics\n4. Implement evaluation metrics for each feature group\n5. Add visualization tools for group comparison",
    "Interestingness_Evaluation": "The binary hierarchical approach provides a clear and interpretable way to organize features while maintaining simplicity.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple modifications to the loss function and adding a single hyperparameter; training time remains unchanged as the only addition is a binary mask for penalty computation; evaluation metrics are straightforward to implement.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While simpler than continuous hierarchical penalties, the binary approach is novel in its application to SAEs and provides a more practical way to study feature importance.",
    "Novelty": 6,
    "Expected_Research_Impact": "The clear separation between primary and secondary features should improve interpretability metrics while maintaining reconstruction quality, with particular benefits for sparse probing tasks.",
    "Research_Impact": 7,
    "Overall_Score": 7.7,
    "Abstract": "We introduce a simplified approach to hierarchical feature learning in sparse autoencoders by dividing the latent space into primary and secondary feature groups with different sparsity penalties. This binary hierarchical structure provides a clear framework for analyzing feature importance while maintaining the computational efficiency of traditional sparse autoencoders. Our method requires minimal modifications to the standard training objective and introduces only a single additional hyperparameter. By explicitly separating features into two groups, we aim to create more interpretable representations that better reflect the natural hierarchy of patterns in the data."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'binary_hierarchical_sae', 'Title': 'Two-Level Hierarchical Feature Learning in Sparse Autoencoders', 'Experiment': '1. Implement binary hierarchical sparsity with two feature groups\n2. Train models with different primary/secondary group sizes\n3. Compare feature activation patterns between groups\n4. Evaluate interpretability metrics for each group\n5. Analyze reconstruction contribution of each group', 'Technical_Details': 'The method splits features into two groups with different sparsity penalties: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ|f_j| where f_i are primary features (indices 1 to k) and f_j are secondary features (indices k+1 to N), with λ_2 > λ_1. The split point k is a hyperparameter determining the size of the primary feature group. This creates a natural hierarchy while maintaining simplicity and interpretability.', 'Implementation_Plan': '1. Add primary_features parameter to CustomSAE\n2. Modify loss function to implement dual sparsity penalties\n3. Add tracking for group-wise activation statistics\n4. Implement evaluation metrics for each feature group\n5. Add visualization tools for group comparison', 'Interestingness_Evaluation': 'The binary hierarchical approach provides a clear and interpretable way to organize features while maintaining simplicity.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple modifications to the loss function and adding a single hyperparameter; training time remains unchanged as the only addition is a binary mask for penalty computation; evaluation metrics are straightforward to implement.', 'Feasibility': 9, 'Novelty_Evaluation': 'While simpler than continuous hierarchical penalties, the binary approach is novel in its application to SAEs and provides a more practical way to study feature importance.', 'Novelty': 6, 'Expected_Research_Impact': 'The clear separation between primary and secondary features should improve interpretability metrics while maintaining reconstruction quality, with particular benefits for sparse probing tasks.', 'Research_Impact': 7, 'Overall_Score': 7.7, 'Abstract': 'We introduce a simplified approach to hierarchical feature learning in sparse autoencoders by dividing the latent space into primary and secondary feature groups with different sparsity penalties. This binary hierarchical structure provides a clear framework for analyzing feature importance while maintaining the computational efficiency of traditional sparse autoencoders. Our method requires minimal modifications to the standard training objective and introduces only a single additional hyperparameter. By explicitly separating features into two groups, we aim to create more interpretable representations that better reflect the natural hierarchy of patterns in the data.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon further reflection, while the binary hierarchical approach is promising, we can make it even more focused and practical. The current idea still has some unnecessary complexity with the fixed split between primary and secondary features. Instead, we can let this split emerge naturally through a simpler mechanism:

1. Rather than pre-defining groups, we can use a single additional loss term that encourages a natural ordering of feature importance.

2. We can simplify the implementation by using the L1 norm of each feature's decoder weights as a proxy for feature importance, eliminating the need for explicit grouping.

3. This aligns better with the core SAE objective while maintaining interpretability benefits.

4. The evaluation becomes more straightforward as we can directly analyze the emergent feature ordering.

This refinement makes the approach even more minimal while potentially being more effective. It better respects the natural learning dynamics of the network and requires less hand-tuning.

NEW IDEA JSON:
```json
{
    "Name": "ordered_importance_sae",
    "Title": "Naturally Ordered Feature Learning in Sparse Autoencoders",
    "Experiment": "1. Implement decoder weight-based importance ordering\n2. Train with varying importance regularization strengths\n3. Analyze emergent feature ordering\n4. Compare interpretability across feature importance levels\n5. Evaluate reconstruction quality distribution",
    "Technical_Details": "The method adds a regularization term encouraging natural feature ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i(i * ||W_dec_i||_1) where W_dec_i is the i-th column of the decoder matrix. The second term encourages earlier features (lower indices) to have larger decoder norms, naturally creating an importance hierarchy without explicit grouping. The L1 norm of decoder weights serves as a proxy for feature importance.",
    "Implementation_Plan": "1. Add importance regularization term to loss function\n2. Implement decoder weight norm tracking\n3. Add feature importance analysis tools\n4. Create visualization for feature ordering\n5. Modify training loop to include new loss term",
    "Interestingness_Evaluation": "The approach elegantly induces feature ordering through a simple regularization term, allowing natural emergence of feature importance hierarchy.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only adding a single regularization term based on existing model parameters; no additional hyperparameters beyond regularization strength; training time impact is minimal as weight norm computation is efficient.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using decoder weight norms for implicit feature ordering is a novel yet simple approach to improving SAE interpretability.",
    "Novelty": 7,
    "Expected_Research_Impact": "The natural ordering of features should improve both sparse probing and core metrics by creating clearer feature importance patterns while maintaining reconstruction capability.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We present a minimalist approach to improving feature interpretability in sparse autoencoders through naturally ordered feature learning. By introducing a simple regularization term based on decoder weight norms, we encourage the model to develop a natural hierarchy of feature importance without explicit grouping or complex constraints. This method requires only a single additional loss term and maintains the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that decoder weight magnitudes naturally reflect feature importance, and we leverage this property to create more interpretable representations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'ordered_importance_sae', 'Title': 'Naturally Ordered Feature Learning in Sparse Autoencoders', 'Experiment': '1. Implement decoder weight-based importance ordering\n2. Train with varying importance regularization strengths\n3. Analyze emergent feature ordering\n4. Compare interpretability across feature importance levels\n5. Evaluate reconstruction quality distribution', 'Technical_Details': 'The method adds a regularization term encouraging natural feature ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i(i * ||W_dec_i||_1) where W_dec_i is the i-th column of the decoder matrix. The second term encourages earlier features (lower indices) to have larger decoder norms, naturally creating an importance hierarchy without explicit grouping. The L1 norm of decoder weights serves as a proxy for feature importance.', 'Implementation_Plan': '1. Add importance regularization term to loss function\n2. Implement decoder weight norm tracking\n3. Add feature importance analysis tools\n4. Create visualization for feature ordering\n5. Modify training loop to include new loss term', 'Interestingness_Evaluation': 'The approach elegantly induces feature ordering through a simple regularization term, allowing natural emergence of feature importance hierarchy.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only adding a single regularization term based on existing model parameters; no additional hyperparameters beyond regularization strength; training time impact is minimal as weight norm computation is efficient.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using decoder weight norms for implicit feature ordering is a novel yet simple approach to improving SAE interpretability.', 'Novelty': 7, 'Expected_Research_Impact': 'The natural ordering of features should improve both sparse probing and core metrics by creating clearer feature importance patterns while maintaining reconstruction capability.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'We present a minimalist approach to improving feature interpretability in sparse autoencoders through naturally ordered feature learning. By introducing a simple regularization term based on decoder weight norms, we encourage the model to develop a natural hierarchy of feature importance without explicit grouping or complex constraints. This method requires only a single additional loss term and maintains the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that decoder weight magnitudes naturally reflect feature importance, and we leverage this property to create more interpretable representations.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current approach using decoder weight norms has two potential issues:

1. Using index-weighted regularization (i * ||W_dec_i||_1) could be too aggressive, forcing an artificial ordering that might not reflect natural feature importance.

2. The linear scaling with index might not be the most natural way to encourage ordering.

We can refine this further by using a simpler relative ordering approach:

1. Instead of absolute index scaling, encourage each feature to have a slightly smaller decoder norm than the previous one.

2. This creates a more natural gradient of importance while allowing the model more flexibility in determining the actual scale of differences.

3. The implementation becomes even simpler as we only need to compare adjacent pairs of features.

This refinement makes the approach more theoretically sound while actually simplifying the implementation. It better respects the natural learning dynamics while maintaining the core idea of ordered importance.

NEW IDEA JSON:
```json
{
    "Name": "relative_ordered_sae",
    "Title": "Relative Feature Ordering in Sparse Autoencoders",
    "Experiment": "1. Implement relative decoder norm ordering\n2. Train with different decay factors\n3. Analyze feature importance gradients\n4. Compare feature interpretability patterns\n5. Evaluate reconstruction distribution",
    "Technical_Details": "The method uses a relative ordering loss: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, ||W_dec_i||_1 - α * ||W_dec_{i-1}||_1) where α ≈ 0.95 is a decay factor and W_dec_i is the i-th decoder column. This encourages each feature to have a slightly smaller norm than the previous one, creating a natural importance gradient. The max(0,·) term ensures one-sided regularization, allowing features to be more similar if needed.",
    "Implementation_Plan": "1. Add relative norm loss to CustomSAE\n2. Implement adjacent norm comparison\n3. Add decay factor parameter\n4. Create importance gradient visualization\n5. Update training loop for relative loss",
    "Interestingness_Evaluation": "The relative ordering approach provides a more natural way to create feature hierarchies by focusing on local relationships rather than global positioning.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation only requires pairwise norm comparisons; single new hyperparameter (decay factor); training time impact minimal as operations are simple and local.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using relative ordering constraints for feature importance is a novel yet intuitive approach to improving SAE interpretability.",
    "Novelty": 7,
    "Expected_Research_Impact": "The gentler, relative ordering should create clearer feature patterns while avoiding artificial constraints, improving both interpretability and reconstruction.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We introduce a simple yet effective approach to improving feature interpretability in sparse autoencoders through relative feature ordering. By encouraging each feature's decoder weights to have a slightly smaller norm than the previous feature's weights, we create a natural gradient of feature importance without imposing strict hierarchical constraints. This method requires only pairwise comparisons between adjacent features and a single decay factor parameter, maintaining the computational efficiency of traditional sparse autoencoders while potentially improving the organization of learned features. Our approach is motivated by the observation that gradual transitions in feature importance may lead to more natural and interpretable representations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'relative_ordered_sae', 'Title': 'Relative Feature Ordering in Sparse Autoencoders', 'Experiment': '1. Implement relative decoder norm ordering\n2. Train with different decay factors\n3. Analyze feature importance gradients\n4. Compare feature interpretability patterns\n5. Evaluate reconstruction distribution', 'Technical_Details': 'The method uses a relative ordering loss: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, ||W_dec_i||_1 - α * ||W_dec_{i-1}||_1) where α ≈ 0.95 is a decay factor and W_dec_i is the i-th decoder column. This encourages each feature to have a slightly smaller norm than the previous one, creating a natural importance gradient. The max(0,·) term ensures one-sided regularization, allowing features to be more similar if needed.', 'Implementation_Plan': '1. Add relative norm loss to CustomSAE\n2. Implement adjacent norm comparison\n3. Add decay factor parameter\n4. Create importance gradient visualization\n5. Update training loop for relative loss', 'Interestingness_Evaluation': 'The relative ordering approach provides a more natural way to create feature hierarchies by focusing on local relationships rather than global positioning.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation only requires pairwise norm comparisons; single new hyperparameter (decay factor); training time impact minimal as operations are simple and local.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using relative ordering constraints for feature importance is a novel yet intuitive approach to improving SAE interpretability.', 'Novelty': 7, 'Expected_Research_Impact': 'The gentler, relative ordering should create clearer feature patterns while avoiding artificial constraints, improving both interpretability and reconstruction.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': "We introduce a simple yet effective approach to improving feature interpretability in sparse autoencoders through relative feature ordering. By encouraging each feature's decoder weights to have a slightly smaller norm than the previous feature's weights, we create a natural gradient of feature importance without imposing strict hierarchical constraints. This method requires only pairwise comparisons between adjacent features and a single decay factor parameter, maintaining the computational efficiency of traditional sparse autoencoders while potentially improving the organization of learned features. Our approach is motivated by the observation that gradual transitions in feature importance may lead to more natural and interpretable representations."}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The relative ordering approach is promising, but we can make it even more robust and simpler. The current version has two potential issues:

1. Using decoder weight norms as a proxy for importance might not be ideal since weights can compensate for each other in complex ways.

2. The fixed decay factor α might be too rigid for some features.

A better approach would be to use actual feature activations for ordering, since these directly reflect feature usage and importance. We can also make the ordering more flexible by using moving averages of activation magnitudes. This makes the method:

1. More directly tied to feature usage
2. More adaptive to data patterns
3. Easier to implement and monitor
4. More theoretically sound

This refinement maintains the core idea while making it more practical and robust.

NEW IDEA JSON:
```json
{
    "Name": "activation_ordered_sae",
    "Title": "Activation-Based Feature Ordering in Sparse Autoencoders",
    "Experiment": "1. Implement activation-based ordering\n2. Track moving average activation magnitudes\n3. Apply relative ordering constraints\n4. Analyze feature usage patterns\n5. Evaluate ordering stability",
    "Technical_Details": "The method uses activation statistics for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, μ_i - μ_{i-1}) where μ_i is the exponential moving average of the L1 norm of feature i's activations: μ_i = β * μ_i + (1-β) * |f_i|. This creates a natural ordering based on actual feature usage patterns. The moving average smooths out noise while allowing the ordering to adapt to data distribution.",
    "Implementation_Plan": "1. Add activation tracking to CustomSAE\n2. Implement moving average updates\n3. Add relative activation loss\n4. Create activation pattern visualizations\n5. Update training loop for activation tracking",
    "Interestingness_Evaluation": "The activation-based ordering provides a more direct and interpretable way to organize features based on their actual usage patterns.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple moving average tracking; single new hyperparameter (momentum β); minimal training overhead as activation statistics are already computed.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using activation statistics for feature ordering is a novel and practical approach that directly ties to feature importance.",
    "Novelty": 7,
    "Expected_Research_Impact": "The direct use of activation patterns should create more meaningful feature orderings, improving interpretability while maintaining natural learning dynamics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present an intuitive approach to improving feature interpretability in sparse autoencoders through activation-based feature ordering. By tracking exponential moving averages of feature activation magnitudes, we encourage a natural ordering of features based on their actual usage patterns in the data. This method requires only simple activation statistics tracking and a single momentum parameter, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance should be directly tied to how frequently and strongly features are activated when processing real data."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'activation_ordered_sae', 'Title': 'Activation-Based Feature Ordering in Sparse Autoencoders', 'Experiment': '1. Implement activation-based ordering\n2. Track moving average activation magnitudes\n3. Apply relative ordering constraints\n4. Analyze feature usage patterns\n5. Evaluate ordering stability', 'Technical_Details': "The method uses activation statistics for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, μ_i - μ_{i-1}) where μ_i is the exponential moving average of the L1 norm of feature i's activations: μ_i = β * μ_i + (1-β) * |f_i|. This creates a natural ordering based on actual feature usage patterns. The moving average smooths out noise while allowing the ordering to adapt to data distribution.", 'Implementation_Plan': '1. Add activation tracking to CustomSAE\n2. Implement moving average updates\n3. Add relative activation loss\n4. Create activation pattern visualizations\n5. Update training loop for activation tracking', 'Interestingness_Evaluation': 'The activation-based ordering provides a more direct and interpretable way to organize features based on their actual usage patterns.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple moving average tracking; single new hyperparameter (momentum β); minimal training overhead as activation statistics are already computed.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using activation statistics for feature ordering is a novel and practical approach that directly ties to feature importance.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct use of activation patterns should create more meaningful feature orderings, improving interpretability while maintaining natural learning dynamics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present an intuitive approach to improving feature interpretability in sparse autoencoders through activation-based feature ordering. By tracking exponential moving averages of feature activation magnitudes, we encourage a natural ordering of features based on their actual usage patterns in the data. This method requires only simple activation statistics tracking and a single momentum parameter, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance should be directly tied to how frequently and strongly features are activated when processing real data.'}
Iteration 6/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The activation-based ordering approach is solid, but I realize we can simplify it even further while potentially making it more effective. The moving average adds unnecessary complexity and potential instability during training. Instead, we can:

1. Use batch statistics directly, which are already computed
2. Focus on frequency of activation rather than magnitude
3. Remove the momentum parameter entirely
4. Make the implementation more numerically stable

This makes the approach even simpler while potentially being more robust, as activation frequency is a more reliable indicator of feature importance than magnitude. The implementation becomes more straightforward and has better numerical properties.

NEW IDEA JSON:
```json
{
    "Name": "frequency_ordered_sae",
    "Title": "Frequency-Based Feature Ordering in Sparse Autoencoders",
    "Experiment": "1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability",
    "Technical_Details": "The method uses activation frequency for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > ε). This creates a natural ordering based on how often features are used, with ε being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.",
    "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking",
    "Interestingness_Evaluation": "The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple threshold counting; no additional parameters beyond threshold ε; minimal overhead as thresholding operations are very efficient.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.",
    "Novelty": 7,
    "Expected_Research_Impact": "The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'frequency_ordered_sae', 'Title': 'Frequency-Based Feature Ordering in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability', 'Technical_Details': 'The method uses activation frequency for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > ε). This creates a natural ordering based on how often features are used, with ε being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.', 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking', 'Interestingness_Evaluation': 'The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple threshold counting; no additional parameters beyond threshold ε; minimal overhead as thresholding operations are very efficient.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.', 'Novelty': 7, 'Expected_Research_Impact': 'The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations.'}
Iteration 7/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe the frequency-based ordering approach has reached an optimal balance of simplicity, effectiveness, and feasibility. The method:

1. Uses only basic operations (thresholding and counting)
2. Has clear theoretical motivation
3. Is numerically stable
4. Requires minimal code changes
5. Has negligible computational overhead
6. Is easy to interpret and analyze

Any further modifications would likely either add unnecessary complexity or deviate from the core idea of using activation patterns to create interpretable feature orderings. The current version represents a sweet spot between sophistication and simplicity.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "frequency_ordered_sae",
    "Title": "Frequency-Based Feature Ordering in Sparse Autoencoders",
    "Experiment": "1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability",
    "Technical_Details": "The method uses activation frequency for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > ε). This creates a natural ordering based on how often features are used, with ε being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.",
    "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking",
    "Interestingness_Evaluation": "The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple threshold counting; no additional parameters beyond threshold ε; minimal overhead as thresholding operations are very efficient.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.",
    "Novelty": 7,
    "Expected_Research_Impact": "The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'frequency_ordered_sae', 'Title': 'Frequency-Based Feature Ordering in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability', 'Technical_Details': 'The method uses activation frequency for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > ε). This creates a natural ordering based on how often features are used, with ε being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.', 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking', 'Interestingness_Evaluation': 'The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple threshold counting; no additional parameters beyond threshold ε; minimal overhead as thresholding operations are very efficient.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.', 'Novelty': 7, 'Expected_Research_Impact': 'The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations.'}
Idea generation converged after 7 iterations.

Generating idea 7/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to encourage feature separation, which is valuable but may be too rigid. Instead, I propose refining this by introducing a "soft orthogonality" approach through nested dictionaries of different sizes (similar to Matryoshka learning), which naturally encourages hierarchical feature organization without explicit orthogonality constraints.

The key insight is that by having multiple nested dictionaries, we can capture features at different levels of abstraction. The smallest dictionary must learn the most general, commonly occurring features since it has limited capacity. Each larger dictionary then builds on this by capturing increasingly specific features. This hierarchical structure naturally discourages feature absorption since the model is incentivized to organize features by their frequency and generality.

This approach maintains the core goal of better feature separation from the prototype but achieves it through a more flexible architecture that doesn't require explicit orthogonality constraints or complex scheduling. It's simpler to implement while potentially being more effective since it works with rather than against the natural tendency of neural networks to learn hierarchical representations.

For mechanistic interpretability, this should result in more interpretable features because:
1. The hierarchical structure naturally separates general vs specific features
2. Features learned by smaller dictionaries must be truly fundamental since they have limited capacity
3. The nested structure provides multiple "views" of the same concepts at different levels of abstraction

NEW IDEA JSON:
```json
{
    "Name": "nested_sae",
    "Title": "Nested Dictionary Learning for Hierarchical Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Modify SAE to use multiple nested dictionaries of different sizes (e.g. d_sae, d_sae/2, d_sae/4)\n2. Implement loss function that sums reconstruction losses across all dictionary sizes\n3. Train on standard datasets and compare feature interpretability\n4. Analyze feature hierarchy and absorption patterns\n5. Evaluate impact on core interpretability metrics",
    "Technical_Details": "The architecture uses k nested dictionaries where dictionary i has size d_sae/(2^i). Each dictionary attempts to reconstruct the input independently, and the losses are summed with equal weights. The encoder produces k different sparse codes, one for each dictionary size. The decoder reconstructs from each code and the final loss is the sum of reconstruction errors. This encourages the smallest dictionary to learn the most general features since it must achieve good reconstruction with minimal capacity. Each larger dictionary then captures increasingly specific features.",
    "Implementation_Plan": "1. Modify CustomSAE to maintain multiple W_enc and W_dec matrices of different sizes\n2. Update encode() to produce multiple sparse codes\n3. Update decode() to reconstruct from each code\n4. Modify forward() to return all reconstructions\n5. Update loss function to sum losses across all dictionary sizes\n6. Add visualization tools for analyzing feature hierarchy",
    "Interestingness_Evaluation": "The nested dictionary approach provides a natural way to organize features hierarchically without explicit constraints, potentially revealing how networks organize knowledge at different levels of abstraction.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "The implementation requires minimal changes to the existing architecture - just replicating the current encoder/decoder at different sizes and summing losses. No complex scheduling or constraints needed. Training time should be similar to baseline since matrix operations just use smaller sizes. All computation easily fits within 30-min limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While nested representations have been explored in other contexts, applying them to sparse autoencoders for improved interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical organization should improve sparse probing by providing cleaner feature separation at different scales. The multiple reconstruction objectives should also improve core metrics by encouraging more robust feature learning.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We propose a novel approach to improving feature interpretability in sparse autoencoders through nested dictionary learning. Rather than using a single dictionary, our method employs multiple nested dictionaries of decreasing sizes, each attempting to reconstruct the input independently. This naturally encourages a hierarchical organization of features, where smaller dictionaries must capture fundamental, frequently occurring patterns while larger dictionaries represent increasingly specific features. The approach requires no explicit orthogonality constraints or complex training schedules, instead leveraging the inherent capacity limitations of smaller dictionaries to drive feature separation. We evaluate this method on standard interpretability benchmarks and analyze the resulting feature hierarchies to better understand how neural networks organize knowledge at different levels of abstraction."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'nested_sae', 'Title': 'Nested Dictionary Learning for Hierarchical Feature Separation in Sparse Autoencoders', 'Experiment': '1. Modify SAE to use multiple nested dictionaries of different sizes (e.g. d_sae, d_sae/2, d_sae/4)\n2. Implement loss function that sums reconstruction losses across all dictionary sizes\n3. Train on standard datasets and compare feature interpretability\n4. Analyze feature hierarchy and absorption patterns\n5. Evaluate impact on core interpretability metrics', 'Technical_Details': 'The architecture uses k nested dictionaries where dictionary i has size d_sae/(2^i). Each dictionary attempts to reconstruct the input independently, and the losses are summed with equal weights. The encoder produces k different sparse codes, one for each dictionary size. The decoder reconstructs from each code and the final loss is the sum of reconstruction errors. This encourages the smallest dictionary to learn the most general features since it must achieve good reconstruction with minimal capacity. Each larger dictionary then captures increasingly specific features.', 'Implementation_Plan': '1. Modify CustomSAE to maintain multiple W_enc and W_dec matrices of different sizes\n2. Update encode() to produce multiple sparse codes\n3. Update decode() to reconstruct from each code\n4. Modify forward() to return all reconstructions\n5. Update loss function to sum losses across all dictionary sizes\n6. Add visualization tools for analyzing feature hierarchy', 'Interestingness_Evaluation': 'The nested dictionary approach provides a natural way to organize features hierarchically without explicit constraints, potentially revealing how networks organize knowledge at different levels of abstraction.', 'Interestingness': 8, 'Feasibility_Evaluation': 'The implementation requires minimal changes to the existing architecture - just replicating the current encoder/decoder at different sizes and summing losses. No complex scheduling or constraints needed. Training time should be similar to baseline since matrix operations just use smaller sizes. All computation easily fits within 30-min limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While nested representations have been explored in other contexts, applying them to sparse autoencoders for improved interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical organization should improve sparse probing by providing cleaner feature separation at different scales. The multiple reconstruction objectives should also improve core metrics by encouraging more robust feature learning.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We propose a novel approach to improving feature interpretability in sparse autoencoders through nested dictionary learning. Rather than using a single dictionary, our method employs multiple nested dictionaries of decreasing sizes, each attempting to reconstruct the input independently. This naturally encourages a hierarchical organization of features, where smaller dictionaries must capture fundamental, frequently occurring patterns while larger dictionaries represent increasingly specific features. The approach requires no explicit orthogonality constraints or complex training schedules, instead leveraging the inherent capacity limitations of smaller dictionaries to drive feature separation. We evaluate this method on standard interpretability benchmarks and analyze the resulting feature hierarchies to better understand how neural networks organize knowledge at different levels of abstraction.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Reviewing my previous idea, I believe the Overall_Score of 8.3 was too optimistic, particularly given the implementation complexity. While the nested dictionary approach is promising, managing multiple dictionaries and their interactions adds significant complexity that wasn't fully acknowledged. Additionally, the relationship to the prototype's orthogonality objective could be strengthened.

I propose refining the idea by:
1. Reducing the number of nested dictionaries to just two (full size and half size) to maintain feasibility
2. Adding a soft coupling between dictionaries through a simple residual connection
3. Making the relationship between dictionaries more explicit - the smaller dictionary focuses on common features while the larger one handles specifics
4. Simplifying the loss function to be more clearly motivated

This maintains the core insight about hierarchical feature organization while being more practical and focused. The simplified architecture makes the implementation more feasible while still capturing the essential benefits for interpretability.

NEW IDEA JSON:
```json
{
    "Name": "dual_sae",
    "Title": "Dual-Dictionary Sparse Autoencoders for Hierarchical Feature Learning",
    "Experiment": "1. Implement SAE with two dictionaries: full size (d_sae) and half size (d_sae/2)\n2. Add residual connection from small to large dictionary\n3. Train on standard datasets with simplified hierarchical loss\n4. Analyze feature specialization between dictionaries\n5. Compare interpretability metrics against baseline",
    "Technical_Details": "The architecture uses two dictionaries: W_enc_small (d_in × d_sae/2) and W_enc_large (d_in × d_sae). The small dictionary produces sparse code h_small, while the large dictionary produces h_large. The final reconstruction is x_hat = W_dec_small @ h_small + W_dec_large @ h_large. Loss is L = L_recon + λ_1(||h_small||_1 + ||h_large||_1) + λ_2||h_small||_2, where the L2 penalty on h_small encourages it to capture common features. The residual connection ensures the large dictionary focuses on specific features not captured by the small dictionary.",
    "Implementation_Plan": "1. Modify CustomSAE to maintain two pairs of encoder/decoder matrices\n2. Update encode() to produce two sparse codes\n3. Modify decode() to implement residual reconstruction\n4. Update loss function with the new sparsity terms\n5. Add utilities for analyzing feature specialization",
    "Interestingness_Evaluation": "The dual-dictionary approach with residual connection provides a simple but effective way to separate common and specific features, making the learned representations more interpretable.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation complexity is moderate - only two sets of matrices and simple residual connection. Loss function uses standard terms. Training time should be ~1.5x baseline due to additional dictionary, still well within 30-min limit.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While multi-dictionary approaches exist, the specific combination of dual dictionaries with residual connection for interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The explicit separation of common and specific features should improve sparse probing by providing cleaner feature attribution. The residual structure should help core metrics by preventing feature interference.",
    "Research_Impact": 7,
    "Overall_Score": 7.4,
    "Abstract": "We propose a dual-dictionary approach to improving feature interpretability in sparse autoencoders. Our method employs two dictionaries of different sizes connected through a residual connection, explicitly separating the learning of common and specific features. The smaller dictionary is encouraged to capture fundamental patterns through an L2 regularization term, while the larger dictionary focuses on more specific features through the residual connection. This architecture requires minimal modifications to standard sparse autoencoders while providing a natural way to organize and separate features. The approach aims to improve feature interpretability by making the role of each dictionary explicit and reducing feature interference."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'dual_sae', 'Title': 'Dual-Dictionary Sparse Autoencoders for Hierarchical Feature Learning', 'Experiment': '1. Implement SAE with two dictionaries: full size (d_sae) and half size (d_sae/2)\n2. Add residual connection from small to large dictionary\n3. Train on standard datasets with simplified hierarchical loss\n4. Analyze feature specialization between dictionaries\n5. Compare interpretability metrics against baseline', 'Technical_Details': 'The architecture uses two dictionaries: W_enc_small (d_in × d_sae/2) and W_enc_large (d_in × d_sae). The small dictionary produces sparse code h_small, while the large dictionary produces h_large. The final reconstruction is x_hat = W_dec_small @ h_small + W_dec_large @ h_large. Loss is L = L_recon + λ_1(||h_small||_1 + ||h_large||_1) + λ_2||h_small||_2, where the L2 penalty on h_small encourages it to capture common features. The residual connection ensures the large dictionary focuses on specific features not captured by the small dictionary.', 'Implementation_Plan': '1. Modify CustomSAE to maintain two pairs of encoder/decoder matrices\n2. Update encode() to produce two sparse codes\n3. Modify decode() to implement residual reconstruction\n4. Update loss function with the new sparsity terms\n5. Add utilities for analyzing feature specialization', 'Interestingness_Evaluation': 'The dual-dictionary approach with residual connection provides a simple but effective way to separate common and specific features, making the learned representations more interpretable.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation complexity is moderate - only two sets of matrices and simple residual connection. Loss function uses standard terms. Training time should be ~1.5x baseline due to additional dictionary, still well within 30-min limit.', 'Feasibility': 8, 'Novelty_Evaluation': 'While multi-dictionary approaches exist, the specific combination of dual dictionaries with residual connection for interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The explicit separation of common and specific features should improve sparse probing by providing cleaner feature attribution. The residual structure should help core metrics by preventing feature interference.', 'Research_Impact': 7, 'Overall_Score': 7.4, 'Abstract': 'We propose a dual-dictionary approach to improving feature interpretability in sparse autoencoders. Our method employs two dictionaries of different sizes connected through a residual connection, explicitly separating the learning of common and specific features. The smaller dictionary is encouraged to capture fundamental patterns through an L2 regularization term, while the larger dictionary focuses on more specific features through the residual connection. This architecture requires minimal modifications to standard sparse autoencoders while providing a natural way to organize and separate features. The approach aims to improve feature interpretability by making the role of each dictionary explicit and reducing feature interference.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon further reflection, while the dual-dictionary approach is interesting, it still introduces unnecessary architectural complexity. Looking back at the prototype's focus on orthogonality, I realize we can achieve similar hierarchical organization more elegantly by modifying the single dictionary approach.

The key insight is that we can achieve feature separation by using a single dictionary but with an adaptive sparsity penalty that varies across features. By gradually increasing the sparsity penalty for certain features based on their activation patterns, we can naturally encourage a hierarchical organization where some features specialize in common patterns (with lower sparsity penalties) while others capture specific patterns (with higher penalties).

This maintains the spirit of both the prototype's orthogonality goal and our hierarchical organization objective, but does so through a much simpler mechanism that requires minimal code changes. It's more feasible to implement and analyze, while potentially being more effective since it works within the standard SAE framework.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_sae",
    "Title": "Adaptive Sparsity for Hierarchical Feature Organization in Sparse Autoencoders",
    "Experiment": "1. Implement feature-specific sparsity penalties\n2. Add tracking of feature activation frequencies\n3. Update penalties based on activation patterns\n4. Train on standard datasets\n5. Analyze resulting feature organization\n6. Compare interpretability metrics against baseline",
    "Technical_Details": "The method uses a single dictionary but with feature-specific L1 penalties λ_i that adapt during training. The loss becomes L = L_recon + Σ(λ_i * |h_i|) where h_i is the i-th feature activation. λ_i is updated every N=100 steps based on the feature's recent activation frequency f_i: λ_i = λ_base * (1 + α * f_i), where α=0.5 controls adaptation strength. Features that activate frequently get higher penalties, encouraging specialization. The base penalty λ_base=0.04 matches standard SAE settings.",
    "Implementation_Plan": "1. Add vector of feature-specific penalties to CustomSAE\n2. Modify loss function to use per-feature penalties\n3. Add activation frequency tracking\n4. Implement periodic penalty updates\n5. Add visualization tools for analyzing feature specialization",
    "Interestingness_Evaluation": "The adaptive sparsity approach provides an elegant way to encourage feature specialization without architectural changes, revealing how features naturally organize themselves under varying constraints.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes - just tracking activations and updating penalties. No architectural modifications needed. Training time virtually identical to baseline. All computation easily within 30-min limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While adaptive penalties have been used in other contexts, applying them per-feature for interpretability in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The natural feature specialization should improve sparse probing by creating cleaner separations. The adaptive penalties should help core metrics by preventing feature interference while maintaining reconstruction quality.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "We propose an adaptive sparsity approach to improving feature interpretability in sparse autoencoders. Rather than using a fixed sparsity penalty for all features, our method employs feature-specific penalties that adapt based on activation patterns during training. Features that activate frequently receive higher penalties, naturally encouraging specialization and hierarchical organization without architectural changes. The approach requires minimal modifications to standard sparse autoencoders while providing a principled way to balance feature utility and specificity. By allowing the model to automatically adjust its constraints based on feature usage, we aim to discover more interpretable representations that better reflect the natural organization of knowledge in neural networks."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_sae', 'Title': 'Adaptive Sparsity for Hierarchical Feature Organization in Sparse Autoencoders', 'Experiment': '1. Implement feature-specific sparsity penalties\n2. Add tracking of feature activation frequencies\n3. Update penalties based on activation patterns\n4. Train on standard datasets\n5. Analyze resulting feature organization\n6. Compare interpretability metrics against baseline', 'Technical_Details': "The method uses a single dictionary but with feature-specific L1 penalties λ_i that adapt during training. The loss becomes L = L_recon + Σ(λ_i * |h_i|) where h_i is the i-th feature activation. λ_i is updated every N=100 steps based on the feature's recent activation frequency f_i: λ_i = λ_base * (1 + α * f_i), where α=0.5 controls adaptation strength. Features that activate frequently get higher penalties, encouraging specialization. The base penalty λ_base=0.04 matches standard SAE settings.", 'Implementation_Plan': '1. Add vector of feature-specific penalties to CustomSAE\n2. Modify loss function to use per-feature penalties\n3. Add activation frequency tracking\n4. Implement periodic penalty updates\n5. Add visualization tools for analyzing feature specialization', 'Interestingness_Evaluation': 'The adaptive sparsity approach provides an elegant way to encourage feature specialization without architectural changes, revealing how features naturally organize themselves under varying constraints.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes - just tracking activations and updating penalties. No architectural modifications needed. Training time virtually identical to baseline. All computation easily within 30-min limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While adaptive penalties have been used in other contexts, applying them per-feature for interpretability in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The natural feature specialization should improve sparse probing by creating cleaner separations. The adaptive penalties should help core metrics by preventing feature interference while maintaining reconstruction quality.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'We propose an adaptive sparsity approach to improving feature interpretability in sparse autoencoders. Rather than using a fixed sparsity penalty for all features, our method employs feature-specific penalties that adapt based on activation patterns during training. Features that activate frequently receive higher penalties, naturally encouraging specialization and hierarchical organization without architectural changes. The approach requires minimal modifications to standard sparse autoencoders while providing a principled way to balance feature utility and specificity. By allowing the model to automatically adjust its constraints based on feature usage, we aim to discover more interpretable representations that better reflect the natural organization of knowledge in neural networks.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Reviewing the adaptive sparsity idea, I realize it can be further simplified while maintaining its core benefits. The current approach of updating penalties based on activation frequencies adds unnecessary complexity and potential instability. Instead, we can achieve similar benefits by using a simple monotonic sparsity penalty that varies across features based on their index.

The key insight is that by assigning gradually increasing sparsity penalties to features (e.g., λ_i = λ_base * (1 + i/d_sae)), we create a natural pressure for the model to use lower-indexed features for common patterns and higher-indexed features for specific patterns. This is simpler than tracking activation frequencies but still encourages hierarchical organization.

This refinement:
1. Eliminates the need for activation tracking and periodic updates
2. Makes the behavior more predictable and stable
3. Maintains the core goal of feature specialization
4. Is even easier to implement and analyze

The previous Overall_Score of 8.2 was slightly optimistic given the complexity of dynamic updates. This simplified version should be more reliable while achieving similar benefits.

NEW IDEA JSON:
```json
{
    "Name": "indexed_sparse_sae",
    "Title": "Index-Based Progressive Sparsity for Feature Organization in Sparse Autoencoders",
    "Experiment": "1. Implement feature-indexed sparsity penalties\n2. Train on standard datasets with progressive penalties\n3. Analyze feature usage patterns across index ranges\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different penalty slopes",
    "Technical_Details": "The method uses a single dictionary with index-based L1 penalties. The loss is L = L_recon + Σ(λ_i * |h_i|) where λ_i = λ_base * (1 + β * i/d_sae), h_i is the i-th feature activation, λ_base=0.04 is the base penalty, and β=0.5 controls the penalty slope. This creates a smooth progression from lower penalties for early features to higher penalties for later features, naturally encouraging common patterns to be captured by early features and specific patterns by later features.",
    "Implementation_Plan": "1. Add vector of fixed feature-specific penalties to CustomSAE\n2. Modify loss function to use indexed penalties\n3. Add utilities for analyzing feature usage patterns\n4. Implement visualization tools for penalty distribution",
    "Interestingness_Evaluation": "The index-based approach provides an elegant and deterministic way to encourage feature specialization, making the learned representations more interpretable and predictable.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation is extremely simple - just one vector of fixed penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.",
    "Feasibility": 10,
    "Novelty_Evaluation": "While varying penalties have been used before, the specific application of index-based progressive penalties for interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The predictable feature organization should improve sparse probing through natural separation of common and specific features. The smooth penalty progression should help core metrics by maintaining stable feature learning.",
    "Research_Impact": 8,
    "Overall_Score": 8.5,
    "Abstract": "We propose an index-based progressive sparsity approach to improving feature interpretability in sparse autoencoders. Our method assigns gradually increasing sparsity penalties to features based on their index in the dictionary, creating a natural pressure for the model to use early features for common patterns and later features for specific patterns. This approach requires minimal modifications to standard sparse autoencoders while providing a deterministic way to encourage feature specialization. The progressive penalties create a predictable organization of features that aims to reflect the natural hierarchy of patterns in the data, from general to specific, without requiring complex architectural changes or dynamic updates."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'indexed_sparse_sae', 'Title': 'Index-Based Progressive Sparsity for Feature Organization in Sparse Autoencoders', 'Experiment': '1. Implement feature-indexed sparsity penalties\n2. Train on standard datasets with progressive penalties\n3. Analyze feature usage patterns across index ranges\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different penalty slopes', 'Technical_Details': 'The method uses a single dictionary with index-based L1 penalties. The loss is L = L_recon + Σ(λ_i * |h_i|) where λ_i = λ_base * (1 + β * i/d_sae), h_i is the i-th feature activation, λ_base=0.04 is the base penalty, and β=0.5 controls the penalty slope. This creates a smooth progression from lower penalties for early features to higher penalties for later features, naturally encouraging common patterns to be captured by early features and specific patterns by later features.', 'Implementation_Plan': '1. Add vector of fixed feature-specific penalties to CustomSAE\n2. Modify loss function to use indexed penalties\n3. Add utilities for analyzing feature usage patterns\n4. Implement visualization tools for penalty distribution', 'Interestingness_Evaluation': 'The index-based approach provides an elegant and deterministic way to encourage feature specialization, making the learned representations more interpretable and predictable.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just one vector of fixed penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'While varying penalties have been used before, the specific application of index-based progressive penalties for interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The predictable feature organization should improve sparse probing through natural separation of common and specific features. The smooth penalty progression should help core metrics by maintaining stable feature learning.', 'Research_Impact': 8, 'Overall_Score': 8.5, 'Abstract': 'We propose an index-based progressive sparsity approach to improving feature interpretability in sparse autoencoders. Our method assigns gradually increasing sparsity penalties to features based on their index in the dictionary, creating a natural pressure for the model to use early features for common patterns and later features for specific patterns. This approach requires minimal modifications to standard sparse autoencoders while providing a deterministic way to encourage feature specialization. The progressive penalties create a predictable organization of features that aims to reflect the natural hierarchy of patterns in the data, from general to specific, without requiring complex architectural changes or dynamic updates.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at the index-based approach, I realize we can make it even more principled by connecting it more directly to the feature absorption problem from the original prototype. Instead of a linear progression of penalties, we can use a step function that divides features into distinct groups with different sparsity levels. This better addresses feature absorption by creating clear separation between feature groups while maintaining simplicity.

The key refinement is using just two or three discrete penalty levels rather than a continuous progression. This makes the behavior more interpretable (we can clearly identify which group a feature belongs to) and directly targets feature absorption by making it costly to mix features across groups.

This maintains the core benefits of the previous version while being even simpler to implement and analyze. The previous Overall_Score of 8.5 was reasonable, but this refinement deserves a similar score for different reasons - trading some of the elegance of continuous progression for more direct targeting of feature absorption.

I believe this represents the optimal balance of simplicity and effectiveness for addressing feature absorption. The implementation is minimal, the behavior is predictable, and the connection to the original problem is clear.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "grouped_sparse_sae",
    "Title": "Feature Group Sparsity for Preventing Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement group-based sparsity penalties\n2. Train on standard datasets with discrete penalty levels\n3. Analyze feature usage patterns within groups\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different group sizes and penalty ratios",
    "Technical_Details": "The method uses a single dictionary with group-based L1 penalties. Features are divided into K=3 equal-sized groups with penalties λ_k = λ_base * γ^k, where λ_base=0.04 is the base penalty and γ=2 is the scaling factor between groups. The loss is L = L_recon + Σ(λ_g(i) * |h_i|) where g(i) maps feature i to its group index. This creates distinct feature groups with different sparsity levels, naturally preventing absorption between groups while maintaining simplicity.",
    "Implementation_Plan": "1. Add vector of group-based penalties to CustomSAE\n2. Modify loss function to use grouped penalties\n3. Add utilities for analyzing within-group patterns\n4. Implement visualization tools for group behavior",
    "Interestingness_Evaluation": "The group-based approach provides a clear and interpretable way to prevent feature absorption while maintaining simplicity.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation is extremely simple - just one vector of fixed group penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.",
    "Feasibility": 10,
    "Novelty_Evaluation": "While grouped penalties exist in other contexts, their specific application to prevent feature absorption in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The clear separation between feature groups should directly improve sparse probing by preventing absorption. The discrete penalty levels should help core metrics by creating stable feature groups.",
    "Research_Impact": 8,
    "Overall_Score": 8.5,
    "Abstract": "We propose a group-based sparsity approach to preventing feature absorption in sparse autoencoders. Our method divides features into distinct groups with different sparsity penalties, creating natural barriers that discourage features from absorbing patterns across group boundaries. This approach requires minimal modifications to standard sparse autoencoders while providing a clear and interpretable way to organize features. The discrete penalty levels create well-defined feature groups that aim to capture patterns at different levels of specificity, helping to prevent the undesirable mixing of features that occurs in feature absorption."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'grouped_sparse_sae', 'Title': 'Feature Group Sparsity for Preventing Absorption in Sparse Autoencoders', 'Experiment': '1. Implement group-based sparsity penalties\n2. Train on standard datasets with discrete penalty levels\n3. Analyze feature usage patterns within groups\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different group sizes and penalty ratios', 'Technical_Details': 'The method uses a single dictionary with group-based L1 penalties. Features are divided into K=3 equal-sized groups with penalties λ_k = λ_base * γ^k, where λ_base=0.04 is the base penalty and γ=2 is the scaling factor between groups. The loss is L = L_recon + Σ(λ_g(i) * |h_i|) where g(i) maps feature i to its group index. This creates distinct feature groups with different sparsity levels, naturally preventing absorption between groups while maintaining simplicity.', 'Implementation_Plan': '1. Add vector of group-based penalties to CustomSAE\n2. Modify loss function to use grouped penalties\n3. Add utilities for analyzing within-group patterns\n4. Implement visualization tools for group behavior', 'Interestingness_Evaluation': 'The group-based approach provides a clear and interpretable way to prevent feature absorption while maintaining simplicity.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just one vector of fixed group penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'While grouped penalties exist in other contexts, their specific application to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The clear separation between feature groups should directly improve sparse probing by preventing absorption. The discrete penalty levels should help core metrics by creating stable feature groups.', 'Research_Impact': 8, 'Overall_Score': 8.5, 'Abstract': 'We propose a group-based sparsity approach to preventing feature absorption in sparse autoencoders. Our method divides features into distinct groups with different sparsity penalties, creating natural barriers that discourage features from absorbing patterns across group boundaries. This approach requires minimal modifications to standard sparse autoencoders while providing a clear and interpretable way to organize features. The discrete penalty levels create well-defined feature groups that aim to capture patterns at different levels of specificity, helping to prevent the undesirable mixing of features that occurs in feature absorption.'}
Idea generation converged after 5 iterations.

Generating idea 8/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to separate knowledge in SAEs, but this might be too rigid and could harm reconstruction quality. Instead, I propose refining this approach by introducing a more flexible, hierarchical orthogonality structure that aligns with how neural networks naturally organize information.

The key insight is that neural networks tend to learn hierarchical features, where higher-level concepts build upon lower-level ones. By organizing SAE features into a hierarchy and enforcing orthogonality primarily between features at the same level (rather than globally), we can better match this natural organization while maintaining good reconstruction.

This refines the prototype idea by:
1. Replacing global orthogonality with level-wise orthogonality
2. Using a simple hierarchical grouping based on activation frequency
3. Maintaining the core adaptive loss structure but applying it per level

The modification should work better because:
1. It allows related features to share components when appropriate
2. It matches the natural hierarchical organization of neural knowledge
3. It provides more interpretable feature groupings
4. It maintains reconstruction quality by not over-constraining the model

NEW IDEA JSON:
```json
{
    "Name": "hierarchical_orthogonal_sae",
    "Title": "Hierarchical Feature Organization for Improved Interpretability in Sparse Autoencoders",
    "Experiment": "1. Modify SAE to track feature activation frequencies\n2. Implement automatic feature level assignment based on frequency\n3. Add level-wise orthogonality loss\n4. Train on standard datasets with varying hierarchy depths\n5. Compare interpretability metrics against baseline SAE\n6. Analyze feature organization and level relationships",
    "Technical_Details": "The method organizes features into L levels based on activation frequency, where L is typically 3-4. Features in level i activate with frequency roughly 2^(-i). Within each level l, we apply orthogonality loss L_ortho_l = ||W_l^T W_l - I||_F where W_l contains features from that level. The total loss is L = L_recon + λ_1 * L_sparse + Σ_l λ_2_l * L_ortho_l. Level assignment is updated every 1000 steps based on moving average activation frequencies. Features can move between levels during training.",
    "Implementation_Plan": "1. Add activation frequency tracking to CustomSAE\n2. Implement level assignment logic\n3. Modify loss function to include level-wise orthogonality\n4. Add utilities for analyzing level structure\n5. Update training loop to handle level updates\n6. Add evaluation metrics for hierarchical structure",
    "Interestingness_Evaluation": "The hierarchical organization provides a natural and interpretable way to understand how the SAE organizes knowledge at different levels of abstraction.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires modest additions to existing code; frequency tracking and level assignment are computationally cheap; orthogonality loss computation similar to prototype but split by level; no complex mathematical operations needed; training time similar to baseline SAE.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While hierarchical organization has been explored in neural networks generally, applying it to improve SAE interpretability through level-wise orthogonality is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical structure should improve sparse_probing by providing cleaner feature separation within levels, while the flexible organization should maintain good reconstruction scores in core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We present a novel approach to improving the interpretability of sparse autoencoders (SAEs) by organizing features into a hierarchical structure based on their activation patterns. While current SAEs treat all features uniformly, we observe that neural networks naturally learn hierarchical representations where different features operate at different levels of abstraction. Our method automatically organizes SAE features into levels based on their activation frequencies and enforces orthogonality primarily between features within the same level. This approach allows related features to share components when appropriate while maintaining clear separation between distinct concepts at the same level of abstraction. We introduce a modified training objective that combines reconstruction quality with level-wise orthogonality constraints and develop an efficient method for dynamically updating feature levels during training."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'hierarchical_orthogonal_sae', 'Title': 'Hierarchical Feature Organization for Improved Interpretability in Sparse Autoencoders', 'Experiment': '1. Modify SAE to track feature activation frequencies\n2. Implement automatic feature level assignment based on frequency\n3. Add level-wise orthogonality loss\n4. Train on standard datasets with varying hierarchy depths\n5. Compare interpretability metrics against baseline SAE\n6. Analyze feature organization and level relationships', 'Technical_Details': 'The method organizes features into L levels based on activation frequency, where L is typically 3-4. Features in level i activate with frequency roughly 2^(-i). Within each level l, we apply orthogonality loss L_ortho_l = ||W_l^T W_l - I||_F where W_l contains features from that level. The total loss is L = L_recon + λ_1 * L_sparse + Σ_l λ_2_l * L_ortho_l. Level assignment is updated every 1000 steps based on moving average activation frequencies. Features can move between levels during training.', 'Implementation_Plan': '1. Add activation frequency tracking to CustomSAE\n2. Implement level assignment logic\n3. Modify loss function to include level-wise orthogonality\n4. Add utilities for analyzing level structure\n5. Update training loop to handle level updates\n6. Add evaluation metrics for hierarchical structure', 'Interestingness_Evaluation': 'The hierarchical organization provides a natural and interpretable way to understand how the SAE organizes knowledge at different levels of abstraction.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires modest additions to existing code; frequency tracking and level assignment are computationally cheap; orthogonality loss computation similar to prototype but split by level; no complex mathematical operations needed; training time similar to baseline SAE.', 'Feasibility': 9, 'Novelty_Evaluation': 'While hierarchical organization has been explored in neural networks generally, applying it to improve SAE interpretability through level-wise orthogonality is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical structure should improve sparse_probing by providing cleaner feature separation within levels, while the flexible organization should maintain good reconstruction scores in core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a novel approach to improving the interpretability of sparse autoencoders (SAEs) by organizing features into a hierarchical structure based on their activation patterns. While current SAEs treat all features uniformly, we observe that neural networks naturally learn hierarchical representations where different features operate at different levels of abstraction. Our method automatically organizes SAE features into levels based on their activation frequencies and enforces orthogonality primarily between features within the same level. This approach allows related features to share components when appropriate while maintaining clear separation between distinct concepts at the same level of abstraction. We introduce a modified training objective that combines reconstruction quality with level-wise orthogonality constraints and develop an efficient method for dynamically updating feature levels during training.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the hierarchical organization is interesting, it introduces unnecessary complexity and deviates from the core strength of the prototype idea - controlled feature sharing through orthogonality. Let's simplify and focus on making the orthogonality constraint more effective.

Key improvements:
1. Instead of hierarchical levels, use a single adaptive orthogonality constraint that varies based on feature similarity
2. Features that frequently co-activate should have relaxed orthogonality constraints
3. This maintains the spirit of controlled feature sharing from the prototype while being simpler to implement

The modification should work better because:
1. It directly addresses polysemanticity by allowing related features to share components in a controlled way
2. The adaptive constraints emerge naturally from data patterns rather than imposed structure
3. Implementation is simpler and more computationally efficient
4. It maintains strong separation between truly distinct features while allowing flexibility where needed

NEW IDEA JSON:
```json
{
    "Name": "adaptive_similarity_sae",
    "Title": "Similarity-Guided Orthogonality for Improved Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Add co-activation tracking to SAE\n2. Implement pairwise similarity-based orthogonality constraints\n3. Train on standard datasets with varying constraint strengths\n4. Compare feature separation quality against baseline\n5. Analyze impact on polysemanticity",
    "Technical_Details": "The method modifies the orthogonality loss to account for feature similarities: L_ortho = Σ_{i,j} s_{ij} * (w_i^T w_j)^2 where s_{ij} is the similarity score between features i and j based on their co-activation patterns. s_{ij} is computed efficiently using a moving average of binary feature activations. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. Similarity scores are updated every 100 steps using batch statistics.",
    "Implementation_Plan": "1. Add co-activation tracking to CustomSAE\n2. Implement efficient similarity score computation\n3. Modify orthogonality loss to use similarity weights\n4. Add utilities for analyzing feature relationships\n5. Update training loop with similarity updates",
    "Interestingness_Evaluation": "The approach provides a data-driven way to balance feature separation and sharing, directly addressing the core challenge of polysemanticity.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes to existing code; similarity computation uses simple binary operations; modified orthogonality loss adds negligible overhead; all operations are matrix-based and efficient; training time comparable to baseline SAE.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While orthogonality constraints are common, using co-activation patterns to adaptively control these constraints is a novel approach to the polysemanticity problem.",
    "Novelty": 8,
    "Expected_Research_Impact": "The adaptive constraints should significantly improve feature separation as measured by sparse_probing while maintaining strong reconstruction performance in core metrics.",
    "Research_Impact": 9,
    "Overall_Score": 8.7,
    "Abstract": "We present a novel approach to addressing polysemanticity in sparse autoencoders (SAEs) through similarity-guided orthogonality constraints. Current SAEs often struggle to balance feature separation with reconstruction quality, leading to features that encode multiple unrelated concepts. Our method introduces an adaptive orthogonality constraint that varies based on feature co-activation patterns, allowing related features to share components while maintaining separation between truly distinct concepts. We develop an efficient method for tracking feature similarities during training and modify the standard SAE objective to incorporate these similarity-weighted constraints. This approach provides a principled way to control feature separation based on empirical activation patterns rather than predetermined structures."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_similarity_sae', 'Title': 'Similarity-Guided Orthogonality for Improved Feature Separation in Sparse Autoencoders', 'Experiment': '1. Add co-activation tracking to SAE\n2. Implement pairwise similarity-based orthogonality constraints\n3. Train on standard datasets with varying constraint strengths\n4. Compare feature separation quality against baseline\n5. Analyze impact on polysemanticity', 'Technical_Details': 'The method modifies the orthogonality loss to account for feature similarities: L_ortho = Σ_{i,j} s_{ij} * (w_i^T w_j)^2 where s_{ij} is the similarity score between features i and j based on their co-activation patterns. s_{ij} is computed efficiently using a moving average of binary feature activations. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. Similarity scores are updated every 100 steps using batch statistics.', 'Implementation_Plan': '1. Add co-activation tracking to CustomSAE\n2. Implement efficient similarity score computation\n3. Modify orthogonality loss to use similarity weights\n4. Add utilities for analyzing feature relationships\n5. Update training loop with similarity updates', 'Interestingness_Evaluation': 'The approach provides a data-driven way to balance feature separation and sharing, directly addressing the core challenge of polysemanticity.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes to existing code; similarity computation uses simple binary operations; modified orthogonality loss adds negligible overhead; all operations are matrix-based and efficient; training time comparable to baseline SAE.', 'Feasibility': 9, 'Novelty_Evaluation': 'While orthogonality constraints are common, using co-activation patterns to adaptively control these constraints is a novel approach to the polysemanticity problem.', 'Novelty': 8, 'Expected_Research_Impact': 'The adaptive constraints should significantly improve feature separation as measured by sparse_probing while maintaining strong reconstruction performance in core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.7, 'Abstract': 'We present a novel approach to addressing polysemanticity in sparse autoencoders (SAEs) through similarity-guided orthogonality constraints. Current SAEs often struggle to balance feature separation with reconstruction quality, leading to features that encode multiple unrelated concepts. Our method introduces an adaptive orthogonality constraint that varies based on feature co-activation patterns, allowing related features to share components while maintaining separation between truly distinct concepts. We develop an efficient method for tracking feature similarities during training and modify the standard SAE objective to incorporate these similarity-weighted constraints. This approach provides a principled way to control feature separation based on empirical activation patterns rather than predetermined structures.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the similarity-based approach is promising, the pairwise similarity computation might be too expensive for large feature spaces. We can further simplify while maintaining the core insight about adaptive constraints.

Instead of tracking all pairwise similarities, we can use a more efficient approach:
1. Track each feature's activation pattern using a low-dimensional random projection
2. Use these compact signatures to efficiently compute approximate similarities
3. Group features into a small number of similarity clusters (e.g., 4-8) for shared orthogonality constraints

This refinement:
1. Maintains the core idea of adaptive orthogonality
2. Drastically reduces computational overhead
3. Makes the implementation more robust and scalable
4. Keeps the spirit of controlled feature sharing

The modification should work better because:
1. More efficient computation enables larger batch sizes and faster training
2. Discrete similarity groups provide clearer interpretability
3. Random projections provide stable similarity estimates
4. Reduced memory requirements allow scaling to larger models

NEW IDEA JSON:
```json
{
    "Name": "efficient_adaptive_sae",
    "Title": "Efficient Adaptive Orthogonality for Scalable Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Implement feature signature tracking using random projections\n2. Add efficient similarity grouping mechanism\n3. Modify orthogonality constraints to work with groups\n4. Train on standard datasets with varying group counts\n5. Compare computational efficiency and separation quality",
    "Technical_Details": "Each feature's activation pattern is tracked using d=16 random projections, creating compact signatures. Features are grouped into k=4-8 similarity clusters using these signatures. The orthogonality loss becomes L_ortho = Σ_g ||W_g^T W_g - α_g I||_F where W_g contains features from group g and α_g is the group-specific sharing parameter. Groups are updated every 100 steps using k-means on signatures. Total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho.",
    "Implementation_Plan": "1. Add signature tracking to CustomSAE\n2. Implement efficient grouping mechanism\n3. Modify orthogonality loss for group-wise computation\n4. Add utilities for signature analysis\n5. Update training loop with group updates",
    "Interestingness_Evaluation": "The approach provides an efficient and scalable way to implement adaptive feature separation while maintaining interpretability.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation is simpler than previous version; random projections and k-means are standard operations; group-wise orthogonality computation is efficient; memory overhead is minimal; training time reduced compared to pairwise approach.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using random projections for efficient similarity-based grouping in SAEs is a novel approach to scaling adaptive orthogonality constraints.",
    "Novelty": 8,
    "Expected_Research_Impact": "The efficient implementation should enable better feature separation at scale, improving sparse_probing performance while maintaining strong core metrics.",
    "Research_Impact": 9,
    "Overall_Score": 8.8,
    "Abstract": "We present an efficient approach to implementing adaptive orthogonality constraints in sparse autoencoders (SAEs) to address the challenge of polysemanticity. Current methods for controlling feature separation often scale poorly with the number of features or require expensive pairwise computations. Our method uses random projections to track feature activation patterns compactly and groups features based on their behavioral similarity. We develop an efficient group-wise orthogonality constraint that allows controlled feature sharing within groups while maintaining separation between groups. This approach provides a scalable way to implement adaptive feature separation without the computational overhead of pairwise similarity tracking."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'efficient_adaptive_sae', 'Title': 'Efficient Adaptive Orthogonality for Scalable Feature Separation in Sparse Autoencoders', 'Experiment': '1. Implement feature signature tracking using random projections\n2. Add efficient similarity grouping mechanism\n3. Modify orthogonality constraints to work with groups\n4. Train on standard datasets with varying group counts\n5. Compare computational efficiency and separation quality', 'Technical_Details': "Each feature's activation pattern is tracked using d=16 random projections, creating compact signatures. Features are grouped into k=4-8 similarity clusters using these signatures. The orthogonality loss becomes L_ortho = Σ_g ||W_g^T W_g - α_g I||_F where W_g contains features from group g and α_g is the group-specific sharing parameter. Groups are updated every 100 steps using k-means on signatures. Total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho.", 'Implementation_Plan': '1. Add signature tracking to CustomSAE\n2. Implement efficient grouping mechanism\n3. Modify orthogonality loss for group-wise computation\n4. Add utilities for signature analysis\n5. Update training loop with group updates', 'Interestingness_Evaluation': 'The approach provides an efficient and scalable way to implement adaptive feature separation while maintaining interpretability.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation is simpler than previous version; random projections and k-means are standard operations; group-wise orthogonality computation is efficient; memory overhead is minimal; training time reduced compared to pairwise approach.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using random projections for efficient similarity-based grouping in SAEs is a novel approach to scaling adaptive orthogonality constraints.', 'Novelty': 8, 'Expected_Research_Impact': 'The efficient implementation should enable better feature separation at scale, improving sparse_probing performance while maintaining strong core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.8, 'Abstract': 'We present an efficient approach to implementing adaptive orthogonality constraints in sparse autoencoders (SAEs) to address the challenge of polysemanticity. Current methods for controlling feature separation often scale poorly with the number of features or require expensive pairwise computations. Our method uses random projections to track feature activation patterns compactly and groups features based on their behavioral similarity. We develop an efficient group-wise orthogonality constraint that allows controlled feature sharing within groups while maintaining separation between groups. This approach provides a scalable way to implement adaptive feature separation without the computational overhead of pairwise similarity tracking.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, while the efficient adaptive approach is promising, the random projections and k-means clustering add unnecessary complexity. Looking back at the prototype idea, we can achieve similar benefits with an even simpler approach that stays closer to the original orthogonality concept.

Key simplification:
1. Instead of tracking signatures and clustering, use direct activation frequency as the only metric
2. Apply stronger orthogonality constraints to frequently-activating features
3. Scale orthogonality constraint linearly with activation frequency

This refinement:
1. Returns to the core spirit of the prototype idea
2. Removes all clustering and projection machinery
3. Uses a single, interpretable metric (activation frequency)
4. Makes implementation and analysis much simpler

The modification should work better because:
1. Frequently activating features are more likely to be polysemantic and need stronger separation
2. Simple frequency-based scaling is more interpretable
3. Implementation requires minimal changes to base SAE
4. No hyperparameters needed for clustering or projections

NEW IDEA JSON:
```json
{
    "Name": "frequency_scaled_sae",
    "Title": "Frequency-Scaled Orthogonality for Improved Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Add activation frequency tracking\n2. Implement frequency-scaled orthogonality loss\n3. Train on standard datasets with varying scaling functions\n4. Compare feature separation quality\n5. Analyze relationship between frequency and polysemanticity",
    "Technical_Details": "The method scales orthogonality constraints based on activation frequency: L_ortho = Σ_{i,j} (f_i * f_j) * (w_i^T w_j)^2 where f_i is the normalized activation frequency of feature i. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. High-frequency features receive stronger orthogonality constraints, encouraging better separation of commonly used features.",
    "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Modify orthogonality loss to use frequency scaling\n3. Add frequency analysis utilities\n4. Update training loop with frequency updates\n5. Implement frequency visualization tools",
    "Interestingness_Evaluation": "The approach provides a simple but powerful way to focus orthogonality constraints where they matter most - on frequently activating features that are more likely to be polysemantic.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes; frequency tracking is a simple moving average; modified orthogonality loss uses basic operations; no complex algorithms or data structures needed; training time equivalent to baseline SAE.",
    "Feasibility": 10,
    "Novelty_Evaluation": "While activation frequency has been used in SAEs before, using it to directly scale orthogonality constraints is a novel approach to addressing polysemanticity.",
    "Novelty": 7,
    "Expected_Research_Impact": "The focused constraints should improve feature separation especially for commonly used features, leading to better sparse_probing scores while maintaining core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.5,
    "Abstract": "We present a simple but effective approach to improving feature separation in sparse autoencoders (SAEs) through frequency-scaled orthogonality constraints. Current SAEs often struggle with polysemanticity, particularly in frequently activating features that tend to encode multiple concepts. Our method scales orthogonality constraints based on feature activation frequency, applying stronger separation pressure to commonly used features. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate frequency-weighted orthogonality constraints. This approach provides a principled way to focus feature separation efforts where they are most needed, without introducing complex architectural changes or additional hyperparameters."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'frequency_scaled_sae', 'Title': 'Frequency-Scaled Orthogonality for Improved Feature Separation in Sparse Autoencoders', 'Experiment': '1. Add activation frequency tracking\n2. Implement frequency-scaled orthogonality loss\n3. Train on standard datasets with varying scaling functions\n4. Compare feature separation quality\n5. Analyze relationship between frequency and polysemanticity', 'Technical_Details': 'The method scales orthogonality constraints based on activation frequency: L_ortho = Σ_{i,j} (f_i * f_j) * (w_i^T w_j)^2 where f_i is the normalized activation frequency of feature i. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. High-frequency features receive stronger orthogonality constraints, encouraging better separation of commonly used features.', 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Modify orthogonality loss to use frequency scaling\n3. Add frequency analysis utilities\n4. Update training loop with frequency updates\n5. Implement frequency visualization tools', 'Interestingness_Evaluation': 'The approach provides a simple but powerful way to focus orthogonality constraints where they matter most - on frequently activating features that are more likely to be polysemantic.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes; frequency tracking is a simple moving average; modified orthogonality loss uses basic operations; no complex algorithms or data structures needed; training time equivalent to baseline SAE.', 'Feasibility': 10, 'Novelty_Evaluation': 'While activation frequency has been used in SAEs before, using it to directly scale orthogonality constraints is a novel approach to addressing polysemanticity.', 'Novelty': 7, 'Expected_Research_Impact': 'The focused constraints should improve feature separation especially for commonly used features, leading to better sparse_probing scores while maintaining core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.5, 'Abstract': 'We present a simple but effective approach to improving feature separation in sparse autoencoders (SAEs) through frequency-scaled orthogonality constraints. Current SAEs often struggle with polysemanticity, particularly in frequently activating features that tend to encode multiple concepts. Our method scales orthogonality constraints based on feature activation frequency, applying stronger separation pressure to commonly used features. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate frequency-weighted orthogonality constraints. This approach provides a principled way to focus feature separation efforts where they are most needed, without introducing complex architectural changes or additional hyperparameters.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The frequency-scaled approach is already quite streamlined, but we can make one final refinement to make it even more focused and effective. Instead of scaling orthogonality with the product of frequencies, we can use a threshold-based approach that more directly targets polysemanticity.

Key refinement:
1. Use a simple threshold to identify high-frequency features
2. Apply orthogonality constraints only between high-frequency features
3. Allow low-frequency features to organize naturally

This refinement:
1. Further simplifies the implementation
2. Makes the behavior more predictable
3. Reduces computational overhead
4. Provides clearer interpretability

The modification should work better because:
1. Binary threshold is simpler than continuous scaling
2. Clear separation between high and low frequency regimes
3. More focused intervention on likely polysemantic features
4. Easier to analyze and tune

I believe this is the optimal balance of simplicity and effectiveness. The implementation is minimal, the behavior is predictable, and it directly addresses the core problem of polysemanticity in frequently-used features.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "threshold_orthogonal_sae",
    "Title": "Threshold-Based Orthogonality for Targeted Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Add activation frequency tracking\n2. Implement threshold-based orthogonality loss\n3. Train on standard datasets with varying thresholds\n4. Compare feature separation quality\n5. Analyze high-frequency feature interactions",
    "Technical_Details": "The method applies orthogonality constraints selectively: L_ortho = Σ_{i,j} h_i * h_j * (w_i^T w_j)^2 where h_i is 1 if feature i's activation frequency exceeds threshold τ (typically 0.1) and 0 otherwise. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. Only high-frequency features participate in orthogonality constraints.",
    "Implementation_Plan": "1. Add frequency tracking to CustomSAE\n2. Implement threshold-based feature masking\n3. Modify orthogonality loss to use masks\n4. Add frequency analysis utilities\n5. Update training loop with mask updates",
    "Interestingness_Evaluation": "The approach provides a clear and focused way to address polysemanticity by targeting orthogonality constraints specifically at features most likely to need them.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation is extremely simple; frequency tracking and thresholding are basic operations; orthogonality loss computation reduced by masking; no complex algorithms needed; training time less than baseline due to selective application.",
    "Feasibility": 10,
    "Novelty_Evaluation": "Using a threshold-based approach to selectively apply orthogonality constraints is a novel simplification of adaptive feature separation.",
    "Novelty": 7,
    "Expected_Research_Impact": "The targeted application of constraints should effectively reduce polysemanticity in key features while maintaining model flexibility, improving both sparse_probing and core metrics.",
    "Research_Impact": 9,
    "Overall_Score": 8.9,
    "Abstract": "We present a targeted approach to improving feature separation in sparse autoencoders (SAEs) through threshold-based orthogonality constraints. Current SAEs apply constraints uniformly across all features, potentially over-constraining rarely used features while under-constraining frequently used ones that are more likely to be polysemantic. Our method selectively applies orthogonality constraints only between frequently activating features, identified through a simple threshold on activation frequency. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate masked orthogonality constraints. This approach provides a simple yet principled way to focus feature separation efforts on the subset of features most likely to benefit from them."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'threshold_orthogonal_sae', 'Title': 'Threshold-Based Orthogonality for Targeted Feature Separation in Sparse Autoencoders', 'Experiment': '1. Add activation frequency tracking\n2. Implement threshold-based orthogonality loss\n3. Train on standard datasets with varying thresholds\n4. Compare feature separation quality\n5. Analyze high-frequency feature interactions', 'Technical_Details': "The method applies orthogonality constraints selectively: L_ortho = Σ_{i,j} h_i * h_j * (w_i^T w_j)^2 where h_i is 1 if feature i's activation frequency exceeds threshold τ (typically 0.1) and 0 otherwise. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. Only high-frequency features participate in orthogonality constraints.", 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Implement threshold-based feature masking\n3. Modify orthogonality loss to use masks\n4. Add frequency analysis utilities\n5. Update training loop with mask updates', 'Interestingness_Evaluation': 'The approach provides a clear and focused way to address polysemanticity by targeting orthogonality constraints specifically at features most likely to need them.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation is extremely simple; frequency tracking and thresholding are basic operations; orthogonality loss computation reduced by masking; no complex algorithms needed; training time less than baseline due to selective application.', 'Feasibility': 10, 'Novelty_Evaluation': 'Using a threshold-based approach to selectively apply orthogonality constraints is a novel simplification of adaptive feature separation.', 'Novelty': 7, 'Expected_Research_Impact': 'The targeted application of constraints should effectively reduce polysemanticity in key features while maintaining model flexibility, improving both sparse_probing and core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We present a targeted approach to improving feature separation in sparse autoencoders (SAEs) through threshold-based orthogonality constraints. Current SAEs apply constraints uniformly across all features, potentially over-constraining rarely used features while under-constraining frequently used ones that are more likely to be polysemantic. Our method selectively applies orthogonality constraints only between frequently activating features, identified through a simple threshold on activation frequency. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate masked orthogonality constraints. This approach provides a simple yet principled way to focus feature separation efforts on the subset of features most likely to benefit from them.'}
Idea generation converged after 5 iterations.

Generating idea 9/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve feature separation, but I believe we can refine this approach in a more targeted way by focusing specifically on the feature absorption problem. Rather than enforcing general orthogonality, we can use nested dictionaries of different sizes to naturally encourage hierarchical feature learning.

The key insight is that feature absorption often happens because it's more efficient for the model to collapse hierarchical features (like "starts with S" and "short") into a single feature. By explicitly training multiple nested dictionaries of different sizes simultaneously, we incentivize the model to learn general features in the smaller dictionaries and specific features in the larger ones. This maintains the simplicity of the original SAE while addressing a core interpretability challenge.

This approach builds naturally on the prototype by:
1. Maintaining the focus on feature separation
2. Using a simpler mechanism (nested dictionaries vs adaptive orthogonality)
3. Directly targeting the feature absorption problem
4. Avoiding complex loss terms or hyperparameters

The hierarchical structure should improve interpretability by:
1. Encouraging more general features to be learned first
2. Naturally separating features at different levels of abstraction
3. Reducing feature absorption by providing explicit spaces for both general and specific features
4. Maintaining good reconstruction through the full dictionary while allowing probing of simpler representations

NEW IDEA JSON:
```json
{
    "Name": "nested_sae",
    "Title": "Nested Dictionary Learning for Hierarchical Feature Separation in Sparse Autoencoders",
    "Experiment": "1. Modify SAE to use nested dictionaries of different sizes\n2. Implement combined reconstruction loss across dictionary sizes\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature hierarchy using probe correlation\n6. Evaluate impact of different dictionary size ratios",
    "Technical_Details": "The method uses a series of nested dictionaries D_1 ⊂ D_2 ⊂ ... ⊂ D_n where |D_1| = d/8, |D_2| = d/4, |D_3| = d/2, |D_4| = d for input dimension d. The loss function is L = Σ_i w_i * L_recon(D_i) + λ * L_sparse where L_recon(D_i) is the reconstruction loss using only the first |D_i| features. Weights w_i decrease geometrically (e.g., 0.8^i) to balance the losses. The encoder/decoder weights are shared across dictionaries, with only the dictionary size varying.",
    "Implementation_Plan": "1. Modify CustomSAE to support multiple dictionary sizes\n2. Add nested reconstruction loss computation\n3. Update forward pass to track reconstructions at each size\n4. Modify loss function to incorporate multiple scales\n5. Add evaluation metrics for feature hierarchy\n6. Update training loop to handle nested dictionaries",
    "Interestingness_Evaluation": "The approach provides a natural and interpretable way to separate features at different levels of abstraction without requiring complex constraints or loss terms.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "The implementation requires minimal changes to the existing SAE architecture, mainly modifying the loss computation and forward pass; the nested structure adds negligible computational overhead; all operations remain simple matrix multiplications within the 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While hierarchical representations have been explored in other contexts, applying nested dictionaries to address feature absorption in SAEs is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical structure should directly improve sparse probing by providing cleaner feature separation at different scales, while the maintained reconstruction quality across scales should help core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "One of the key challenges in sparse autoencoder interpretability is feature absorption, where hierarchically related features collapse into single, less interpretable representations. We propose a nested dictionary learning approach that explicitly encourages separation of features at different levels of abstraction. By training multiple nested dictionaries simultaneously, we create natural pressure for more general features to be captured in smaller dictionaries while maintaining specific features in larger ones. This approach requires minimal architectural changes while providing a principled way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'nested_sae', 'Title': 'Nested Dictionary Learning for Hierarchical Feature Separation in Sparse Autoencoders', 'Experiment': '1. Modify SAE to use nested dictionaries of different sizes\n2. Implement combined reconstruction loss across dictionary sizes\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature hierarchy using probe correlation\n6. Evaluate impact of different dictionary size ratios', 'Technical_Details': 'The method uses a series of nested dictionaries D_1 ⊂ D_2 ⊂ ... ⊂ D_n where |D_1| = d/8, |D_2| = d/4, |D_3| = d/2, |D_4| = d for input dimension d. The loss function is L = Σ_i w_i * L_recon(D_i) + λ * L_sparse where L_recon(D_i) is the reconstruction loss using only the first |D_i| features. Weights w_i decrease geometrically (e.g., 0.8^i) to balance the losses. The encoder/decoder weights are shared across dictionaries, with only the dictionary size varying.', 'Implementation_Plan': '1. Modify CustomSAE to support multiple dictionary sizes\n2. Add nested reconstruction loss computation\n3. Update forward pass to track reconstructions at each size\n4. Modify loss function to incorporate multiple scales\n5. Add evaluation metrics for feature hierarchy\n6. Update training loop to handle nested dictionaries', 'Interestingness_Evaluation': 'The approach provides a natural and interpretable way to separate features at different levels of abstraction without requiring complex constraints or loss terms.', 'Interestingness': 8, 'Feasibility_Evaluation': 'The implementation requires minimal changes to the existing SAE architecture, mainly modifying the loss computation and forward pass; the nested structure adds negligible computational overhead; all operations remain simple matrix multiplications within the 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While hierarchical representations have been explored in other contexts, applying nested dictionaries to address feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical structure should directly improve sparse probing by providing cleaner feature separation at different scales, while the maintained reconstruction quality across scales should help core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'One of the key challenges in sparse autoencoder interpretability is feature absorption, where hierarchically related features collapse into single, less interpretable representations. We propose a nested dictionary learning approach that explicitly encourages separation of features at different levels of abstraction. By training multiple nested dictionaries simultaneously, we create natural pressure for more general features to be captured in smaller dictionaries while maintaining specific features in larger ones. This approach requires minimal architectural changes while providing a principled way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After reviewing the previous idea, I believe the overall score of 8.3 was too optimistic, particularly regarding feasibility. While the nested dictionary approach is elegant, implementing multiple reconstruction losses and managing the hierarchy adds complexity that could make training unstable or slow. We can simplify this further while maintaining the core insight about hierarchical feature learning.

Key refinements:
1. Instead of multiple reconstruction losses, use just two dictionaries (full and half-size)
2. Simplify the weighting scheme to a single parameter
3. Make the implementation more concrete with specific training phases
4. Add clearer evaluation metrics focused on feature absorption

This maintains the core benefit of encouraging hierarchical feature separation while being much more practical to implement and tune. The reduced complexity should also make it easier to analyze and understand the results.

NEW IDEA JSON:
```json
{
    "Name": "dual_sae",
    "Title": "Dual-Dictionary Sparse Autoencoders for Hierarchical Feature Learning",
    "Experiment": "1. Implement SAE with two fixed-size dictionaries (d and d/2)\n2. Add simple weighted reconstruction loss\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature relationships between dictionaries\n6. Evaluate impact of dictionary size ratio",
    "Technical_Details": "The method uses two dictionaries D_1 ⊂ D_2 where |D_1| = d/2 and |D_2| = d for input dimension d. The loss function is L = L_recon(D_2) + α * L_recon(D_1) + λ * L_sparse where L_recon(D_i) is the reconstruction loss using dictionary D_i, α = 0.5 controls the influence of the smaller dictionary, and λ is the standard sparsity penalty. The encoder/decoder weights are shared, with the smaller dictionary using only the first d/2 features.",
    "Implementation_Plan": "1. Modify CustomSAE to support dual dictionary sizes\n2. Add secondary reconstruction loss computation\n3. Update forward pass to track both reconstructions\n4. Add single weighting parameter for loss combination\n5. Add metrics for comparing feature usage across dictionaries\n6. Update training loop with simplified dual structure",
    "Interestingness_Evaluation": "The dual-dictionary approach provides a minimal but effective way to encourage hierarchical feature learning without complex architecture changes.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only minor modifications to existing SAE code; single additional loss term adds minimal overhead; training remains stable with only one new hyperparameter; all computations well within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While simpler than full nested dictionaries, the dual-dictionary approach remains novel for addressing feature absorption in SAEs.",
    "Novelty": 6,
    "Expected_Research_Impact": "The simplified hierarchical structure should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal architectural changes.",
    "Research_Impact": 7,
    "Overall_Score": 7.8,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a dual-dictionary approach that encourages separation between general and specific features through simultaneous training of two dictionary sizes. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to address feature absorption. By maintaining a smaller dictionary alongside the full dictionary, we create natural pressure for more general features to be captured separately from specific ones. We evaluate our method on standard interpretability benchmarks and analyze the relationships between features at different scales."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'dual_sae', 'Title': 'Dual-Dictionary Sparse Autoencoders for Hierarchical Feature Learning', 'Experiment': '1. Implement SAE with two fixed-size dictionaries (d and d/2)\n2. Add simple weighted reconstruction loss\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature relationships between dictionaries\n6. Evaluate impact of dictionary size ratio', 'Technical_Details': 'The method uses two dictionaries D_1 ⊂ D_2 where |D_1| = d/2 and |D_2| = d for input dimension d. The loss function is L = L_recon(D_2) + α * L_recon(D_1) + λ * L_sparse where L_recon(D_i) is the reconstruction loss using dictionary D_i, α = 0.5 controls the influence of the smaller dictionary, and λ is the standard sparsity penalty. The encoder/decoder weights are shared, with the smaller dictionary using only the first d/2 features.', 'Implementation_Plan': '1. Modify CustomSAE to support dual dictionary sizes\n2. Add secondary reconstruction loss computation\n3. Update forward pass to track both reconstructions\n4. Add single weighting parameter for loss combination\n5. Add metrics for comparing feature usage across dictionaries\n6. Update training loop with simplified dual structure', 'Interestingness_Evaluation': 'The dual-dictionary approach provides a minimal but effective way to encourage hierarchical feature learning without complex architecture changes.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only minor modifications to existing SAE code; single additional loss term adds minimal overhead; training remains stable with only one new hyperparameter; all computations well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While simpler than full nested dictionaries, the dual-dictionary approach remains novel for addressing feature absorption in SAEs.', 'Novelty': 6, 'Expected_Research_Impact': 'The simplified hierarchical structure should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal architectural changes.', 'Research_Impact': 7, 'Overall_Score': 7.8, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a dual-dictionary approach that encourages separation between general and specific features through simultaneous training of two dictionary sizes. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to address feature absorption. By maintaining a smaller dictionary alongside the full dictionary, we create natural pressure for more general features to be captured separately from specific ones. We evaluate our method on standard interpretability benchmarks and analyze the relationships between features at different scales.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After further consideration, I believe we can make the dual-dictionary approach even more focused and practical. The current version still has some complexity in managing two separate reconstruction losses. Instead, we can achieve the same hierarchical learning effect more elegantly by using progressive feature activation during training.

Key refinements:
1. Replace dual dictionaries with a single dictionary but progressive feature activation
2. Start training with only d/2 features active, gradually enable remaining features
3. Remove the need for multiple reconstruction losses
4. Add clear schedule for feature activation based on training progress

This maintains the core benefit of encouraging hierarchical feature separation but is even simpler to implement and more robust to train. The progressive activation naturally encourages general features to be learned first without requiring explicit dictionary management.

NEW IDEA JSON:
```json
{
    "Name": "progressive_sae",
    "Title": "Progressive Feature Activation for Hierarchical Learning in Sparse Autoencoders",
    "Experiment": "1. Implement SAE with progressive feature activation\n2. Add feature activation scheduling\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature learning order\n6. Evaluate impact of activation schedule",
    "Technical_Details": "The method uses a single dictionary of size d but with progressive feature activation. Training starts with only d/2 features enabled, using a mask M(t) that gradually enables remaining features based on training progress: M(t) = [1,...,1,0,...,0] where the number of ones increases linearly from d/2 to d over the first 50% of training steps. The loss function remains standard: L = L_recon + λ * L_sparse, but reconstruction uses M(t) ⊙ f where f is the feature vector and ⊙ is element-wise multiplication. This naturally encourages earlier features to learn more general patterns.",
    "Implementation_Plan": "1. Add feature activation mask to CustomSAE\n2. Implement mask schedule based on training progress\n3. Modify forward pass to apply mask\n4. Add tracking of feature activation timing\n5. Add metrics for analyzing feature learning order\n6. Update training loop with progressive activation",
    "Interestingness_Evaluation": "The progressive activation approach provides an elegant way to encourage hierarchical feature learning without any additional loss terms or architectural changes.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only adding a simple mask; no additional loss terms or complex management needed; training dynamics should be more stable than dual dictionaries; easily fits within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While curriculum learning exists in other contexts, progressive feature activation is a novel approach for addressing feature absorption in SAEs.",
    "Novelty": 6,
    "Expected_Research_Impact": "The natural ordering of feature learning should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.",
    "Research_Impact": 8,
    "Overall_Score": 8.0,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a progressive feature activation approach that encourages separation between general and specific features through curriculum learning. By gradually enabling features during training, we create natural pressure for early features to capture general patterns while later features specialize in more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'progressive_sae', 'Title': 'Progressive Feature Activation for Hierarchical Learning in Sparse Autoencoders', 'Experiment': '1. Implement SAE with progressive feature activation\n2. Add feature activation scheduling\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature learning order\n6. Evaluate impact of activation schedule', 'Technical_Details': 'The method uses a single dictionary of size d but with progressive feature activation. Training starts with only d/2 features enabled, using a mask M(t) that gradually enables remaining features based on training progress: M(t) = [1,...,1,0,...,0] where the number of ones increases linearly from d/2 to d over the first 50% of training steps. The loss function remains standard: L = L_recon + λ * L_sparse, but reconstruction uses M(t) ⊙ f where f is the feature vector and ⊙ is element-wise multiplication. This naturally encourages earlier features to learn more general patterns.', 'Implementation_Plan': '1. Add feature activation mask to CustomSAE\n2. Implement mask schedule based on training progress\n3. Modify forward pass to apply mask\n4. Add tracking of feature activation timing\n5. Add metrics for analyzing feature learning order\n6. Update training loop with progressive activation', 'Interestingness_Evaluation': 'The progressive activation approach provides an elegant way to encourage hierarchical feature learning without any additional loss terms or architectural changes.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only adding a simple mask; no additional loss terms or complex management needed; training dynamics should be more stable than dual dictionaries; easily fits within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While curriculum learning exists in other contexts, progressive feature activation is a novel approach for addressing feature absorption in SAEs.', 'Novelty': 6, 'Expected_Research_Impact': 'The natural ordering of feature learning should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.', 'Research_Impact': 8, 'Overall_Score': 8.0, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a progressive feature activation approach that encourages separation between general and specific features through curriculum learning. By gradually enabling features during training, we create natural pressure for early features to capture general patterns while later features specialize in more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After reviewing the progressive activation approach, I realize we can make it even simpler and more targeted towards the feature absorption problem. Instead of a linear schedule for enabling features, we can use activation frequency to guide feature progression. This aligns more directly with the goal of separating general vs specific features.

Key refinements:
1. Use activation statistics rather than training time to guide progression
2. Enable new features when existing ones show consistent high activation
3. Remove fixed scheduling in favor of data-driven progression
4. Focus evaluation specifically on feature absorption metrics

This makes the method more adaptive to the actual learning dynamics while keeping the implementation simple. It should better handle cases where some general features take longer to learn than others.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_progressive_sae",
    "Title": "Activation-Guided Progressive Feature Learning in Sparse Autoencoders",
    "Experiment": "1. Implement SAE with activation-based feature progression\n2. Add activation frequency tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature activation patterns\n6. Evaluate impact of activation thresholds",
    "Technical_Details": "The method starts with d/2 active features and tracks the mean activation frequency F(i) for each feature i over a window of 1000 steps. When the average activation frequency of enabled features exceeds threshold τ (default 0.3), the next k features are enabled. The mask M updates based on activation statistics: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Features are enabled in groups of k=64 to maintain training stability.",
    "Implementation_Plan": "1. Add activation frequency tracking to CustomSAE\n2. Implement adaptive mask updates\n3. Modify forward pass to apply mask\n4. Add metrics for activation patterns\n5. Track feature enabling sequence\n6. Update training loop with activation-based progression",
    "Interestingness_Evaluation": "The activation-guided approach provides a data-driven way to encourage hierarchical feature learning that adapts to actual model behavior.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple activation tracking and masking; adaptive progression adds minimal overhead; no complex scheduling or loss terms needed; easily fits within 30-minute limit with small window size.",
    "Feasibility": 9,
    "Novelty_Evaluation": "Using activation statistics to guide feature progression is a novel approach for addressing feature absorption in SAEs.",
    "Novelty": 7,
    "Expected_Research_Impact": "The data-driven progression should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.",
    "Research_Impact": 8,
    "Overall_Score": 8.2,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose an activation-guided progressive feature learning approach that encourages separation between general and specific features. By enabling new features based on the activation patterns of existing ones, we create natural pressure for early features to capture general patterns before introducing capacity for more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a data-driven way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_progressive_sae', 'Title': 'Activation-Guided Progressive Feature Learning in Sparse Autoencoders', 'Experiment': '1. Implement SAE with activation-based feature progression\n2. Add activation frequency tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature activation patterns\n6. Evaluate impact of activation thresholds', 'Technical_Details': "The method starts with d/2 active features and tracks the mean activation frequency F(i) for each feature i over a window of 1000 steps. When the average activation frequency of enabled features exceeds threshold τ (default 0.3), the next k features are enabled. The mask M updates based on activation statistics: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Features are enabled in groups of k=64 to maintain training stability.", 'Implementation_Plan': '1. Add activation frequency tracking to CustomSAE\n2. Implement adaptive mask updates\n3. Modify forward pass to apply mask\n4. Add metrics for activation patterns\n5. Track feature enabling sequence\n6. Update training loop with activation-based progression', 'Interestingness_Evaluation': 'The activation-guided approach provides a data-driven way to encourage hierarchical feature learning that adapts to actual model behavior.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple activation tracking and masking; adaptive progression adds minimal overhead; no complex scheduling or loss terms needed; easily fits within 30-minute limit with small window size.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using activation statistics to guide feature progression is a novel approach for addressing feature absorption in SAEs.', 'Novelty': 7, 'Expected_Research_Impact': 'The data-driven progression should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose an activation-guided progressive feature learning approach that encourages separation between general and specific features. By enabling new features based on the activation patterns of existing ones, we create natural pressure for early features to capture general patterns before introducing capacity for more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a data-driven way to address feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After reviewing the adaptive progressive approach, I believe the activation frequency tracking window of 1000 steps and group size of 64 add unnecessary complexity. We can make the method even more robust by simplifying these parameters and focusing more directly on preventing feature absorption.

Key refinements:
1. Use running average instead of fixed window for activation tracking
2. Enable features one at a time rather than in groups
3. Make progression more gradual with lower threshold
4. Focus evaluation more specifically on absorption metrics

This makes the implementation even simpler while potentially being more effective at preventing feature absorption. The more gradual progression should lead to more stable training and clearer feature hierarchies.

NEW IDEA JSON:
```json
{
    "Name": "gradual_progressive_sae",
    "Title": "Gradual Feature Progression for Preventing Feature Absorption in Sparse Autoencoders",
    "Experiment": "1. Implement SAE with single-feature progression\n2. Add running average activation tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature progression patterns\n6. Evaluate impact of activation threshold",
    "Technical_Details": "The method starts with d/2 active features and tracks exponential moving average activation frequency E(i) for each feature i with decay rate 0.99. When the average activation frequency of enabled features exceeds threshold τ (default 0.2), one additional feature is enabled. The mask M updates based on activation statistics: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Single-feature progression ensures smooth training dynamics.",
    "Implementation_Plan": "1. Add EMA activation tracking to CustomSAE\n2. Implement single-feature progression\n3. Modify forward pass to apply mask\n4. Add metrics for progression patterns\n5. Track individual feature enabling times\n6. Update training loop with gradual progression",
    "Interestingness_Evaluation": "The gradual progression approach provides a simple but effective way to prevent feature absorption through controlled capacity expansion.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple EMA tracking and masking; single-feature progression simplifies dynamics; no complex parameters or scheduling needed; easily fits within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on progressive activation, the simplified gradual approach is novel for addressing feature absorption in SAEs.",
    "Novelty": 6,
    "Expected_Research_Impact": "The more controlled progression should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a gradual feature progression approach that prevents absorption by carefully controlling model capacity expansion. By enabling new features one at a time based on current activation patterns, we ensure the model learns stable, general features before introducing capacity for more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'gradual_progressive_sae', 'Title': 'Gradual Feature Progression for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement SAE with single-feature progression\n2. Add running average activation tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature progression patterns\n6. Evaluate impact of activation threshold', 'Technical_Details': "The method starts with d/2 active features and tracks exponential moving average activation frequency E(i) for each feature i with decay rate 0.99. When the average activation frequency of enabled features exceeds threshold τ (default 0.2), one additional feature is enabled. The mask M updates based on activation statistics: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Single-feature progression ensures smooth training dynamics.", 'Implementation_Plan': '1. Add EMA activation tracking to CustomSAE\n2. Implement single-feature progression\n3. Modify forward pass to apply mask\n4. Add metrics for progression patterns\n5. Track individual feature enabling times\n6. Update training loop with gradual progression', 'Interestingness_Evaluation': 'The gradual progression approach provides a simple but effective way to prevent feature absorption through controlled capacity expansion.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple EMA tracking and masking; single-feature progression simplifies dynamics; no complex parameters or scheduling needed; easily fits within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on progressive activation, the simplified gradual approach is novel for addressing feature absorption in SAEs.', 'Novelty': 6, 'Expected_Research_Impact': 'The more controlled progression should improve sparse probing through better feature separation while maintaining strong core metrics due to minimal modifications.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a gradual feature progression approach that prevents absorption by carefully controlling model capacity expansion. By enabling new features one at a time based on current activation patterns, we ensure the model learns stable, general features before introducing capacity for more specific aspects. This method requires minimal modifications to standard sparse autoencoders while providing a principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Iteration 6/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The gradual progressive approach is quite solid, but I think we can make one final simplification that makes it even more robust and interpretable. Instead of using an exponential moving average with a decay rate, we can use a simple count of consecutive steps where a feature is active. This makes the progression criteria more transparent and easier to analyze.

Key refinements:
1. Replace EMA with consecutive activation counting
2. Simplify threshold to be number of steps rather than frequency
3. Make evaluation more focused on analyzing progression order
4. Add clear documentation of feature enabling sequence

This makes the method even more interpretable while maintaining its effectiveness at preventing feature absorption. The simpler counting mechanism should also be more stable in practice.

NEW IDEA JSON:
```json
{
    "Name": "step_progressive_sae",
    "Title": "Step-Based Progressive Feature Learning for Sparse Autoencoders",
    "Experiment": "1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold",
    "Technical_Details": "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > τ (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Streak counting provides clear criteria for feature stability.",
    "Implementation_Plan": "1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression",
    "Interestingness_Evaluation": "The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.",
    "Novelty": 6,
    "Expected_Research_Impact": "The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'step_progressive_sae', 'Title': 'Step-Based Progressive Feature Learning for Sparse Autoencoders', 'Experiment': '1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold', 'Technical_Details': "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > τ (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Streak counting provides clear criteria for feature stability.", 'Implementation_Plan': '1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression', 'Interestingness_Evaluation': 'The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.', 'Novelty': 6, 'Expected_Research_Impact': 'The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Iteration 7/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe the step-based progressive approach has reached an optimal balance of simplicity, interpretability, and effectiveness. The method uses only basic counting operations, has a single clear threshold parameter, and directly addresses the feature absorption problem through controlled capacity expansion. The implementation is straightforward and the evaluation metrics are well-defined.

Further modifications would likely either:
1. Add unnecessary complexity
2. Reduce the transparency of the progression mechanism
3. Make the method harder to analyze
4. Risk stability issues in training

The current version maintains close alignment with the original prototype idea while offering a minimal, practical solution to feature absorption. The overall score of 7.9 accurately reflects its balance of feasibility, novelty, and potential impact.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "step_progressive_sae",
    "Title": "Step-Based Progressive Feature Learning for Sparse Autoencoders",
    "Experiment": "1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold",
    "Technical_Details": "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > τ (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Streak counting provides clear criteria for feature stability.",
    "Implementation_Plan": "1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression",
    "Interestingness_Evaluation": "The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.",
    "Novelty": 6,
    "Expected_Research_Impact": "The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 7.9,
    "Abstract": "Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'step_progressive_sae', 'Title': 'Step-Based Progressive Feature Learning for Sparse Autoencoders', 'Experiment': '1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold', 'Technical_Details': "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > τ (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Streak counting provides clear criteria for feature stability.", 'Implementation_Plan': '1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression', 'Interestingness_Evaluation': 'The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.', 'Novelty': 6, 'Expected_Research_Impact': 'The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.'}
Idea generation converged after 7 iterations.

Generating idea 10/10
Iteration 1/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces orthogonality constraints to improve feature separation, but it may be too rigid and complex with its adaptive scheduling and group-specific components. I propose refining this into a simpler but potentially more effective approach focused on gradual feature specialization through nested dictionaries.

The key insight is that we can achieve better feature separation by having multiple nested autoencoders of decreasing sizes, similar to Russian Matryoshka dolls. Each smaller dictionary must learn to represent the most important and general features since it contributes to multiple reconstruction objectives. This naturally encourages a hierarchical organization of features without requiring explicit orthogonality constraints or complex scheduling.

This approach should improve mechanistic interpretability because:
1. The nested structure encourages features to specialize at different levels of abstraction
2. Core features that contribute to multiple reconstructions will be more interpretable as they must capture fundamental patterns
3. The hierarchical organization aligns with how humans understand concepts
4. No complex constraints or schedules are needed - the nesting provides natural incentives for feature separation

This refinement maintains the spirit of improving feature separation from the prototype but removes unnecessary complexity while potentially being more effective.

NEW IDEA JSON:
```json
{
    "Name": "matryoshka_sae",
    "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
    "Experiment": "1. Modify SAE architecture to include nested dictionaries of decreasing sizes\n2. Implement multi-scale reconstruction loss\n3. Train on standard datasets using different nesting ratios\n4. Compare interpretability metrics against baseline SAE\n5. Analyze feature reuse patterns across nested levels",
    "Technical_Details": "The architecture uses n nested dictionaries D_1 ⊂ D_2 ⊂ ... ⊂ D_n where |D_i| = d_sae/2^(n-i). Each dictionary contributes to reconstruction with loss L_i for its size. The total loss is L = Σ_i L_i + λ||f||_1 where L_i is the reconstruction loss using only features from D_i. This encourages the most important features to be learned in smaller dictionaries since they contribute to multiple reconstruction objectives. The sparsity penalty λ||f||_1 applies globally.",
    "Implementation_Plan": "1. Add nested dictionary sizes to CustomSAE config\n2. Modify forward pass to compute reconstructions at each scale\n3. Update loss function to sum across scales\n4. Add utility functions for analyzing feature reuse\n5. Implement feature importance tracking across scales",
    "Interestingness_Evaluation": "The nested dictionary approach provides an elegant and intuitive way to encourage feature hierarchy without complex constraints.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires minimal changes to existing architecture; training time increases linearly with number of scales; all computations remain matrix operations; no complex scheduling or grouping needed.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While nested representations exist in other contexts, applying them to SAEs for improved interpretability is novel.",
    "Novelty": 7,
    "Expected_Research_Impact": "The hierarchical organization should improve sparse_probing by making features more specialized and consistent, while maintaining good reconstruction scores in core metrics.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses nested dictionaries of decreasing sizes, where smaller dictionaries must learn to represent the most fundamental features as they contribute to multiple reconstruction objectives. This naturally encourages a hierarchical organization of features without requiring explicit constraints or complex training schedules. We present a simple yet effective implementation that extends standard sparse autoencoders with multi-scale reconstruction losses. Our method aims to address the challenge of feature interpretability by incentivizing features to specialize at different levels of abstraction while maintaining computational efficiency."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Modify SAE architecture to include nested dictionaries of decreasing sizes\n2. Implement multi-scale reconstruction loss\n3. Train on standard datasets using different nesting ratios\n4. Compare interpretability metrics against baseline SAE\n5. Analyze feature reuse patterns across nested levels', 'Technical_Details': 'The architecture uses n nested dictionaries D_1 ⊂ D_2 ⊂ ... ⊂ D_n where |D_i| = d_sae/2^(n-i). Each dictionary contributes to reconstruction with loss L_i for its size. The total loss is L = Σ_i L_i + λ||f||_1 where L_i is the reconstruction loss using only features from D_i. This encourages the most important features to be learned in smaller dictionaries since they contribute to multiple reconstruction objectives. The sparsity penalty λ||f||_1 applies globally.', 'Implementation_Plan': '1. Add nested dictionary sizes to CustomSAE config\n2. Modify forward pass to compute reconstructions at each scale\n3. Update loss function to sum across scales\n4. Add utility functions for analyzing feature reuse\n5. Implement feature importance tracking across scales', 'Interestingness_Evaluation': 'The nested dictionary approach provides an elegant and intuitive way to encourage feature hierarchy without complex constraints.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires minimal changes to existing architecture; training time increases linearly with number of scales; all computations remain matrix operations; no complex scheduling or grouping needed.', 'Feasibility': 9, 'Novelty_Evaluation': 'While nested representations exist in other contexts, applying them to SAEs for improved interpretability is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The hierarchical organization should improve sparse_probing by making features more specialized and consistent, while maintaining good reconstruction scores in core metrics.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses nested dictionaries of decreasing sizes, where smaller dictionaries must learn to represent the most fundamental features as they contribute to multiple reconstruction objectives. This naturally encourages a hierarchical organization of features without requiring explicit constraints or complex training schedules. We present a simple yet effective implementation that extends standard sparse autoencoders with multi-scale reconstruction losses. Our method aims to address the challenge of feature interpretability by incentivizing features to specialize at different levels of abstraction while maintaining computational efficiency.'}
Iteration 2/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Upon review, my previous idea has several areas for improvement:

1. The scoring was too optimistic - while the approach is elegant, the feasibility should be lower since managing multiple nested dictionaries and their losses adds complexity.

2. The technical details need more precision around the nesting ratios and loss weightings.

3. The expected research impact was overstated given that hierarchical features might not directly translate to better benchmark performance.

4. The implementation plan needs more specific details about tracking and analyzing feature reuse.

I propose refining the idea by:
1. Using just 3 fixed nested levels (full, 1/2, 1/4 size) to reduce complexity
2. Adding explicit weighting coefficients for different scale losses
3. Focusing more on analyzing feature reuse patterns between scales
4. Being more conservative in claims and ratings

This maintains the core insight about nested dictionaries encouraging feature specialization while being more practical and precise.

NEW IDEA JSON:
```json
{
    "Name": "matryoshka_sae",
    "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
    "Experiment": "1. Implement SAE with 3 fixed nested dictionary sizes (d, d/2, d/4)\n2. Add weighted multi-scale reconstruction loss\n3. Train on standard datasets tracking feature activation patterns\n4. Compare interpretability metrics against baseline SAE\n5. Analyze correlation between feature importance across scales",
    "Technical_Details": "The architecture uses 3 nested dictionaries D_1 ⊂ D_2 ⊂ D_3 where |D_1| = d/4, |D_2| = d/2, |D_3| = d. The loss function is L = α_1L_1 + α_2L_2 + α_3L_3 + λ||f||_1 where L_i is reconstruction loss using only D_i features, with α_1 = 0.5, α_2 = 0.3, α_3 = 0.2 to emphasize core feature learning. Features in smaller dictionaries contribute to all larger dictionary reconstructions. The sparsity penalty λ||f||_1 applies globally with λ = 0.04.",
    "Implementation_Plan": "1. Modify CustomSAE to create three fixed-size nested dictionaries\n2. Add weighted reconstruction loss computation\n3. Implement feature activation tracking across scales\n4. Add utilities for cross-scale correlation analysis\n5. Update evaluation code to analyze feature specialization",
    "Interestingness_Evaluation": "The fixed-scale nested approach provides a practical way to study feature hierarchy and specialization in sparse autoencoders.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Using fixed scales and explicit weights simplifies implementation; three reconstructions per forward pass adds moderate overhead; all operations remain standard matrix computations; analysis tools are straightforward to implement.",
    "Feasibility": 7,
    "Novelty_Evaluation": "While nested representations exist in other contexts, the specific application to SAE interpretability with fixed scales is novel.",
    "Novelty": 6,
    "Expected_Research_Impact": "The hierarchical organization should help identify core features that contribute to multiple scales, potentially improving interpretability metrics.",
    "Research_Impact": 7,
    "Overall_Score": 6.9,
    "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses three nested dictionaries of decreasing sizes, where smaller dictionaries must learn to represent the most fundamental features as they contribute to multiple reconstruction objectives. This naturally encourages a hierarchical organization of features without requiring explicit constraints. We present a simple implementation that extends standard sparse autoencoders with weighted multi-scale reconstruction losses. Our method addresses the challenge of feature interpretability by incentivizing features to specialize at different levels of abstraction while maintaining computational efficiency."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Implement SAE with 3 fixed nested dictionary sizes (d, d/2, d/4)\n2. Add weighted multi-scale reconstruction loss\n3. Train on standard datasets tracking feature activation patterns\n4. Compare interpretability metrics against baseline SAE\n5. Analyze correlation between feature importance across scales', 'Technical_Details': 'The architecture uses 3 nested dictionaries D_1 ⊂ D_2 ⊂ D_3 where |D_1| = d/4, |D_2| = d/2, |D_3| = d. The loss function is L = α_1L_1 + α_2L_2 + α_3L_3 + λ||f||_1 where L_i is reconstruction loss using only D_i features, with α_1 = 0.5, α_2 = 0.3, α_3 = 0.2 to emphasize core feature learning. Features in smaller dictionaries contribute to all larger dictionary reconstructions. The sparsity penalty λ||f||_1 applies globally with λ = 0.04.', 'Implementation_Plan': '1. Modify CustomSAE to create three fixed-size nested dictionaries\n2. Add weighted reconstruction loss computation\n3. Implement feature activation tracking across scales\n4. Add utilities for cross-scale correlation analysis\n5. Update evaluation code to analyze feature specialization', 'Interestingness_Evaluation': 'The fixed-scale nested approach provides a practical way to study feature hierarchy and specialization in sparse autoencoders.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Using fixed scales and explicit weights simplifies implementation; three reconstructions per forward pass adds moderate overhead; all operations remain standard matrix computations; analysis tools are straightforward to implement.', 'Feasibility': 7, 'Novelty_Evaluation': 'While nested representations exist in other contexts, the specific application to SAE interpretability with fixed scales is novel.', 'Novelty': 6, 'Expected_Research_Impact': 'The hierarchical organization should help identify core features that contribute to multiple scales, potentially improving interpretability metrics.', 'Research_Impact': 7, 'Overall_Score': 6.9, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses three nested dictionaries of decreasing sizes, where smaller dictionaries must learn to represent the most fundamental features as they contribute to multiple reconstruction objectives. This naturally encourages a hierarchical organization of features without requiring explicit constraints. We present a simple implementation that extends standard sparse autoencoders with weighted multi-scale reconstruction losses. Our method addresses the challenge of feature interpretability by incentivizing features to specialize at different levels of abstraction while maintaining computational efficiency.'}
Iteration 3/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After further analysis, I see several ways to simplify and strengthen the idea:

1. Instead of three nested levels which adds complexity, we can achieve the same benefits with just two levels (full and half size). This significantly reduces implementation complexity while maintaining the core insight.

2. The weighted loss coefficients add unnecessary hyperparameters. We can simply sum the reconstruction losses equally since the smaller dictionary naturally contributes to both reconstructions.

3. The technical details should better explain how this helps with the polysemanticity problem - features in the smaller dictionary must be more specialized since they're used in both reconstructions.

4. The implementation plan needs to be more specific about modifications to the existing code template.

This refinement makes the idea more focused and practical while better addressing the core challenge of polysemanticity in SAEs.

NEW IDEA JSON:
```json
{
    "Name": "matryoshka_sae",
    "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
    "Experiment": "1. Implement SAE with 2 nested dictionaries (d and d/2 size)\n2. Train on standard datasets tracking feature reuse\n3. Compare interpretability metrics against baseline SAE\n4. Analyze polysemanticity of features across dictionaries\n5. Measure feature specialization through activation patterns",
    "Technical_Details": "The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = d/2, |D_2| = d. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 is reconstruction loss using only D_1 features, L_2 uses all features, and λ = 0.04 for sparsity. Features in D_1 contribute to both reconstructions, forcing them to learn more fundamental, monosemantic patterns. This directly addresses polysemanticity as features that must work well for both reconstructions cannot afford to represent multiple distinct concepts.",
    "Implementation_Plan": "1. Modify CustomSAE.__init__ to create two weight matrices with shared components\n2. Update forward pass to compute both full and partial reconstructions\n3. Modify loss function in CustomTrainer to sum both reconstruction errors\n4. Add feature activation tracking between dictionaries\n5. Implement analysis tools for measuring feature specialization",
    "Interestingness_Evaluation": "The two-level nested approach provides a simple yet effective way to force feature specialization and reduce polysemanticity.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Using just two fixed scales keeps implementation simple; only doubles forward pass computation; all operations remain standard matrix operations; minimal code changes needed to existing template.",
    "Feasibility": 8,
    "Novelty_Evaluation": "While nested representations exist, using them specifically to address polysemanticity in SAEs is novel.",
    "Novelty": 6,
    "Expected_Research_Impact": "The forced specialization of features in the smaller dictionary should directly improve interpretability metrics by reducing polysemanticity.",
    "Research_Impact": 7,
    "Overall_Score": 7.3,
    "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where the smaller dictionary must learn to represent fundamental features that contribute to multiple reconstruction objectives. This architecture directly addresses the challenge of polysemanticity by forcing features in the smaller dictionary to specialize, as they cannot afford to represent multiple distinct concepts while maintaining good reconstruction at both scales. We present a simple implementation that extends standard sparse autoencoders with nested dictionaries and multi-scale reconstruction losses, requiring minimal modifications to existing architectures."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Implement SAE with 2 nested dictionaries (d and d/2 size)\n2. Train on standard datasets tracking feature reuse\n3. Compare interpretability metrics against baseline SAE\n4. Analyze polysemanticity of features across dictionaries\n5. Measure feature specialization through activation patterns', 'Technical_Details': 'The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = d/2, |D_2| = d. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 is reconstruction loss using only D_1 features, L_2 uses all features, and λ = 0.04 for sparsity. Features in D_1 contribute to both reconstructions, forcing them to learn more fundamental, monosemantic patterns. This directly addresses polysemanticity as features that must work well for both reconstructions cannot afford to represent multiple distinct concepts.', 'Implementation_Plan': '1. Modify CustomSAE.__init__ to create two weight matrices with shared components\n2. Update forward pass to compute both full and partial reconstructions\n3. Modify loss function in CustomTrainer to sum both reconstruction errors\n4. Add feature activation tracking between dictionaries\n5. Implement analysis tools for measuring feature specialization', 'Interestingness_Evaluation': 'The two-level nested approach provides a simple yet effective way to force feature specialization and reduce polysemanticity.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Using just two fixed scales keeps implementation simple; only doubles forward pass computation; all operations remain standard matrix operations; minimal code changes needed to existing template.', 'Feasibility': 8, 'Novelty_Evaluation': 'While nested representations exist, using them specifically to address polysemanticity in SAEs is novel.', 'Novelty': 6, 'Expected_Research_Impact': 'The forced specialization of features in the smaller dictionary should directly improve interpretability metrics by reducing polysemanticity.', 'Research_Impact': 7, 'Overall_Score': 7.3, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where the smaller dictionary must learn to represent fundamental features that contribute to multiple reconstruction objectives. This architecture directly addresses the challenge of polysemanticity by forcing features in the smaller dictionary to specialize, as they cannot afford to represent multiple distinct concepts while maintaining good reconstruction at both scales. We present a simple implementation that extends standard sparse autoencoders with nested dictionaries and multi-scale reconstruction losses, requiring minimal modifications to existing architectures.'}
Iteration 4/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The previous iteration made good progress in simplifying the approach, but I notice a few key areas that need attention:

1. The implementation plan needs more specific details about how the shared components between dictionaries will work in practice.

2. The technical details should better explain the relationship between dictionary sizes - using exactly half size might be too restrictive. We could allow the smaller dictionary to be a tunable fraction.

3. We should clarify how this directly helps with the core benchmark metrics.

However, I want to be careful not to add unnecessary complexity. These refinements just make the existing idea more precise and practical while keeping the core mechanism the same.

NEW IDEA JSON:
```json
{
    "Name": "matryoshka_sae",
    "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
    "Experiment": "1. Implement SAE with 2 nested dictionaries (d and αd size, α ∈ [0.3, 0.7])\n2. Train on standard datasets with different α values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of α on feature monosemanticity",
    "Technical_Details": "The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = αd, |D_2| = d, with α as a tunable hyperparameter. The encoder matrix W_enc has its first αd columns shared between both dictionaries, while the remaining (1-α)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and λ = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.",
    "Implementation_Plan": "1. Add α parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over α values",
    "Interestingness_Evaluation": "The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; α parameter provides easy tuning.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.",
    "Novelty": 6,
    "Expected_Research_Impact": "The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.",
    "Research_Impact": 7,
    "Overall_Score": 7.6,
    "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter α that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Implement SAE with 2 nested dictionaries (d and αd size, α ∈ [0.3, 0.7])\n2. Train on standard datasets with different α values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of α on feature monosemanticity', 'Technical_Details': 'The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = αd, |D_2| = d, with α as a tunable hyperparameter. The encoder matrix W_enc has its first αd columns shared between both dictionaries, while the remaining (1-α)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and λ = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.', 'Implementation_Plan': '1. Add α parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over α values', 'Interestingness_Evaluation': 'The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; α parameter provides easy tuning.', 'Feasibility': 9, 'Novelty_Evaluation': 'The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.', 'Novelty': 6, 'Expected_Research_Impact': 'The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.', 'Research_Impact': 7, 'Overall_Score': 7.6, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter α that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures.'}
Iteration 5/7
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe the current formulation has reached a good balance of simplicity, feasibility, and potential impact. The idea:

1. Uses a minimal set of modifications (shared dictionary components and double reconstruction)
2. Has a single clear hyperparameter (α)
3. Directly addresses polysemanticity through feature specialization
4. Can be implemented with simple matrix operations
5. Maintains reasonable training time
6. Requires minimal code changes

While we could add more complexity (e.g., different loss weightings, more nested levels, sophisticated feature analysis), this would likely detract from the core contribution rather than enhance it. The current version is both theoretically sound and practically implementable while maintaining focus on the key goal of improving interpretability.

I am done.

NEW IDEA JSON:
```json
{
    "Name": "matryoshka_sae",
    "Title": "Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability",
    "Experiment": "1. Implement SAE with 2 nested dictionaries (d and αd size, α ∈ [0.3, 0.7])\n2. Train on standard datasets with different α values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of α on feature monosemanticity",
    "Technical_Details": "The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = αd, |D_2| = d, with α as a tunable hyperparameter. The encoder matrix W_enc has its first αd columns shared between both dictionaries, while the remaining (1-α)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and λ = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.",
    "Implementation_Plan": "1. Add α parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over α values",
    "Interestingness_Evaluation": "The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.",
    "Interestingness": 7,
    "Feasibility_Evaluation": "Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; α parameter provides easy tuning.",
    "Feasibility": 9,
    "Novelty_Evaluation": "The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.",
    "Novelty": 6,
    "Expected_Research_Impact": "The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.",
    "Research_Impact": 7,
    "Overall_Score": 7.6,
    "Abstract": "We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter α that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Implement SAE with 2 nested dictionaries (d and αd size, α ∈ [0.3, 0.7])\n2. Train on standard datasets with different α values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of α on feature monosemanticity', 'Technical_Details': 'The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = αd, |D_2| = d, with α as a tunable hyperparameter. The encoder matrix W_enc has its first αd columns shared between both dictionaries, while the remaining (1-α)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and λ = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.', 'Implementation_Plan': '1. Add α parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over α values', 'Interestingness_Evaluation': 'The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; α parameter provides easy tuning.', 'Feasibility': 9, 'Novelty_Evaluation': 'The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.', 'Novelty': 6, 'Expected_Research_Impact': 'The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.', 'Research_Impact': 7, 'Overall_Score': 7.6, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter α that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures.'}
Idea generation converged after 5 iterations.
Skipping seed idea 0
Skipping seed idea 1
Skipping seed idea 2
Skipping seed idea 3
Skipping seed idea 4
Skipping seed idea 5
Skipping seed idea 6
Skipping seed idea 7

Checking novelty of idea 8: coactivation_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes using co-activation patterns to prevent feature absorption in sparse autoencoders. The key innovation is modifying the L1 penalty to scale with each feature's co-activation frequency. While co-activation analysis is common in neuroscience, I should check if this specific approach has been used in sparse autoencoders or similar neural network contexts. Let me search for papers that discuss co-activation or feature interaction in the context of sparse autoencoders.

I want to first look for papers that explicitly discuss co-activation patterns or feature interaction analysis in sparse autoencoders, as this would be the most direct overlap with the proposed idea.

RESPONSE:
```json
{
    "Query": "co-activation patterns sparse autoencoder feature interaction",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 163, "offset": 0, "next": 10, "data": [{"paperId": "eb6997806eb8914363adc58e9b1797bf88510a43", "title": "A model of the mouse cortex with attractor dynamics explains the structure and emergence of rsfMRI co-activation patterns", "abstract": "Neural network models have been instrumental in revealing the foundational principles of whole-brain dynamics. Here we describe a new whole-cortex model of mouse resting-state fMRI (rsfMRI) activity. Our model implements neural input-output nonline
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results don't show direct overlap with the proposed co-activation based sparsity approach for SAEs. While paper #2 ("Scaling and evaluating sparse autoencoders") is highly relevant to the field, it focuses on k-sparse autoencoders rather than co-activation penalties. The other papers discussing co-activation patterns are mostly in neuroscience or other domains.

However, I should specifically search for papers about feature absorption in sparse autoencoders and adaptive penalties, as these might reveal similar approaches that have already been tried. Let me search for papers specifically addressing feature absorption prevention in autoencoders.

RESPONSE:
```json
{
    "Query": "feature absorption prevention sparse autoencoder adaptive penalty",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 4, "offset": 0, "data": [{"paperId": "00fd0d102b747f729757cf10d3aea9bae2273ed5", "title": "Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder", "abstract": "We introduce a novel nonlinear model, Sparse Adaptive Bottleneck Centroid-Encoder (SABCE), for determining the features that discriminate between two or more classes. The algorithm aims to extract discriminatory features in groups while reconstructing the class centroids in the ambient space and simultaneously use 
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results show various applications of sparse autoencoders and adaptive penalties, but none specifically address feature absorption through co-activation patterns. Paper #0 (SABCE) uses adaptive penalties but focuses on class discrimination rather than feature absorption. Let me make one final search specifically looking for papers about preventing feature interaction or redundancy in sparse autoencoders, as this might reveal any existing work that tackles the same problem through different means.

RESPONSE:
```json
{
    "Query": "preventing feature redundancy sparse autoencoder interaction",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 228, "offset": 0, "next": 10, "data": [{"paperId": "aea07ab7eb11e225f82d45ab24c450c9c202e35c", "title": "Multi-Classification Decision Fusion Based on Stacked Sparse Shrink AutoEncoder and GS-Tabnet for Network Intrusion Detection", "abstract": "With the rapid development of the Internet, various network invasive behaviors are increasing rapidly. This seriously threatens the economic development of individuals, enterprises, and society. Network intrusion detection is important in netwo
Decision made: novel after round 3

Checking novelty of idea 9: active_feature_absorption_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes a specific mechanism to prevent feature absorption in sparse autoencoders by tracking activation ratios between features with similar decoder weights. Before making a decision, I should check if similar approaches for preventing feature absorption in sparse autoencoders already exist. A good starting point would be to search for papers specifically addressing feature absorption in sparse autoencoders, particularly any that use activation statistics or weight similarities.

I'll search for papers about feature absorption in sparse autoencoders, as this is the core problem being addressed.

RESPONSE:
```json
{
    "Query": "sparse autoencoder feature absorption",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 1172, "offset": 0, "next": 10, "data": [{"paperId": "f64f0b8d2b0886196aa38edff165f3797c76108a", "title": "The Geometry of Concepts: Sparse Autoencoder Feature Structure", "abstract": "Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The\"atomic\"small-scale structure contains\"crystals\"whose 
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
I found a highly relevant paper: "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders" by Chanin et al. (2024). This paper specifically investigates feature absorption in sparse autoencoders and concludes that varying SAE size or sparsity alone is insufficient to solve the absorption problem. Let me search for more recent papers specifically about preventing feature absorption, as there might be papers proposing solutions similar to our active feature-guided approach.

RESPONSE:
```json
{
    "Query": "preventing feature absorption sparse autoencoder mechanistic interpretability",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 15, "offset": 0, "next": 10, "data": [{"paperId": "8cbc7c757766697a56e7cf881605b8e414ab2fdc", "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to
Decision made: novel after round 2

Checking novelty of idea 10: decoder_selective_orthogonal_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes using decoder weight similarities to selectively apply orthogonality constraints in sparse autoencoders, specifically targeting feature absorption. While orthogonality constraints are common in neural networks, the selective application based on decoder weight similarities and the specific focus on preventing feature absorption in sparse autoencoders seems novel. However, I should check the literature for similar approaches in sparse autoencoders, particularly focusing on orthogonality constraints and feature absorption prevention.

Let me search for papers specifically about orthogonality constraints in sparse autoencoders and feature absorption prevention methods.

RESPONSE:
```json
{
    "Query": "sparse autoencoder orthogonality constraint feature absorption",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 239, "offset": 0, "next": 10, "data": [{"paperId": "5a3769507469826c75e1192eec520c36657cdcf1", "title": "An Innovative PolSAR Image Classification Method Based on Non-Negative Constraints Stacked Sparse Autoencoder Network with Multi-Features Joint Representation Learning", "abstract": ": This paper proposed a framework based on joint multi-feature representation learning to reduce the inherent speckle phenomenon in Polarimetric Synthetic Aperture Radar (PolSAR) images interfere with t
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results are quite relevant, particularly paper #9 "A is for Absorption" which directly studies feature absorption in sparse autoencoders. However, while this paper identifies the problem, it doesn't propose a solution using selective orthogonality constraints. Let me search specifically for papers about using selective or adaptive orthogonality constraints in autoencoders to ensure there isn't prior work using this specific approach.

Papers #4 and #6 mention orthogonality constraints but use them in different contexts (2D feature extraction and non-negative matrix factorization) and not for preventing feature absorption in sparse autoencoders. I should verify there aren't any papers specifically about selective orthogonality in autoencoders.

RESPONSE:
```json
{
    "Query": "selective orthogonality constraint autoencoder adaptive orthogonal",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 2697, "offset": 0, "next": 10, "data": [{"paperId": "9b563edcd737eb616142bbb7d31af8ca14c491e5", "title": "MOLA: Enhancing Industrial Process Monitoring Using Multi-Block Orthogonal Long Short-Term Memory Autoencoder", "abstract": "In this work, we introduce MOLA, a multi-block orthogonal long short-term memory autoencoder paradigm, to conduct accurate, reliable fault detection of industrial processes. To achieve this, MOLA effectively extracts dynamic orthogonal features by introducing
Decision made: novel after round 2

Checking novelty of idea 11: model_aligned_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
This idea proposes a dual-dictionary sparse autoencoder with model-aligned sizes to encourage natural feature hierarchies. The key novelty claims are:
1. Using model-aligned dictionary sizes (full dimension and d/3)
2. Size-weighted reconstruction losses
3. Feature hierarchy analysis through presence patterns

The approach seems related to hierarchical autoencoders and multi-scale representations. I should search for papers about hierarchical sparse autoencoders and multi-scale feature learning to check if this specific approach has been done before.

Let me search for papers about hierarchical sparse autoencoders first.

RESPONSE:
```json
{
    "Query": "hierarchical sparse autoencoder multi scale feature learning",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 2122, "offset": 0, "next": 10, "data": [{"paperId": "4b47e5454cdaf4253e7f208350c205ae8a1a5a2a", "title": "Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature Learning in Chest CT Segmentation", "abstract": "In the field of medical image segmentation, challenges such as indistinct lesion features, ambiguous boundaries,and multi-scale characteristics have long revailed. This paper proposes an improved method named Intensity-Spatial Dual Masked AutoEncoder (ISD-MAE). Based o
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, I see several papers using multi-scale feature learning and hierarchical approaches, but none specifically address sparse autoencoders with model-aligned dictionary sizes. The closest papers are about multi-scale feature fusion in various computer vision tasks, but they don't address the specific problem of polysemanticity in language models.

Let me search specifically for papers about sparse autoencoders and language model interpretability to see if similar approaches have been proposed in this context.

RESPONSE:
```json
{
    "Query": "sparse autoencoder language model interpretability polysemantic",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 633, "offset": 0, "next": 10, "data": [{"paperId": "b244b80d0a2d36412b56c0156532d9cbeb298ffa", "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models", "abstract": "What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is diffi
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, I see several papers on sparse autoencoders for language model interpretability, including recent work on improving their training and applications. However, none of them propose using model-aligned dictionary sizes or dual dictionaries specifically for addressing feature hierarchies. Let me do one final search focusing on dual dictionary or hierarchical dictionary learning approaches in sparse autoencoders.

RESPONSE:
```json
{
    "Query": "dual dictionary hierarchical sparse autoencoder dictionary learning",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 471, "offset": 0, "next": 10, "data": [{"paperId": "bbf218fd97d3c18801eb6eb09345bb38e7bcc871", "title": "Improving Dictionary Learning with Gated Sparse Autoencoders", "abstract": "Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Par
Decision made: novel after round 3

Checking novelty of idea 12: frequency_ordered_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes ordering SAE features based on activation frequency rather than magnitude. This seems like a natural extension of existing SAE work, so I should check if this specific approach has been explored before. I want to search for papers that discuss feature ordering or organization in sparse autoencoders, particularly any that might use activation frequency as a criterion.

Let me search for papers about feature ordering in sparse autoencoders specifically.

RESPONSE:
```json
{
    "Query": "sparse autoencoder feature ordering activation frequency",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 1224, "offset": 0, "next": 10, "data": [{"paperId": "30c70e2b32ff2166f5275f1ceb0a49fc25038bdd", "title": "Sparse Autoencoder with SGReLU Activation Based Network Intrusion Detection System", "abstract": "A Network Intrusion Detection System (NIDS) is the most important function, which is utilized to monitor malevolent activities frequently and detect these security attacks in Internet of Things (IoT) network. An improvement of effective performance and security is the most needed in th
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results show various applications of sparse autoencoders but don't directly address feature ordering based on activation frequency. However, I notice that paper #2 discusses sparsity patterns and feature selection in autoencoders, and there's mention of activation analysis in several papers. Let me search more specifically for papers about ordering or organizing sparse autoencoder features, particularly in the context of interpretability, since this is closer to the proposed idea's core contribution.

RESPONSE:
```json
{
    "Query": "sparse autoencoder feature organization interpretability ordering latent space",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 40, "offset": 0, "next": 10, "data": [{"paperId": "2f85ff64230bb50d1852fa48b996f10e5d49164c", "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models", "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representati
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The results show significant work in sparse autoencoders and feature interpretability, particularly papers #0 and #6 which discuss interpretability in the context of large language models. While these papers discuss feature spaces and interpretability, none directly address the specific approach of using activation frequency for ordering features. Let me make one final search focusing specifically on activation frequency or feature importance ordering in neural networks to ensure we haven't missed any similar approaches.

RESPONSE:
```json
{
    "Query": "neural network feature importance ordering activation frequency sparse",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 356, "offset": 0, "next": 10, "data": [{"paperId": "a21b931c720ea6a8f94fc351010acac4edb1e017", "title": "Bispectrum analysis and Convolutional Neural Network Based Feature Extraction and Classification", "abstract": "Presently, the dominant technologies in device condition detection is to analyze ID vibration signals from time domain, frequency domain, or time-frequency domain. Based on statistical feature parameters calculation or adaptive features extraction, feature mining and condi
Decision made: novel after round 3

Checking novelty of idea 13: grouped_sparse_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes using group-based sparsity penalties in sparse autoencoders to prevent feature absorption. While group-based penalties are common in other contexts (like group LASSO), I need to verify if this specific application to prevent feature absorption in SAEs is truly novel. Let me search for any papers that combine group sparsity with autoencoders or specifically address feature absorption using similar techniques.

I'll start by searching for papers that combine group sparsity with autoencoders, as this would be the most direct overlap.

RESPONSE:
```json
{
    "Query": "group sparse autoencoder feature absorption",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 46, "offset": 0, "next": 10, "data": [{"paperId": "cb6c27e3a77ddfd744c57ce1c49db71709915102", "title": "A novel multimodal framework for automatic recognition of individual cattle based on hybrid features using sparse stacked denoising autoencoder and group sparse representation techniques", "abstract": null, "venue": "Multimedia tools and applications", "year": 2022, "citationCount": 5, "citationStyles": {"bibtex": "@Article{Kumar2022ANM,\n author = {Santosh Kumar and Sunil Kumar and 
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, I see a highly relevant paper "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders" by Chanin et al. (2024) that specifically studies the feature absorption problem in SAEs. They mention that varying SAE size or sparsity is insufficient to solve the absorption issue, but they don't propose using group-based penalties as a solution. I should search specifically for papers combining group sparsity with preventing feature absorption, as this might be a novel approach to addressing the problem they identified.

Let me search for papers that specifically combine group sparsity with preventing feature absorption in neural networks or autoencoders.

RESPONSE:
```json
{
    "Query": "group sparsity prevent feature absorption neural network",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 540, "offset": 0, "next": 10, "data": [{"paperId": "2b358cf2c33f11389235deed753531f65866598b", "title": "Sparse Neural Additive Model: Interpretable Deep Learning with Feature Selection via Group Sparsity", "abstract": null, "venue": "ECML/PKDD", "year": 2022, "citationCount": 18, "citationStyles": {"bibtex": "@Article{Xu2022SparseNA,\n author = {Shiyun Xu and Zhiqi Bu and P. Chaudhari and I. Barnett},\n booktitle = {ECML/PKDD},\n pages = {343-359},\n title = {Sparse Neural Additive Mo
Decision made: novel after round 2

Checking novelty of idea 14: threshold_orthogonal_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes a novel modification to sparse autoencoders by selectively applying orthogonality constraints based on activation frequency thresholds. This seems like a targeted approach to addressing polysemanticity. Before making a decision, I should check if there are existing papers that use similar threshold-based or selective orthogonality constraints in autoencoders or related neural network architectures. Let me search for papers that might discuss selective orthogonality or frequency-based constraints in autoencoders.

RESPONSE:
```json
{
    "Query": "selective orthogonality constraints sparse autoencoder threshold activation frequency",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 2, "offset": 0, "data": [{"paperId": "c61d74519290ab66ad76b28b7d67da2808419bfa", "title": "PRICAI 2008: Trends in Artificial Intelligence, 10th Pacific Rim International Conference on Artificial Intelligence, Hanoi, Vietnam, December 15-19, 2008. Proceedings", "abstract": null, "venue": "Pacific Rim International Conference on Artificial Intelligence", "year": 2008, "citationCount": 48, "citationStyles": {"bibtex": "@Inproceedings{Ho2008PRICAI2T,\n author = {T. Ho and Zhi-Hua Zhou},\n 
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The previous search results mostly discuss traditional independent component analysis and basic orthogonality constraints, but don't address the specific approach of using activation frequency thresholds to selectively apply orthogonality constraints. Let me search more specifically for papers that might discuss adaptive or dynamic orthogonality constraints in neural networks, as this might reveal similar approaches.

RESPONSE:
```json
{
    "Query": "adaptive orthogonality constraints neural networks dynamic feature separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 279, "offset": 0, "next": 10, "data": [{"paperId": "73f58680843d9c6a30cb997a2d32b0546495f1fe", "title": "Adaptive physics-informed neural networks for dynamic thermo-mechanical coupling problems in large-size-ratio functionally graded materials", "abstract": "In this paper, we present the adaptive physics-informed neural networks (PINNs) for resolving three dimensional (3D) dynamic thermo-mechanical coupling problems in large-size-ratio functionally graded materials (FGMs). The physica
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results show various adaptive and dynamic approaches in neural networks, but none specifically address the use of activation frequency thresholds for selective orthogonality constraints in autoencoders. Let me make one final search specifically focusing on frequency-based feature selection in autoencoders to ensure we haven't missed any similar approaches in the autoencoder literature.

RESPONSE:
```json
{
    "Query": "frequency based feature selection sparse autoencoder activation threshold",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 59, "offset": 0, "next": 10, "data": [{"paperId": "73dc951acfc9df058744720560cd5447ed8fbe61", "title": "An intelligent grinding burn detection system based on two-stage feature selection and stacked sparse autoencoder", "abstract": null, "venue": "The International Journal of Advanced Manufacturing Technology", "year": 2019, "citationCount": 23, "citationStyles": {"bibtex": "@Article{Guo2019AnIG,\n author = {Weicheng Guo and Beizhi Li and Shouguo Shen and Qinzhi Zhou},\n booktitle = {T
Decision made: novel after round 3

Checking novelty of idea 15: step_progressive_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes a step-based progressive feature learning approach for sparse autoencoders, focusing on preventing feature absorption by gradually enabling features based on activation stability. While the concept of progressive learning isn't entirely new, I should check for existing work on progressive feature activation in sparse autoencoders specifically. I want to search for papers that discuss progressive or curriculum-based approaches to sparse autoencoder training.

Let me search for papers that combine the concepts of progressive learning and sparse autoencoders to see if this specific approach has been explored before.

RESPONSE:
```json
{
    "Query": "progressive sparse autoencoder curriculum learning feature activation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 59, "offset": 0, "next": 10, "data": [{"paperId": "0aba87cf02635a40636c37949610654fce3e867d", "title": "The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning", "abstract": "In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successful
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
From the search results, I see several relevant papers but none that directly address the specific approach of step-based progressive feature learning with activation streak tracking. While paper #3 "Improving Dictionary Learning with Gated Sparse Autoencoders" and paper #5 discuss progressive learning aspects, they focus on different mechanisms. The Gated SAE paper addresses shrinkage through gating mechanisms, while paper #5 uses attention-based progressive learning for fault detection.

Let me make one more search focusing specifically on step-based or streak-based feature activation in neural networks to ensure we haven't missed any similar approaches.

RESPONSE:
```json
{
    "Query": "step based progressive activation neural network feature tracking stability",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 46, "offset": 0, "next": 10, "data": [{"paperId": "f7b717960c4f637d9461a9919f370f39e797589e", "title": "Real-Time Progressive Learning: Mutually Reinforcing Learning and Control with Neural-Network-Based Selective Memory", "abstract": "\u2014Memory, as the basis of learning, determines the storage, update and forgetting of the knowledge and further determines the efficiency of learning. Featured with a mechanism of memory, a radial basis function neural network (RBFNN) based learning c
Decision made: novel after round 2

Checking novelty of idea 16: matryoshka_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea of using nested dictionaries in sparse autoencoders is interesting and draws inspiration from Matryoshka representation learning. I should first check if there are any existing papers that specifically apply Matryoshka-style nested representations to sparse autoencoders or neural network interpretability. Let me search for papers containing both "Matryoshka" and "sparse autoencoder" or similar representation learning terms.

RESPONSE:
```json
{
    "Query": "Matryoshka representation learning sparse autoencoder neural network interpretability",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 396, "offset": 0, "next": 10, "data": [{"paperId": "c626f4c2ad1ff8501de1fd930deb887e6358c9ba", "title": "Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) have shown promise in improving the interpretability of neural network activations, but can learn features that are not features of the input, limiting their effectiveness. We propose \\textsc{Mutual Feature Regularization} \\textbf{(MFR)}, a regularization te
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
While the search results show various applications of autoencoders for representation learning and interpretability, none specifically address the concept of nested dictionaries or Matryoshka-style feature hierarchies in sparse autoencoders. Let me search more specifically for papers about hierarchical or nested sparse coding/dictionary learning, as this might be closer to the proposed approach.

RESPONSE:
```json
{
    "Query": "hierarchical sparse coding nested dictionary learning matryoshka neural networks",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 15, "offset": 0, "next": 10, "data": [{"paperId": "4d7549ea0ca944babb2657eb18e0346d5faa7935", "title": "Multilayer Convolutional Sparse Modeling: Pursuit and Dictionary Learning", "abstract": "The recently proposed multilayer convolutional sparse coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of convolutional neural networks (CNNs). Under this framework, the forward pass in a CNN is equivalent to a pursuit algorithm aiming t
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
While the search results reveal several papers on hierarchical dictionary learning and sparse coding, none of them specifically address the concept of nested dictionaries with shared features for improving interpretability in sparse autoencoders. The closest work is the Multilayer Convolutional Sparse Modeling paper by Sulam et al., but it focuses on convolutional architectures and doesn't address the specific problem of polysemanticity through nested feature sharing. Let me make one final search specifically about techniques for addressing polysemanticity in sparse autoencoders or neural networks.

RESPONSE:
```json
{
    "Query": "polysemantic features sparse autoencoder neural network interpretability",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 897, "offset": 0, "next": 10, "data": [{"paperId": "c626f4c2ad1ff8501de1fd930deb887e6358c9ba", "title": "Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) have shown promise in improving the interpretability of neural network activations, but can learn features that are not features of the input, limiting their effectiveness. We propose \\textsc{Mutual Feature Regularization} \\textbf{(MFR)}, a regularization te
Decision made: novel after round 3
saved
[{'Name': 'matryoshka_sae', 'Title': 'Matryoshka Sparse Autoencoders: Learning Nested Feature Hierarchies for Improved Interpretability', 'Experiment': '1. Implement SAE with 2 nested dictionaries (d and αd size, α ∈ [0.3, 0.7])\n2. Train on standard datasets with different α values\n3. Compare interpretability metrics against baseline SAE\n4. Analyze feature specialization through activation patterns\n5. Measure impact of α on feature monosemanticity', 'Technical_Details': 'The architecture uses 2 nested dictionaries D_1 ⊂ D_2 where |D_1| = αd, |D_2| = d, with α as a tunable hyperparameter. The encoder matrix W_enc has its first αd columns shared between both dictionaries, while the remaining (1-α)d columns are used only for full reconstruction. The loss function is L = L_1 + L_2 + λ||f||_1 where L_1 uses only the shared columns for reconstruction, L_2 uses all columns, and λ = 0.04. This structure forces the shared features to learn fundamental patterns that work well at both scales, naturally reducing polysemanticity.', 'Implementation_Plan': '1. Add α parameter to CustomSAE.__init__\n2. Create encoder matrix with shared structure by slicing\n3. Implement two-part forward pass using matrix slicing\n4. Update loss computation in CustomTrainer\n5. Add activation pattern analysis tools\n6. Implement grid search over α values', 'Interestingness_Evaluation': 'The tunable nested dictionary approach provides a simple yet flexible way to study the trade-off between feature capacity and specialization.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only matrix slicing operations; training time roughly doubles; all operations remain standard matrix computations; α parameter provides easy tuning.', 'Feasibility': 9, 'Novelty_Evaluation': 'The specific application to reducing polysemanticity through shared dictionary components with tunable size is novel.', 'Novelty': 6, 'Expected_Research_Impact': 'The forced specialization through shared components should improve core metrics by making features more interpretable and monosemantic.', 'Research_Impact': 7, 'Overall_Score': 7.6, 'Abstract': 'We propose Matryoshka Sparse Autoencoders (M-SAE), a novel approach to improving the interpretability of learned features in neural networks. M-SAE uses two nested dictionaries where a tunable fraction of features must contribute to reconstructions at both full and reduced scales. This architecture directly addresses the challenge of polysemanticity by forcing shared features to learn fundamental patterns that work well at multiple scales. The method introduces a single hyperparameter α that controls the proportion of shared features, allowing for systematic study of the trade-off between feature capacity and specialization. We present a simple implementation that extends standard sparse autoencoders through matrix slicing operations, requiring minimal modifications to existing architectures.', 'novel': True}, {'Name': 'step_progressive_sae', 'Title': 'Step-Based Progressive Feature Learning for Sparse Autoencoders', 'Experiment': '1. Implement SAE with consecutive step counting\n2. Add activation streak tracking\n3. Train on Gemma-2B using standard datasets\n4. Compare feature absorption metrics against baseline\n5. Analyze feature enabling sequence\n6. Evaluate impact of streak threshold', 'Technical_Details': "The method starts with d/2 active features and tracks consecutive steps C(i) where each feature i is active (activation > 0.01). When all enabled features maintain C(i) > τ (default 100 steps), one additional feature is enabled. The mask M updates based on activation streaks: M = [1,...,1,0,...,0] where ones correspond to enabled features. The loss function remains L = L_recon + λ * L_sparse with masked features f' = M ⊙ f. Streak counting provides clear criteria for feature stability.", 'Implementation_Plan': '1. Add consecutive step counting to CustomSAE\n2. Implement streak-based progression\n3. Modify forward pass to apply mask\n4. Add metrics for streak patterns\n5. Track feature enabling sequence\n6. Update training loop with step-based progression', 'Interestingness_Evaluation': 'The step-based progression provides an extremely clear and interpretable way to prevent feature absorption through controlled capacity expansion.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation requires only simple step counting and masking; progression criteria is maximally simple; no complex tracking or parameters needed; easily fits within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on progressive activation, the simplified step-based approach offers a novel and more interpretable method for preventing feature absorption.', 'Novelty': 6, 'Expected_Research_Impact': 'The simplified progression criteria should improve sparse probing through more stable feature separation while maintaining strong core metrics.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': 'Feature absorption in sparse autoencoders often results in the collapse of hierarchically related features into less interpretable representations. We propose a step-based progressive feature learning approach that prevents absorption by ensuring feature stability before capacity expansion. By enabling new features only after existing ones demonstrate consistent activation over many consecutive steps, we create a natural progression from general to specific features. This method requires minimal modifications to standard sparse autoencoders while providing a transparent and principled way to prevent feature absorption. We evaluate our method on standard interpretability benchmarks and analyze the resulting feature hierarchies.', 'novel': True}, {'Name': 'threshold_orthogonal_sae', 'Title': 'Threshold-Based Orthogonality for Targeted Feature Separation in Sparse Autoencoders', 'Experiment': '1. Add activation frequency tracking\n2. Implement threshold-based orthogonality loss\n3. Train on standard datasets with varying thresholds\n4. Compare feature separation quality\n5. Analyze high-frequency feature interactions', 'Technical_Details': "The method applies orthogonality constraints selectively: L_ortho = Σ_{i,j} h_i * h_j * (w_i^T w_j)^2 where h_i is 1 if feature i's activation frequency exceeds threshold τ (typically 0.1) and 0 otherwise. Frequencies are tracked using exponential moving average with decay 0.99. The total loss remains L = L_recon + λ_1 * L_sparse + λ_2 * L_ortho. Only high-frequency features participate in orthogonality constraints.", 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Implement threshold-based feature masking\n3. Modify orthogonality loss to use masks\n4. Add frequency analysis utilities\n5. Update training loop with mask updates', 'Interestingness_Evaluation': 'The approach provides a clear and focused way to address polysemanticity by targeting orthogonality constraints specifically at features most likely to need them.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation is extremely simple; frequency tracking and thresholding are basic operations; orthogonality loss computation reduced by masking; no complex algorithms needed; training time less than baseline due to selective application.', 'Feasibility': 10, 'Novelty_Evaluation': 'Using a threshold-based approach to selectively apply orthogonality constraints is a novel simplification of adaptive feature separation.', 'Novelty': 7, 'Expected_Research_Impact': 'The targeted application of constraints should effectively reduce polysemanticity in key features while maintaining model flexibility, improving both sparse_probing and core metrics.', 'Research_Impact': 9, 'Overall_Score': 8.9, 'Abstract': 'We present a targeted approach to improving feature separation in sparse autoencoders (SAEs) through threshold-based orthogonality constraints. Current SAEs apply constraints uniformly across all features, potentially over-constraining rarely used features while under-constraining frequently used ones that are more likely to be polysemantic. Our method selectively applies orthogonality constraints only between frequently activating features, identified through a simple threshold on activation frequency. We develop an efficient tracking mechanism for feature frequencies and modify the standard SAE objective to incorporate masked orthogonality constraints. This approach provides a simple yet principled way to focus feature separation efforts on the subset of features most likely to benefit from them.', 'novel': True}, {'Name': 'grouped_sparse_sae', 'Title': 'Feature Group Sparsity for Preventing Absorption in Sparse Autoencoders', 'Experiment': '1. Implement group-based sparsity penalties\n2. Train on standard datasets with discrete penalty levels\n3. Analyze feature usage patterns within groups\n4. Compare interpretability metrics against baseline\n5. Evaluate impact of different group sizes and penalty ratios', 'Technical_Details': 'The method uses a single dictionary with group-based L1 penalties. Features are divided into K=3 equal-sized groups with penalties λ_k = λ_base * γ^k, where λ_base=0.04 is the base penalty and γ=2 is the scaling factor between groups. The loss is L = L_recon + Σ(λ_g(i) * |h_i|) where g(i) maps feature i to its group index. This creates distinct feature groups with different sparsity levels, naturally preventing absorption between groups while maintaining simplicity.', 'Implementation_Plan': '1. Add vector of group-based penalties to CustomSAE\n2. Modify loss function to use grouped penalties\n3. Add utilities for analyzing within-group patterns\n4. Implement visualization tools for group behavior', 'Interestingness_Evaluation': 'The group-based approach provides a clear and interpretable way to prevent feature absorption while maintaining simplicity.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just one vector of fixed group penalties. No tracking or updates needed. Training time identical to baseline. All computation easily within 30-min limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'While grouped penalties exist in other contexts, their specific application to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The clear separation between feature groups should directly improve sparse probing by preventing absorption. The discrete penalty levels should help core metrics by creating stable feature groups.', 'Research_Impact': 8, 'Overall_Score': 8.5, 'Abstract': 'We propose a group-based sparsity approach to preventing feature absorption in sparse autoencoders. Our method divides features into distinct groups with different sparsity penalties, creating natural barriers that discourage features from absorbing patterns across group boundaries. This approach requires minimal modifications to standard sparse autoencoders while providing a clear and interpretable way to organize features. The discrete penalty levels create well-defined feature groups that aim to capture patterns at different levels of specificity, helping to prevent the undesirable mixing of features that occurs in feature absorption.', 'novel': True}, {'Name': 'frequency_ordered_sae', 'Title': 'Frequency-Based Feature Ordering in Sparse Autoencoders', 'Experiment': '1. Implement activation frequency tracking\n2. Apply batch-wise ordering constraints\n3. Analyze feature activation patterns\n4. Compare feature interpretability\n5. Evaluate ordering stability', 'Technical_Details': 'The method uses activation frequency for ordering: L = L_recon + λ_1 * Σ|f_i| + λ_2 * Σ_i max(0, p_i - p_{i-1}) where p_i is the proportion of non-zero activations for feature i in the current batch: p_i = mean(|f_i| > ε). This creates a natural ordering based on how often features are used, with ε being a small threshold (e.g., 1e-4). The frequency-based approach is more stable than magnitude-based ordering and directly reflects feature importance.', 'Implementation_Plan': '1. Add frequency tracking to CustomSAE\n2. Implement threshold-based activation counting\n3. Add relative frequency loss\n4. Create frequency visualization tools\n5. Update training loop for frequency tracking', 'Interestingness_Evaluation': 'The frequency-based ordering provides an elegantly simple way to organize features based on their actual usage patterns, with better numerical properties.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple threshold counting; no additional parameters beyond threshold ε; minimal overhead as thresholding operations are very efficient.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using activation frequency for feature ordering is a novel and numerically stable approach that directly reflects feature importance.', 'Novelty': 7, 'Expected_Research_Impact': 'The simple frequency-based approach should create reliable feature orderings while being more numerically stable and easier to interpret.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'We present a simplified approach to improving feature interpretability in sparse autoencoders through frequency-based feature ordering. By tracking the proportion of non-zero activations for each feature, we encourage a natural ordering of features based on their activation frequency in the data. This method requires only simple threshold-based counting with no additional parameters beyond a small threshold value, maintaining the computational efficiency of traditional sparse autoencoders. Our approach is motivated by the observation that feature importance is often better reflected by how frequently a feature is used rather than the magnitude of its activations.', 'novel': True}, {'Name': 'model_aligned_sae', 'Title': 'Model-Aligned Dual-Dictionary Sparse Autoencoders for Natural Feature Hierarchies', 'Experiment': '1. Implement SAE with model-aligned dictionary sizes (full and d/3)\n2. Train on Gemma-2-2b using size-weighted reconstruction losses\n3. Track feature activation patterns across both dictionaries\n4. Measure impact on feature absorption benchmark\n5. Analyze hierarchy through feature presence patterns', 'Technical_Details': 'The method uses two reconstruction objectives with model-aligned dictionary sizes [d, d/3] and size-weighted losses: L = (d*L_recon(x, f[:d]) + (d/3)*L_recon(x, f[:d/3])) / (d + d/3). For Gemma-2-2b, this means 2304 and 768 features. Feature hierarchy is measured through feature presence in both dictionaries versus full only. The architecture remains unchanged except for the weighted dual reconstruction loss. Features present in both dictionaries are considered fundamental concepts, while those only in the full dictionary represent more specific features.', 'Implementation_Plan': '1. Add MODEL_ALIGNED_SIZES = [d, d/3] parameter to CustomSAE\n2. Modify forward pass to return two reconstructions\n3. Update CustomTrainer with size-weighted reconstruction losses\n4. Add feature_presence_analysis() to track activation patterns\n5. Implement hierarchy_score based on feature presence in both dictionaries', 'Interestingness_Evaluation': 'The model-aligned approach provides a principled way to encourage natural feature hierarchies that match the underlying model architecture.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation remains very straightforward with just two dictionaries; memory usage is minimal; evaluation metrics are simple binary checks; well within 30-minute limit on H100.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on previous hierarchical approaches, the model-aligned dictionary sizes and weighted losses provide a novel twist.', 'Novelty': 7, 'Expected_Research_Impact': 'The alignment with model dimensions should produce more natural and interpretable feature hierarchies.', 'Research_Impact': 8, 'Overall_Score': 8.2, 'Abstract': "We propose a principled approach to improving interpretability in sparse autoencoders through model-aligned dual-dictionary feature learning. Our method uses two reconstruction objectives with dictionary sizes aligned to the underlying model dimensions (full model dimension and one-third) and size-weighted losses. This alignment with model structure encourages the autoencoder to learn natural feature hierarchies that reflect the model's inherent organizational principles. Features that activate in both dictionaries are interpreted as fundamental concepts, while those only present in the full dictionary represent more specific features. The approach requires minimal modifications to standard sparse autoencoder architectures, adding only a second reconstruction objective with model-aligned dimensions and simple feature presence analysis.", 'novel': True}, {'Name': 'decoder_selective_orthogonal_sae', 'Title': 'Decoder-Based Selective Orthogonality for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement decoder weight similarity computation\n2. Add time-dependent selective orthogonality loss\n3. Train on standard datasets with varying warmup periods\n4. Compare feature absorption metrics against baseline SAE\n5. Analyze feature hierarchy preservation', 'Technical_Details': 'The method uses a weight-based selective orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(t) * L_selective_ortho. L_selective_ortho applies to decoder weight columns with cosine similarity exceeding τ=0.3. λ_2(t) decreases linearly: λ_2(t) = λ_2_max * max(0, 1 - t/t_warmup) where t_warmup is 20% of total steps. The similarity mask M is computed from normalized decoder weights: M_ij = 1 if cos_sim(W_dec[:,i], W_dec[:,j]) > τ else 0. L_selective_ortho = ||W_dec^T W_dec ⊙ M||_F.', 'Implementation_Plan': '1. Add decoder weight similarity computation to update step\n2. Modify CustomSAE loss function for weight-based orthogonality\n3. Add simple linear scheduling for orthogonality weight\n4. Implement efficient masked matrix operations\n5. Add evaluation metrics for decoder weight diversity', 'Interestingness_Evaluation': 'The focus on decoder weight similarities provides a direct and efficient way to prevent feature absorption at its source.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only one similarity computation per step; uses existing decoder weights; highly efficient with simple matrix operations; well within 30-minute limit.', 'Feasibility': 9, 'Novelty_Evaluation': 'Using decoder weight similarities for selective orthogonality is a novel and theoretically well-motivated approach.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct intervention on decoder weights should effectively prevent feature absorption while being computationally efficient.', 'Research_Impact': 8, 'Overall_Score': 8.1, 'Abstract': 'We present a decoder-based selective orthogonality approach for sparse autoencoders that addresses the challenge of feature absorption. Our method applies orthogonality constraints selectively to pairs of decoder weight columns that exhibit high cosine similarity, directly targeting the mechanism through which feature absorption occurs. The constraint strength decreases linearly during training, focusing intervention in early phases when feature absorption is most likely to develop. This approach is motivated by the observation that feature absorption manifests through similar decoder weight patterns. The method requires minimal computational overhead, using only the existing decoder weight matrix for similarity calculations. We evaluate our method on standard benchmarks focusing on feature interpretability and absorption metrics, with particular attention to the preservation of meaningful feature hierarchies.', 'novel': True}, {'Name': 'active_feature_absorption_sae', 'Title': 'Active Feature-Guided Prevention of Absorption in Sparse Autoencoders', 'Experiment': '1. Implement active feature detection per batch\n2. Add normalized weight dot product computation\n3. Train with activation ratio thresholds={5, 10, 20}\n4. Evaluate feature separation on standard tasks\n5. Analyze active feature patterns', 'Technical_Details': 'The method prevents feature absorption by focusing on active features: L = L_recon + λ_1 * L_sparse + λ_2 * L_absorb. In each batch, we identify features with activation frequency > 0.1%. For these active features, we compute normalized dot products between decoder weight vectors. When dot(w_i, w_j) > τ (typically 0.8), we compute the running average activation ratio R = avg(f_i & f_j) / avg(f_j) with decay 0.99. If R > ρ (typically 10), suggesting f_i has absorbed f_j, we apply a penalty proportional to (R - ρ) * dot(w_i, w_j). This efficiently targets actual absorption cases while ignoring inactive or unrelated features.', 'Implementation_Plan': '1. Add active feature identification\n2. Implement normalized dot product computation\n3. Add running average activation tracking\n4. Add configurable ratio threshold\n5. Add evaluation code for activation patterns', 'Interestingness_Evaluation': 'The focus on active features and clear activation ratio metric provides an efficient and interpretable approach to preventing absorption.', 'Interestingness': 7, 'Feasibility_Evaluation': 'Implementation is extremely simple - just basic dot products and running averages; minimal overhead; single clear threshold parameter; fits easily within training loop.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing work in feature absorption, the active feature focus and ratio-based penalty provide an incremental but useful improvement.', 'Novelty': 6, 'Expected_Research_Impact': 'The focused approach should provide reliable improvements in feature separation while being highly efficient and interpretable.', 'Research_Impact': 7, 'Overall_Score': 7.7, 'Abstract': 'Feature absorption in sparse autoencoders presents a significant challenge for interpretability, particularly among frequently active features. We propose an efficient approach that focuses specifically on preventing absorption between active features with similar decoder weights. Our method identifies potential absorption cases by computing activation ratios between similar features and applies a targeted penalty when these ratios indicate absorption. By focusing only on active features and using a clear ratio-based metric, the method provides an interpretable and computationally efficient way to prevent feature absorption. The approach maintains the simplicity of standard sparse autoencoders while adding minimal computational overhead and requiring only basic statistical calculations.', 'novel': True}, {'Name': 'coactivation_sae', 'Title': 'Co-activation Based Sparsity for Preventing Feature Absorption in Sparse Autoencoders', 'Experiment': '1. Implement co-activation tracking\n2. Add co-activation based sparsity scaling\n3. Train model with adaptive penalties\n4. Evaluate feature interaction metrics\n5. Analyze co-activation patterns', 'Technical_Details': 'The method scales sparsity penalties based on co-activation patterns: L = L_recon + Σ_i (λ_1 * (1 + γ*c_i) * |f_i|) where c_i is the average number of other features that co-activate with feature i, tracked using exponential moving average. γ controls co-activation sensitivity (typically 0.1). Co-activation counts are updated each batch: c_i = mean_j≠i(f_i != 0 && f_j != 0). This directly penalizes features that might be absorbing or being absorbed by others.', 'Implementation_Plan': '1. Add co-activation tracking to CustomSAE\n2. Modify L1 penalty calculation\n3. Update training loop with co-activation updates\n4. Add sensitivity parameter\n5. Implement co-activation analysis metrics', 'Interestingness_Evaluation': 'The use of co-activation patterns provides a direct and interpretable approach to preventing feature absorption.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple co-activation counting and modified L1 calculation; efficient batch-wise updates; training time slightly higher than baseline but still within limits; single new hyperparameter.', 'Feasibility': 8, 'Novelty_Evaluation': 'While co-activation analysis exists in neuroscience, applying it to prevent feature absorption in SAEs is novel.', 'Novelty': 7, 'Expected_Research_Impact': 'The direct targeting of co-activation patterns should significantly improve feature separation and interpretability.', 'Research_Impact': 8, 'Overall_Score': 7.9, 'Abstract': "We present a co-activation based approach to improving sparse autoencoder interpretability by directly targeting feature absorption patterns. Our method modifies the standard L1 penalty to scale with each feature's co-activation frequency with other features, creating a precise mechanism for preventing feature absorption. This adaptive penalty uses batch-wise co-activation statistics to dynamically adjust regularization strength, encouraging features to develop independent, interpretable representations. Unlike previous approaches that use general sparsity or complex architectural modifications, our method achieves similar goals by directly monitoring and preventing problematic feature interactions. The approach maintains computational efficiency while providing an interpretable mechanism for preventing feature absorption based on observable activation patterns.", 'novel': True}]
Running 2 parallel processes
Step 0: {'loss': 1437.1258544921875, 'l1_loss': 11565.46875, 'l2_loss': 974.507080078125}
Step 100: {'loss': 1017.1106567382812, 'l1_loss': 8204.494140625, 'l2_loss': 688.930908203125}
Step 200: {'loss': 784.43701171875, 'l1_loss': 3691.763427734375, 'l2_loss': 636.7664794921875}
Step 300: {'loss': 692.053955078125, 'l1_loss': 1656.397705078125, 'l2_loss': 625.7980346679688}
Step 400: {'loss': 655.4166870117188, 'l1_loss': 1144.89453125, 'l2_loss': 609.6209106445312}
Step 500: {'loss': 607.6644287109375, 'l1_loss': 1409.1800537109375, 'l2_loss': 551.2972412109375}
Step 600: {'loss': 565.9019165039062, 'l1_loss': 1508.752685546875, 'l2_loss': 505.5517883300781}
Step 700: {'loss': 530.829345703125, 'l1_loss': 1478.357177734375, 'l2_loss': 471.695068359375}
Step 800: {'loss': 514.3995971679688, 'l1_loss': 1467.8189697265625, 'l2_loss': 455.68682861328125}
Step 900: {'loss': 492.7655334472656, 'l1_loss': 1285.419677734375, 'l2_loss': 441.3487548828125}
Step 1000: {'loss': 490.29180908203125, 'l1_loss': 1412.078125, 'l2_loss': 433.8086853027344}
Step 1100: {'loss': 476.3499450683594, 'l1_loss': 1203.361083984375, 'l2_loss': 428.21551513671875}
Step 1200: {'loss': 462.3206787109375, 'l1_loss': 1008.7631225585938, 'l2_loss': 421.97015380859375}
Step 1300: {'loss': 465.8956298828125, 'l1_loss': 1123.107177734375, 'l2_loss': 420.9713439941406}
Step 1400: {'loss': 459.6485595703125, 'l1_loss': 1149.386474609375, 'l2_loss': 413.673095703125}
Step 1500: {'loss': 456.0144958496094, 'l1_loss': 1050.8583984375, 'l2_loss': 413.98016357421875}
Step 1600: {'loss': 453.9500732421875, 'l1_loss': 1053.42041015625, 'l2_loss': 411.8132629394531}
Step 1700: {'loss': 449.00335693359375, 'l1_loss': 1002.4117431640625, 'l2_loss': 408.9068908691406}
Step 1800: {'loss': 449.3970031738281, 'l1_loss': 1063.017578125, 'l2_loss': 406.8763122558594}
Step 1900: {'loss': 445.7697448730469, 'l1_loss': 1027.533203125, 'l2_loss': 404.6684265136719}
Step 2000: {'loss': 443.9060363769531, 'l1_loss': 1024.707763671875, 'l2_loss': 402.917724609375}
Step 2100: {'loss': 444.4630126953125, 'l1_loss': 988.5729370117188, 'l2_loss': 404.92010498046875}
Step 2200: {'loss': 436.08599853515625, 'l1_loss': 913.133056640625, 'l2_loss': 399.5606689453125}
Step 2300: {'loss': 437.36627197265625, 'l1_loss': 1010.91064453125, 'l2_loss': 396.9298400878906}
Step 2400: {'loss': 437.60443115234375, 'l1_loss': 986.3865356445312, 'l2_loss': 398.14898681640625}
Step 2500: {'loss': 432.9288330078125, 'l1_loss': 1000.7174072265625, 'l2_loss': 392.900146484375}
Step 2600: {'loss': 432.94281005859375, 'l1_loss': 1044.445556640625, 'l2_loss': 391.16497802734375}
Step 2700: {'loss': 432.6243591308594, 'l1_loss': 991.1851806640625, 'l2_loss': 392.9769592285156}
Step 2800: {'loss': 430.64862060546875, 'l1_loss': 950.3992309570312, 'l2_loss': 392.6326599121094}
Step 2900: {'loss': 426.7047424316406, 'l1_loss': 919.2120361328125, 'l2_loss': 389.9362487792969}
Step 3000: {'loss': 429.2130126953125, 'l1_loss': 990.098876953125, 'l2_loss': 389.60906982421875}
Step 3100: {'loss': 425.43780517578125, 'l1_loss': 968.104248046875, 'l2_loss': 386.713623046875}
Step 3200: {'loss': 424.7178039550781, 'l1_loss': 929.723388671875, 'l2_loss': 387.52886962890625}
Step 3300: {'loss': 422.9954528808594, 'l1_loss': 932.16796875, 'l2_loss': 385.708740234375}
Step 3400: {'loss': 424.52685546875, 'l1_loss': 977.15625, 'l2_loss': 385.44061279296875}
Step 3500: {'loss': 417.343505859375, 'l1_loss': 916.6128540039062, 'l2_loss': 380.6789855957031}
Step 3600: {'loss': 421.99212646484375, 'l1_loss': 959.6696166992188, 'l2_loss': 383.6053466796875}
Step 3700: {'loss': 420.0968017578125, 'l1_loss': 937.986572265625, 'l2_loss': 382.57733154296875}
Step 3800: {'loss': 416.97698974609375, 'l1_loss': 955.961181640625, 'l2_loss': 378.7385559082031}
Step 3900: {'loss': 417.3282775878906, 'l1_loss': 941.7456665039062, 'l2_loss': 379.658447265625}
Step 4000: {'loss': 415.81280517578125, 'l1_loss': 911.5252075195312, 'l2_loss': 379.351806640625}
Step 4100: {'loss': 415.90692138671875, 'l1_loss': 914.511962890625, 'l2_loss': 379.3264465332031}
Step 4200: {'loss': 415.4447326660156, 'l1_loss': 959.4534912109375, 'l2_loss': 377.06658935546875}
Step 4300: {'loss': 416.0540771484375, 'l1_loss': 948.28369140625, 'l2_loss': 378.12274169921875}
Step 4400: {'loss': 412.7254333496094, 'l1_loss': 929.7110595703125, 'l2_loss': 375.5369873046875}
Step 4500: {'loss': 413.7205505371094, 'l1_loss': 947.2483520507812, 'l2_loss': 375.83062744140625}
Step 4600: {'loss': 412.7112731933594, 'l1_loss': 941.3998413085938, 'l2_loss': 375.0552673339844}
Step 4700: {'loss': 411.28155517578125, 'l1_loss': 977.4938354492188, 'l2_loss': 372.1817932128906}
Step 4800: {'loss': 411.6326904296875, 'l1_loss': 951.9332275390625, 'l2_loss': 373.55535888671875}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Step 100: {'loss': 467.17498779296875, 'l1_loss': 3255.898193359375, 'l2_loss': 336.9390563964844}
Step 200: {'loss': 375.7010498046875, 'l1_loss': 1522.224853515625, 'l2_loss': 314.81207275390625}
Step 300: {'loss': 338.71881103515625, 'l1_loss': 738.626953125, 'l2_loss': 309.1737365722656}
Step 400: {'loss': 314.4864196777344, 'l1_loss': 679.998291015625, 'l2_loss': 287.2864990234375}
Step 500: {'loss': 290.0771179199219, 'l1_loss': 806.890625, 'l2_loss': 257.8014831542969}
Step 600: {'loss': 273.9638366699219, 'l1_loss': 827.7991943359375, 'l2_loss': 240.8518829345703}
Step 700: {'loss': 258.5910339355469, 'l1_loss': 819.0880126953125, 'l2_loss': 225.8275146484375}
Step 800: {'loss': 251.28565979003906, 'l1_loss': 817.823486328125, 'l2_loss': 218.57272338867188}
Step 900: {'loss': 242.8711395263672, 'l1_loss': 700.2772216796875, 'l2_loss': 214.86004638671875}
Step 1000: {'loss': 241.58338928222656, 'l1_loss': 750.8533325195312, 'l2_loss': 211.54925537109375}
Step 1100: {'loss': 234.541015625, 'l1_loss': 633.6325073242188, 'l2_loss': 209.19570922851562}
Step 1200: {'loss': 227.93734741210938, 'l1_loss': 533.1725463867188, 'l2_loss': 206.61044311523438}
Step 1300: {'loss': 229.73849487304688, 'l1_loss': 578.0140380859375, 'l2_loss': 206.61793518066406}
Step 1400: {'loss': 226.5792999267578, 'l1_loss': 585.1002197265625, 'l2_loss': 203.17529296875}
Step 1500: {'loss': 224.97412109375, 'l1_loss': 536.0109252929688, 'l2_loss': 203.53369140625}
Step 1600: {'loss': 224.02197265625, 'l1_loss': 535.0396728515625, 'l2_loss': 202.62039184570312}
Step 1700: {'loss': 221.52767944335938, 'l1_loss': 510.0685729980469, 'l2_loss': 201.12493896484375}
Step 1800: {'loss': 221.51443481445312, 'l1_loss': 535.8087158203125, 'l2_loss': 200.08209228515625}
Step 1900: {'loss': 219.86631774902344, 'l1_loss': 515.2140502929688, 'l2_loss': 199.25775146484375}
Step 2000: {'loss': 218.75064086914062, 'l1_loss': 510.550048828125, 'l2_loss': 198.32864379882812}
Step 2100: {'loss': 218.67974853515625, 'l1_loss': 491.6463623046875, 'l2_loss': 199.01390075683594}
Step 2200: {'loss': 214.6620330810547, 'l1_loss': 453.89776611328125, 'l2_loss': 196.50611877441406}
Step 2300: {'loss': 215.2037353515625, 'l1_loss': 499.33453369140625, 'l2_loss': 195.2303466796875}
Step 2400: {'loss': 215.3161163330078, 'l1_loss': 488.50531005859375, 'l2_loss': 195.77590942382812}
Step 2500: {'loss': 212.91586303710938, 'l1_loss': 494.22149658203125, 'l2_loss': 193.14700317382812}
Step 2600: {'loss': 212.79495239257812, 'l1_loss': 514.818603515625, 'l2_loss': 192.20220947265625}
Step 2700: {'loss': 212.4857940673828, 'l1_loss': 491.69744873046875, 'l2_loss': 192.81790161132812}
Step 2800: {'loss': 211.439453125, 'l1_loss': 472.43902587890625, 'l2_loss': 192.54190063476562}
Step 2900: {'loss': 209.58633422851562, 'l1_loss': 460.3796691894531, 'l2_loss': 191.171142578125}
Step 3000: {'loss': 210.8347625732422, 'l1_loss': 495.0528869628906, 'l2_loss': 191.03265380859375}
Step 3100: {'loss': 208.90760803222656, 'l1_loss': 486.3028869628906, 'l2_loss': 189.4554901123047}
Step 3200: {'loss': 208.60748291015625, 'l1_loss': 468.57733154296875, 'l2_loss': 189.86439514160156}
Step 3300: {'loss': 207.92916870117188, 'l1_loss': 470.60345458984375, 'l2_loss': 189.1050262451172}
Step 3400: {'loss': 208.91224670410156, 'l1_loss': 493.04925537109375, 'l2_loss': 189.19027709960938}
Step 3500: {'loss': 205.50619506835938, 'l1_loss': 463.98126220703125, 'l2_loss': 186.9469451904297}
Step 3600: {'loss': 207.74160766601562, 'l1_loss': 488.82769775390625, 'l2_loss': 188.18850708007812}
Step 3700: {'loss': 206.8702392578125, 'l1_loss': 477.2414855957031, 'l2_loss': 187.78057861328125}
Step 3800: {'loss': 205.14971923828125, 'l1_loss': 487.33441162109375, 'l2_loss': 185.65634155273438}
Step 3900: {'loss': 205.3732452392578, 'l1_loss': 481.3116760253906, 'l2_loss': 186.1207733154297}
Step 4000: {'loss': 204.7164306640625, 'l1_loss': 467.72015380859375, 'l2_loss': 186.00762939453125}
Step 4100: {'loss': 204.75836181640625, 'l1_loss': 470.21722412109375, 'l2_loss': 185.94967651367188}
Step 4200: {'loss': 204.56454467773438, 'l1_loss': 493.02001953125, 'l2_loss': 184.84375}
Step 4300: {'loss': 204.554931640625, 'l1_loss': 488.14263916015625, 'l2_loss': 185.0292205810547}
Step 4400: {'loss': 203.0932159423828, 'l1_loss': 478.8529968261719, 'l2_loss': 183.93910217285156}
Step 4500: {'loss': 203.61915588378906, 'l1_loss': 488.8935852050781, 'l2_loss': 184.06341552734375}
Step 4600: {'loss': 203.08319091796875, 'l1_loss': 487.5282287597656, 'l2_loss': 183.58206176757812}
Step 4700: {'loss': 202.27235412597656, 'l1_loss': 504.82525634765625, 'l2_loss': 182.079345703125}
Step 4800: {'loss': 202.3258819580078, 'l1_loss': 494.7270202636719, 'l2_loss': 182.53680419921875}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Saved evaluation results to: run_1/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8460000157356262, scr_score: -0.7761204121084663
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8170000314712524, scr_score: -1.2089561798922313
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8210000395774841, scr_score: -1.1492545412975848
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7850000262260437, scr_score: -1.6865683990274654
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6110000014305115, scr_score: 0.03787874910931702
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6580000519752502, scr_score: 0.1565657371453411
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6890000104904175, scr_score: 0.23484845520124242
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6910000443458557, scr_score: 0.23989904539254622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6890000104904175, scr_score: 0.23484845520124242
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7050000429153442, scr_score: 0.27525257466459596
Enabled 1 new features. Total active: 1153
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9540000557899475
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9940000176429749
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8470000624656677, scr_score: 0.018182123594007893
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8540000319480896, scr_score: 0.08181820152219406
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8530000448226929, scr_score: 0.0727274106553584
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8230000138282776, scr_score: -0.2000001083720673
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8130000233650208, scr_score: -0.29090910076109705
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7890000343322754, scr_score: -0.5090907908668356
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7070000171661377, scr_score: 0.1700878740213084
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7040000557899475, scr_score: 0.16129033385765607
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7010000348091125, scr_score: 0.15249261890033458
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7020000219345093, scr_score: 0.1554251322882187
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6980000138282776, scr_score: 0.14369490394301307
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6990000605583191, scr_score: 0.14662759212456636
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6990000605583191, scr_score: 0.14662759212456636
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.968000054359436
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9970000386238098
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9170000553131104, scr_score: 0.01818158173367131
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9140000343322754, scr_score: -0.036364247188015786
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9040000438690186, scr_score: -0.2181822319660752
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.23636381369974652
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9050000309944153, scr_score: -0.2000006502324039
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.7090914410992396
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8690000176429749, scr_score: -0.8545462624099563
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.593000054359436, scr_score: 0.03194105964755349
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5950000286102295, scr_score: 0.03685500123236629
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6000000238418579, scr_score: 0.04914000164315505
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6050000190734863, scr_score: 0.061425002053943814
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6140000224113464, scr_score: 0.08353803208311494
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6170000433921814, scr_score: 0.09090909090909091
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6220000386238098, scr_score: 0.10319409131987967
Enabled 1 new features. Total active: 1153
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Enabled 1 new features. Total active: 1154
Enabled 1 new features. Total active: 1155
Enabled 1 new features. Total active: 1156
Enabled 1 new features. Total active: 1157
Enabled 1 new features. Total active: 1158
Enabled 1 new features. Total active: 1159
Enabled 1 new features. Total active: 1160
Enabled 1 new features. Total active: 1161
Enabled 1 new features. Total active: 1162
Enabled 1 new features. Total active: 1163
Enabled 1 new features. Total active: 1164
Enabled 1 new features. Total active: 1165
Enabled 1 new features. Total active: 1166
Enabled 1 new features. Total active: 1167
Enabled 1 new features. Total active: 1168
Enabled 1 new features. Total active: 1169
Enabled 1 new features. Total active: 1170
Enabled 1 new features. Total active: 1171
Enabled 1 new features. Total active: 1172
Enabled 1 new features. Total active: 1173
Enabled 1 new features. Total active: 1174
Enabled 1 new features. Total active: 1175
Enabled 1 new features. Total active: 1176
Enabled 1 new features. Total active: 1177
Enabled 1 new features. Total active: 1178
Enabled 1 new features. Total active: 1179
Enabled 1 new features. Total active: 1180
Enabled 1 new features. Total active: 1181
Enabled 1 new features. Total active: 1182
Enabled 1 new features. Total active: 1183
Enabled 1 new features. Total active: 1184
Enabled 1 new features. Total active: 1185
Enabled 1 new features. Total active: 1186
Enabled 1 new features. Total active: 1187
Enabled 1 new features. Total active: 1188
Enabled 1 new features. Total active: 1189
Enabled 1 new features. Total active: 1190
Enabled 1 new features. Total active: 1191
Enabled 1 new features. Total active: 1192
Enabled 1 new features. Total active: 1193
Enabled 1 new features. Total active: 1194
Enabled 1 new features. Total active: 1195
Enabled 1 new features. Total active: 1196
Enabled 1 new features. Total active: 1197
Enabled 1 new features. Total active: 1198
Enabled 1 new features. Total active: 1199
Enabled 1 new features. Total active: 1200
Enabled 1 new features. Total active: 1201
Enabled 1 new features. Total active: 1202
Enabled 1 new features. Total active: 1203
Enabled 1 new features. Total active: 1204
Enabled 1 new features. Total active: 1205
Enabled 1 new features. Total active: 1206
Enabled 1 new features. Total active: 1207
Enabled 1 new features. Total active: 1208
Enabled 1 new features. Total active: 1209
Enabled 1 new features. Total active: 1210
Enabled 1 new features. Total active: 1211
Enabled 1 new features. Total active: 1212
Enabled 1 new features. Total active: 1213
Enabled 1 new features. Total active: 1214
Enabled 1 new features. Total active: 1215
Enabled 1 new features. Total active: 1216
Enabled 1 new features. Total active: 1217
Enabled 1 new features. Total active: 1218
Enabled 1 new features. Total active: 1219
Enabled 1 new features. Total active: 1220
Enabled 1 new features. Total active: 1221
Enabled 1 new features. Total active: 1222
Enabled 1 new features. Total active: 1223
Enabled 1 new features. Total active: 1224
Enabled 1 new features. Total active: 1225
Enabled 1 new features. Total active: 1226
Enabled 1 new features. Total active: 1227
Enabled 1 new features. Total active: 1228
Enabled 1 new features. Total active: 1229
Enabled 1 new features. Total active: 1230
Enabled 1 new features. Total active: 1231
Enabled 1 new features. Total active: 1232
Enabled 1 new features. Total active: 1233
Enabled 1 new features. Total active: 1234
Enabled 1 new features. Total active: 1235
Enabled 1 new features. Total active: 1236
Enabled 1 new features. Total active: 1237
Enabled 1 new features. Total active: 1238
Enabled 1 new features. Total active: 1239
Enabled 1 new features. Total active: 1240
Enabled 1 new features. Total active: 1241
Enabled 1 new features. Total active: 1242
Enabled 1 new features. Total active: 1243
Enabled 1 new features. Total active: 1244
Enabled 1 new features. Total active: 1245
Enabled 1 new features. Total active: 1246
Enabled 1 new features. Total active: 1247
Enabled 1 new features. Total active: 1248
Enabled 1 new features. Total active: 1249
Enabled 1 new features. Total active: 1250
Enabled 1 new features. Total active: 1251
Enabled 1 new features. Total active: 1252
Enabled 1 new features. Total active: 1253
Step 100: {'loss': 481.2099304199219, 'l1_loss': 3541.18115234375, 'l2_loss': 339.56268310546875}
Enabled 1 new features. Total active: 1254
Enabled 1 new features. Total active: 1255
Enabled 1 new features. Total active: 1256
Enabled 1 new features. Total active: 1257
Enabled 1 new features. Total active: 1258
Enabled 1 new features. Total active: 1259
Enabled 1 new features. Total active: 1260
Enabled 1 new features. Total active: 1261
Enabled 1 new features. Total active: 1262
Enabled 1 new features. Total active: 1263
Enabled 1 new features. Total active: 1264
Enabled 1 new features. Total active: 1265
Enabled 1 new features. Total active: 1266
Enabled 1 new features. Total active: 1267
Enabled 1 new features. Total active: 1268
Enabled 1 new features. Total active: 1269
Enabled 1 new features. Total active: 1270
Enabled 1 new features. Total active: 1271
Enabled 1 new features. Total active: 1272
Enabled 1 new features. Total active: 1273
Enabled 1 new features. Total active: 1274
Enabled 1 new features. Total active: 1275
Enabled 1 new features. Total active: 1276
Enabled 1 new features. Total active: 1277
Enabled 1 new features. Total active: 1278
Enabled 1 new features. Total active: 1279
Enabled 1 new features. Total active: 1280
Enabled 1 new features. Total active: 1281
Enabled 1 new features. Total active: 1282
Enabled 1 new features. Total active: 1283
Enabled 1 new features. Total active: 1284
Enabled 1 new features. Total active: 1285
Enabled 1 new features. Total active: 1286
Enabled 1 new features. Total active: 1287
Enabled 1 new features. Total active: 1288
Enabled 1 new features. Total active: 1289
Enabled 1 new features. Total active: 1290
Enabled 1 new features. Total active: 1291
Enabled 1 new features. Total active: 1292
Enabled 1 new features. Total active: 1293
Enabled 1 new features. Total active: 1294
Enabled 1 new features. Total active: 1295
Enabled 1 new features. Total active: 1296
Enabled 1 new features. Total active: 1297
Step 200: {'loss': 381.58306884765625, 'l1_loss': 1666.2921142578125, 'l2_loss': 314.931396484375}
Step 300: {'loss': 340.7734375, 'l1_loss': 792.018798828125, 'l2_loss': 309.0926818847656}
Step 400: {'loss': 316.2876281738281, 'l1_loss': 702.9044189453125, 'l2_loss': 288.17144775390625}
Step 500: {'loss': 290.5782775878906, 'l1_loss': 843.7872314453125, 'l2_loss': 256.8267822265625}
Step 600: {'loss': 274.0348815917969, 'l1_loss': 861.3532104492188, 'l2_loss': 239.58074951171875}
Step 700: {'loss': 258.49774169921875, 'l1_loss': 852.011474609375, 'l2_loss': 224.41729736328125}
Step 800: {'loss': 251.28221130371094, 'l1_loss': 848.234619140625, 'l2_loss': 217.3528289794922}
Step 900: {'loss': 242.73635864257812, 'l1_loss': 723.9234619140625, 'l2_loss': 213.7794189453125}
Step 1000: {'loss': 241.5872802734375, 'l1_loss': 770.9268798828125, 'l2_loss': 210.75021362304688}
Step 1100: {'loss': 234.95875549316406, 'l1_loss': 650.2155151367188, 'l2_loss': 208.95013427734375}
Step 1200: {'loss': 227.99722290039062, 'l1_loss': 547.3679809570312, 'l2_loss': 206.10250854492188}
Step 1300: {'loss': 229.1379852294922, 'l1_loss': 591.3851318359375, 'l2_loss': 205.48257446289062}
Step 1400: {'loss': 225.69912719726562, 'l1_loss': 594.4755859375, 'l2_loss': 201.92010498046875}
Step 1500: {'loss': 224.09095764160156, 'l1_loss': 540.1959838867188, 'l2_loss': 202.48312377929688}
Step 1600: {'loss': 223.15335083007812, 'l1_loss': 532.8870849609375, 'l2_loss': 201.83786010742188}
Step 1700: {'loss': 220.38525390625, 'l1_loss': 503.7796936035156, 'l2_loss': 200.23406982421875}
Step 1800: {'loss': 220.12696838378906, 'l1_loss': 526.980712890625, 'l2_loss': 199.04774475097656}
Step 1900: {'loss': 218.44833374023438, 'l1_loss': 504.94525146484375, 'l2_loss': 198.25051879882812}Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9690000414848328
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9640000462532043
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8370000123977661, scr_score: -0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8300000429153442, scr_score: -0.07751930104548388
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8390000462532043, scr_score: -0.007751837694252218
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7820000648498535, scr_score: -0.4496121308843989
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6780000329017639, scr_score: 0.05373135709263166
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6920000314712524, scr_score: 0.09552239283975711
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7040000557899475, scr_score: 0.1313433568762757
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7170000076293945, scr_score: 0.17014917884380523
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.706000030040741, scr_score: 0.13731342858688256
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7080000042915344, scr_score: 0.14328350029748943
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.718000054359436, scr_score: 0.17313439262340116
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9310000538825989
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8230000138282776, scr_score: -0.006024377580324436
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8230000138282776, scr_score: -0.006024377580324436
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7940000295639038, scr_score: -0.18072306893528156
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8070000410079956, scr_score: -0.10240975103212978
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8140000104904175, scr_score: -0.06024126235449825
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8170000314712524, scr_score: -0.04216884774173812
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8110000491142273, scr_score: -0.0783133179031518
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7090000510215759, scr_score: 0.13503650349568203
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7290000319480896, scr_score: 0.20802913753598304
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7400000095367432, scr_score: 0.24817504275110688
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7360000610351562, scr_score: 0.23357668997121356
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7200000286102295, scr_score: 0.17518240871080587
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.706000030040741, scr_score: 0.12408752137555344
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6850000619888306, scr_score: 0.047445299140279104
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9790000319480896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9790000319480896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8160000443458557, scr_score: 0.04117663764621982
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8270000219345093, scr_score: 0.10588238181539601
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8410000205039978, scr_score: 0.18823530649231257
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.7950000166893005, scr_score: -0.08235292467691656
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8290000557899475, scr_score: 0.11764728569239728
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8220000267028809, scr_score: 0.07647064804617747
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8050000667572021, scr_score: -0.023529106522956377
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7040000557899475, scr_score: 0.056391140868614346
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7100000381469727, scr_score: 0.07894746276950931
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.718000054359436, scr_score: 0.10902270802242568
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7050000429153442, scr_score: 0.06015049050583274
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7140000462532043, scr_score: 0.0939850853959675
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184

Step 2000: {'loss': 217.38583374023438, 'l1_loss': 499.8209228515625, 'l2_loss': 197.39300537109375}
Step 2100: {'loss': 217.2660675048828, 'l1_loss': 482.7727966308594, 'l2_loss': 197.9551544189453}
Step 2200: {'loss': 213.35745239257812, 'l1_loss': 444.793701171875, 'l2_loss': 195.56570434570312}
Step 2300: {'loss': 214.00637817382812, 'l1_loss': 490.9206237792969, 'l2_loss': 194.3695526123047}
Step 2400: {'loss': 214.017578125, 'l1_loss': 481.862060546875, 'l2_loss': 194.74310302734375}
Step 2500: {'loss': 211.56736755371094, 'l1_loss': 489.22808837890625, 'l2_loss': 191.9982452392578}
Step 2600: {'loss': 211.73402404785156, 'l1_loss': 510.47442626953125, 'l2_loss': 191.31504821777344}
Step 2700: {'loss': 211.46047973632812, 'l1_loss': 488.03900146484375, 'l2_loss': 191.9389190673828}
Step 2800: {'loss': 210.64547729492188, 'l1_loss': 469.6737976074219, 'l2_loss': 191.8585205078125}
Step 2900: {'loss': 208.82225036621094, 'l1_loss': 456.9937744140625, 'l2_loss': 190.54249572753906}
Step 3000: {'loss': 210.3185577392578, 'l1_loss': 491.111328125, 'l2_loss': 190.67410278320312}
Step 3100: {'loss': 208.47560119628906, 'l1_loss': 482.6107177734375, 'l2_loss': 189.17117309570312}
Step 3200: {'loss': 208.1592254638672, 'l1_loss': 464.97454833984375, 'l2_loss': 189.56024169921875}
Step 3300: {'loss': 207.2714080810547, 'l1_loss': 468.8512268066406, 'l2_loss': 188.51736450195312}
Step 3400: {'loss': 208.138671875, 'l1_loss': 489.39825439453125, 'l2_loss': 188.562744140625}
Step 3500: {'loss': 205.10263061523438, 'l1_loss': 462.19940185546875, 'l2_loss': 186.61465454101562}
Step 3600: {'loss': 207.42010498046875, 'l1_loss': 485.64971923828125, 'l2_loss': 187.99411010742188}
Step 3700: {'loss': 206.40216064453125, 'l1_loss': 473.31524658203125, 'l2_loss': 187.46954345703125}
Step 3800: {'loss': 204.6769561767578, 'l1_loss': 483.65631103515625, 'l2_loss': 185.33070373535156}
Step 3900: {'loss': 205.16107177734375, 'l1_loss': 476.9210205078125, 'l2_loss': 186.084228515625}
Step 4000: {'loss': 204.33905029296875, 'l1_loss': 465.20001220703125, 'l2_loss': 185.73104858398438}
Step 4100: {'loss': 204.62448120117188, 'l1_loss': 465.2846374511719, 'l2_loss': 186.01309204101562}
Step 4200: {'loss': 204.23709106445312, 'l1_loss': 486.6907043457031, 'l2_loss': 184.76947021484375}
Step 4300: {'loss': 204.54779052734375, 'l1_loss': 481.4493408203125, 'l2_loss': 185.28982543945312}
Step 4400: {'loss': 203.17721557617188, 'l1_loss': 473.2479248046875, 'l2_loss': 184.24729919433594}
Step 4500: {'loss': 203.64166259765625, 'l1_loss': 481.9273681640625, 'l2_loss': 184.36456298828125}
Step 4600: {'loss': 203.04974365234375, 'l1_loss': 481.5833740234375, 'l2_loss': 183.78640747070312}
Step 4700: {'loss': 202.32025146484375, 'l1_loss': 497.6534118652344, 'l2_loss': 182.41412353515625}
Step 4800: {'loss': 202.74374389648438, 'l1_loss': 488.00054931640625, 'l2_loss': 183.22372436523438}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9580000638961792
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9820000529289246
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.999000072479248
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.12499993347698757
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8840000629425049, scr_score: 0.13392891829856485
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8850000500679016, scr_score: 0.14285737093604264
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8840000629425049, scr_score: 0.13392891829856485
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.12499993347698757
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8880000710487366, scr_score: 0.16964326103257552
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8040000200271606, scr_score: -0.5803568720134493
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6390000581741333, scr_score: 0.030395283805928035
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6550000309944153, scr_score: 0.07902741179102869
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6680000424385071, scr_score: 0.11854111768654303
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6810000538825989, scr_score: 0.15805482358205739
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7170000076293945, scr_score: 0.2674771568408094
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.706000030040741, scr_score: 0.23404258017412163
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6840000152587891, scr_score: 0.16717324567164368
Loaded pretrained model google/gemma-2-2b into HookedTransformer

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9610000252723694
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9600000381469727
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9610000252723694
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.024038438117628045
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7660000324249268, scr_score: 0.06250005373014743
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.718000054359436, scr_score: -0.16826906682339632
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.718000054359436, scr_score: -0.16826906682339632
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7780000567436218, scr_score: 0.1201924771489265
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7610000371932983, scr_score: 0.03846161561251938
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.737000048160553, scr_score: -0.02752285551095536
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7410000562667847, scr_score: -0.009174194031715687
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7430000305175781, scr_score: 0.0
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7670000195503235, scr_score: 0.11009169545962974
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7620000243186951, scr_score: 0.08715593696453222
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.10091750142791406
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7350000143051147, scr_score: -0.03669732295847935
Enabled 1 new features. Total active: 1153
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Enabled 1 new features. Total active: 1154
Enabled 1 new features. Total active: 1155
Enabled 1 new features. Total active: 1156
Enabled 1 new features. Total active: 1157
Enabled 1 new features. Total active: 1158
Enabled 1 new features. Total active: 1159
Enabled 1 new features. Total active: 1160
Enabled 1 new features. Total active: 1161
Enabled 1 new features. Total active: 1162
Enabled 1 new features. Total active: 1163
Enabled 1 new features. Total active: 1164
Enabled 1 new features. Total active: 1165
Enabled 1 new features. Total active: 1166
Enabled 1 new features. Total active: 1167
Enabled 1 new features. Total active: 1168
Enabled 1 new features. Total active: 1169
Enabled 1 new features. Total active: 1170
Enabled 1 new features. Total active: 1171
Enabled 1 new features. Total active: 1172
Enabled 1 new features. Total active: 1173
Enabled 1 new features. Total active: 1174
Enabled 1 new features. Total active: 1175
Enabled 1 new features. Total active: 1176
Enabled 1 new features. Total active: 1177
Enabled 1 new features. Total active: 1178
Enabled 1 new features. Total active: 1179
Enabled 1 new features. Total active: 1180
Enabled 1 new features. Total active: 1181
Enabled 1 new features. Total active: 1182
Enabled 1 new features. Total active: 1183
Enabled 1 new features. Total active: 1184
Enabled 1 new features. Total active: 1185
Enabled 1 new features. Total active: 1186
Enabled 1 new features. Total active: 1187
Enabled 1 new features. Total active: 1188
Enabled 1 new features. Total active: 1189
Enabled 1 new features. Total active: 1190
Enabled 1 new features. Total active: 1191
Enabled 1 new features. Total active: 1192
Enabled 1 new features. Total active: 1193
Enabled 1 new features. Total active: 1194
Enabled 1 new features. Total active: 1195
Enabled 1 new features. Total active: 1196
Enabled 1 new features. Total active: 1197
Enabled 1 new features. Total active: 1198
Enabled 1 new features. Total active: 1199
Enabled 1 new features. Total active: 1200
Enabled 1 new features. Total active: 1201
Enabled 1 new features. Total active: 1202
Enabled 1 new features. Total active: 1203
Enabled 1 new features. Total active: 1204
Enabled 1 new features. Total active: 1205
Enabled 1 new features. Total active: 1206
Enabled 1 new features. Total active: 1207
Enabled 1 new features. Total active: 1208
Enabled 1 new features. Total active: 1209
Enabled 1 new features. Total active: 1210
Enabled 1 new features. Total active: 1211
Enabled 1 new features. Total active: 1212
Enabled 1 new features. Total active: 1213
Enabled 1 new features. Total active: 1214
Enabled 1 new features. Total active: 1215
Enabled 1 new features. Total active: 1216
Enabled 1 new features. Total active: 1217
Enabled 1 new features. Total active: 1218
Enabled 1 new features. Total active: 1219
Enabled 1 new features. Total active: 1220
Enabled 1 new features. Total active: 1221
Enabled 1 new features. Total active: 1222
Enabled 1 new features. Total active: 1223
Enabled 1 new features. Total active: 1224
Enabled 1 new features. Total active: 1225
Enabled 1 new features. Total active: 1226
Enabled 1 new features. Total active: 1227
Enabled 1 new features. Total active: 1228
Enabled 1 new features. Total active: 1229
Enabled 1 new features. Total active: 1230
Enabled 1 new features. Total active: 1231
Enabled 1 new features. Total active: 1232
Enabled 1 new features. Total active: 1233
Enabled 1 new features. Total active: 1234
Enabled 1 new features. Total active: 1235
Enabled 1 new features. Total active: 1236
Enabled 1 new features. Total active: 1237
Enabled 1 new features. Total active: 1238
Enabled 1 new features. Total active: 1239
Enabled 1 new features. Total active: 1240
Enabled 1 new features. Total active: 1241
Enabled 1 new features. Total active: 1242
Enabled 1 new features. Total active: 1243
Enabled 1 new features. Total active: 1244
Enabled 1 new features. Total active: 1245
Enabled 1 new features. Total active: 1246
Enabled 1 new features. Total active: 1247
Enabled 1 new features. Total active: 1248
Enabled 1 new features. Total active: 1249
Enabled 1 new features. Total active: 1250
Enabled 1 new features. Total active: 1251
Enabled 1 new features. Total active: 1252
Enabled 1 new features. Total active: 1253
Step 100: {'loss': 481.2099304199219, 'l1_loss': 3541.18115234375, 'l2_loss': 339.56268310546875}
Enabled 1 new features. Total active: 1254
Enabled 1 new features. Total active: 1255
Enabled 1 new features. Total active: 1256
Enabled 1 new features. Total active: 1257
Enabled 1 new features. Total active: 1258
Enabled 1 new features. Total active: 1259
Enabled 1 new features. Total active: 1260
Enabled 1 new features. Total active: 1261
Enabled 1 new features. Total active: 1262
Enabled 1 new features. Total active: 1263
Enabled 1 new features. Total active: 1264
Enabled 1 new features. Total active: 1265
Enabled 1 new features. Total active: 1266
Enabled 1 new features. Total active: 1267
Enabled 1 new features. Total active: 1268
Enabled 1 new features. Total active: 1269
Enabled 1 new features. Total active: 1270
Enabled 1 new features. Total active: 1271
Enabled 1 new features. Total active: 1272
Enabled 1 new features. Total active: 1273
Enabled 1 new features. Total active: 1274
Enabled 1 new features. Total active: 1275
Enabled 1 new features. Total active: 1276
Enabled 1 new features. Total active: 1277
Enabled 1 new features. Total active: 1278
Enabled 1 new features. Total active: 1279
Enabled 1 new features. Total active: 1280
Enabled 1 new features. Total active: 1281
Enabled 1 new features. Total active: 1282
Enabled 1 new features. Total active: 1283
Enabled 1 new features. Total active: 1284
Enabled 1 new features. Total active: 1285
Enabled 1 new features. Total active: 1286
Enabled 1 new features. Total active: 1287
Enabled 1 new features. Total active: 1288
Enabled 1 new features. Total active: 1289
Enabled 1 new features. Total active: 1290
Enabled 1 new features. Total active: 1291
Enabled 1 new features. Total active: 1292
Enabled 1 new features. Total active: 1293
Enabled 1 new features. Total active: 1294
Enabled 1 new features. Total active: 1295
Enabled 1 new features. Total active: 1296
Enabled 1 new features. Total active: 1297
Step 200: {'loss': 381.58306884765625, 'l1_loss': 1666.2921142578125, 'l2_loss': 314.931396484375}
Step 300: {'loss': 340.7734375, 'l1_loss': 792.018798828125, 'l2_loss': 309.0926818847656}
Step 400: {'loss': 316.2876281738281, 'l1_loss': 702.9044189453125, 'l2_loss': 288.17144775390625}
Step 500: {'loss': 290.5782775878906, 'l1_loss': 843.7872314453125, 'l2_loss': 256.8267822265625}
Step 600: {'loss': 274.0348815917969, 'l1_loss': 861.3532104492188, 'l2_loss': 239.58074951171875}
Step 700: {'loss': 258.49774169921875, 'l1_loss': 852.011474609375, 'l2_loss': 224.41729736328125}
Step 800: {'loss': 251.28221130371094, 'l1_loss': 848.234619140625, 'l2_loss': 217.3528289794922}
Step 900: {'loss': 242.73635864257812, 'l1_loss': 723.9234619140625, 'l2_loss': 213.7794189453125}
Step 1000: {'loss': 241.5872802734375, 'l1_loss': 770.9268798828125, 'l2_loss': 210.75021362304688}
Step 1100: {'loss': 234.95875549316406, 'l1_loss': 650.2155151367188, 'l2_loss': 208.95013427734375}
Step 1200: {'loss': 227.99722290039062, 'l1_loss': 547.3679809570312, 'l2_loss': 206.10250854492188}
Step 1300: {'loss': 229.1379852294922, 'l1_loss': 591.3851318359375, 'l2_loss': 205.48257446289062}
Step 1400: {'loss': 225.69912719726562, 'l1_loss': 594.4755859375, 'l2_loss': 201.92010498046875}
Step 1500: {'loss': 224.09095764160156, 'l1_loss': 540.1959838867188, 'l2_loss': 202.48312377929688}
Step 1600: {'loss': 223.15335083007812, 'l1_loss': 532.8870849609375, 'l2_loss': 201.83786010742188}
Step 1700: {'loss': 220.38525390625, 'l1_loss': 503.7796936035156, 'l2_loss': 200.23406982421875}
Step 1800: {'loss': 220.12696838378906, 'l1_loss': 526.980712890625, 'l2_loss': 199.04774475097656}
Step 1900: {'loss': 218.44833374023438, 'l1_loss': 504.94525146484375, 'l2_loss': 198.25051879882812}
Step 2000: {'loss': 217.38583374023438, 'l1_loss': 499.8209228515625, 'l2_loss': 197.39300537109375}
Step 2100: {'loss': 217.2660675048828, 'l1_loss': 482.7727966308594, 'l2_loss': 197.9551544189453}
Step 2200: {'loss': 213.35745239257812, 'l1_loss': 444.793701171875, 'l2_loss': 195.56570434570312}
Step 2300: {'loss': 214.00637817382812, 'l1_loss': 490.9206237792969, 'l2_loss': 194.3695526123047}
Step 2400: {'loss': 214.017578125, 'l1_loss': 481.862060546875, 'l2_loss': 194.74310302734375}
Step 2500: {'loss': 211.56736755371094, 'l1_loss': 489.22808837890625, 'l2_loss': 191.9982452392578}
Step 2600: {'loss': 211.73402404785156, 'l1_loss': 510.47442626953125, 'l2_loss': 191.31504821777344}
Step 2700: {'loss': 211.46047973632812, 'l1_loss': 488.03900146484375, 'l2_loss': 191.9389190673828}
Step 2800: {'loss': 210.64547729492188, 'l1_loss': 469.6737976074219, 'l2_loss': 191.8585205078125}
Step 2900: {'loss': 208.82225036621094, 'l1_loss': 456.9937744140625, 'l2_loss': 190.54249572753906}
Step 3000: {'loss': 210.3185577392578, 'l1_loss': 491.111328125, 'l2_loss': 190.67410278320312}
Step 3100: {'loss': 208.47560119628906, 'l1_loss': 482.6107177734375, 'l2_loss': 189.17117309570312}
Step 3200: {'loss': 208.1592254638672, 'l1_loss': 464.97454833984375, 'l2_loss': 189.56024169921875}
Step 3300: {'loss': 207.2714080810547, 'l1_loss': 468.8512268066406, 'l2_loss': 188.51736450195312}
Step 3400: {'loss': 208.138671875, 'l1_loss': 489.39825439453125, 'l2_loss': 188.562744140625}
Step 3500: {'loss': 205.10263061523438, 'l1_loss': 462.19940185546875, 'l2_loss': 186.61465454101562}
Step 3600: {'loss': 207.42010498046875, 'l1_loss': 485.64971923828125, 'l2_loss': 187.99411010742188}
Step 3700: {'loss': 206.40216064453125, 'l1_loss': 473.31524658203125, 'l2_loss': 187.46954345703125}
Step 3800: {'loss': 204.6769561767578, 'l1_loss': 483.65631103515625, 'l2_loss': 185.33070373535156}
Step 3900: {'loss': 205.16107177734375, 'l1_loss': 476.9210205078125, 'l2_loss': 186.084228515625}
Step 4000: {'loss': 204.33905029296875, 'l1_loss': 465.20001220703125, 'l2_loss': 185.73104858398438}
Step 4100: {'loss': 204.62448120117188, 'l1_loss': 465.2846374511719, 'l2_loss': 186.01309204101562}
Step 4200: {'loss': 204.23709106445312, 'l1_loss': 486.6907043457031, 'l2_loss': 184.76947021484375}
Step 4300: {'loss': 204.54779052734375, 'l1_loss': 481.4493408203125, 'l2_loss': 185.28982543945312}
Step 4400: {'loss': 203.17721557617188, 'l1_loss': 473.2479248046875, 'l2_loss': 184.24729919433594}
Step 4500: {'loss': 203.64166259765625, 'l1_loss': 481.9273681640625, 'l2_loss': 184.36456298828125}
Step 4600: {'loss': 203.04974365234375, 'l1_loss': 481.5833740234375, 'l2_loss': 183.78640747070312}
Step 4700: {'loss': 202.32025146484375, 'l1_loss': 497.6534118652344, 'l2_loss': 182.41412353515625}
Step 4800: {'loss': 202.74374389648438, 'l1_loss': 488.00054931640625, 'l2_loss': 183.22372436523438}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.906000018119812, scr_score: 0.1194023875673555
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9010000228881836, scr_score: 0.0447755617295318
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8700000643730164, scr_score: -0.41791058054058766
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8700000643730164, scr_score: -0.41791058054058766
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8550000190734863, scr_score: -0.6417919476759961
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7970000505447388, scr_score: -1.507463483243526
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6190000176429749, scr_score: 0.058080808840993786
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6650000214576721, scr_score: 0.17424242652298136
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6680000424385071, scr_score: 0.18181823655155244
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6630000472068787, scr_score: 0.16919198684844677
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6950000524520874, scr_score: 0.25000007525838464
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6970000267028809, scr_score: 0.2550505149329192
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.690000057220459, scr_score: 0.23737382555527894
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9570000171661377
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9940000176429749
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8490000367164612, scr_score: 0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8560000658035278, scr_score: 0.09821462783281747
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8470000624656677, scr_score: 0.017857446962504415
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8370000123977661, scr_score: -0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8240000605583191, scr_score: -0.18749973390780864
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8030000329017639, scr_score: -0.375
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6550000309944153, scr_score: 0.02332365873061937
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6730000376701355, scr_score: 0.07580180398728945
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6740000247955322, scr_score: 0.0787172178850051
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.675000011920929, scr_score: 0.08163263178272077
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6850000619888306, scr_score: 0.1107872920832185
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6800000071525574, scr_score: 0.09620987504574613
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6810000538825989, scr_score: 0.09912546271790881
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8740000128746033, scr_score: -0.8653841745222463
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8710000514984131, scr_score: -0.9230756886622895
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8840000629425049, scr_score: -0.6730756886622895
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8880000710487366, scr_score: -0.5961525235667388
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.893000066280365, scr_score: -0.4999988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8800000548362732, scr_score: -0.7499988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.859000027179718, scr_score: -1.1538451839489419
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.612000048160553, scr_score: 0.08515821189552046
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6150000095367432, scr_score: 0.0924573877561912
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6190000176429749, scr_score: 0.10218976726054742
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6300000548362732, scr_score: 0.12895384715339248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9940000176429749
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9660000205039978
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8250000476837158, scr_score: -0.07692300638503319
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8270000219345093, scr_score: -0.06153858850694027
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8130000233650208, scr_score: -0.1692308891454436
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -0.09230788276041041
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8030000329017639, scr_score: -0.24615389553047679
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7900000214576721, scr_score: -0.3461539872299336
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.781000018119812, scr_score: -0.4153847846759204
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6810000538825989, scr_score: 0.0484849404369056
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6930000185966492, scr_score: 0.08484846514447261
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6970000267028809, scr_score: 0.09696970025369901
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7000000476837158, scr_score: 0.10606067174064686
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7050000429153442, scr_score: 0.12121217047215181
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7130000591278076, scr_score: 0.14545464069060463
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7190000414848328, scr_score: 0.16363640304438812
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9360000491142273
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9340000152587891
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8260000348091125, scr_score: 0.05202312935342067
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8370000123977661, scr_score: 0.11560680099184864
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8460000157356262, scr_score: 0.1676299303452693
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.781000018119812, scr_score: -0.20809251741368268
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7830000519752502, scr_score: -0.19653163059323148
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7860000133514404, scr_score: -0.17919081716572052
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7820000648498535, scr_score: -0.2023119017357351
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7430000305175781, scr_score: 0.24313716782097103
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.737000048160553, scr_score: 0.21960782480441496
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7360000610351562, scr_score: 0.21568630659227384
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7390000224113464, scr_score: 0.22745086122869718
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7350000143051147, scr_score: 0.21176455463642335
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7190000414848328, scr_score: 0.1490195620110374
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7140000462532043, scr_score: 0.12941173720662244
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9770000576972961
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9970000386238098
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8070000410079956, scr_score: 0.06451624274209494
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8290000557899475, scr_score: 0.1827958608751049
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8290000557899475, scr_score: 0.1827958608751049
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.831000030040741, scr_score: 0.19354840777128998
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.812000036239624, scr_score: 0.09139793043755245
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8050000667572021, scr_score: 0.053763695845909856
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7910000085830688, scr_score: -0.02150541424736498
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7140000462532043, scr_score: 0.04365093537887616
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7110000252723694, scr_score: 0.031746091816212414
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.03571429416165487
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7200000286102295, scr_score: 0.06746038597786728
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.08333343188597349
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: 0.07142858832330974
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7070000171661377, scr_score: 0.015873045908106207
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9570000171661377
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9800000190734863
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8560000658035278, scr_score: 0.07407441747519233
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8690000176429749, scr_score: 0.17037032131306776
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8410000205039978, scr_score: -0.037036987979734426
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8640000224113464, scr_score: 0.13333333333333333
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8730000257492065, scr_score: 0.2
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8820000290870667, scr_score: 0.26666666666666666
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.796000063419342, scr_score: -0.37036987979734426
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6740000247955322, scr_score: 0.07213119431485049
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6780000329017639, scr_score: 0.08524597468346515
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6860000491142273, scr_score: 0.11147553542069445
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6940000057220459, scr_score: 0.1377049007328635
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7240000367164612, scr_score: 0.23606565578494332
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7250000238418579, scr_score: 0.23934430202083193
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6940000057220459, scr_score: 0.1377049007328635
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9620000720024109
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9620000720024109
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9580000638961792
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7620000243186951, scr_score: -0.06486499548132361
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7970000505447388, scr_score: 0.12432433303208824
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7940000295639038, scr_score: 0.1081080036149411
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7240000367164612, scr_score: 0.0
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7360000610351562, scr_score: 0.05042026277129501
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7250000238418579, scr_score: 0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7150000333786011, scr_score: -0.03781513446855956
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7130000591278076, scr_score: -0.04621838637728532
Step 0: {'loss': 1498.513916015625, 'l1_loss': 12487.2587890625, 'l2_loss': 999.0235595703125}
Step 100: {'loss': 1047.2498779296875, 'l1_loss': 8862.8603515625, 'l2_loss': 692.7354736328125}
Step 200: {'loss': 795.8009643554688, 'l1_loss': 3984.1298828125, 'l2_loss': 636.435791015625}
Step 300: {'loss': 695.7577514648438, 'l1_loss': 1781.12353515625, 'l2_loss': 624.5128173828125}
Step 400: {'loss': 657.0755615234375, 'l1_loss': 1219.1683349609375, 'l2_loss': 608.308837890625}
Step 500: {'loss': 608.715087890625, 'l1_loss': 1480.468017578125, 'l2_loss': 549.496337890625}
Step 600: {'loss': 565.714111328125, 'l1_loss': 1574.396240234375, 'l2_loss': 502.73828125}
Step 700: {'loss': 529.650634765625, 'l1_loss': 1543.406494140625, 'l2_loss': 467.9143981933594}
Step 800: {'loss': 512.11376953125, 'l1_loss': 1535.429443359375, 'l2_loss': 450.69659423828125}
Step 900: {'loss': 491.63482666015625, 'l1_loss': 1309.5985107421875, 'l2_loss': 439.2508850097656}
Step 1000: {'loss': 488.87738037109375, 'l1_loss': 1413.5364990234375, 'l2_loss': 432.3359069824219}
Step 1100: {'loss': 474.44805908203125, 'l1_loss': 1194.581298828125, 'l2_loss': 426.664794921875}
Step 1200: {'loss': 460.67071533203125, 'l1_loss': 1003.5546875, 'l2_loss': 420.5285339355469}
Step 1300: {'loss': 463.8619384765625, 'l1_loss': 1096.5330810546875, 'l2_loss': 420.0006103515625}
Step 1400: {'loss': 457.2279052734375, 'l1_loss': 1110.1455078125, 'l2_loss': 412.82208251953125}
Step 1500: {'loss': 453.5606689453125, 'l1_loss': 1019.3511352539062, 'l2_loss': 412.78662109375}
Step 1600: {'loss': 451.92498779296875, 'l1_loss': 1021.28271484375, 'l2_loss': 411.07366943359375}
Step 1700: {'loss': 447.15924072265625, 'l1_loss': 978.9476928710938, 'l2_loss': 408.0013427734375}
Step 1800: {'loss': 447.26202392578125, 'l1_loss': 1035.4267578125, 'l2_loss': 405.844970703125}
Step 1900: {'loss': 443.6641845703125, 'l1_loss': 1005.9251708984375, 'l2_loss': 403.42718505859375}
Step 2000: {'loss': 441.2823791503906, 'l1_loss': 1006.183837890625, 'l2_loss': 401.0350341796875}
Step 2100: {'loss': 441.8796081542969, 'l1_loss': 978.2005615234375, 'l2_loss': 402.7515869140625}
Step 2200: {'loss': 433.6889953613281, 'l1_loss': 911.7867431640625, 'l2_loss': 397.217529296875}
Step 2300: {'loss': 434.7720947265625, 'l1_loss': 999.5848388671875, 'l2_loss': 394.7886962890625}
Step 2400: {'loss': 434.47845458984375, 'l1_loss': 980.7127685546875, 'l2_loss': 395.24993896484375}
Step 2500: {'loss': 429.9486389160156, 'l1_loss': 993.10986328125, 'l2_loss': 390.2242431640625}
Step 2600: {'loss': 430.0940246582031, 'l1_loss': 1036.7750244140625, 'l2_loss': 388.6230163574219}
Step 2700: {'loss': 429.60589599609375, 'l1_loss': 995.2310791015625, 'l2_loss': 389.7966613769531}
Step 2800: {'loss': 427.61090087890625, 'l1_loss': 959.5966796875, 'l2_loss': 389.22705078125}
Step 2900: {'loss': 423.9181213378906, 'l1_loss': 933.14453125, 'l2_loss': 386.59234619140625}
Step 3000: {'loss': 426.760986328125, 'l1_loss': 1002.3779296875, 'l2_loss': 386.6658630371094}
Step 3100: {'loss': 422.6874694824219, 'l1_loss': 984.005126953125, 'l2_loss': 383.3272705078125}
Step 3200: {'loss': 422.08447265625, 'l1_loss': 945.2852783203125, 'l2_loss': 384.2730712890625}
Step 3300: {'loss': 420.3460388183594, 'l1_loss': 952.3751220703125, 'l2_loss': 382.25103759765625}
Step 3400: {'loss': 421.8597412109375, 'l1_loss': 993.4945068359375, 'l2_loss': 382.1199645996094}
Step 3500: {'loss': 415.1004333496094, 'l1_loss': 933.814453125, 'l2_loss': 377.74786376953125}
Step 3600: {'loss': 419.33685302734375, 'l1_loss': 973.0511474609375, 'l2_loss': 380.414794921875}
Step 3700: {'loss': 417.2848205566406, 'l1_loss': 950.616943359375, 'l2_loss': 379.2601318359375}
Step 3800: {'loss': 414.151123046875, 'l1_loss': 965.7557373046875, 'l2_loss': 375.5209045410156}
Step 3900: {'loss': 415.08013916015625, 'l1_loss': 952.6870727539062, 'l2_loss': 376.97265625}
Step 4000: {'loss': 413.4460144042969, 'l1_loss': 922.3121948242188, 'l2_loss': 376.55352783203125}
Step 4100: {'loss': 413.73291015625, 'l1_loss': 925.5953979492188, 'l2_loss': 376.7091064453125}
Step 4200: {'loss': 412.89312744140625, 'l1_loss': 966.7529296875, 'l2_loss': 374.2230224609375}
Step 4300: {'loss': 413.27740478515625, 'l1_loss': 955.8079223632812, 'l2_loss': 375.04510498046875}
Step 4400: {'loss': 409.98333740234375, 'l1_loss': 934.8653564453125, 'l2_loss': 372.5887145996094}
Step 4500: {'loss': 411.21826171875, 'l1_loss': 951.6209106445312, 'l2_loss': 373.1534118652344}
Step 4600: {'loss': 409.78424072265625, 'l1_loss': 946.6080932617188, 'l2_loss': 371.919921875}
Step 4700: {'loss': 408.38177490234375, 'l1_loss': 976.6857299804688, 'l2_loss': 369.3143615722656}
Step 4800: {'loss': 409.0137023925781, 'l1_loss': 954.6972045898438, 'l2_loss': 370.8258056640625}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_2/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Enabled 5 new features. Total active: 1157
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Enabled 5 new features. Total active: 1162
Enabled 5 new features. Total active: 1167
Enabled 5 new features. Total active: 1172
Enabled 5 new features. Total active: 1177
Enabled 5 new features. Total active: 1182
Enabled 5 new features. Total active: 1187
Enabled 5 new features. Total active: 1192
Enabled 5 new features. Total active: 1197
Enabled 5 new features. Total active: 1202
Enabled 5 new features. Total active: 1207
Enabled 5 new features. Total active: 1212
Enabled 5 new features. Total active: 1217
Enabled 5 new features. Total active: 1222
Enabled 5 new features. Total active: 1227
Enabled 5 new features. Total active: 1232
Enabled 5 new features. Total active: 1237
Enabled 5 new features. Total active: 1242
Enabled 5 new features. Total active: 1247
Enabled 5 new features. Total active: 1252
Enabled 5 new features. Total active: 1257
Enabled 5 new features. Total active: 1262
Enabled 5 new features. Total active: 1267
Enabled 5 new features. Total active: 1272
Enabled 5 new features. Total active: 1277
Enabled 5 new features. Total active: 1282
Enabled 5 new features. Total active: 1287
Enabled 5 new features. Total active: 1292
Enabled 5 new features. Total active: 1297
Enabled 5 new features. Total active: 1302
Enabled 5 new features. Total active: 1307
Enabled 5 new features. Total active: 1312
Enabled 5 new features. Total active: 1317
Enabled 5 new features. Total active: 1322
Enabled 5 new features. Total active: 1327
Enabled 5 new features. Total active: 1332
Enabled 5 new features. Total active: 1337
Enabled 5 new features. Total active: 1342
Enabled 5 new features. Total active: 1347
Enabled 5 new features. Total active: 1352
Enabled 5 new features. Total active: 1357
Enabled 5 new features. Total active: 1362
Enabled 5 new features. Total active: 1367
Enabled 5 new features. Total active: 1372
Enabled 5 new features. Total active: 1377
Enabled 5 new features. Total active: 1382
Enabled 5 new features. Total active: 1387
Enabled 5 new features. Total active: 1392
Enabled 5 new features. Total active: 1397
Enabled 5 new features. Total active: 1402
Enabled 5 new features. Total active: 1407
Enabled 5 new features. Total active: 1412
Enabled 5 new features. Total active: 1417
Enabled 5 new features. Total active: 1422
Enabled 5 new features. Total active: 1427
Enabled 5 new features. Total active: 1432
Enabled 5 new features. Total active: 1437
Enabled 5 new features. Total active: 1442
Enabled 5 new features. Total active: 1447
Enabled 5 new features. Total active: 1452
Enabled 5 new features. Total active: 1457
Enabled 5 new features. Total active: 1462
Enabled 5 new features. Total active: 1467
Enabled 5 new features. Total active: 1472
Enabled 5 new features. Total active: 1477
Enabled 5 new features. Total active: 1482
Enabled 5 new features. Total active: 1487
Enabled 5 new features. Total active: 1492
Enabled 5 new features. Total active: 1497
Enabled 5 new features. Total active: 1502
Enabled 5 new features. Total active: 1507
Enabled 5 new features. Total active: 1512
Enabled 5 new features. Total active: 1517
Enabled 5 new features. Total active: 1522
Enabled 5 new features. Total active: 1527
Enabled 5 new features. Total active: 1532
Enabled 5 new features. Total active: 1537
Enabled 5 new features. Total active: 1542
Enabled 5 new features. Total active: 1547
Enabled 5 new features. Total active: 1552
Enabled 5 new features. Total active: 1557
Enabled 5 new features. Total active: 1562
Enabled 5 new features. Total active: 1567
Enabled 5 new features. Total active: 1572
Enabled 5 new features. Total active: 1577
Enabled 5 new features. Total active: 1582
Enabled 5 new features. Total active: 1587
Enabled 5 new features. Total active: 1592
Enabled 5 new features. Total active: 1597
Enabled 5 new features. Total active: 1602
Enabled 5 new features. Total active: 1607
Enabled 5 new features. Total active: 1612
Enabled 5 new features. Total active: 1617
Enabled 5 new features. Total active: 1622
Enabled 5 new features. Total active: 1627
Enabled 5 new features. Total active: 1632
Enabled 5 new features. Total active: 1637
Enabled 5 new features. Total active: 1642
Enabled 5 new features. Total active: 1647
Enabled 5 new features. Total active: 1652
Enabled 5 new features. Total active: 1657
Step 100: {'loss': 532.419921875, 'l1_loss': 4634.44384765625, 'l2_loss': 347.04217529296875}
Enabled 5 new features. Total active: 1662
Enabled 5 new features. Total active: 1667
Enabled 5 new features. Total active: 1672
Enabled 5 new features. Total active: 1677
Enabled 5 new features. Total active: 1682
Enabled 5 new features. Total active: 1687
Enabled 5 new features. Total active: 1692
Enabled 5 new features. Total active: 1697
Enabled 5 new features. Total active: 1702
Enabled 5 new features. Total active: 1707
Enabled 5 new features. Total active: 1712
Enabled 5 new features. Total active: 1717
Enabled 5 new features. Total active: 1722
Enabled 5 new features. Total active: 1727
Enabled 5 new features. Total active: 1732
Enabled 5 new features. Total active: 1737
Enabled 5 new features. Total active: 1742
Enabled 5 new features. Total active: 1747
Enabled 5 new features. Total active: 1752
Enabled 5 new features. Total active: 1757
Enabled 5 new features. Total active: 1762
Enabled 5 new features. Total active: 1767
Enabled 5 new features. Total active: 1772
Enabled 5 new features. Total active: 1777
Enabled 5 new features. Total active: 1782
Enabled 5 new features. Total active: 1787
Enabled 5 new features. Total active: 1792
Enabled 5 new features. Total active: 1797
Enabled 5 new features. Total active: 1802
Enabled 5 new features. Total active: 1807
Enabled 5 new features. Total active: 1812
Enabled 5 new features. Total active: 1817
Enabled 5 new features. Total active: 1822
Enabled 5 new features. Total active: 1827
Enabled 5 new features. Total active: 1832
Enabled 5 new features. Total active: 1837
Enabled 5 new features. Total active: 1842
Enabled 5 new features. Total active: 1847
Enabled 5 new features. Total active: 1852
Enabled 5 new features. Total active: 1857
Enabled 5 new features. Total active: 1862
Enabled 5 new features. Total active: 1867
Enabled 5 new features. Total active: 1872
Enabled 5 new features. Total active: 1877
Enabled 5 new features. Total active: 1882
Enabled 5 new features. Total active: 1887
Enabled 5 new features. Total active: 1892
Enabled 5 new features. Total active: 1897
Enabled 5 new features. Total active: 1902
Enabled 5 new features. Total active: 1907
Enabled 5 new features. Total active: 1912
Enabled 5 new features. Total active: 1917
Enabled 5 new features. Total active: 1922
Enabled 5 new features. Total active: 1927
Enabled 5 new features. Total active: 1932
Enabled 5 new features. Total active: 1937
Enabled 5 new features. Total active: 1942
Enabled 5 new features. Total active: 1947
Enabled 5 new features. Total active: 1952
Enabled 5 new features. Total active: 1957
Step 200: {'loss': 406.6170349121094, 'l1_loss': 2252.38134765625, 'l2_loss': 316.52178955078125}
Step 300: {'loss': 349.1876525878906, 'l1_loss': 1002.097900390625, 'l2_loss': 309.1037292480469}
Step 400: {'loss': 322.7314758300781, 'l1_loss': 768.811279296875, 'l2_loss': 291.9790344238281}
Step 500: {'loss': 295.400390625, 'l1_loss': 867.3533935546875, 'l2_loss': 260.7062683105469}
Step 600: {'loss': 275.4814453125, 'l1_loss': 900.9006958007812, 'l2_loss': 239.4454345703125}
Step 700: {'loss': 258.740966796875, 'l1_loss': 885.1150512695312, 'l2_loss': 223.33636474609375}
Step 800: {'loss': 251.4671630859375, 'l1_loss': 858.748291015625, 'l2_loss': 217.1172332763672}
Step 900: {'loss': 241.8627166748047, 'l1_loss': 717.9920654296875, 'l2_loss': 213.14303588867188}
Step 1000: {'loss': 241.3281707763672, 'l1_loss': 770.55126953125, 'l2_loss': 210.50611877441406}
Step 1100: {'loss': 233.93417358398438, 'l1_loss': 643.795166015625, 'l2_loss': 208.182373046875}
Step 1200: {'loss': 226.74111938476562, 'l1_loss': 535.359375, 'l2_loss': 205.3267364501953}
Step 1300: {'loss': 228.86712646484375, 'l1_loss': 582.600341796875, 'l2_loss': 205.5631103515625}Saved evaluation results to: run_2/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8910000324249268, scr_score: -0.1044780899461156
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8830000162124634, scr_score: -0.22388136713540846
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8080000281333923, scr_score: -1.3432846443247013
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7970000505447388, scr_score: -1.507463483243526
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7880000472068787, scr_score: -1.6417919476759961
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6200000047683716, scr_score: 0.06060602867826107
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6240000128746033, scr_score: 0.07070705854409946
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6300000548362732, scr_score: 0.08585867860124165
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6470000147819519, scr_score: 0.12878786738509324
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6970000267028809, scr_score: 0.2550505149329192
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7050000429153442, scr_score: 0.27525257466459596
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.706000030040741, scr_score: 0.27777779450186324
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.843000054359436, scr_score: -0.01818158173367131
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8440000414848328, scr_score: -0.009090790866835655
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8550000190734863, scr_score: 0.09090899238902972
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8580000400543213, scr_score: 0.11818190684987326
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8570000529289246, scr_score: 0.1090911159830376
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8300000429153442, scr_score: -0.13636348858354458
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.812000036239624, scr_score: -0.29999989162793267
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6620000600814819, scr_score: 0.03812319842350094
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6700000166893005, scr_score: 0.061583480320242974
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6760000586509705, scr_score: 0.07917891023488599
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6740000247955322, scr_score: 0.07331370866544859
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6790000200271606, scr_score: 0.08797645039853831
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08211142362277009
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08211142362277009
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9130000472068787, scr_score: -0.05454582892168709
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9110000729560852, scr_score: -0.09090899238902972
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9120000600814819, scr_score: -0.0727274106553584
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.018182665454344472
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8940000534057617, scr_score: -0.4000002167441346
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8640000224113464, scr_score: -0.9454552547989861
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8580000400543213, scr_score: -1.0545458289216871
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5900000333786011, scr_score: 0.024570000821577526
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5980000495910645, scr_score: 0.04422606005834226
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5920000076293945, scr_score: 0.02948394240639032
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5960000157356262, scr_score: 0.039311972024772684
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6050000190734863, scr_score: 0.061425002053943814
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6050000190734863, scr_score: 0.061425002053943814
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6310000419616699, scr_score: 0.1253071213490508
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8320000171661377, scr_score: -0.06201562565697945
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8260000348091125, scr_score: -0.10852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.831000030040741, scr_score: -0.06976746335123167
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.843000054359436, scr_score: 0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8390000462532043, scr_score: -0.007751837694252218
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8370000123977661, scr_score: -0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8160000443458557, scr_score: -0.1860464149194575
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6620000600814819, scr_score: 0.005970249634899351
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6770000457763672, scr_score: 0.05074632123732823
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6920000314712524, scr_score: 0.09552239283975711
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6910000443458557, scr_score: 0.09253735698445369
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6980000138282776, scr_score: 0.11343278589587016
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6930000185966492, scr_score: 0.09850742869506053
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6840000152587891, scr_score: 0.07164175014874472
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8920000195503235, scr_score: 0.40963828600030594
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8910000324249268, scr_score: 0.4036142674840881
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.843000054359436, scr_score: 0.11445778806456547
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8450000286102295, scr_score: 0.12650582509700117
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8350000381469727, scr_score: 0.0662649218066095
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7290000319480896, scr_score: 0.20802913753598304
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7310000061988831, scr_score: 0.2153283139259297
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7380000352859497, scr_score: 0.2408758663611602
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7480000257492065, scr_score: 0.2773721833813107
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6980000138282776, scr_score: 0.09489038074534958
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6840000152587891, scr_score: 0.043795493410097155
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6610000133514404, scr_score: -0.04014612275033245
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8480000495910645, scr_score: 0.22941194413853241
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8580000400543213, scr_score: 0.2882354116769695
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8630000352859497, scr_score: 0.31764714544618805
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8640000224113464, scr_score: 0.32352942207692714
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8490000367164612, scr_score: 0.2352942207692715
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8340000510215759, scr_score: 0.14705901946161584
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8090000152587891, scr_score: 0.0
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7050000429153442, scr_score: 0.06015049050583274
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.14285730291256044
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6980000138282776, scr_score: 0.03383459489013476
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7010000348091125, scr_score: 0.04511286787937455
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6920000314712524, scr_score: 0.011278272989239795
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6850000619888306, scr_score: -0.015037398548873562
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8850000500679016, scr_score: 0.14285737093604264
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8910000324249268, scr_score: 0.19642861894500888
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8980000615119934, scr_score: 0.2589288517755524
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9000000357627869, scr_score: 0.276785757050508
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8920000195503235, scr_score: 0.20535707158248667
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8070000410079956, scr_score: -0.5535709819169164
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6500000357627869, scr_score: 0.06382986047261584
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6540000438690186, scr_score: 0.07598793776116658
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6730000376701355, scr_score: 0.13373866900495587
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6680000424385071, scr_score: 0.11854111768654303
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6950000524520874, scr_score: 0.20060800350743382
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6980000138282776, scr_score: 0.20972642559702012
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6850000619888306, scr_score: 0.1702129008706081
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7760000228881836, scr_score: 0.11057692996540351
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.781000018119812, scr_score: 0.13461536808303157
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7850000262260437, scr_score: 0.15384617588929125
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7820000648498535, scr_score: 0.1394232849551862
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7850000262260437, scr_score: 0.15384617588929125
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7780000567436218, scr_score: 0.1201924771489265
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7720000147819519, scr_score: 0.09134612215914382
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7550000548362732, scr_score: 0.05504598443771902
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7600000500679016, scr_score: 0.07798174293281654
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7640000581741333, scr_score: 0.09633040441205622
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7700000405311584, scr_score: 0.12385325992301158
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7380000352859497, scr_score: -0.02293575849509752
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7390000224113464, scr_score: -0.018348661479239674
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7240000367164612, scr_score: -0.08715593696453222

Step 1400: {'loss': 225.2779998779297, 'l1_loss': 594.983154296875, 'l2_loss': 201.47866821289062}
Step 1500: {'loss': 223.87152099609375, 'l1_loss': 545.8014526367188, 'l2_loss': 202.03945922851562}
Step 1600: {'loss': 222.58834838867188, 'l1_loss': 549.9234008789062, 'l2_loss': 200.59141540527344}
Step 1700: {'loss': 220.09042358398438, 'l1_loss': 527.7576904296875, 'l2_loss': 198.98011779785156}
Step 1800: {'loss': 220.22218322753906, 'l1_loss': 564.4293823242188, 'l2_loss': 197.64500427246094}
Step 1900: {'loss': 218.39292907714844, 'l1_loss': 546.63720703125, 'l2_loss': 196.52743530273438}
Step 2000: {'loss': 217.2555694580078, 'l1_loss': 547.2137451171875, 'l2_loss': 195.3670196533203}
Step 2100: {'loss': 217.18582153320312, 'l1_loss': 529.9644775390625, 'l2_loss': 195.98724365234375}
Step 2200: {'loss': 212.77357482910156, 'l1_loss': 485.1292419433594, 'l2_loss': 193.368408203125}
Step 2300: {'loss': 214.07235717773438, 'l1_loss': 549.7340698242188, 'l2_loss': 192.08299255371094}
Step 2400: {'loss': 214.29721069335938, 'l1_loss': 539.48681640625, 'l2_loss': 192.71774291992188}
Step 2500: {'loss': 211.85971069335938, 'l1_loss': 549.0997314453125, 'l2_loss': 189.89572143554688}
Step 2600: {'loss': 212.34121704101562, 'l1_loss': 581.274658203125, 'l2_loss': 189.09022521972656}
Step 2700: {'loss': 211.671142578125, 'l1_loss': 548.8394775390625, 'l2_loss': 189.71755981445312}
Step 2800: {'loss': 210.39114379882812, 'l1_loss': 524.0731201171875, 'l2_loss': 189.42822265625}
Step 2900: {'loss': 208.14395141601562, 'l1_loss': 505.42724609375, 'l2_loss': 187.92686462402344}
Step 3000: {'loss': 210.1141815185547, 'l1_loss': 549.8447875976562, 'l2_loss': 188.12039184570312}
Step 3100: {'loss': 207.89129638671875, 'l1_loss': 539.390625, 'l2_loss': 186.315673828125}
Step 3200: {'loss': 207.73817443847656, 'l1_loss': 514.1759643554688, 'l2_loss': 187.171142578125}
Step 3300: {'loss': 206.71075439453125, 'l1_loss': 513.978271484375, 'l2_loss': 186.15162658691406}
Step 3400: {'loss': 208.05177307128906, 'l1_loss': 543.0478515625, 'l2_loss': 186.32986450195312}
Step 3500: {'loss': 204.35569763183594, 'l1_loss': 503.51995849609375, 'l2_loss': 184.21490478515625}
Step 3600: {'loss': 206.9109649658203, 'l1_loss': 531.9976806640625, 'l2_loss': 185.6310577392578}
Step 3700: {'loss': 205.73492431640625, 'l1_loss': 514.690673828125, 'l2_loss': 185.1472930908203}
Step 3800: {'loss': 204.033935546875, 'l1_loss': 526.6580200195312, 'l2_loss': 182.96762084960938}
Step 3900: {'loss': 204.5602569580078, 'l1_loss': 516.854736328125, 'l2_loss': 183.8860626220703}
Step 4000: {'loss': 203.69618225097656, 'l1_loss': 498.608154296875, 'l2_loss': 183.75186157226562}
Step 4100: {'loss': 203.7084503173828, 'l1_loss': 498.23724365234375, 'l2_loss': 183.77896118164062}
Step 4200: {'loss': 203.73878479003906, 'l1_loss': 524.9974365234375, 'l2_loss': 182.7388916015625}
Step 4300: {'loss': 203.85667419433594, 'l1_loss': 516.7508544921875, 'l2_loss': 183.1866455078125}
Step 4400: {'loss': 202.29092407226562, 'l1_loss': 504.13128662109375, 'l2_loss': 182.12567138671875}
Step 4500: {'loss': 203.0398406982422, 'l1_loss': 513.4912109375, 'l2_loss': 182.5001983642578}
Step 4600: {'loss': 202.35328674316406, 'l1_loss': 509.7049255371094, 'l2_loss': 181.965087890625}
Step 4700: {'loss': 201.56784057617188, 'l1_loss': 528.7057495117188, 'l2_loss': 180.41961669921875}
Step 4800: {'loss': 201.81378173828125, 'l1_loss': 513.38232421875, 'l2_loss': 181.2784881591797}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_2/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_2/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9040000438690186, scr_score: 0.08955201308100097
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9020000696182251, scr_score: 0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8520000576972961, scr_score: -0.6865675094055279
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8510000109672546, scr_score: -0.7014935862706425
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8010000586509705, scr_score: -1.4477618446488796
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7830000519752502, scr_score: -1.7164187735138199
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6810000538825989, scr_score: 0.2146465459863349
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6680000424385071, scr_score: 0.18181823655155244
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.690000057220459, scr_score: 0.23737382555527894
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6920000314712524, scr_score: 0.24242426522981353
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7040000557899475, scr_score: 0.2727273548273287
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7070000171661377, scr_score: 0.2803030143391305
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.703000009059906, scr_score: 0.27020198447329213
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8490000367164612, scr_score: 0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8460000157356262, scr_score: 0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8470000624656677, scr_score: 0.017857446962504415
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8370000123977661, scr_score: -0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8300000429153442, scr_score: -0.13392845738906084
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.796000063419342, scr_score: -0.4374997339078086
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6680000424385071, scr_score: 0.06122456072426409
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06997080241741109
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06997080241741109
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6780000329017639, scr_score: 0.09037904725031479
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6910000443458557, scr_score: 0.12827994924395952
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6940000057220459, scr_score: 0.1370261909371065
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6920000314712524, scr_score: 0.13119536314167518
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9190000295639038, scr_score: 0.0
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.07692316509555075
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9000000357627869, scr_score: -0.36538417452224625
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9010000228881836, scr_score: -0.3461536698088985
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8720000386238098, scr_score: -0.9038451839489418
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.593000054359436, scr_score: 0.038929518017424856
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6020000576972961, scr_score: 0.06082733564636087
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6070000529289246, scr_score: 0.07299277377094067
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6160000562667847, scr_score: 0.09489059139987667
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6220000386238098, scr_score: 0.10948908814468004
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.843000054359436, scr_score: 0.06153858850694027
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8320000171661377, scr_score: -0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8480000495910645, scr_score: 0.10000009169945687
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8460000157356262, scr_score: 0.08461521532407963
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8440000414848328, scr_score: 0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.812000036239624, scr_score: -0.17692309808449005
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6710000038146973, scr_score: 0.018181762353783503
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6580000519752502, scr_score: -0.021212025976062057
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.03333344170540065
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6890000104904175, scr_score: 0.07272723003524621
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6890000104904175, scr_score: 0.07272723003524621
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6850000619888306, scr_score: 0.060606175546132
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.687000036239624, scr_score: 0.0666667027906891
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8500000238418579, scr_score: 0.19075135945072783
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8630000352859497, scr_score: 0.265895917909607
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.862000048160553, scr_score: 0.26011564676710336
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7770000100135803, scr_score: -0.23121394651914118
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7590000629425049, scr_score: -0.3352598606905386
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7680000066757202, scr_score: -0.28323707587256186
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7760000228881836, scr_score: -0.23699421766164483
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7760000228881836, scr_score: 0.37254890502759347
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7760000228881836, scr_score: 0.37254890502759347
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.784000039100647, scr_score: 0.4039215182121411
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7900000214576721, scr_score: 0.4274508612286972
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7620000243186951, scr_score: 0.3176469488264897
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7580000162124634, scr_score: 0.30196064223421587
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.718000054359436, scr_score: 0.14509804379889626
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8170000314712524, scr_score: 0.11827961813300997
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8350000381469727, scr_score: 0.21505382201865497
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8380000591278076, scr_score: 0.2311829628179274
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7880000472068787, scr_score: -0.0376342345916426
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7920000553131104, scr_score: -0.01612882034427762
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7250000238418579, scr_score: 0.08730163423141594
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7310000061988831, scr_score: 0.11111108483040708
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7230000495910645, scr_score: 0.07936522954053103
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.734000027179718, scr_score: 0.12301592839307082
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7450000643730164, scr_score: 0.16666686377194698
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7290000319480896, scr_score: 0.10317468013952215
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.706000030040741, scr_score: 0.01190484356266375
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8570000529289246, scr_score: 0.08148172676799452
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8490000367164612, scr_score: 0.022222369394130047
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8670000433921814, scr_score: 0.15555570272746339
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8760000467300415, scr_score: 0.22222236939413004
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.859000027179718, scr_score: 0.0962963453535989
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8640000224113464, scr_score: 0.13333333333333333
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7900000214576721, scr_score: -0.41481461858560437
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6690000295639038, scr_score: 0.055737767710347226
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6840000152587891, scr_score: 0.104918047523857
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6970000267028809, scr_score: 0.1475410348655896
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7210000157356262, scr_score: 0.22622952165221727
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7250000238418579, scr_score: 0.23934430202083193
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6910000443458557, scr_score: 0.1278689620251977
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6980000138282776, scr_score: 0.15081968110147817
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8040000200271606, scr_score: 0.16216200542241166
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7930000424385071, scr_score: 0.10270266787164704
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7980000376701355, scr_score: 0.1297296687753823
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8070000410079956, scr_score: 0.17837833483955878
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.796000063419342, scr_score: 0.11891899728879417
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7240000367164612, scr_score: 0.0
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.734000027179718, scr_score: 0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7350000143051147, scr_score: 0.04621838637728532
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.734000027179718, scr_score: 0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7160000205039978, scr_score: -0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.6980000138282776, scr_score: -0.10924377745131579
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Enabled 3 new features. Total active: 1155
Step 0: {'loss': 643.5555419921875, 'l1_loss': 4554.9228515625, 'l2_loss': 461.358642578125}
Enabled 1 new features. Total active: 1153
Enabled 1 new features. Total active: 1154
Enabled 1 new features. Total active: 1155
Enabled 1 new features. Total active: 1156
Enabled 1 new features. Total active: 1157
Enabled 1 new features. Total active: 1158
Enabled 1 new features. Total active: 1159
Enabled 1 new features. Total active: 1160
Enabled 1 new features. Total active: 1161
Enabled 1 new features. Total active: 1162
Enabled 1 new features. Total active: 1163
Enabled 1 new features. Total active: 1164
Enabled 1 new features. Total active: 1165
Enabled 1 new features. Total active: 1166
Enabled 1 new features. Total active: 1167
Enabled 1 new features. Total active: 1168
Enabled 1 new features. Total active: 1169
Enabled 1 new features. Total active: 1170
Enabled 1 new features. Total active: 1171
Enabled 1 new features. Total active: 1172
Enabled 1 new features. Total active: 1173
Enabled 1 new features. Total active: 1174
Enabled 1 new features. Total active: 1175
Enabled 1 new features. Total active: 1176
Enabled 1 new features. Total active: 1177
Enabled 1 new features. Total active: 1178
Enabled 1 new features. Total active: 1179
Enabled 1 new features. Total active: 1180
Enabled 1 new features. Total active: 1181
Enabled 1 new features. Total active: 1182
Enabled 1 new features. Total active: 1183
Enabled 1 new features. Total active: 1184
Enabled 1 new features. Total active: 1185
Enabled 1 new features. Total active: 1186
Enabled 1 new features. Total active: 1187
Enabled 1 new features. Total active: 1188
Enabled 1 new features. Total active: 1189
Enabled 1 new features. Total active: 1190
Enabled 1 new features. Total active: 1191
Enabled 1 new features. Total active: 1192
Enabled 1 new features. Total active: 1193
Enabled 1 new features. Total active: 1194
Enabled 1 new features. Total active: 1195
Enabled 1 new features. Total active: 1196
Enabled 1 new features. Total active: 1197
Enabled 1 new features. Total active: 1198
Enabled 1 new features. Total active: 1199
Enabled 1 new features. Total active: 1200
Enabled 1 new features. Total active: 1201
Enabled 1 new features. Total active: 1202
Enabled 1 new features. Total active: 1203
Enabled 1 new features. Total active: 1204
Enabled 1 new features. Total active: 1205
Enabled 1 new features. Total active: 1206
Enabled 1 new features. Total active: 1207
Enabled 1 new features. Total active: 1208
Enabled 1 new features. Total active: 1209
Enabled 1 new features. Total active: 1210
Enabled 1 new features. Total active: 1211
Enabled 1 new features. Total active: 1212
Enabled 1 new features. Total active: 1213
Enabled 1 new features. Total active: 1214
Enabled 1 new features. Total active: 1215
Enabled 1 new features. Total active: 1216
Enabled 1 new features. Total active: 1217
Enabled 1 new features. Total active: 1218
Enabled 1 new features. Total active: 1219
Enabled 1 new features. Total active: 1220
Enabled 1 new features. Total active: 1221
Enabled 1 new features. Total active: 1222
Enabled 1 new features. Total active: 1223
Enabled 1 new features. Total active: 1224
Enabled 1 new features. Total active: 1225
Enabled 1 new features. Total active: 1226
Enabled 1 new features. Total active: 1227
Enabled 1 new features. Total active: 1228
Enabled 1 new features. Total active: 1229
Enabled 1 new features. Total active: 1230
Enabled 1 new features. Total active: 1231
Enabled 1 new features. Total active: 1232
Enabled 1 new features. Total active: 1233
Enabled 1 new features. Total active: 1234
Enabled 1 new features. Total active: 1235
Enabled 1 new features. Total active: 1236
Enabled 1 new features. Total active: 1237
Enabled 1 new features. Total active: 1238
Enabled 1 new features. Total active: 1239
Enabled 1 new features. Total active: 1240
Enabled 1 new features. Total active: 1241
Enabled 1 new features. Total active: 1242
Enabled 1 new features. Total active: 1243
Enabled 1 new features. Total active: 1244
Enabled 1 new features. Total active: 1245
Enabled 1 new features. Total active: 1246
Enabled 1 new features. Total active: 1247
Step 100: {'loss': 479.90264892578125, 'l1_loss': 3523.4189453125, 'l2_loss': 338.96588134765625}
Enabled 1 new features. Total active: 1248
Enabled 1 new features. Total active: 1249
Enabled 1 new features. Total active: 1250
Enabled 1 new features. Total active: 1251
Enabled 1 new features. Total active: 1252
Enabled 1 new features. Total active: 1253
Enabled 1 new features. Total active: 1254
Enabled 1 new features. Total active: 1255
Enabled 1 new features. Total active: 1256
Enabled 1 new features. Total active: 1257
Enabled 1 new features. Total active: 1258
Enabled 1 new features. Total active: 1259
Enabled 1 new features. Total active: 1260
Enabled 1 new features. Total active: 1261
Enabled 1 new features. Total active: 1262
Enabled 1 new features. Total active: 1263
Enabled 1 new features. Total active: 1264
Enabled 1 new features. Total active: 1265
Enabled 1 new features. Total active: 1266
Enabled 1 new features. Total active: 1267
Enabled 1 new features. Total active: 1268
Enabled 1 new features. Total active: 1269
Enabled 1 new features. Total active: 1270
Enabled 1 new features. Total active: 1271
Enabled 1 new features. Total active: 1272
Enabled 1 new features. Total active: 1273
Enabled 1 new features. Total active: 1274
Enabled 1 new features. Total active: 1275
Enabled 1 new features. Total active: 1276
Enabled 1 new features. Total active: 1277
Enabled 1 new features. Total active: 1278
Enabled 1 new features. Total active: 1279
Enabled 1 new features. Total active: 1280
Enabled 1 new features. Total active: 1281
Enabled 1 new features. Total active: 1282
Enabled 1 new features. Total active: 1283
Enabled 1 new features. Total active: 1284
Enabled 1 new features. Total active: 1285
Enabled 1 new features. Total active: 1286
Enabled 1 new features. Total active: 1287
Enabled 1 new features. Total active: 1288
Enabled 1 new features. Total active: 1289
Enabled 1 new features. Total active: 1290
Enabled 1 new features. Total active: 1291
Enabled 1 new features. Total active: 1292
Enabled 1 new features. Total active: 1293
Enabled 1 new features. Total active: 1294
Enabled 1 new features. Total active: 1295
Enabled 1 new features. Total active: 1296
Enabled 1 new features. Total active: 1297
Enabled 1 new features. Total active: 1298
Enabled 1 new features. Total active: 1299
Enabled 1 new features. Total active: 1300
Enabled 1 new features. Total active: 1301
Enabled 1 new features. Total active: 1302
Enabled 1 new features. Total active: 1303
Enabled 1 new features. Total active: 1304
Enabled 1 new features. Total active: 1305
Enabled 1 new features. Total active: 1306
Enabled 1 new features. Total active: 1307
Enabled 1 new features. Total active: 1308
Enabled 1 new features. Total active: 1309
Enabled 1 new features. Total active: 1310
Step 200: {'loss': 382.2629699707031, 'l1_loss': 1685.6826171875, 'l2_loss': 314.8356628417969}
Step 300: {'loss': 340.82421875, 'l1_loss': 796.029052734375, 'l2_loss': 308.9830627441406}
Step 400: {'loss': 316.3453063964844, 'l1_loss': 704.67724609375, 'l2_loss': 288.158203125}
Step 500: {'loss': 290.5958557128906, 'l1_loss': 845.816650390625, 'l2_loss': 256.76318359375}
Step 600: {'loss': 273.88909912109375, 'l1_loss': 864.40771484375, 'l2_loss': 239.31280517578125}
Step 700: {'loss': 258.2216796875, 'l1_loss': 855.4015502929688, 'l2_loss': 224.005615234375}
Step 800: {'loss': 251.08938598632812, 'l1_loss': 848.2652587890625, 'l2_loss': 217.15878295898438}
Step 900: {'loss': 242.47442626953125, 'l1_loss': 723.4365234375, 'l2_loss': 213.53697204589844}
Step 1000: {'loss': 241.56942749023438, 'l1_loss': 770.7086181640625, 'l2_loss': 210.7410888671875}
Step 1100: {'loss': 234.587158203125, 'l1_loss': 651.239990234375, 'l2_loss': 208.53756713867188}
Step 1200: {'loss': 227.5654296875, 'l1_loss': 547.7420043945312, 'l2_loss': 205.65574645996094}
Step 1300: {'loss': 228.66529846191406, 'l1_loss': 590.966552734375, 'l2_loss': 205.02664184570312}
Step 1400: {'loss': 225.2729949951172, 'l1_loss': 593.165283203125, 'l2_loss': 201.54638671875}
Step 1500: {'loss': 223.6145782470703, 'l1_loss': 538.4746704101562, 'l2_loss': 202.07559204101562}
Step 1600: {'loss': 222.64759826660156, 'l1_loss': 531.7206420898438, 'l2_loss': 201.37876892089844}
Step 1700: {'loss': 219.96859741210938, 'l1_loss': 502.96533203125, 'l2_loss': 199.84999084472656}
Step 1800: {'loss': 219.64950561523438, 'l1_loss': 526.2555541992188, 'l2_loss': 198.5992889404297}
Step 1900: {'loss': 217.98919677734375, 'l1_loss': 505.2101135253906, 'l2_loss': 197.78079223632812}
Step 2000: {'loss': 216.96142578125, 'l1_loss': 500.6673889160156, 'l2_loss': 196.93472290039062}
Step 2100: {'loss': 216.86199951171875, 'l1_loss': 485.49658203125, 'l2_loss': 197.442138671875}
Step 2200: {'loss': 212.94952392578125, 'l1_loss': 448.1367492675781, 'l2_loss': 195.0240478515625}
Step 2300: {'loss': 213.6964874267578, 'l1_loss': 495.267333984375, 'l2_loss': 193.8857879638672}
Step 2400: {'loss': 213.8316192626953, 'l1_loss': 485.49249267578125, 'l2_loss': 194.41192626953125}
Step 2500: {'loss': 211.49208068847656, 'l1_loss': 492.818115234375, 'l2_loss': 191.77935791015625}
Step 2600: {'loss': 211.77159118652344, 'l1_loss': 514.635986328125, 'l2_loss': 191.1861572265625}
Step 2700: {'loss': 211.42762756347656, 'l1_loss': 491.06298828125, 'l2_loss': 191.7851104736328}
Step 2800: {'loss': 210.61953735351562, 'l1_loss': 471.908447265625, 'l2_loss': 191.74319458007812}
Step 2900: {'loss': 208.73443603515625, 'l1_loss': 459.25738525390625, 'l2_loss': 190.3641357421875}
Step 3000: {'loss': 210.20330810546875, 'l1_loss': 493.788818359375, 'l2_loss': 190.45175170898438}
Step 3100: {'loss': 208.41778564453125, 'l1_loss': 484.70916748046875, 'l2_loss': 189.0294189453125}
Step 3200: {'loss': 208.21319580078125, 'l1_loss': 466.47772216796875, 'l2_loss': 189.55409240722656}
Step 3300: {'loss': 207.3755645751953, 'l1_loss': 469.66998291015625, 'l2_loss': 188.58876037597656}
Step 3400: {'loss': 208.24998474121094, 'l1_loss': 491.54156494140625, 'l2_loss': 188.58831787109375}
Step 3500: {'loss': 205.18807983398438, 'l1_loss': 463.0377197265625, 'l2_loss': 186.66656494140625}
Step 3600: {'loss': 207.54388427734375, 'l1_loss': 487.25396728515625, 'l2_loss': 188.05372619628906}
Step 3700: {'loss': 206.47683715820312, 'l1_loss': 475.2328186035156, 'l2_loss': 187.467529296875}
Step 3800: {'loss': 204.87069702148438, 'l1_loss': 486.10546875, 'l2_loss': 185.42648315429688}
Step 3900: {'loss': 205.09347534179688, 'l1_loss': 479.3537902832031, 'l2_loss': 185.91932678222656}
Step 4000: {'loss': 204.500244140625, 'l1_loss': 466.9704895019531, 'l2_loss': 185.82142639160156}
Step 4100: {'loss': 204.6234588623047, 'l1_loss': 468.245361328125, 'l2_loss': 185.89364624023438}
Step 4200: {'loss': 204.2891845703125, 'l1_loss': 489.7235412597656, 'l2_loss': 184.7002410888672}
Step 4300: {'loss': 204.6460418701172, 'l1_loss': 484.35784912109375, 'l2_loss': 185.271728515625}
Step 4400: {'loss': 203.0194854736328, 'l1_loss': 475.1007080078125, 'l2_loss': 184.0154571533203}
Step 4500: {'loss': 203.7095184326172, 'l1_loss': 484.25640869140625, 'l2_loss': 184.33926391601562}
Step 4600: {'loss': 202.91500854492188, 'l1_loss': 483.54547119140625, 'l2_loss': 183.5731964111328}
Step 4700: {'loss': 202.27835083007812, 'l1_loss': 500.0028381347656, 'l2_loss': 182.2782440185547}
Step 4800: {'loss': 202.59597778320312, 'l1_loss': 490.5518798828125, 'l2_loss': 182.97390747070312}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_3/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Step 0: {'loss': 1556.73779296875, 'l1_loss': 13369.0908203125, 'l2_loss': 1021.97412109375}
Step 100: {'loss': 1075.6484375, 'l1_loss': 9488.275390625, 'l2_loss': 696.1174926757812}
Step 200: {'loss': 806.3853759765625, 'l1_loss': 4256.51806640625, 'l2_loss': 636.1246337890625}
Step 300: {'loss': 699.1574096679688, 'l1_loss': 1899.2708740234375, 'l2_loss': 623.1865844726562}
Step 400: {'loss': 657.65185546875, 'l1_loss': 1307.010498046875, 'l2_loss': 605.3714599609375}
Step 500: {'loss': 606.6934814453125, 'l1_loss': 1533.256103515625, 'l2_loss': 545.3632202148438}
Step 600: {'loss': 564.6168212890625, 'l1_loss': 1625.49951171875, 'l2_loss': 499.59686279296875}
Step 700: {'loss': 528.18310546875, 'l1_loss': 1603.331787109375, 'l2_loss': 464.04986572265625}
Step 800: {'loss': 511.1153564453125, 'l1_loss': 1602.697998046875, 'l2_loss': 447.0074462890625}
Step 900: {'loss': 491.1976013183594, 'l1_loss': 1342.4462890625, 'l2_loss': 437.499755859375}
Step 1000: {'loss': 488.6184997558594, 'l1_loss': 1429.3828125, 'l2_loss': 431.44317626953125}
Step 1100: {'loss': 473.72998046875, 'l1_loss': 1200.2900390625, 'l2_loss': 425.7183837890625}
Step 1200: {'loss': 459.8295593261719, 'l1_loss': 1001.7034301757812, 'l2_loss': 419.76141357421875}
Step 1300: {'loss': 462.5688781738281, 'l1_loss': 1090.366943359375, 'l2_loss': 418.9541931152344}
Step 1400: {'loss': 455.27813720703125, 'l1_loss': 1100.0484619140625, 'l2_loss': 411.27618408203125}
Step 1500: {'loss': 451.79583740234375, 'l1_loss': 1000.4879760742188, 'l2_loss': 411.77630615234375}
Step 1600: {'loss': 449.8680114746094, 'l1_loss': 998.3159790039062, 'l2_loss': 409.93536376953125}
Step 1700: {'loss': 444.9524230957031, 'l1_loss': 950.8291015625, 'l2_loss': 406.91925048828125}
Step 1800: {'loss': 445.0567626953125, 'l1_loss': 1002.583740234375, 'l2_loss': 404.9533996582031}
Step 1900: {'loss': 441.3016052246094, 'l1_loss': 966.04150390625, 'l2_loss': 402.6599426269531}
Step 2000: {'loss': 438.9721984863281, 'l1_loss': 964.711669921875, 'l2_loss': 400.38372802734375}
Step 2100: {'loss': 439.18798828125, 'l1_loss': 933.235595703125, 'l2_loss': 401.85858154296875}
Step 2200: {'loss': 431.5372619628906, 'l1_loss': 866.63134765625, 'l2_loss': 396.87200927734375}
Step 2300: {'loss': 432.1247863769531, 'l1_loss': 957.1513061523438, 'l2_loss': 393.8387451171875}
Step 2400: {'loss': 432.2324523925781, 'l1_loss': 937.5025024414062, 'l2_loss': 394.73236083984375}
Step 2500: {'loss': 427.9025573730469, 'l1_loss': 954.9461669921875, 'l2_loss': 389.7047119140625}
Step 2600: {'loss': 427.7556457519531, 'l1_loss': 997.9071044921875, 'l2_loss': 387.83935546875}
Step 2700: {'loss': 427.300048828125, 'l1_loss': 956.09326171875, 'l2_loss': 389.05633544921875}
Step 2800: {'loss': 425.1301574707031, 'l1_loss': 922.1536865234375, 'l2_loss': 388.2440185546875}
Step 2900: {'loss': 421.48175048828125, 'l1_loss': 902.1650390625, 'l2_loss': 385.3951416015625}
Step 3000: {'loss': 423.9385986328125, 'l1_loss': 972.8099975585938, 'l2_loss': 385.0262145996094}
Step 3100: {'loss': 419.9744873046875, 'l1_loss': 956.6382446289062, 'l2_loss': 381.7089538574219}
Step 3200: {'loss': 419.2213134765625, 'l1_loss': 923.3271484375, 'l2_loss': 382.2882385253906}
Step 3300: {'loss': 417.2763366699219, 'l1_loss': 928.7222900390625, 'l2_loss': 380.12744140625}
Step 3400: {'loss': 419.10113525390625, 'l1_loss': 969.7955322265625, 'l2_loss': 380.309326171875}
Step 3500: {'loss': 412.472900390625, 'l1_loss': 912.50048828125, 'l2_loss': 375.9728698730469}
Step 3600: {'loss': 417.1113586425781, 'l1_loss': 959.6654052734375, 'l2_loss': 378.7247314453125}
Step 3700: {'loss': 414.98828125, 'l1_loss': 936.896484375, 'l2_loss': 377.5124206542969}
Step 3800: {'loss': 411.77825927734375, 'l1_loss': 957.189697265625, 'l2_loss': 373.49066162109375}
Step 3900: {'loss': 412.612548828125, 'l1_loss': 944.323486328125, 'l2_loss': 374.839599609375}
Step 4000: {'loss': 410.9195251464844, 'l1_loss': 919.49560546875, 'l2_loss': 374.13970947265625}
Step 4100: {'loss': 411.0610046386719, 'l1_loss': 922.20947265625, 'l2_loss': 374.1726379394531}
Step 4200: {'loss': 410.4701232910156, 'l1_loss': 966.0303344726562, 'l2_loss': 371.82891845703125}
Step 4300: {'loss': 411.2596435546875, 'l1_loss': 957.0933837890625, 'l2_loss': 372.9759216308594}
Step 4400: {'loss': 407.8253479003906, 'l1_loss': 938.0120849609375, 'l2_loss': 370.30487060546875}
Step 4500: {'loss': 409.0443115234375, 'l1_loss': 956.6950073242188, 'l2_loss': 370.7765197753906}
Step 4600: {'loss': 407.8675842285156, 'l1_loss': 952.5823364257812, 'l2_loss': 369.7642822265625}
Step 4700: {'loss': 406.1844482421875, 'l1_loss': 987.002685546875, 'l2_loss': 366.704345703125}
Step 4800: {'loss': 407.1426696777344, 'l1_loss': 966.196533203125, 'l2_loss': 368.49481201171875}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_3/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_3/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9010000228881836, scr_score: 0.0447755617295318
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.906000018119812, scr_score: 0.1194023875673555
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.906000018119812, scr_score: 0.1194023875673555
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8680000305175781, scr_score: -0.4477618446488796
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8400000333786011, scr_score: -0.8656724251894672
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8350000381469727, scr_score: -0.940299251027291
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8060000538825989, scr_score: -1.3731350188110558
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6760000586509705, scr_score: 0.2020202962832292
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6830000281333923, scr_score: 0.21969698566086945
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7110000252723694, scr_score: 0.2904040442049689
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7190000414848328, scr_score: 0.31060610393664567
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6880000233650208, scr_score: 0.23232323536397514
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8350000381469727, scr_score: -0.0892856382593739
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8190000653266907, scr_score: -0.2321425530374956
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8060000538825989, scr_score: -0.34821409564843475
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7910000085830688, scr_score: -0.4821430852218783
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6610000133514404, scr_score: 0.04081631589136039
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6730000376701355, scr_score: 0.07580180398728945
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6800000071525574, scr_score: 0.09620987504574613
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6890000104904175, scr_score: 0.12244894767408115
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6880000233650208, scr_score: 0.1195335337763655
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6930000185966492, scr_score: 0.13411077703939084
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6950000524520874, scr_score: 0.1399417786092692
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8820000290870667, scr_score: -0.7115378443311448
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8970000147819519, scr_score: -0.42307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9020000696182251, scr_score: -0.32692201885339106
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9040000438690186, scr_score: -0.2884610094266955
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8960000276565552, scr_score: -0.442307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.3076915141400433
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.8076915141400433
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.609000027179718, scr_score: 0.07785889101138782
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6100000143051147, scr_score: 0.0802919496316114
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5990000367164612, scr_score: 0.05352801476222823
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6000000238418579, scr_score: 0.05596107338245181
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6060000061988831, scr_score: 0.07055957012725518
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6070000529289246, scr_score: 0.07299277377094067
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6150000095367432, scr_score: 0.0924573877561912
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8170000314712524, scr_score: -0.13846159489197346
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8060000538825989, scr_score: -0.22307681021605308
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8130000233650208, scr_score: -0.1692308891454436
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -0.16153868020639714
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8210000395774841, scr_score: -0.10769230063850332
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7560000419616699, scr_score: -0.6076923006385033
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7000000476837158, scr_score: 0.10606067174064686
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7070000171661377, scr_score: 0.1272726977167089
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.706000030040741, scr_score: 0.12424243409443037
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7140000462532043, scr_score: 0.14848490431288316
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7300000190734863, scr_score: 0.19696966412967656
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7350000143051147, scr_score: 0.21212116286118152
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7400000095367432, scr_score: 0.22727266159268647
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8440000414848328, scr_score: 0.156069388060262
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8560000658035278, scr_score: 0.22543367537663753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8400000333786011, scr_score: 0.1329479589548035
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8210000395774841, scr_score: 0.02312142910545851
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.812000036239624, scr_score: -0.02890170024796216
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7930000424385071, scr_score: -0.13872823009730714
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8020000457763672, scr_score: -0.08670510074388647
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7460000514984131, scr_score: 0.25490195620110373
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7140000462532043, scr_score: 0.12941173720662244
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.718000054359436, scr_score: 0.14509804379889626
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7210000157356262, scr_score: 0.1568625984353196
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7200000286102295, scr_score: 0.15294108022317848
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7100000381469727, scr_score: 0.11372543061434859
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6940000057220459, scr_score: 0.05098020424525325
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8110000491142273, scr_score: 0.08602165698945992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8190000653266907, scr_score: 0.12903248548418988
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8140000104904175, scr_score: 0.10215047733373753
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7820000648498535, scr_score: -0.06989219573519266
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7190000414848328, scr_score: 0.06349218363242483
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: 0.07142858832330974
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7380000352859497, scr_score: 0.13888897430117703
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7410000562667847, scr_score: 0.15079381786384077
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7390000224113464, scr_score: 0.14285717664661948
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7280000448226929, scr_score: 0.09920647779407969
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.051587340069761074
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8420000672340393, scr_score: -0.029629237171208758
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.843000054359436, scr_score: -0.02222192787840657
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8500000238418579, scr_score: 0.02962967868693224
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8660000562667847, scr_score: 0.14814839343466119
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8670000433921814, scr_score: 0.15555570272746339
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8630000352859497, scr_score: 0.12592602404053113
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7920000553131104, scr_score: -0.39999955848427654
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6790000200271606, scr_score: 0.08852462091935374
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6830000281333923, scr_score: 0.1016394012879684
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6960000395774841, scr_score: 0.14426238862970098
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7230000495910645, scr_score: 0.2327870095490547
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7240000367164612, scr_score: 0.23606565578494332
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6980000138282776, scr_score: 0.15081968110147817
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7880000472068787, scr_score: 0.07567566696791175
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7470000386238098, scr_score: -0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7710000276565552, scr_score: -0.016216329417147127
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.796000063419342, scr_score: 0.11891899728879417
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7910000085830688, scr_score: 0.09189167419779398
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7980000376701355, scr_score: 0.1297296687753823
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7220000624656677, scr_score: -0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7220000624656677, scr_score: -0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7250000238418579, scr_score: 0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7130000591278076, scr_score: -0.04621838637728532
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7100000381469727, scr_score: -0.058823514680020776
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.6980000138282776, scr_score: -0.10924377745131579
Saved evaluation results to: run_3/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8820000290870667, scr_score: -0.23880655437858572
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8300000429153442, scr_score: -1.0149260768651147
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8220000267028809, scr_score: -1.1343293540544075
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7770000100135803, scr_score: -1.8059716762167581
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6130000352859497, scr_score: 0.042929339300620824
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6630000472068787, scr_score: 0.16919198684844677
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6810000538825989, scr_score: 0.2146465459863349
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.690000057220459, scr_score: 0.23737382555527894
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7020000219345093, scr_score: 0.26767676463602486
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7080000042915344, scr_score: 0.28282823417639785
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7090000510215759, scr_score: 0.28535360453043435
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8540000319480896, scr_score: 0.08181820152219406
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8470000624656677, scr_score: 0.018182123594007893
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8470000624656677, scr_score: 0.018182123594007893
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8340000510215759, scr_score: -0.09999978325586537
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.831000030040741, scr_score: -0.1272726977167089
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.815000057220459, scr_score: -0.27272697716708916
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.659000039100647, scr_score: 0.029325483466179435
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6570000052452087, scr_score: 0.023460281896742038
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6650000214576721, scr_score: 0.04692073858715326
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06451599370812708
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6630000472068787, scr_score: 0.041055711811385044
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6790000200271606, scr_score: 0.08797645039853831
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6730000376701355, scr_score: 0.07038119527756448
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9170000553131104, scr_score: 0.01818158173367131
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: 0.0
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: 0.03636316346734262
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.018182665454344472
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9010000228881836, scr_score: -0.2727280608877623
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8840000629425049, scr_score: -0.5818182015221941
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8500000238418579, scr_score: -1.2000006502324039
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5990000367164612, scr_score: 0.04668303085074865
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6030000448226929, scr_score: 0.05651106046913102
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6060000061988831, scr_score: 0.06388197284635021
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6300000548362732, scr_score: 0.1228501505566444
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6290000081062317, scr_score: 0.12039303331548123
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6330000162124634, scr_score: 0.1302210629338636
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.64000004529953, scr_score: 0.14742015137822193
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.843000054359436, scr_score: 0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8380000591278076, scr_score: -0.015503675388504437
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.843000054359436, scr_score: 0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8220000267028809, scr_score: -0.13953492670246334
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8110000491142273, scr_score: -0.22480606544219944
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7890000343322754, scr_score: -0.3953488049731525
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6710000038146973, scr_score: 0.03283575025692268
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6700000166893005, scr_score: 0.029850714401619252
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6980000138282776, scr_score: 0.11343278589587016
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7050000429153442, scr_score: 0.13432839273157915
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7270000576972961, scr_score: 0.200000071169717
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7230000495910645, scr_score: 0.1880597498242108
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7110000252723694, scr_score: 0.1522387857876922
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8640000224113464, scr_score: 0.24096361316156664
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8630000352859497, scr_score: 0.2349395946453488
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8100000619888306, scr_score: -0.08433733641936964
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8110000491142273, scr_score: -0.0783133179031518
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8160000443458557, scr_score: -0.04819286625795596
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8060000538825989, scr_score: -0.10843376954834762
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8100000619888306, scr_score: -0.08433733641936964
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7360000610351562, scr_score: 0.23357668997121356
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.737000048160553, scr_score: 0.23722627816618688
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7290000319480896, scr_score: 0.20802913753598304
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7300000190734863, scr_score: 0.21167872573095636
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7410000562667847, scr_score: 0.25182484848128883
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7020000219345093, scr_score: 0.1094889510604515
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6860000491142273, scr_score: 0.05109488733525243
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8550000190734863, scr_score: 0.27058823116922914
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.859000027179718, scr_score: 0.2941176883077086
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8520000576972961, scr_score: 0.25294140127701187
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8180000185966492, scr_score: 0.052941190907698006
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8450000286102295, scr_score: 0.21176476363079202
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8480000495910645, scr_score: 0.22941194413853241
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8110000491142273, scr_score: 0.01176490387700127
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.0864661620439461
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6960000395774841, scr_score: 0.026315895615697978
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7000000476837158, scr_score: 0.04135351824215616
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.703000009059906, scr_score: 0.052631567153811336
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7110000252723694, scr_score: 0.08270681240672771
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7050000429153442, scr_score: 0.06015049050583274
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.687000036239624, scr_score: -0.007518699274436781
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8870000243186951, scr_score: 0.16071427621099824
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8870000243186951, scr_score: 0.16071427621099824
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8950000405311584, scr_score: 0.23214296167901954
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8920000195503235, scr_score: 0.20535707158248667
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8800000548362732, scr_score: 0.0982145755645542
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: -0.5624994345543942
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6320000290870667, scr_score: 0.009118603258688643
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6420000195503235, scr_score: 0.039513705895514346
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6600000262260437, scr_score: 0.09422496310944153
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.675000011920929, scr_score: 0.1398176170646801
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.687000036239624, scr_score: 0.17629184893033234
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.690000057220459, scr_score: 0.18541045218902097
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6840000152587891, scr_score: 0.16717324567164368
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7860000133514404, scr_score: 0.1586538062006596
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7680000066757202, scr_score: 0.07211531435288414
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7720000147819519, scr_score: 0.09134612215914382
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7740000486373901, scr_score: 0.10096166934266682
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7630000114440918, scr_score: 0.04807687623525609
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7550000548362732, scr_score: 0.05504598443771902
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7560000419616699, scr_score: 0.05963308145357687
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7590000629425049, scr_score: 0.0733946459169587
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.10091750142791406
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7670000195503235, scr_score: 0.11009169545962974
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7590000629425049, scr_score: 0.0733946459169587
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7310000061988831, scr_score: -0.05504598443771902
Step 0: {'loss': 496.58343505859375, 'l1_loss': 2349.1640625, 'l2_loss': 402.61688232421875}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 447.442626953125, 'l1_loss': 2797.677734375, 'l2_loss': 335.5355224609375}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Enabled 5 new features. Total active: 1251
Enabled 5 new features. Total active: 1256
Enabled 5 new features. Total active: 1261
Step 200: {'loss': 375.41314697265625, 'l1_loss': 1529.05078125, 'l2_loss': 314.2511291503906}
Step 300: {'loss': 336.6236267089844, 'l1_loss': 722.4618530273438, 'l2_loss': 307.72515869140625}
Step 400: {'loss': 308.5605163574219, 'l1_loss': 766.8917236328125, 'l2_loss': 277.8848571777344}
Step 500: {'loss': 284.03131103515625, 'l1_loss': 862.8422241210938, 'l2_loss': 249.51763916015625}
Step 600: {'loss': 268.41943359375, 'l1_loss': 877.3311157226562, 'l2_loss': 233.32620239257812}
Step 700: {'loss': 253.77688598632812, 'l1_loss': 865.1170654296875, 'l2_loss': 219.17221069335938}
Step 800: {'loss': 246.86190795898438, 'l1_loss': 823.2918701171875, 'l2_loss': 213.93023681640625}
Step 900: {'loss': 238.40428161621094, 'l1_loss': 697.38671875, 'l2_loss': 210.50881958007812}
Step 1000: {'loss': 237.30410766601562, 'l1_loss': 738.011962890625, 'l2_loss': 207.78363037109375}
Step 1100: {'loss': 230.5354461669922, 'l1_loss': 633.403564453125, 'l2_loss': 205.19931030273438}
Step 1200: {'loss': 223.76756286621094, 'l1_loss': 537.2891235351562, 'l2_loss': 202.2760009765625}
Step 1300: {'loss': 225.74415588378906, 'l1_loss': 580.3573608398438, 'l2_loss': 202.5298614501953}
Step 1400: {'loss': 222.35968017578125, 'l1_loss': 592.5555419921875, 'l2_loss': 198.65745544433594}
Step 1500: {'loss': 220.76651000976562, 'l1_loss': 546.913818359375, 'l2_loss': 198.88995361328125}
Step 1600: {'loss': 220.01351928710938, 'l1_loss': 548.2496337890625, 'l2_loss': 198.08352661132812}
Step 1700: {'loss': 217.4015655517578, 'l1_loss': 527.8065795898438, 'l2_loss': 196.289306640625}
Step 1800: {'loss': 217.56919860839844, 'l1_loss': 558.2059326171875, 'l2_loss': 195.240966796875}
Step 1900: {'loss': 216.22882080078125, 'l1_loss': 539.8201904296875, 'l2_loss': 194.63601684570312}
Step 2000: {'loss': 215.15528869628906, 'l1_loss': 537.2202758789062, 'l2_loss': 193.66647338867188}
Step 2100: {'loss': 215.08460998535156, 'l1_loss': 520.6642456054688, 'l2_loss': 194.25804138183594}
Step 2200: {'loss': 211.21450805664062, 'l1_loss': 481.6160583496094, 'l2_loss': 191.94985961914062}
Step 2300: {'loss': 211.4887237548828, 'l1_loss': 531.9351196289062, 'l2_loss': 190.21131896972656}
Step 2400: {'loss': 211.73097229003906, 'l1_loss': 522.7025146484375, 'l2_loss': 190.8228759765625}
Step 2500: {'loss': 209.39419555664062, 'l1_loss': 525.8517456054688, 'l2_loss': 188.36012268066406}
Step 2600: {'loss': 209.35154724121094, 'l1_loss': 547.5391845703125, 'l2_loss': 187.44998168945312}
Step 2700: {'loss': 209.3294677734375, 'l1_loss': 523.126220703125, 'l2_loss': 188.4044189453125}
Step 2800: {'loss': 208.5826873779297, 'l1_loss': 503.8358154296875, 'l2_loss': 188.42926025390625}
Step 2900: {'loss': 206.6222381591797, 'l1_loss': 491.8001708984375, 'l2_loss': 186.95022583007812}
Step 3000: {'loss': 208.05615234375, 'l1_loss': 525.871337890625, 'l2_loss': 187.02130126953125}
Step 3100: {'loss': 206.23524475097656, 'l1_loss': 516.9085083007812, 'l2_loss': 185.55889892578125}
Step 3200: {'loss': 206.23748779296875, 'l1_loss': 498.9915466308594, 'l2_loss': 186.27783203125}
Step 3300: {'loss': 205.3775634765625, 'l1_loss': 500.6471252441406, 'l2_loss': 185.3516845703125}
Step 3400: {'loss': 206.2816925048828, 'l1_loss': 522.3321533203125, 'l2_loss': 185.38841247558594}
Step 3500: {'loss': 203.29159545898438, 'l1_loss': 495.2689514160156, 'l2_loss': 183.4808349609375}
Step 3600: {'loss': 205.6530303955078, 'l1_loss': 513.937744140625, 'l2_loss': 185.09552001953125}
Step 3700: {'loss': 204.67294311523438, 'l1_loss': 503.277587890625, 'l2_loss': 184.54183959960938}
Step 3800: {'loss': 203.0613555908203, 'l1_loss': 510.5847473144531, 'l2_loss': 182.63796997070312}
Step 3900: {'loss': 203.41964721679688, 'l1_loss': 505.3779602050781, 'l2_loss': 183.20452880859375}
Step 4000: {'loss': 202.966064453125, 'l1_loss': 492.49127197265625, 'l2_loss': 183.26641845703125}
Step 4100: {'loss': 203.1782989501953, 'l1_loss': 493.64202880859375, 'l2_loss': 183.4326171875}
Step 4200: {'loss': 202.85308837890625, 'l1_loss': 512.2490844726562, 'l2_loss': 182.36312866210938}
Step 4300: {'loss': 203.03517150878906, 'l1_loss': 508.0475158691406, 'l2_loss': 182.71327209472656}
Step 4400: {'loss': 201.47207641601562, 'l1_loss': 498.24835205078125, 'l2_loss': 181.54214477539062}
Step 4500: {'loss': 202.06246948242188, 'l1_loss': 504.7928161621094, 'l2_loss': 181.87075805664062}
Step 4600: {'loss': 201.69667053222656, 'l1_loss': 502.5653381347656, 'l2_loss': 181.59405517578125}
Step 4700: {'loss': 200.76763916015625, 'l1_loss': 516.8275756835938, 'l2_loss': 180.09454345703125}
Step 4800: {'loss': 200.9911651611328, 'l1_loss': 506.1922912597656, 'l2_loss': 180.74346923828125}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_4/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Step 0: {'loss': 1437.1258544921875, 'l1_loss': 11565.46875, 'l2_loss': 974.507080078125}
Step 100: {'loss': 1017.1106567382812, 'l1_loss': 8204.494140625, 'l2_loss': 688.930908203125}
Step 200: {'loss': 784.43701171875, 'l1_loss': 3691.763427734375, 'l2_loss': 636.7664794921875}
Step 300: {'loss': 692.053955078125, 'l1_loss': 1656.397705078125, 'l2_loss': 625.7980346679688}
Step 400: {'loss': 655.4166870117188, 'l1_loss': 1144.89453125, 'l2_loss': 609.6209106445312}
Step 500: {'loss': 607.6644287109375, 'l1_loss': 1409.1800537109375, 'l2_loss': 551.2972412109375}
Step 600: {'loss': 565.9019165039062, 'l1_loss': 1508.752685546875, 'l2_loss': 505.5517883300781}
Step 700: {'loss': 530.829345703125, 'l1_loss': 1478.357177734375, 'l2_loss': 471.695068359375}
Step 800: {'loss': 514.3995971679688, 'l1_loss': 1467.8189697265625, 'l2_loss': 455.68682861328125}
Step 900: {'loss': 492.7655334472656, 'l1_loss': 1285.419677734375, 'l2_loss': 441.3487548828125}
Step 1000: {'loss': 490.29180908203125, 'l1_loss': 1412.078125, 'l2_loss': 433.8086853027344}
Step 1100: {'loss': 476.3499450683594, 'l1_loss': 1203.361083984375, 'l2_loss': 428.21551513671875}
Step 1200: {'loss': 462.3206787109375, 'l1_loss': 1008.7631225585938, 'l2_loss': 421.97015380859375}
Step 1300: {'loss': 465.8956298828125, 'l1_loss': 1123.107177734375, 'l2_loss': 420.9713439941406}
Step 1400: {'loss': 459.6485595703125, 'l1_loss': 1149.386474609375, 'l2_loss': 413.673095703125}
Step 1500: {'loss': 456.0144958496094, 'l1_loss': 1050.8583984375, 'l2_loss': 413.98016357421875}
Step 1600: {'loss': 453.9500732421875, 'l1_loss': 1053.42041015625, 'l2_loss': 411.8132629394531}
Step 1700: {'loss': 449.00335693359375, 'l1_loss': 1002.4117431640625, 'l2_loss': 408.9068908691406}
Step 1800: {'loss': 449.3970031738281, 'l1_loss': 1063.017578125, 'l2_loss': 406.8763122558594}
Step 1900: {'loss': 445.7697448730469, 'l1_loss': 1027.533203125, 'l2_loss': 404.6684265136719}
Step 2000: {'loss': 443.9060363769531, 'l1_loss': 1024.707763671875, 'l2_loss': 402.917724609375}
Step 2100: {'loss': 444.4630126953125, 'l1_loss': 988.5729370117188, 'l2_loss': 404.92010498046875}
Step 2200: {'loss': 436.08599853515625, 'l1_loss': 913.133056640625, 'l2_loss': 399.5606689453125}
Step 2300: {'loss': 437.36627197265625, 'l1_loss': 1010.91064453125, 'l2_loss': 396.9298400878906}
Step 2400: {'loss': 437.60443115234375, 'l1_loss': 986.3865356445312, 'l2_loss': 398.14898681640625}
Step 2500: {'loss': 432.9288330078125, 'l1_loss': 1000.7174072265625, 'l2_loss': 392.900146484375}
Step 2600: {'loss': 432.94281005859375, 'l1_loss': 1044.445556640625, 'l2_loss': 391.16497802734375}
Step 2700: {'loss': 432.6243591308594, 'l1_loss': 991.1851806640625, 'l2_loss': 392.9769592285156}
Step 2800: {'loss': 430.64862060546875, 'l1_loss': 950.3992309570312, 'l2_loss': 392.6326599121094}
Step 2900: {'loss': 426.7047424316406, 'l1_loss': 919.2120361328125, 'l2_loss': 389.9362487792969}
Step 3000: {'loss': 429.2130126953125, 'l1_loss': 990.098876953125, 'l2_loss': 389.60906982421875}
Step 3100: {'loss': 425.43780517578125, 'l1_loss': 968.104248046875, 'l2_loss': 386.713623046875}
Step 3200: {'loss': 424.7178039550781, 'l1_loss': 929.723388671875, 'l2_loss': 387.52886962890625}
Step 3300: {'loss': 422.9954528808594, 'l1_loss': 932.16796875, 'l2_loss': 385.708740234375}
Step 3400: {'loss': 424.52685546875, 'l1_loss': 977.15625, 'l2_loss': 385.44061279296875}
Step 3500: {'loss': 417.343505859375, 'l1_loss': 916.6128540039062, 'l2_loss': 380.6789855957031}
Step 3600: {'loss': 421.99212646484375, 'l1_loss': 959.6696166992188, 'l2_loss': 383.6053466796875}
Step 3700: {'loss': 420.0968017578125, 'l1_loss': 937.986572265625, 'l2_loss': 382.57733154296875}
Step 3800: {'loss': 416.97698974609375, 'l1_loss': 955.961181640625, 'l2_loss': 378.7385559082031}
Step 3900: {'loss': 417.3282775878906, 'l1_loss': 941.7456665039062, 'l2_loss': 379.658447265625}
Step 4000: {'loss': 415.81280517578125, 'l1_loss': 911.5252075195312, 'l2_loss': 379.351806640625}
Step 4100: {'loss': 415.90692138671875, 'l1_loss': 914.511962890625, 'l2_loss': 379.3264465332031}
Step 4200: {'loss': 415.4447326660156, 'l1_loss': 959.4534912109375, 'l2_loss': 377.06658935546875}
Step 4300: {'loss': 416.0540771484375, 'l1_loss': 948.28369140625, 'l2_loss': 378.12274169921875}
Step 4400: {'loss': 412.7254333496094, 'l1_loss': 929.7110595703125, 'l2_loss': 375.5369873046875}
Step 4500: {'loss': 413.7205505371094, 'l1_loss': 947.2483520507812, 'l2_loss': 375.83062744140625}
Step 4600: {'loss': 412.7112731933594, 'l1_loss': 941.3998413085938, 'l2_loss': 375.0552673339844}
Step 4700: {'loss': 411.28155517578125, 'l1_loss': 977.4938354492188, 'l2_loss': 372.1817932128906}
Step 4800: {'loss': 411.6326904296875, 'l1_loss': 951.9332275390625, 'l2_loss': 373.55535888671875}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_4/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_4/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -1.1194041668112302
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8080000281333923, scr_score: -1.3432846443247013
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7750000357627869, scr_score: -1.8358220507031127
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6150000095367432, scr_score: 0.0479797789751554
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6170000433921814, scr_score: 0.0530303691664592
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7070000171661377, scr_score: 0.2803030143391305
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8400000333786011, scr_score: -0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7610000371932983, scr_score: -0.75
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7460000514984131, scr_score: -0.8839284573890609
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7660000324249268, scr_score: -0.7053571808703131
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7450000643730164, scr_score: 0.28571438501396973
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7410000562667847, scr_score: 0.27405255564866005
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: 0.27113696797649733
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7540000081062317, scr_score: 0.31195328386785776
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.753000020980835, scr_score: 0.3090378699701421
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7510000467300415, scr_score: 0.30320704217471073
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7220000624656677, scr_score: 0.2186589964942743
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9230000376701355, scr_score: 0.07692316509555075
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8800000548362732, scr_score: -0.7499988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8960000276565552, scr_score: -0.442307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8660000562667847, scr_score: -1.019229358471188
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8290000557899475, scr_score: -1.7307672028023329
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6060000061988831, scr_score: 0.07055957012725518
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6220000386238098, scr_score: 0.10948908814468004
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6350000500679016, scr_score: 0.14111928527797227
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6570000052452087, scr_score: 0.1946471550167386
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8190000653266907, scr_score: -0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -0.16153868020639714
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7790000438690186, scr_score: -0.4307692025540133
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6720000505447388, scr_score: 0.02121220659617425
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.03333344170540065
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6880000233650208, scr_score: 0.06969696641296766
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7100000381469727, scr_score: 0.13636366920365675
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1515151679351617
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7140000462532043, scr_score: 0.14848490431288316
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8210000395774841, scr_score: 0.02312142910545851
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7200000286102295, scr_score: -0.5606935360671761
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7280000448226929, scr_score: -0.5144506778562591
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7680000066757202, scr_score: 0.34117629184304576
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7540000081062317, scr_score: 0.28627433564194205
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7850000262260437, scr_score: 0.4078430364242822
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7690000534057617, scr_score: 0.34509804379889625
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7420000433921814, scr_score: 0.2392156496088299
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8180000185966492, scr_score: 0.12365589158110252
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8270000219345093, scr_score: 0.17204299352392502
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: 0.1559141731796474
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7820000648498535, scr_score: -0.06989219573519266
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7250000238418579, scr_score: 0.08730163423141594
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7510000467300415, scr_score: 0.1904763143709381
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7520000338554382, scr_score: 0.19444451671638055
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7500000596046448, scr_score: 0.18650811202549564
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7640000581741333, scr_score: 0.2420636544406992
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.76500004529953, scr_score: 0.24603185678614164
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.051587340069761074
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.17777807212159344
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8740000128746033, scr_score: 0.20740730929280218
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7930000424385071, scr_score: -0.39259224919147434
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: -0.1629625705045421
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8130000233650208, scr_score: -0.2444442972725366
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: -0.1555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7720000147819519, scr_score: -0.5481479519189377
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6690000295639038, scr_score: 0.055737767710347226
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7220000624656677, scr_score: 0.22950836331316612
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7190000414848328, scr_score: 0.21967222918044005
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7080000042915344, scr_score: 0.18360653431048468
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7920000553131104, scr_score: 0.09729733212835297
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7900000214576721, scr_score: 0.0864863384544999
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7250000238418579, scr_score: 0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7110000252723694, scr_score: -0.054621888725657895
Saved evaluation results to: run_4/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8460000157356262, scr_score: -0.7761204121084663
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8170000314712524, scr_score: -1.2089561798922313
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8210000395774841, scr_score: -1.1492545412975848
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7850000262260437, scr_score: -1.6865683990274654
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6110000014305115, scr_score: 0.03787874910931702
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6580000519752502, scr_score: 0.1565657371453411
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6890000104904175, scr_score: 0.23484845520124242
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6910000443458557, scr_score: 0.23989904539254622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6890000104904175, scr_score: 0.23484845520124242
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7050000429153442, scr_score: 0.27525257466459596
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8470000624656677, scr_score: 0.018182123594007893
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8540000319480896, scr_score: 0.08181820152219406
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8530000448226929, scr_score: 0.0727274106553584
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8230000138282776, scr_score: -0.2000001083720673
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8130000233650208, scr_score: -0.29090910076109705
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7890000343322754, scr_score: -0.5090907908668356
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7070000171661377, scr_score: 0.1700878740213084
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7040000557899475, scr_score: 0.16129033385765607
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7010000348091125, scr_score: 0.15249261890033458
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7020000219345093, scr_score: 0.1554251322882187
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6980000138282776, scr_score: 0.14369490394301307
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6990000605583191, scr_score: 0.14662759212456636
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6990000605583191, scr_score: 0.14662759212456636
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9170000553131104, scr_score: 0.01818158173367131
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9140000343322754, scr_score: -0.036364247188015786
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9040000438690186, scr_score: -0.2181822319660752
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.23636381369974652
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9050000309944153, scr_score: -0.2000006502324039
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.7090914410992396
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8690000176429749, scr_score: -0.8545462624099563
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.593000054359436, scr_score: 0.03194105964755349
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5950000286102295, scr_score: 0.03685500123236629
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6000000238418579, scr_score: 0.04914000164315505
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6050000190734863, scr_score: 0.061425002053943814
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6140000224113464, scr_score: 0.08353803208311494
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6170000433921814, scr_score: 0.09090909090909091
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6220000386238098, scr_score: 0.10319409131987967
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8370000123977661, scr_score: -0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8300000429153442, scr_score: -0.07751930104548388
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8390000462532043, scr_score: -0.007751837694252218
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7820000648498535, scr_score: -0.4496121308843989
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6780000329017639, scr_score: 0.05373135709263166
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6920000314712524, scr_score: 0.09552239283975711
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7040000557899475, scr_score: 0.1313433568762757
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7170000076293945, scr_score: 0.17014917884380523
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.706000030040741, scr_score: 0.13731342858688256
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7080000042915344, scr_score: 0.14328350029748943
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.718000054359436, scr_score: 0.17313439262340116
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8230000138282776, scr_score: -0.006024377580324436
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8230000138282776, scr_score: -0.006024377580324436
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7940000295639038, scr_score: -0.18072306893528156
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8070000410079956, scr_score: -0.10240975103212978
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8140000104904175, scr_score: -0.06024126235449825
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8170000314712524, scr_score: -0.04216884774173812
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8110000491142273, scr_score: -0.0783133179031518
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7090000510215759, scr_score: 0.13503650349568203
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7290000319480896, scr_score: 0.20802913753598304
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7400000095367432, scr_score: 0.24817504275110688
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7360000610351562, scr_score: 0.23357668997121356
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7200000286102295, scr_score: 0.17518240871080587
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.706000030040741, scr_score: 0.12408752137555344
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6850000619888306, scr_score: 0.047445299140279104
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8160000443458557, scr_score: 0.04117663764621982
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8270000219345093, scr_score: 0.10588238181539601
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8410000205039978, scr_score: 0.18823530649231257
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.7950000166893005, scr_score: -0.08235292467691656
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8290000557899475, scr_score: 0.11764728569239728
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8220000267028809, scr_score: 0.07647064804617747
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8050000667572021, scr_score: -0.023529106522956377
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7040000557899475, scr_score: 0.056391140868614346
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7100000381469727, scr_score: 0.07894746276950931
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.718000054359436, scr_score: 0.10902270802242568
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7050000429153442, scr_score: 0.06015049050583274
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7140000462532043, scr_score: 0.0939850853959675
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.12499993347698757
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8840000629425049, scr_score: 0.13392891829856485
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8850000500679016, scr_score: 0.14285737093604264
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8840000629425049, scr_score: 0.13392891829856485
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.12499993347698757
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8880000710487366, scr_score: 0.16964326103257552
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8040000200271606, scr_score: -0.5803568720134493
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6390000581741333, scr_score: 0.030395283805928035
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6550000309944153, scr_score: 0.07902741179102869
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6680000424385071, scr_score: 0.11854111768654303
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6810000538825989, scr_score: 0.15805482358205739
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7170000076293945, scr_score: 0.2674771568408094
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.706000030040741, scr_score: 0.23404258017412163
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6840000152587891, scr_score: 0.16717324567164368
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.024038438117628045
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7660000324249268, scr_score: 0.06250005373014743
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.718000054359436, scr_score: -0.16826906682339632
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.718000054359436, scr_score: -0.16826906682339632
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7780000567436218, scr_score: 0.1201924771489265
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7610000371932983, scr_score: 0.03846161561251938
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.737000048160553, scr_score: -0.02752285551095536
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7410000562667847, scr_score: -0.009174194031715687
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7430000305175781, scr_score: 0.0
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7670000195503235, scr_score: 0.11009169545962974
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7620000243186951, scr_score: 0.08715593696453222
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.10091750142791406
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7350000143051147, scr_score: -0.03669732295847935
Step 0: {'loss': 496.58343505859375, 'l1_loss': 2349.1640625, 'l2_loss': 402.61688232421875}
Enabled 2 new features. Total active: 578
Enabled 2 new features. Total active: 580
Enabled 2 new features. Total active: 582
Enabled 4 new features. Total active: 586
Enabled 7 new features. Total active: 593
Enabled 10 new features. Total active: 603
Enabled 10 new features. Total active: 613
Enabled 10 new features. Total active: 623
Enabled 10 new features. Total active: 633
Enabled 10 new features. Total active: 643
Enabled 10 new features. Total active: 653
Enabled 10 new features. Total active: 663
Enabled 10 new features. Total active: 673
Enabled 10 new features. Total active: 683
Enabled 10 new features. Total active: 693
Enabled 10 new features. Total active: 703
Enabled 10 new features. Total active: 713
Enabled 10 new features. Total active: 723
Enabled 10 new features. Total active: 733
Enabled 10 new features. Total active: 743
Enabled 10 new features. Total active: 753
Enabled 10 new features. Total active: 763
Enabled 10 new features. Total active: 773
Enabled 10 new features. Total active: 783
Enabled 10 new features. Total active: 793
Enabled 10 new features. Total active: 803
Enabled 10 new features. Total active: 813
Enabled 10 new features. Total active: 823
Enabled 10 new features. Total active: 833
Enabled 10 new features. Total active: 843
Enabled 10 new features. Total active: 853
Enabled 10 new features. Total active: 863
Enabled 10 new features. Total active: 873
Enabled 10 new features. Total active: 883
Enabled 10 new features. Total active: 893
Enabled 10 new features. Total active: 903
Enabled 10 new features. Total active: 913
Enabled 10 new features. Total active: 923
Enabled 10 new features. Total active: 933
Enabled 10 new features. Total active: 943
Enabled 10 new features. Total active: 953
Enabled 10 new features. Total active: 963
Enabled 10 new features. Total active: 973
Enabled 10 new features. Total active: 983
Enabled 10 new features. Total active: 993
Enabled 10 new features. Total active: 1003
Enabled 10 new features. Total active: 1013
Enabled 10 new features. Total active: 1023
Enabled 10 new features. Total active: 1033
Enabled 10 new features. Total active: 1043
Enabled 10 new features. Total active: 1053
Enabled 10 new features. Total active: 1063
Enabled 10 new features. Total active: 1073
Enabled 10 new features. Total active: 1083
Enabled 10 new features. Total active: 1093
Enabled 10 new features. Total active: 1103
Enabled 10 new features. Total active: 1113
Enabled 10 new features. Total active: 1123
Enabled 10 new features. Total active: 1133
Enabled 10 new features. Total active: 1143
Enabled 10 new features. Total active: 1153
Enabled 10 new features. Total active: 1163
Enabled 10 new features. Total active: 1173
Enabled 10 new features. Total active: 1183
Enabled 10 new features. Total active: 1193
Step 100: {'loss': 476.88885498046875, 'l1_loss': 3429.7470703125, 'l2_loss': 339.698974609375}
Enabled 10 new features. Total active: 1203
Enabled 10 new features. Total active: 1213
Enabled 10 new features. Total active: 1223
Enabled 10 new features. Total active: 1233
Enabled 10 new features. Total active: 1243
Enabled 10 new features. Total active: 1253
Enabled 10 new features. Total active: 1263
Enabled 10 new features. Total active: 1273
Enabled 10 new features. Total active: 1283
Enabled 10 new features. Total active: 1293
Enabled 10 new features. Total active: 1303
Enabled 10 new features. Total active: 1313
Enabled 10 new features. Total active: 1323
Enabled 10 new features. Total active: 1333
Enabled 10 new features. Total active: 1343
Enabled 10 new features. Total active: 1353
Enabled 10 new features. Total active: 1363
Enabled 10 new features. Total active: 1373
Enabled 10 new features. Total active: 1383
Enabled 10 new features. Total active: 1393
Enabled 10 new features. Total active: 1403
Enabled 10 new features. Total active: 1413
Enabled 10 new features. Total active: 1423
Enabled 10 new features. Total active: 1433
Enabled 10 new features. Total active: 1443
Enabled 10 new features. Total active: 1453
Enabled 10 new features. Total active: 1463
Enabled 10 new features. Total active: 1473
Enabled 10 new features. Total active: 1483
Enabled 10 new features. Total active: 1493
Enabled 10 new features. Total active: 1503
Enabled 10 new features. Total active: 1513
Enabled 10 new features. Total active: 1523
Enabled 10 new features. Total active: 1533
Enabled 10 new features. Total active: 1543
Enabled 10 new features. Total active: 1553
Enabled 10 new features. Total active: 1563
Enabled 10 new features. Total active: 1573
Step 200: {'loss': 385.40496826171875, 'l1_loss': 1772.5926513671875, 'l2_loss': 314.50128173828125}
Step 300: {'loss': 341.1281433105469, 'l1_loss': 818.0457153320312, 'l2_loss': 308.40631103515625}
Step 400: {'loss': 314.8347473144531, 'l1_loss': 745.9503173828125, 'l2_loss': 284.9967346191406}
Step 500: {'loss': 287.4359130859375, 'l1_loss': 889.7650146484375, 'l2_loss': 251.84530639648438}
Step 600: {'loss': 270.1116027832031, 'l1_loss': 905.9159545898438, 'l2_loss': 233.8749542236328}
Step 700: {'loss': 254.31320190429688, 'l1_loss': 890.2877197265625, 'l2_loss': 218.70169067382812}
Step 800: {'loss': 248.243408203125, 'l1_loss': 843.2721557617188, 'l2_loss': 214.5125274658203}
Step 900: {'loss': 239.39028930664062, 'l1_loss': 711.183349609375, 'l2_loss': 210.94296264648438}
Step 1000: {'loss': 238.42042541503906, 'l1_loss': 757.1942138671875, 'l2_loss': 208.13265991210938}
Step 1100: {'loss': 231.9888153076172, 'l1_loss': 642.8868408203125, 'l2_loss': 206.27334594726562}
Step 1200: {'loss': 224.93356323242188, 'l1_loss': 539.16943359375, 'l2_loss': 203.36679077148438}
Step 1300: {'loss': 227.29600524902344, 'l1_loss': 585.073486328125, 'l2_loss': 203.89306640625}
Step 1400: {'loss': 223.408447265625, 'l1_loss': 595.188720703125, 'l2_loss': 199.6009063720703}
Step 1500: {'loss': 221.97125244140625, 'l1_loss': 543.6478881835938, 'l2_loss': 200.225341796875}
Step 1600: {'loss': 221.04940795898438, 'l1_loss': 542.9951782226562, 'l2_loss': 199.32960510253906}
Step 1700: {'loss': 218.15692138671875, 'l1_loss': 517.658447265625, 'l2_loss': 197.45059204101562}
Step 1800: {'loss': 218.29254150390625, 'l1_loss': 544.6427001953125, 'l2_loss': 196.5068359375}
Step 1900: {'loss': 216.54843139648438, 'l1_loss': 522.637939453125, 'l2_loss': 195.64291381835938}
Step 2000: {'loss': 215.03958129882812, 'l1_loss': 520.0264282226562, 'l2_loss': 194.238525390625}
Step 2100: {'loss': 215.13401794433594, 'l1_loss': 503.01171875, 'l2_loss': 195.0135498046875}
Step 2200: {'loss': 211.3026885986328, 'l1_loss': 465.88653564453125, 'l2_loss': 192.66722106933594}
Step 2300: {'loss': 211.5066375732422, 'l1_loss': 511.78521728515625, 'l2_loss': 191.0352325439453}
Step 2400: {'loss': 211.71609497070312, 'l1_loss': 503.33148193359375, 'l2_loss': 191.5828399658203}
Step 2500: {'loss': 209.4105682373047, 'l1_loss': 509.8408203125, 'l2_loss': 189.01693725585938}
Step 2600: {'loss': 209.3712921142578, 'l1_loss': 531.6953125, 'l2_loss': 188.10348510742188}
Step 2700: {'loss': 209.18475341796875, 'l1_loss': 510.7647705078125, 'l2_loss': 188.75416564941406}
Step 2800: {'loss': 208.29835510253906, 'l1_loss': 494.0861511230469, 'l2_loss': 188.534912109375}
Step 2900: {'loss': 206.33180236816406, 'l1_loss': 483.1646728515625, 'l2_loss': 187.00521850585938}
Step 3000: {'loss': 207.95852661132812, 'l1_loss': 514.2828369140625, 'l2_loss': 187.38720703125}
Step 3100: {'loss': 206.05465698242188, 'l1_loss': 507.90081787109375, 'l2_loss': 185.73861694335938}
Step 3200: {'loss': 205.76739501953125, 'l1_loss': 490.8433837890625, 'l2_loss': 186.1336669921875}
Step 3300: {'loss': 205.089599609375, 'l1_loss': 494.69921875, 'l2_loss': 185.3016357421875}
Step 3400: {'loss': 205.95242309570312, 'l1_loss': 514.6317749023438, 'l2_loss': 185.36715698242188}
Step 3500: {'loss': 202.9159393310547, 'l1_loss': 487.5859375, 'l2_loss': 183.41250610351562}
Step 3600: {'loss': 205.2711944580078, 'l1_loss': 511.92919921875, 'l2_loss': 184.7940216064453}
Step 3700: {'loss': 204.3599395751953, 'l1_loss': 500.4709777832031, 'l2_loss': 184.34109497070312}
Step 3800: {'loss': 202.61691284179688, 'l1_loss': 510.29150390625, 'l2_loss': 182.20526123046875}
Step 3900: {'loss': 203.26466369628906, 'l1_loss': 504.762939453125, 'l2_loss': 183.0741424560547}
Step 4000: {'loss': 202.70916748046875, 'l1_loss': 491.53369140625, 'l2_loss': 183.04782104492188}
Step 4100: {'loss': 202.7298126220703, 'l1_loss': 492.6479797363281, 'l2_loss': 183.02389526367188}
Step 4200: {'loss': 202.45404052734375, 'l1_loss': 514.870361328125, 'l2_loss': 181.85922241210938}
Step 4300: {'loss': 202.67044067382812, 'l1_loss': 508.4798583984375, 'l2_loss': 182.3312530517578}
Step 4400: {'loss': 201.43804931640625, 'l1_loss': 499.5211486816406, 'l2_loss': 181.4571990966797}
Step 4500: {'loss': 202.08168029785156, 'l1_loss': 507.8539733886719, 'l2_loss': 181.76751708984375}
Step 4600: {'loss': 201.63050842285156, 'l1_loss': 505.6567077636719, 'l2_loss': 181.40423583984375}
Step 4700: {'loss': 200.75961303710938, 'l1_loss': 521.0870361328125, 'l2_loss': 179.9161376953125}
Step 4800: {'loss': 201.07601928710938, 'l1_loss': 510.7968444824219, 'l2_loss': 180.64414978027344}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_5/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Step 0: {'loss': 1310.8929443359375, 'l1_loss': 9773.7431640625, 'l2_loss': 919.9432373046875}
Step 100: {'loss': 958.5499267578125, 'l1_loss': 6931.56494140625, 'l2_loss': 681.2872924804688}
Step 200: {'loss': 761.9007568359375, 'l1_loss': 3098.72998046875, 'l2_loss': 637.9515380859375}
Step 300: {'loss': 684.5242919921875, 'l1_loss': 1382.4345703125, 'l2_loss': 629.2269287109375}
Step 400: {'loss': 653.8421630859375, 'l1_loss': 946.583740234375, 'l2_loss': 615.9788208007812}
Step 500: {'loss': 615.5477294921875, 'l1_loss': 1091.4852294921875, 'l2_loss': 571.8883056640625}
Step 600: {'loss': 575.214599609375, 'l1_loss': 1252.073974609375, 'l2_loss': 525.1316528320312}
Step 700: {'loss': 534.94482421875, 'l1_loss': 1316.418212890625, 'l2_loss': 482.2880859375}
Step 800: {'loss': 517.0052490234375, 'l1_loss': 1307.9295654296875, 'l2_loss': 464.6880798339844}
Step 900: {'loss': 493.58172607421875, 'l1_loss': 1124.61083984375, 'l2_loss': 448.5972900390625}
Step 1000: {'loss': 490.4474182128906, 'l1_loss': 1211.163818359375, 'l2_loss': 442.0008544921875}
Step 1100: {'loss': 474.60394287109375, 'l1_loss': 1039.374267578125, 'l2_loss': 433.0289611816406}
Step 1200: {'loss': 460.9996337890625, 'l1_loss': 876.6796875, 'l2_loss': 425.93243408203125}
Step 1300: {'loss': 464.97772216796875, 'l1_loss': 964.9742431640625, 'l2_loss': 426.3787536621094}
Step 1400: {'loss': 458.02294921875, 'l1_loss': 982.7283325195312, 'l2_loss': 418.71380615234375}
Step 1500: {'loss': 454.72039794921875, 'l1_loss': 895.7962646484375, 'l2_loss': 418.8885498046875}
Step 1600: {'loss': 453.4906921386719, 'l1_loss': 895.8565673828125, 'l2_loss': 417.65643310546875}
Step 1700: {'loss': 448.85498046875, 'l1_loss': 851.7642822265625, 'l2_loss': 414.784423828125}
Step 1800: {'loss': 449.43951416015625, 'l1_loss': 904.9573974609375, 'l2_loss': 413.2412109375}
Step 1900: {'loss': 445.6580505371094, 'l1_loss': 869.92138671875, 'l2_loss': 410.8612060546875}
Step 2000: {'loss': 443.9939880371094, 'l1_loss': 868.366943359375, 'l2_loss': 409.2593078613281}
Step 2100: {'loss': 444.75238037109375, 'l1_loss': 841.4811401367188, 'l2_loss': 411.0931396484375}
Step 2200: {'loss': 437.727294921875, 'l1_loss': 773.3211059570312, 'l2_loss': 406.7944641113281}
Step 2300: {'loss': 438.3112487792969, 'l1_loss': 868.75, 'l2_loss': 403.5612487792969}
Step 2400: {'loss': 439.42071533203125, 'l1_loss': 852.190673828125, 'l2_loss': 405.3330993652344}
Step 2500: {'loss': 434.48333740234375, 'l1_loss': 869.594970703125, 'l2_loss': 399.69952392578125}
Step 2600: {'loss': 435.1297302246094, 'l1_loss': 919.459228515625, 'l2_loss': 398.3513488769531}
Step 2700: {'loss': 435.1574401855469, 'l1_loss': 872.0696411132812, 'l2_loss': 400.274658203125}
Step 2800: {'loss': 433.7222595214844, 'l1_loss': 838.5216064453125, 'l2_loss': 400.181396484375}
Step 2900: {'loss': 429.8143310546875, 'l1_loss': 812.208984375, 'l2_loss': 397.3259582519531}
Step 3000: {'loss': 433.2729797363281, 'l1_loss': 885.2789306640625, 'l2_loss': 397.86181640625}
Step 3100: {'loss': 429.7647705078125, 'l1_loss': 870.6907348632812, 'l2_loss': 394.9371337890625}
Step 3200: {'loss': 429.0373229980469, 'l1_loss': 831.8818359375, 'l2_loss': 395.7620544433594}
Step 3300: {'loss': 427.7371826171875, 'l1_loss': 836.103515625, 'l2_loss': 394.29302978515625}
Step 3400: {'loss': 429.3574523925781, 'l1_loss': 884.0162353515625, 'l2_loss': 393.9967956542969}
Step 3500: {'loss': 423.62200927734375, 'l1_loss': 824.0745849609375, 'l2_loss': 390.6590270996094}
Step 3600: {'loss': 428.0062255859375, 'l1_loss': 873.00732421875, 'l2_loss': 393.0859375}
Step 3700: {'loss': 426.0922546386719, 'l1_loss': 851.8001098632812, 'l2_loss': 392.020263671875}
Step 3800: {'loss': 423.1139831542969, 'l1_loss': 871.6124267578125, 'l2_loss': 388.2494812011719}
Step 3900: {'loss': 423.8321533203125, 'l1_loss': 857.5823974609375, 'l2_loss': 389.52886962890625}
Step 4000: {'loss': 422.9089660644531, 'l1_loss': 828.778076171875, 'l2_loss': 389.7578430175781}
Step 4100: {'loss': 422.67657470703125, 'l1_loss': 832.1221923828125, 'l2_loss': 389.3916931152344}
Step 4200: {'loss': 422.8845520019531, 'l1_loss': 876.4989013671875, 'l2_loss': 387.8245849609375}
Step 4300: {'loss': 423.5275573730469, 'l1_loss': 867.642822265625, 'l2_loss': 388.82183837890625}
Step 4400: {'loss': 420.5125732421875, 'l1_loss': 848.1512451171875, 'l2_loss': 386.5865173339844}
Step 4500: {'loss': 421.65362548828125, 'l1_loss': 867.6835327148438, 'l2_loss': 386.9462890625}
Step 4600: {'loss': 420.6914367675781, 'l1_loss': 862.0216064453125, 'l2_loss': 386.2105712890625}
Step 4700: {'loss': 419.287109375, 'l1_loss': 901.919189453125, 'l2_loss': 383.2103576660156}
Step 4800: {'loss': 419.60888671875, 'l1_loss': 875.6607666015625, 'l2_loss': 384.58245849609375}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_5/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_5/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8800000548362732, scr_score: -0.26865692886494025
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8480000495910645, scr_score: -0.7462691480001744
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8280000686645508, scr_score: -1.0447764513514692
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7930000424385071, scr_score: -1.5671651218381724
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6410000324249268, scr_score: 0.11363639784472028
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6520000100135803, scr_score: 0.14141411708819893
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6300000548362732, scr_score: 0.08585867860124165
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.659000039100647, scr_score: 0.1590909569826084
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6670000553131104, scr_score: 0.17929301671428516
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.690000057220459, scr_score: 0.23737382555527894
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8520000576972961, scr_score: 0.06250026609219136
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8470000624656677, scr_score: 0.017857446962504415
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8370000123977661, scr_score: -0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8260000348091125, scr_score: -0.16964281912968696
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8290000557899475, scr_score: -0.14285691477812168
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8090000152587891, scr_score: -0.32142872348125223
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6540000438690186, scr_score: 0.020408244832903703
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6650000214576721, scr_score: 0.05247814525667007
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6780000329017639, scr_score: 0.09037904725031479
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6880000233650208, scr_score: 0.1195335337763655
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6800000071525574, scr_score: 0.09620987504574613
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6850000619888306, scr_score: 0.1107872920832185
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6790000200271606, scr_score: 0.09329446114803046
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: -0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.906000018119812, scr_score: -0.25
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.906000018119812, scr_score: -0.25
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8730000257492065, scr_score: -0.884614679235594
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5870000123977661, scr_score: 0.02433087624915959
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6000000238418579, scr_score: 0.05596107338245181
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.593000054359436, scr_score: 0.038929518017424856
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6070000529289246, scr_score: 0.07299277377094067
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6150000095367432, scr_score: 0.0924573877561912
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8490000367164612, scr_score: 0.10769230063850332
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8400000333786011, scr_score: 0.038461503192516594
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -0.09230788276041041
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8180000185966492, scr_score: -0.130769385952927
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8220000267028809, scr_score: -0.10000009169945687
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7920000553131104, scr_score: -0.3307691108545564
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7790000438690186, scr_score: -0.4307692025540133
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6880000233650208, scr_score: 0.06969696641296766
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6970000267028809, scr_score: 0.09696970025369901
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7240000367164612, scr_score: 0.17878790177589307
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.718000054359436, scr_score: 0.16060613942210955
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7280000448226929, scr_score: 0.19090913688511946
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7290000319480896, scr_score: 0.19393940050739802
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7210000157356262, scr_score: 0.16969693028894522
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8160000443458557, scr_score: -0.0057802711425036495
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.040462587068413366
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7490000128746033, scr_score: -0.39306360572190685
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7550000548362732, scr_score: -0.3583812897959971
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7710000276565552, scr_score: -0.265895917909607
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7540000081062317, scr_score: -0.36416190547394467
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7610000371932983, scr_score: -0.3236993184055313
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.76500004529953, scr_score: 0.3294117372066224
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.796000063419342, scr_score: 0.45098043798896265
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.8030000329017639, scr_score: 0.4784312992176598
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.8020000457763672, scr_score: 0.4745097810055187
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7970000505447388, scr_score: 0.45490195620110374
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7670000195503235, scr_score: 0.33725477363090467
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7330000400543213, scr_score: 0.2039215182121411
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8020000457763672, scr_score: 0.037634555046637416
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8090000152587891, scr_score: 0.07526878963828001
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.815000057220459, scr_score: 0.1075270712368249
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7940000295639038, scr_score: -0.00537627344809254
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8070000410079956, scr_score: 0.06451624274209494
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7910000085830688, scr_score: -0.02150541424736498
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7260000109672546, scr_score: 0.0912698365768584
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.734000027179718, scr_score: 0.12301592839307082
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7410000562667847, scr_score: 0.15079381786384077
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7380000352859497, scr_score: 0.13888897430117703
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7670000195503235, scr_score: 0.25396826147702656
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7490000128746033, scr_score: 0.1825396731537168
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7070000171661377, scr_score: 0.015873045908106207
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.17777807212159344
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.27407397595946886
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8850000500679016, scr_score: 0.2888890360607967
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: -0.1629625705045421
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7930000424385071, scr_score: -0.39259224919147434
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8140000104904175, scr_score: -0.23703698797973444
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7690000534057617, scr_score: -0.5703698797973443
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6820000410079956, scr_score: 0.0983607550520798
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6860000491142273, scr_score: 0.11147553542069445
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7260000109672546, scr_score: 0.2426229482567205
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7280000448226929, scr_score: 0.24918043615355798
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7260000109672546, scr_score: 0.2426229482567205
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7220000624656677, scr_score: 0.22950836331316612
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7170000076293945, scr_score: 0.2131147412836026
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7770000100135803, scr_score: 0.016216007229882225
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.784000039100647, scr_score: 0.05405400180747055
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7110000252723694, scr_score: -0.054621888725657895
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7100000381469727, scr_score: -0.058823514680020776
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7150000333786011, scr_score: -0.03781513446855956
Saved evaluation results to: run_5/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8960000276565552, scr_score: -0.0298512641082919
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8810000419616699, scr_score: -0.253731741621763
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8570000529289246, scr_score: -0.6119406835677043
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -1.2537326312437003
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7990000247955322, scr_score: -1.4776131087571716
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.612000048160553, scr_score: 0.04040411946335353
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.612000048160553, scr_score: 0.04040411946335353
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6380000114440918, scr_score: 0.10606058781614919
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6320000290870667, scr_score: 0.09090911827577622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6840000152587891, scr_score: 0.22222220549813676
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6970000267028809, scr_score: 0.2550505149329192
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.843000054359436, scr_score: -0.01818158173367131
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8340000510215759, scr_score: -0.09999978325586537
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8510000109672546, scr_score: 0.05454528706135051
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8480000495910645, scr_score: 0.027272914460843546
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.843000054359436, scr_score: -0.01818158173367131
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6570000052452087, scr_score: 0.023460281896742038
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6580000519752502, scr_score: 0.026392970078295327
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6550000309944153, scr_score: 0.017595255120973825
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6530000567436218, scr_score: 0.011730228345205611
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.659000039100647, scr_score: 0.029325483466179435
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6600000262260437, scr_score: 0.03225799685406354
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6490000486373901, scr_score: 0.0
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9140000343322754, scr_score: -0.036364247188015786
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9040000438690186, scr_score: -0.2181822319660752
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9020000696182251, scr_score: -0.2545453954334178
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.1272732395770455
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.23636381369974652
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8870000243186951, scr_score: -0.5272734563211802
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8690000176429749, scr_score: -0.8545462624099563
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5910000205039978, scr_score: 0.027026971613983923
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.597000002861023, scr_score: 0.041768942817179085
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5920000076293945, scr_score: 0.02948394240639032
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6000000238418579, scr_score: 0.04914000164315505
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6040000319480896, scr_score: 0.058968031261537414
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6150000095367432, scr_score: 0.08599500287552134
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6200000047683716, scr_score: 0.0982800032863101
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8420000672340393, scr_score: 0.015504137439985289
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8280000686645508, scr_score: -0.09302297643398832
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8380000591278076, scr_score: -0.015503675388504437
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8260000348091125, scr_score: -0.10852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8160000443458557, scr_score: -0.1860464149194575
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8400000333786011, scr_score: 0.0
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7910000085830688, scr_score: -0.37984512958464806
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.04776128538202481
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.690000057220459, scr_score: 0.08955232112915026
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7250000238418579, scr_score: 0.19402982153481765
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7260000109672546, scr_score: 0.19701485739012106
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7190000414848328, scr_score: 0.1761194284787046
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1641791071331984
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7090000510215759, scr_score: 0.14626871407708533
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.843000054359436, scr_score: 0.11445778806456547
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8490000367164612, scr_score: 0.15060225822597914
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8380000591278076, scr_score: 0.08433733641936964
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8480000495910645, scr_score: 0.1445782397097613
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7820000648498535, scr_score: -0.2530120092581089
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7930000424385071, scr_score: -0.18674708745149943
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8010000586509705, scr_score: -0.13855422119354346
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7480000257492065, scr_score: 0.2773721833813107
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7410000562667847, scr_score: 0.25182484848128883
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7540000081062317, scr_score: 0.2992699300863593
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7520000338554382, scr_score: 0.2919707536964126
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7270000576972961, scr_score: 0.2007299611460364
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7400000095367432, scr_score: 0.24817504275110688
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6930000185966492, scr_score: 0.07664222223527434
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8180000185966492, scr_score: 0.052941190907698006
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8390000462532043, scr_score: 0.1764707532308344
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8320000171661377, scr_score: 0.13529411558461457
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.831000030040741, scr_score: 0.12941183895387548
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8350000381469727, scr_score: 0.15294129609235493
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8200000524520874, scr_score: 0.06470609478469928
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.815000057220459, scr_score: 0.035294361015480724
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6980000138282776, scr_score: 0.03383459489013476
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7000000476837158, scr_score: 0.04135351824215616
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7090000510215759, scr_score: 0.07518811313229093
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7200000286102295, scr_score: 0.11654140729686246
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7040000557899475, scr_score: 0.056391140868614346
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7000000476837158, scr_score: 0.04135351824215616
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6830000281333923, scr_score: -0.022556321900894967
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8880000710487366, scr_score: 0.16964326103257552
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8950000405311584, scr_score: 0.23214296167901954
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8910000324249268, scr_score: 0.19642861894500888
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8970000147819519, scr_score: 0.24999986695397514
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9010000228881836, scr_score: 0.2857142096879858
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8980000615119934, scr_score: 0.2589288517755524
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.7950000166893005, scr_score: -0.6607140101189485
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6380000114440918, scr_score: 0.027355628606963597
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6630000472068787, scr_score: 0.10334356636813018
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6790000200271606, scr_score: 0.15197569435323083
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7270000576972961, scr_score: 0.29787244064673746
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7240000367164612, scr_score: 0.28875383738804883
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7240000367164612, scr_score: 0.28875383738804883
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6910000443458557, scr_score: 0.18844992621888307
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7560000419616699, scr_score: 0.014423177494891337
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7720000147819519, scr_score: 0.09134612215914382
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7780000567436218, scr_score: 0.1201924771489265
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7740000486373901, scr_score: 0.10096166934266682
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7550000548362732, scr_score: 0.009615547183522984
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7790000438690186, scr_score: 0.12500010746029486
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7700000405311584, scr_score: 0.08173086153640712
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7480000257492065, scr_score: 0.02293575849509752
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7500000596046448, scr_score: 0.03211022594262151
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7380000352859497, scr_score: -0.02293575849509752
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7470000386238098, scr_score: 0.018348661479239674
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7570000290870667, scr_score: 0.06422017846943472
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7690000534057617, scr_score: 0.11926616290715374
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7260000109672546, scr_score: -0.07798174293281654
Step 0: {'loss': 496.58343505859375, 'l1_loss': 2349.1640625, 'l2_loss': 402.61688232421875}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 447.442626953125, 'l1_loss': 2797.677734375, 'l2_loss': 335.5355224609375}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Enabled 5 new features. Total active: 1251
Enabled 5 new features. Total active: 1256
Enabled 5 new features. Total active: 1261
Step 200: {'loss': 375.41314697265625, 'l1_loss': 1529.05078125, 'l2_loss': 314.2511291503906}
Step 300: {'loss': 336.6236267089844, 'l1_loss': 722.4618530273438, 'l2_loss': 307.72515869140625}
Step 400: {'loss': 308.5605163574219, 'l1_loss': 766.8917236328125, 'l2_loss': 277.8848571777344}
Step 500: {'loss': 284.03131103515625, 'l1_loss': 862.8422241210938, 'l2_loss': 249.51763916015625}
Step 600: {'loss': 268.41943359375, 'l1_loss': 877.3311157226562, 'l2_loss': 233.32620239257812}
Step 700: {'loss': 253.77688598632812, 'l1_loss': 865.1170654296875, 'l2_loss': 219.17221069335938}
Step 800: {'loss': 246.86190795898438, 'l1_loss': 823.2918701171875, 'l2_loss': 213.93023681640625}
Step 900: {'loss': 238.40428161621094, 'l1_loss': 697.38671875, 'l2_loss': 210.50881958007812}
Step 1000: {'loss': 237.30410766601562, 'l1_loss': 738.011962890625, 'l2_loss': 207.78363037109375}
Step 1100: {'loss': 230.5354461669922, 'l1_loss': 633.403564453125, 'l2_loss': 205.19931030273438}
Step 1200: {'loss': 223.76756286621094, 'l1_loss': 537.2891235351562, 'l2_loss': 202.2760009765625}
Step 1300: {'loss': 225.74415588378906, 'l1_loss': 580.3573608398438, 'l2_loss': 202.5298614501953}
Step 1400: {'loss': 222.35968017578125, 'l1_loss': 592.5555419921875, 'l2_loss': 198.65745544433594}
Step 1500: {'loss': 220.76651000976562, 'l1_loss': 546.913818359375, 'l2_loss': 198.88995361328125}
Step 1600: {'loss': 220.01351928710938, 'l1_loss': 548.2496337890625, 'l2_loss': 198.08352661132812}
Step 1700: {'loss': 217.4015655517578, 'l1_loss': 527.8065795898438, 'l2_loss': 196.289306640625}
Step 1800: {'loss': 217.56919860839844, 'l1_loss': 558.2059326171875, 'l2_loss': 195.240966796875}
Step 1900: {'loss': 216.22882080078125, 'l1_loss': 539.8201904296875, 'l2_loss': 194.63601684570312}
Step 2000: {'loss': 215.15528869628906, 'l1_loss': 537.2202758789062, 'l2_loss': 193.66647338867188}
Step 2100: {'loss': 215.08460998535156, 'l1_loss': 520.6642456054688, 'l2_loss': 194.25804138183594}
Step 2200: {'loss': 211.21450805664062, 'l1_loss': 481.6160583496094, 'l2_loss': 191.94985961914062}
Step 2300: {'loss': 211.4887237548828, 'l1_loss': 531.9351196289062, 'l2_loss': 190.21131896972656}
Step 2400: {'loss': 211.73097229003906, 'l1_loss': 522.7025146484375, 'l2_loss': 190.8228759765625}
Step 2500: {'loss': 209.39419555664062, 'l1_loss': 525.8517456054688, 'l2_loss': 188.36012268066406}
Step 2600: {'loss': 209.35154724121094, 'l1_loss': 547.5391845703125, 'l2_loss': 187.44998168945312}
Step 2700: {'loss': 209.3294677734375, 'l1_loss': 523.126220703125, 'l2_loss': 188.4044189453125}
Step 2800: {'loss': 208.5826873779297, 'l1_loss': 503.8358154296875, 'l2_loss': 188.42926025390625}
Step 2900: {'loss': 206.6222381591797, 'l1_loss': 491.8001708984375, 'l2_loss': 186.95022583007812}
Step 3000: {'loss': 208.05615234375, 'l1_loss': 525.871337890625, 'l2_loss': 187.02130126953125}
Step 3100: {'loss': 206.23524475097656, 'l1_loss': 516.9085083007812, 'l2_loss': 185.55889892578125}
Step 3200: {'loss': 206.23748779296875, 'l1_loss': 498.9915466308594, 'l2_loss': 186.27783203125}
Step 3300: {'loss': 205.3775634765625, 'l1_loss': 500.6471252441406, 'l2_loss': 185.3516845703125}
Step 3400: {'loss': 206.2816925048828, 'l1_loss': 522.3321533203125, 'l2_loss': 185.38841247558594}
Step 3500: {'loss': 203.29159545898438, 'l1_loss': 495.2689514160156, 'l2_loss': 183.4808349609375}
Step 3600: {'loss': 205.6530303955078, 'l1_loss': 513.937744140625, 'l2_loss': 185.09552001953125}
Step 3700: {'loss': 204.67294311523438, 'l1_loss': 503.277587890625, 'l2_loss': 184.54183959960938}
Step 3800: {'loss': 203.0613555908203, 'l1_loss': 510.5847473144531, 'l2_loss': 182.63796997070312}
Step 3900: {'loss': 203.41964721679688, 'l1_loss': 505.3779602050781, 'l2_loss': 183.20452880859375}
Step 4000: {'loss': 202.966064453125, 'l1_loss': 492.49127197265625, 'l2_loss': 183.26641845703125}
Step 4100: {'loss': 203.1782989501953, 'l1_loss': 493.64202880859375, 'l2_loss': 183.4326171875}
Step 4200: {'loss': 202.85308837890625, 'l1_loss': 512.2490844726562, 'l2_loss': 182.36312866210938}
Step 4300: {'loss': 203.03517150878906, 'l1_loss': 508.0475158691406, 'l2_loss': 182.71327209472656}
Step 4400: {'loss': 201.47207641601562, 'l1_loss': 498.24835205078125, 'l2_loss': 181.54214477539062}
Step 4500: {'loss': 202.06246948242188, 'l1_loss': 504.7928161621094, 'l2_loss': 181.87075805664062}
Step 4600: {'loss': 201.69667053222656, 'l1_loss': 502.5653381347656, 'l2_loss': 181.59405517578125}
Step 4700: {'loss': 200.76763916015625, 'l1_loss': 516.8275756835938, 'l2_loss': 180.09454345703125}
Step 4800: {'loss': 200.9911651611328, 'l1_loss': 506.1922912597656, 'l2_loss': 180.74346923828125}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_6/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Step 0: {'loss': 1377.567626953125, 'l1_loss': 10703.931640625, 'l2_loss': 949.4103393554688}
Step 100: {'loss': 989.0537109375, 'l1_loss': 7596.3662109375, 'l2_loss': 685.1990966796875}
Step 200: {'loss': 773.6849365234375, 'l1_loss': 3414.1259765625, 'l2_loss': 637.119873046875}
Step 300: {'loss': 688.5020751953125, 'l1_loss': 1529.6356201171875, 'l2_loss': 627.316650390625}
Step 400: {'loss': 654.3843383789062, 'l1_loss': 1064.686279296875, 'l2_loss': 611.796875}
Step 500: {'loss': 606.8079833984375, 'l1_loss': 1353.91748046875, 'l2_loss': 552.6513061523438}
Step 600: {'loss': 565.6700439453125, 'l1_loss': 1457.51416015625, 'l2_loss': 507.3694763183594}
Step 700: {'loss': 531.1502685546875, 'l1_loss': 1421.96875, 'l2_loss': 474.27154541015625}
Step 800: {'loss': 515.3731079101562, 'l1_loss': 1403.5213623046875, 'l2_loss': 459.2322692871094}
Step 900: {'loss': 493.1520690917969, 'l1_loss': 1244.291259765625, 'l2_loss': 443.38043212890625}
Step 1000: {'loss': 490.8755798339844, 'l1_loss': 1374.1632080078125, 'l2_loss': 435.9090576171875}
Step 1100: {'loss': 475.7573547363281, 'l1_loss': 1177.75634765625, 'l2_loss': 428.6470947265625}
Step 1200: {'loss': 461.6303405761719, 'l1_loss': 984.677978515625, 'l2_loss': 422.24322509765625}
Step 1300: {'loss': 466.0283203125, 'l1_loss': 1092.485107421875, 'l2_loss': 422.32891845703125}
Step 1400: {'loss': 459.75341796875, 'l1_loss': 1111.9012451171875, 'l2_loss': 415.2773742675781}
Step 1500: {'loss': 456.676513671875, 'l1_loss': 1010.616455078125, 'l2_loss': 416.2518615722656}
Step 1600: {'loss': 454.91912841796875, 'l1_loss': 1011.7438354492188, 'l2_loss': 414.4493713378906}
Step 1700: {'loss': 450.0162353515625, 'l1_loss': 962.2208862304688, 'l2_loss': 411.52740478515625}
Step 1800: {'loss': 450.66912841796875, 'l1_loss': 1023.22119140625, 'l2_loss': 409.74029541015625}
Step 1900: {'loss': 446.48345947265625, 'l1_loss': 985.178955078125, 'l2_loss': 407.0762939453125}
Step 2000: {'loss': 444.427978515625, 'l1_loss': 981.6998291015625, 'l2_loss': 405.15997314453125}
Step 2100: {'loss': 444.9327392578125, 'l1_loss': 944.7828979492188, 'l2_loss': 407.14141845703125}
Step 2200: {'loss': 437.0947265625, 'l1_loss': 872.4149169921875, 'l2_loss': 402.1981201171875}
Step 2300: {'loss': 437.9506530761719, 'l1_loss': 973.0758666992188, 'l2_loss': 399.0276184082031}
Step 2400: {'loss': 438.0518493652344, 'l1_loss': 951.5601196289062, 'l2_loss': 399.98944091796875}
Step 2500: {'loss': 433.6670227050781, 'l1_loss': 968.8399658203125, 'l2_loss': 394.9134216308594}
Step 2600: {'loss': 433.4328918457031, 'l1_loss': 1016.1529541015625, 'l2_loss': 392.7867736816406}
Step 2700: {'loss': 433.20306396484375, 'l1_loss': 966.2166748046875, 'l2_loss': 394.55438232421875}
Step 2800: {'loss': 431.7756652832031, 'l1_loss': 934.7220458984375, 'l2_loss': 394.38677978515625}
Step 2900: {'loss': 427.671630859375, 'l1_loss': 912.0975341796875, 'l2_loss': 391.187744140625}
Step 3000: {'loss': 430.1190185546875, 'l1_loss': 984.1566162109375, 'l2_loss': 390.75274658203125}
Step 3100: {'loss': 426.79193115234375, 'l1_loss': 964.447998046875, 'l2_loss': 388.2140197753906}
Step 3200: {'loss': 426.3712463378906, 'l1_loss': 928.26025390625, 'l2_loss': 389.2408447265625}
Step 3300: {'loss': 424.8282775878906, 'l1_loss': 936.1351928710938, 'l2_loss': 387.38287353515625}
Step 3400: {'loss': 426.62298583984375, 'l1_loss': 978.7432861328125, 'l2_loss': 387.4732666015625}
Step 3500: {'loss': 420.00274658203125, 'l1_loss': 922.8435668945312, 'l2_loss': 383.0889892578125}
Step 3600: {'loss': 424.70147705078125, 'l1_loss': 967.4503173828125, 'l2_loss': 386.0034484863281}
Step 3700: {'loss': 422.94512939453125, 'l1_loss': 946.6587524414062, 'l2_loss': 385.0787658691406}
Step 3800: {'loss': 420.10784912109375, 'l1_loss': 968.0156860351562, 'l2_loss': 381.3872375488281}
Step 3900: {'loss': 420.71136474609375, 'l1_loss': 957.572998046875, 'l2_loss': 382.408447265625}
Step 4000: {'loss': 419.6266174316406, 'l1_loss': 930.0853271484375, 'l2_loss': 382.4232177734375}
Step 4100: {'loss': 419.5326843261719, 'l1_loss': 936.1152954101562, 'l2_loss': 382.08807373046875}
Step 4200: {'loss': 419.3035583496094, 'l1_loss': 980.8111572265625, 'l2_loss': 380.07110595703125}
Step 4300: {'loss': 420.1541748046875, 'l1_loss': 970.6104736328125, 'l2_loss': 381.3297424316406}
Step 4400: {'loss': 417.0933837890625, 'l1_loss': 951.5975341796875, 'l2_loss': 379.02947998046875}
Step 4500: {'loss': 418.3027038574219, 'l1_loss': 970.6444091796875, 'l2_loss': 379.4769287109375}
Step 4600: {'loss': 417.46954345703125, 'l1_loss': 966.4891357421875, 'l2_loss': 378.8099670410156}
Step 4700: {'loss': 416.1938171386719, 'l1_loss': 1002.930419921875, 'l2_loss': 376.07659912109375}
Step 4800: {'loss': 416.5561218261719, 'l1_loss': 975.3547973632812, 'l2_loss': 377.54193115234375}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_140808_matryoshka_sae/run_6/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_6/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_6/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -1.1194041668112302
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8080000281333923, scr_score: -1.3432846443247013
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7750000357627869, scr_score: -1.8358220507031127
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6150000095367432, scr_score: 0.0479797789751554
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6170000433921814, scr_score: 0.0530303691664592
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7070000171661377, scr_score: 0.2803030143391305
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8400000333786011, scr_score: -0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7610000371932983, scr_score: -0.75
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7460000514984131, scr_score: -0.8839284573890609
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7660000324249268, scr_score: -0.7053571808703131
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7450000643730164, scr_score: 0.28571438501396973
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7410000562667847, scr_score: 0.27405255564866005
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: 0.27113696797649733
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7540000081062317, scr_score: 0.31195328386785776
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.753000020980835, scr_score: 0.3090378699701421
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7510000467300415, scr_score: 0.30320704217471073
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7220000624656677, scr_score: 0.2186589964942743
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9230000376701355, scr_score: 0.07692316509555075
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8800000548362732, scr_score: -0.7499988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8960000276565552, scr_score: -0.442307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8660000562667847, scr_score: -1.019229358471188
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8290000557899475, scr_score: -1.7307672028023329
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6060000061988831, scr_score: 0.07055957012725518
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6220000386238098, scr_score: 0.10948908814468004
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6350000500679016, scr_score: 0.14111928527797227
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6570000052452087, scr_score: 0.1946471550167386
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8190000653266907, scr_score: -0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -0.16153868020639714
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7790000438690186, scr_score: -0.4307692025540133
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6720000505447388, scr_score: 0.02121220659617425
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.03333344170540065
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6880000233650208, scr_score: 0.06969696641296766Loaded pretrained model google/gemma-2-2b into HookedTransformer

dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7100000381469727, scr_score: 0.13636366920365675
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1515151679351617
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7140000462532043, scr_score: 0.14848490431288316
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8210000395774841, scr_score: 0.02312142910545851
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7200000286102295, scr_score: -0.5606935360671761
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7280000448226929, scr_score: -0.5144506778562591
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7680000066757202, scr_score: 0.34117629184304576
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7540000081062317, scr_score: 0.28627433564194205
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7850000262260437, scr_score: 0.4078430364242822
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7690000534057617, scr_score: 0.34509804379889625
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7420000433921814, scr_score: 0.2392156496088299
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8180000185966492, scr_score: 0.12365589158110252
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8270000219345093, scr_score: 0.17204299352392502
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: 0.1559141731796474
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7820000648498535, scr_score: -0.06989219573519266
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7250000238418579, scr_score: 0.08730163423141594
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7510000467300415, scr_score: 0.1904763143709381
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7520000338554382, scr_score: 0.19444451671638055
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7500000596046448, scr_score: 0.18650811202549564
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7640000581741333, scr_score: 0.2420636544406992
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.76500004529953, scr_score: 0.24603185678614164
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.051587340069761074
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.17777807212159344
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8740000128746033, scr_score: 0.20740730929280218
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7930000424385071, scr_score: -0.39259224919147434
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: -0.1629625705045421
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8130000233650208, scr_score: -0.2444442972725366
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: -0.1555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7720000147819519, scr_score: -0.5481479519189377
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6690000295639038, scr_score: 0.055737767710347226
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7220000624656677, scr_score: 0.22950836331316612
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7190000414848328, scr_score: 0.21967222918044005
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7080000042915344, scr_score: 0.18360653431048468
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7920000553131104, scr_score: 0.09729733212835297
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7900000214576721, scr_score: 0.0864863384544999
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7250000238418579, scr_score: 0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7110000252723694, scr_score: -0.054621888725657895
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.893000066280365, scr_score: -0.0746268258378237
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8820000290870667, scr_score: -0.23880655437858572
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8110000491142273, scr_score: -1.298508192973232
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -1.2388065543785858
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.781000018119812, scr_score: -1.7462700376221116
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6940000057220459, scr_score: 0.2474747049043481
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.690000057220459, scr_score: 0.23737382555527894
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6930000185966492, scr_score: 0.2449494850670808
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.706000030040741, scr_score: 0.27777779450186324
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7090000510215759, scr_score: 0.28535360453043435
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8540000319480896, scr_score: 0.08181820152219406
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8260000348091125, scr_score: -0.17272719391122376
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8190000653266907, scr_score: -0.23636327183940994
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8240000605583191, scr_score: -0.19090877564489508
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8200000524520874, scr_score: -0.22727248097257427
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8010000586509705, scr_score: -0.39999967488379806
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8090000152587891, scr_score: -0.32727280608877624
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6480000019073486, scr_score: -0.00293268818155329
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.687000036239624, scr_score: 0.11143690708894953
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6990000605583191, scr_score: 0.14662759212456636
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7120000123977661, scr_score: 0.1847506157543981
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6880000233650208, scr_score: 0.11436942047683363
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6910000443458557, scr_score: 0.12316713543415514
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6810000538825989, scr_score: 0.0938416519679757
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.018182665454344472
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9130000472068787, scr_score: -0.05454582892168709
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9120000600814819, scr_score: -0.0727274106553584
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: 0.0
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.1272732395770455
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8830000162124634, scr_score: -0.6000008669765385
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.7090914410992396
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5879999995231628, scr_score: 0.019655912788007955
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5830000042915344, scr_score: 0.007370912377219193
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5830000042915344, scr_score: 0.007370912377219193
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5890000462532043, scr_score: 0.02211303002917113
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5980000495910645, scr_score: 0.04422606005834226
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.597000002861023, scr_score: 0.041768942817179085
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6170000433921814, scr_score: 0.09090909090909091
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8290000557899475, scr_score: -0.0852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8210000395774841, scr_score: -0.14728676439671556
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8220000267028809, scr_score: -0.13953492670246334
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8250000476837158, scr_score: -0.11627895156822583
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8170000314712524, scr_score: -0.17829457722520528
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7820000648498535, scr_score: -0.4496121308843989
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.784000039100647, scr_score: -0.43410845549589444
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6720000505447388, scr_score: 0.035820964036518604
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6840000152587891, scr_score: 0.07164175014874472
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.690000057220459, scr_score: 0.08955232112915026
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.718000054359436, scr_score: 0.17313439262340116
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7190000414848328, scr_score: 0.1761194284787046
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7120000123977661, scr_score: 0.1552238216429956
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1641791071331984
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8280000686645508, scr_score: 0.02409643312897798
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8460000157356262, scr_score: 0.132529843613219
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8680000305175781, scr_score: 0.26506004629054464
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.734000027179718, scr_score: -0.5421688477417381
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.76500004529953, scr_score: -0.3554217602902387
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7660000324249268, scr_score: -0.34939774177402083
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7720000147819519, scr_score: -0.3132532716126072
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7600000500679016, scr_score: 0.3211678943266165
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7640000581741333, scr_score: 0.33576646464171844
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.76500004529953, scr_score: 0.33941605283669174
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7670000195503235, scr_score: 0.3467152292266384
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7460000514984131, scr_score: 0.27007300699136405
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7520000338554382, scr_score: 0.2919707536964126
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.718000054359436, scr_score: 0.16788323232085922
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.843000054359436, scr_score: 0.20000021036931384
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8410000205039978, scr_score: 0.18823530649231257
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8250000476837158, scr_score: 0.09411782855391783
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8500000238418579, scr_score: 0.2411764974000106
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.843000054359436, scr_score: 0.20000021036931384
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8290000557899475, scr_score: 0.11764728569239728
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8050000667572021, scr_score: -0.023529106522956377
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6990000605583191, scr_score: 0.03759416860493777
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7190000414848328, scr_score: 0.11278205765964408
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.13157902992332066
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.14285730291256044
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.706000030040741, scr_score: 0.06390984014305114
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6910000443458557, scr_score: 0.007518923352021404
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8810000419616699, scr_score: 0.10714302820203199
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8830000162124634, scr_score: 0.12499993347698757
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8890000581741333, scr_score: 0.1785717136700533
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.859000027179718, scr_score: -0.0892855907429769
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8580000400543213, scr_score: -0.09821404338045468
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8420000672340393, scr_score: -0.24107088213239783
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.796000063419342, scr_score: -0.6517850252973711
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6440000534057617, scr_score: 0.04559283512434088
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6600000262260437, scr_score: 0.09422496310944153
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6760000586509705, scr_score: 0.14285727226364453
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7100000381469727, scr_score: 0.24620065746267236
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6970000267028809, scr_score: 0.20668695156715802
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6710000038146973, scr_score: 0.12765953977612934
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6950000524520874, scr_score: 0.20060800350743382
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.024038438117628045
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7680000066757202, scr_score: 0.07211531435288414
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7320000529289246, scr_score: -0.10096138278188053
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7350000143051147, scr_score: -0.08653849184777547
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.024038438117628045
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7620000243186951, scr_score: 0.043269245923887735
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7200000286102295, scr_score: -0.1055045984437719
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7110000252723694, scr_score: -0.1467890184181091
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7120000123977661, scr_score: -0.14220192140225124
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7210000157356262, scr_score: -0.10091750142791406
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7350000143051147, scr_score: -0.03669732295847935
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.734000027179718, scr_score: -0.04128441997433719
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7240000367164612, scr_score: -0.08715593696453222
Generating plots...
Plots generated successfully!
Step 0: {'loss': 496.58343505859375, 'l1_loss': 2349.1640625, 'l2_loss': 402.61688232421875}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 447.442626953125, 'l1_loss': 2797.677734375, 'l2_loss': 335.5355224609375}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Enabled 5 new features. Total active: 1251
Enabled 5 new features. Total active: 1256
Enabled 5 new features. Total active: 1261
Step 200: {'loss': 375.41314697265625, 'l1_loss': 1529.05078125, 'l2_loss': 314.2511291503906}
Step 300: {'loss': 336.6236267089844, 'l1_loss': 722.4618530273438, 'l2_loss': 307.72515869140625}
Step 400: {'loss': 308.5605163574219, 'l1_loss': 766.8917236328125, 'l2_loss': 277.8848571777344}
Step 500: {'loss': 284.03131103515625, 'l1_loss': 862.8422241210938, 'l2_loss': 249.51763916015625}
Step 600: {'loss': 268.41943359375, 'l1_loss': 877.3311157226562, 'l2_loss': 233.32620239257812}
Step 700: {'loss': 253.77688598632812, 'l1_loss': 865.1170654296875, 'l2_loss': 219.17221069335938}
Step 800: {'loss': 246.86190795898438, 'l1_loss': 823.2918701171875, 'l2_loss': 213.93023681640625}
Step 900: {'loss': 238.40428161621094, 'l1_loss': 697.38671875, 'l2_loss': 210.50881958007812}
Step 1000: {'loss': 237.30410766601562, 'l1_loss': 738.011962890625, 'l2_loss': 207.78363037109375}
Step 1100: {'loss': 230.5354461669922, 'l1_loss': 633.403564453125, 'l2_loss': 205.19931030273438}
Step 1200: {'loss': 223.76756286621094, 'l1_loss': 537.2891235351562, 'l2_loss': 202.2760009765625}
Step 1300: {'loss': 225.74415588378906, 'l1_loss': 580.3573608398438, 'l2_loss': 202.5298614501953}
Step 1400: {'loss': 222.35968017578125, 'l1_loss': 592.5555419921875, 'l2_loss': 198.65745544433594}
Step 1500: {'loss': 220.76651000976562, 'l1_loss': 546.913818359375, 'l2_loss': 198.88995361328125}
Step 1600: {'loss': 220.01351928710938, 'l1_loss': 548.2496337890625, 'l2_loss': 198.08352661132812}
Step 1700: {'loss': 217.4015655517578, 'l1_loss': 527.8065795898438, 'l2_loss': 196.289306640625}
Step 1800: {'loss': 217.56919860839844, 'l1_loss': 558.2059326171875, 'l2_loss': 195.240966796875}
Step 1900: {'loss': 216.22882080078125, 'l1_loss': 539.8201904296875, 'l2_loss': 194.63601684570312}
Step 2000: {'loss': 215.15528869628906, 'l1_loss': 537.2202758789062, 'l2_loss': 193.66647338867188}
Step 2100: {'loss': 215.08460998535156, 'l1_loss': 520.6642456054688, 'l2_loss': 194.25804138183594}
Step 2200: {'loss': 211.21450805664062, 'l1_loss': 481.6160583496094, 'l2_loss': 191.94985961914062}
Step 2300: {'loss': 211.4887237548828, 'l1_loss': 531.9351196289062, 'l2_loss': 190.21131896972656}
Step 2400: {'loss': 211.73097229003906, 'l1_loss': 522.7025146484375, 'l2_loss': 190.8228759765625}
Step 2500: {'loss': 209.39419555664062, 'l1_loss': 525.8517456054688, 'l2_loss': 188.36012268066406}
Step 2600: {'loss': 209.35154724121094, 'l1_loss': 547.5391845703125, 'l2_loss': 187.44998168945312}
Step 2700: {'loss': 209.3294677734375, 'l1_loss': 523.126220703125, 'l2_loss': 188.4044189453125}
Step 2800: {'loss': 208.5826873779297, 'l1_loss': 503.8358154296875, 'l2_loss': 188.42926025390625}
Step 2900: {'loss': 206.6222381591797, 'l1_loss': 491.8001708984375, 'l2_loss': 186.95022583007812}
Step 3000: {'loss': 208.05615234375, 'l1_loss': 525.871337890625, 'l2_loss': 187.02130126953125}
Step 3100: {'loss': 206.23524475097656, 'l1_loss': 516.9085083007812, 'l2_loss': 185.55889892578125}
Step 3200: {'loss': 206.23748779296875, 'l1_loss': 498.9915466308594, 'l2_loss': 186.27783203125}
Step 3300: {'loss': 205.3775634765625, 'l1_loss': 500.6471252441406, 'l2_loss': 185.3516845703125}
Step 3400: {'loss': 206.2816925048828, 'l1_loss': 522.3321533203125, 'l2_loss': 185.38841247558594}
Step 3500: {'loss': 203.29159545898438, 'l1_loss': 495.2689514160156, 'l2_loss': 183.4808349609375}
Step 3600: {'loss': 205.6530303955078, 'l1_loss': 513.937744140625, 'l2_loss': 185.09552001953125}
Step 3700: {'loss': 204.67294311523438, 'l1_loss': 503.277587890625, 'l2_loss': 184.54183959960938}
Step 3800: {'loss': 203.0613555908203, 'l1_loss': 510.5847473144531, 'l2_loss': 182.63796997070312}
Step 3900: {'loss': 203.41964721679688, 'l1_loss': 505.3779602050781, 'l2_loss': 183.20452880859375}
Step 4000: {'loss': 202.966064453125, 'l1_loss': 492.49127197265625, 'l2_loss': 183.26641845703125}
Step 4100: {'loss': 203.1782989501953, 'l1_loss': 493.64202880859375, 'l2_loss': 183.4326171875}
Step 4200: {'loss': 202.85308837890625, 'l1_loss': 512.2490844726562, 'l2_loss': 182.36312866210938}
Step 4300: {'loss': 203.03517150878906, 'l1_loss': 508.0475158691406, 'l2_loss': 182.71327209472656}
Step 4400: {'loss': 201.47207641601562, 'l1_loss': 498.24835205078125, 'l2_loss': 181.54214477539062}
Step 4500: {'loss': 202.06246948242188, 'l1_loss': 504.7928161621094, 'l2_loss': 181.87075805664062}
Step 4600: {'loss': 201.69667053222656, 'l1_loss': 502.5653381347656, 'l2_loss': 181.59405517578125}
Step 4700: {'loss': 200.76763916015625, 'l1_loss': 516.8275756835938, 'l2_loss': 180.09454345703125}
Step 4800: {'loss': 200.9911651611328, 'l1_loss': 506.1922912597656, 'l2_loss': 180.74346923828125}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_7/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_7/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -1.1194041668112302
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8080000281333923, scr_score: -1.3432846443247013
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7750000357627869, scr_score: -1.8358220507031127
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6150000095367432, scr_score: 0.0479797789751554
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6170000433921814, scr_score: 0.0530303691664592
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7070000171661377, scr_score: 0.2803030143391305
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8400000333786011, scr_score: -0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7610000371932983, scr_score: -0.75
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7460000514984131, scr_score: -0.8839284573890609
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.7660000324249268, scr_score: -0.7053571808703131
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7450000643730164, scr_score: 0.28571438501396973
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7410000562667847, scr_score: 0.27405255564866005
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: 0.27113696797649733
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7540000081062317, scr_score: 0.31195328386785776
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.753000020980835, scr_score: 0.3090378699701421
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7510000467300415, scr_score: 0.30320704217471073
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7220000624656677, scr_score: 0.2186589964942743
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9230000376701355, scr_score: 0.07692316509555075
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8800000548362732, scr_score: -0.7499988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8960000276565552, scr_score: -0.442307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8660000562667847, scr_score: -1.019229358471188
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8290000557899475, scr_score: -1.7307672028023329
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6060000061988831, scr_score: 0.07055957012725518
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6220000386238098, scr_score: 0.10948908814468004
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6350000500679016, scr_score: 0.14111928527797227
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6570000052452087, scr_score: 0.1946471550167386
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8190000653266907, scr_score: -0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -0.16153868020639714
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7790000438690186, scr_score: -0.4307692025540133
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6720000505447388, scr_score: 0.02121220659617425
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.03333344170540065
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6880000233650208, scr_score: 0.06969696641296766
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7100000381469727, scr_score: 0.13636366920365675
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1515151679351617
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7140000462532043, scr_score: 0.14848490431288316
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8210000395774841, scr_score: 0.02312142910545851
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7200000286102295, scr_score: -0.5606935360671761
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7280000448226929, scr_score: -0.5144506778562591
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7400000095367432, scr_score: -0.44508673507532753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3872829900439593
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7680000066757202, scr_score: 0.34117629184304576
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7540000081062317, scr_score: 0.28627433564194205
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7850000262260437, scr_score: 0.4078430364242822
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7860000133514404, scr_score: 0.41176455463642336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7690000534057617, scr_score: 0.34509804379889625
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7420000433921814, scr_score: 0.2392156496088299
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8180000185966492, scr_score: 0.12365589158110252
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8270000219345093, scr_score: 0.17204299352392502
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: 0.1559141731796474
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7820000648498535, scr_score: -0.06989219573519266
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7250000238418579, scr_score: 0.08730163423141594
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7510000467300415, scr_score: 0.1904763143709381
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7520000338554382, scr_score: 0.19444451671638055
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7500000596046448, scr_score: 0.18650811202549564
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7640000581741333, scr_score: 0.2420636544406992
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.76500004529953, scr_score: 0.24603185678614164
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.051587340069761074
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.17777807212159344
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8740000128746033, scr_score: 0.20740730929280218
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7930000424385071, scr_score: -0.39259224919147434
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: -0.1629625705045421
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8130000233650208, scr_score: -0.2444442972725366
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: -0.1555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7720000147819519, scr_score: -0.5481479519189377
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6690000295639038, scr_score: 0.055737767710347226
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7220000624656677, scr_score: 0.22950836331316612
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7190000414848328, scr_score: 0.21967222918044005
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.718000054359436, scr_score: 0.21639358294455147
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7080000042915344, scr_score: 0.18360653431048468
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7920000553131104, scr_score: 0.09729733212835297
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8010000586509705, scr_score: 0.14594599819252943
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7900000214576721, scr_score: 0.0864863384544999
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7250000238418579, scr_score: 0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7110000252723694, scr_score: -0.054621888725657895
Step 0: {'loss': 764.490966796875, 'l1_loss': 4697.18115234375, 'l2_loss': 576.6036987304688}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 620.5947265625, 'l1_loss': 5621.68359375, 'l2_loss': 395.7274169921875}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Step 200: {'loss': 450.95050048828125, 'l1_loss': 3012.5654296875, 'l2_loss': 330.4478759765625}
Step 300: {'loss': 358.8421630859375, 'l1_loss': 1292.104736328125, 'l2_loss': 307.157958984375}
Step 400: {'loss': 329.0157775878906, 'l1_loss': 923.051513671875, 'l2_loss': 292.0937194824219}
Step 500: {'loss': 297.54718017578125, 'l1_loss': 1028.9849853515625, 'l2_loss': 256.3877868652344}
Step 600: {'loss': 277.6986999511719, 'l1_loss': 967.2818603515625, 'l2_loss': 239.00741577148438}
Step 700: {'loss': 260.67559814453125, 'l1_loss': 872.2363891601562, 'l2_loss': 225.7861328125}
Step 800: {'loss': 253.36605834960938, 'l1_loss': 790.6005859375, 'l2_loss': 221.74203491210938}
Step 900: {'loss': 244.75042724609375, 'l1_loss': 639.8676147460938, 'l2_loss': 219.15573120117188}
Step 1000: {'loss': 242.831787109375, 'l1_loss': 652.94140625, 'l2_loss': 216.71412658691406}
Step 1100: {'loss': 236.99832153320312, 'l1_loss': 541.129638671875, 'l2_loss': 215.35313415527344}
Step 1200: {'loss': 231.47320556640625, 'l1_loss': 451.30755615234375, 'l2_loss': 213.4208984375}
Step 1300: {'loss': 233.11622619628906, 'l1_loss': 480.84686279296875, 'l2_loss': 213.88235473632812}
Step 1400: {'loss': 230.06411743164062, 'l1_loss': 485.0699462890625, 'l2_loss': 210.66131591796875}
Step 1500: {'loss': 228.53509521484375, 'l1_loss': 442.5909423828125, 'l2_loss': 210.83145141601562}
Step 1600: {'loss': 227.86029052734375, 'l1_loss': 441.44927978515625, 'l2_loss': 210.2023162841797}
Step 1700: {'loss': 225.88787841796875, 'l1_loss': 421.6441345214844, 'l2_loss': 209.02210998535156}
Step 1800: {'loss': 226.098388671875, 'l1_loss': 444.23516845703125, 'l2_loss': 208.3289794921875}
Step 1900: {'loss': 224.7614288330078, 'l1_loss': 426.6150817871094, 'l2_loss': 207.6968231201172}
Step 2000: {'loss': 223.5319366455078, 'l1_loss': 424.747802734375, 'l2_loss': 206.54202270507812}
Step 2100: {'loss': 223.92503356933594, 'l1_loss': 409.51141357421875, 'l2_loss': 207.54457092285156}
Step 2200: {'loss': 220.66795349121094, 'l1_loss': 375.9255676269531, 'l2_loss': 205.6309356689453}
Step 2300: {'loss': 221.0088653564453, 'l1_loss': 415.501953125, 'l2_loss': 204.3887939453125}
Step 2400: {'loss': 221.38381958007812, 'l1_loss': 406.2405700683594, 'l2_loss': 205.1342010498047}
Step 2500: {'loss': 219.33462524414062, 'l1_loss': 411.7301025390625, 'l2_loss': 202.86541748046875}
Step 2600: {'loss': 219.01950073242188, 'l1_loss': 430.8478088378906, 'l2_loss': 201.78558349609375}
Step 2700: {'loss': 218.88722229003906, 'l1_loss': 409.31536865234375, 'l2_loss': 202.5146026611328}
Step 2800: {'loss': 218.82916259765625, 'l1_loss': 392.77325439453125, 'l2_loss': 203.11822509765625}
Step 2900: {'loss': 216.96153259277344, 'l1_loss': 379.7023010253906, 'l2_loss': 201.7734375}
Step 3000: {'loss': 218.31787109375, 'l1_loss': 409.1409606933594, 'l2_loss': 201.95223999023438}
Step 3100: {'loss': 216.9287567138672, 'l1_loss': 400.1580810546875, 'l2_loss': 200.9224395751953}
Step 3200: {'loss': 216.85177612304688, 'l1_loss': 381.9655456542969, 'l2_loss': 201.57315063476562}
Step 3300: {'loss': 216.21910095214844, 'l1_loss': 382.48944091796875, 'l2_loss': 200.91952514648438}
Step 3400: {'loss': 216.78411865234375, 'l1_loss': 400.97906494140625, 'l2_loss': 200.74496459960938}
Step 3500: {'loss': 214.16790771484375, 'l1_loss': 374.8169860839844, 'l2_loss': 199.17523193359375}
Step 3600: {'loss': 216.02505493164062, 'l1_loss': 395.7593994140625, 'l2_loss': 200.19467163085938}
Step 3700: {'loss': 214.94874572753906, 'l1_loss': 385.21051025390625, 'l2_loss': 199.5403289794922}
Step 3800: {'loss': 213.5736541748047, 'l1_loss': 394.8001708984375, 'l2_loss': 197.78164672851562}
Step 3900: {'loss': 213.94708251953125, 'l1_loss': 387.16192626953125, 'l2_loss': 198.46060180664062}
Step 4000: {'loss': 213.26116943359375, 'l1_loss': 375.198974609375, 'l2_loss': 198.25320434570312}
Step 4100: {'loss': 213.60122680664062, 'l1_loss': 376.57861328125, 'l2_loss': 198.5380859375}
Step 4200: {'loss': 213.18441772460938, 'l1_loss': 398.2740478515625, 'l2_loss': 197.25344848632812}
Step 4300: {'loss': 213.41722106933594, 'l1_loss': 393.19219970703125, 'l2_loss': 197.6895294189453}
Step 4400: {'loss': 212.13442993164062, 'l1_loss': 384.6526794433594, 'l2_loss': 196.74832153320312}
Step 4500: {'loss': 212.60556030273438, 'l1_loss': 392.3837585449219, 'l2_loss': 196.91021728515625}
Step 4600: {'loss': 212.15460205078125, 'l1_loss': 390.2541809082031, 'l2_loss': 196.54443359375}
Step 4700: {'loss': 211.3887939453125, 'l1_loss': 408.2592468261719, 'l2_loss': 195.0584259033203}
Step 4800: {'loss': 211.63172912597656, 'l1_loss': 397.2782897949219, 'l2_loss': 195.7406005859375}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_8/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_8/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8890000581741333, scr_score: -0.13432846443247012
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8850000500679016, scr_score: -0.19403010302711657
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8680000305175781, scr_score: -0.4477618446488796
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8760000467300415, scr_score: -0.3283585674595867
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8650000691413879, scr_score: -0.49253740637841137
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8710000514984131, scr_score: -0.4029853932974104
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8680000305175781, scr_score: -0.4477618446488796
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6070000529289246, scr_score: 0.027777869760247858
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6080000400543213, scr_score: 0.03030308959751515
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6260000467300415, scr_score: 0.07575764873540326
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6260000467300415, scr_score: 0.07575764873540326
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.625, scr_score: 0.07323227838136674
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6260000467300415, scr_score: 0.07575764873540326
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8490000367164612, scr_score: 0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8460000157356262, scr_score: 0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8530000448226929, scr_score: 0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8510000109672546, scr_score: 0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8460000157356262, scr_score: 0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8490000367164612, scr_score: 0.0357143617406261
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6510000228881836, scr_score: 0.011661829365309686
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6470000147819519, scr_score: 0.0
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6490000486373901, scr_score: 0.005831001569878354
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6480000019073486, scr_score: 0.002915413897715665
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6480000019073486, scr_score: 0.002915413897715665
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6450000405311584, scr_score: -0.00583082779543133
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6430000066757202, scr_score: -0.011661829365309686
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9220000505447388, scr_score: 0.057692660382202984
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9220000505447388, scr_score: 0.057692660382202984
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9080000519752502, scr_score: -0.21153784433114478
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.192307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.192307339617797
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5879999995231628, scr_score: 0.02676393486938317
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5850000381469727, scr_score: 0.019464759008712428
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8280000686645508, scr_score: -0.053845921070609495
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8390000462532043, scr_score: 0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8320000171661377, scr_score: -0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8420000672340393, scr_score: 0.05384637956789382
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8340000510215759, scr_score: -0.007692208939046454
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8400000333786011, scr_score: 0.038461503192516594
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8400000333786011, scr_score: 0.038461503192516594
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6650000214576721, scr_score: 0.0
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6540000438690186, scr_score: -0.03333326108528845
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6570000052452087, scr_score: -0.0242424702184528
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6600000262260437, scr_score: -0.015151498731504951
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6550000309944153, scr_score: -0.030302997463009903
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.659000039100647, scr_score: -0.018181762353783503
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6600000262260437, scr_score: -0.015151498731504951
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8400000333786011, scr_score: 0.1329479589548035
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8420000672340393, scr_score: 0.14450884577525472
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8390000462532043, scr_score: 0.12716768781229984
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8340000510215759, scr_score: 0.09826598756433769
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8350000381469727, scr_score: 0.10404625870684134
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.831000030040741, scr_score: 0.08092482960138282
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8330000638961792, scr_score: 0.09248571642183404
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6800000071525574, scr_score: -0.003921751955850488
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6820000410079956, scr_score: 0.003921518212141116
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6880000233650208, scr_score: 0.027450861228697183
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6850000619888306, scr_score: 0.015686306592273835
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6620000600814819, scr_score: -0.0745097810055187
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6680000424385071, scr_score: -0.050980437988962624
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6650000214576721, scr_score: -0.06274522636909534
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8040000200271606, scr_score: 0.048387101942822495
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8170000314712524, scr_score: 0.11827961813300997
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8030000329017639, scr_score: 0.04301082849472996
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7920000553131104, scr_score: -0.01612882034427762
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7980000376701355, scr_score: 0.01612914079927244
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7130000591278076, scr_score: 0.039682733033433705
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.03571429416165487
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.718000054359436, scr_score: 0.05952398128698237
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7010000348091125, scr_score: -0.007936404690884913
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7100000381469727, scr_score: 0.027777889470769958
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.6960000395774841, scr_score: -0.027777652944433576
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7000000476837158, scr_score: -0.011904607036327371
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8570000529289246, scr_score: 0.08148172676799452
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8530000448226929, scr_score: 0.05185204808106229
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8560000658035278, scr_score: 0.07407441747519233
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8360000252723694, scr_score: -0.07407397595946885
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: -0.1481479519189377
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8270000219345093, scr_score: -0.14074064262613553
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: -0.1629625705045421
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6610000133514404, scr_score: 0.029508206973117915
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6660000085830688, scr_score: 0.04590163357762117
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6810000538825989, scr_score: 0.0950821088161912
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.675000011920929, scr_score: 0.07540984055073909
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6730000376701355, scr_score: 0.06885254807896189
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6710000038146973, scr_score: 0.06229506018212443
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7880000472068787, scr_score: 0.07567566696791175
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7230000495910645, scr_score: -0.004201625954362884
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7350000143051147, scr_score: 0.04621838637728532
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7140000462532043, scr_score: -0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7160000205039978, scr_score: -0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7170000076293945, scr_score: -0.029411882559833787
Step 0: {'loss': 764.490966796875, 'l1_loss': 4697.18115234375, 'l2_loss': 576.6036987304688}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 620.5953369140625, 'l1_loss': 5621.693359375, 'l2_loss': 395.7275695800781}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Step 200: {'loss': 450.9514465332031, 'l1_loss': 3012.5849609375, 'l2_loss': 330.44805908203125}
Step 300: {'loss': 358.8424377441406, 'l1_loss': 1292.112548828125, 'l2_loss': 307.1579284667969}
Step 400: {'loss': 329.0160217285156, 'l1_loss': 923.0570678710938, 'l2_loss': 292.09375}
Step 500: {'loss': 297.5472106933594, 'l1_loss': 1028.9619140625, 'l2_loss': 256.38873291015625}
Step 600: {'loss': 277.6505432128906, 'l1_loss': 966.9688720703125, 'l2_loss': 238.9718017578125}
Step 700: {'loss': 260.6564025878906, 'l1_loss': 872.1704711914062, 'l2_loss': 225.76959228515625}
Step 800: {'loss': 253.442138671875, 'l1_loss': 790.140380859375, 'l2_loss': 221.83651733398438}
Step 900: {'loss': 244.7196044921875, 'l1_loss': 639.3046875, 'l2_loss': 219.1474151611328}
Step 1000: {'loss': 243.05003356933594, 'l1_loss': 652.00830078125, 'l2_loss': 216.96969604492188}
Step 1100: {'loss': 237.05897521972656, 'l1_loss': 541.00146484375, 'l2_loss': 215.41891479492188}
Step 1200: {'loss': 231.4762725830078, 'l1_loss': 451.27801513671875, 'l2_loss': 213.42515563964844}
Step 1300: {'loss': 233.16139221191406, 'l1_loss': 481.2338562011719, 'l2_loss': 213.9120330810547}
Step 1400: {'loss': 230.0882568359375, 'l1_loss': 484.8128356933594, 'l2_loss': 210.69573974609375}
Step 1500: {'loss': 228.5841827392578, 'l1_loss': 442.798583984375, 'l2_loss': 210.8722381591797}
Step 1600: {'loss': 227.90887451171875, 'l1_loss': 441.9923400878906, 'l2_loss': 210.22918701171875}
Step 1700: {'loss': 225.917724609375, 'l1_loss': 422.0927734375, 'l2_loss': 209.0340118408203}
Step 1800: {'loss': 226.088134765625, 'l1_loss': 444.6700744628906, 'l2_loss': 208.30133056640625}
Step 1900: {'loss': 224.76718139648438, 'l1_loss': 426.82806396484375, 'l2_loss': 207.69406127929688}
Step 2000: {'loss': 223.490478515625, 'l1_loss': 425.0455017089844, 'l2_loss': 206.48866271972656}
Step 2100: {'loss': 223.91680908203125, 'l1_loss': 409.6708068847656, 'l2_loss': 207.5299835205078}
Step 2200: {'loss': 220.63734436035156, 'l1_loss': 376.3819580078125, 'l2_loss': 205.58206176757812}
Step 2300: {'loss': 221.05711364746094, 'l1_loss': 415.8770751953125, 'l2_loss': 204.42202758789062}
Step 2400: {'loss': 221.34120178222656, 'l1_loss': 406.2610168457031, 'l2_loss': 205.09075927734375}
Step 2500: {'loss': 219.20852661132812, 'l1_loss': 411.84149169921875, 'l2_loss': 202.73486328125}
Step 2600: {'loss': 219.01922607421875, 'l1_loss': 430.5161437988281, 'l2_loss': 201.798583984375}
Step 2700: {'loss': 218.85411071777344, 'l1_loss': 408.6865234375, 'l2_loss': 202.50665283203125}
Step 2800: {'loss': 218.80856323242188, 'l1_loss': 392.1022033691406, 'l2_loss': 203.12448120117188}
Step 2900: {'loss': 216.99044799804688, 'l1_loss': 379.1639709472656, 'l2_loss': 201.82388305664062}
Step 3000: {'loss': 218.31503295898438, 'l1_loss': 408.1299743652344, 'l2_loss': 201.98983764648438}
Step 3100: {'loss': 216.9422607421875, 'l1_loss': 399.819091796875, 'l2_loss': 200.94949340820312}
Step 3200: {'loss': 216.8874053955078, 'l1_loss': 381.65240478515625, 'l2_loss': 201.62130737304688}
Step 3300: {'loss': 216.3189697265625, 'l1_loss': 382.1937255859375, 'l2_loss': 201.03121948242188}
Step 3400: {'loss': 216.8059539794922, 'l1_loss': 400.92852783203125, 'l2_loss': 200.76881408691406}
Step 3500: {'loss': 214.18142700195312, 'l1_loss': 374.6618347167969, 'l2_loss': 199.1949462890625}
Step 3600: {'loss': 216.00155639648438, 'l1_loss': 396.0758361816406, 'l2_loss': 200.1585235595703}
Step 3700: {'loss': 215.0021514892578, 'l1_loss': 386.062255859375, 'l2_loss': 199.55966186523438}
Step 3800: {'loss': 213.59793090820312, 'l1_loss': 395.8377990722656, 'l2_loss': 197.76441955566406}
Step 3900: {'loss': 214.0377655029297, 'l1_loss': 388.318359375, 'l2_loss': 198.50503540039062}
Step 4000: {'loss': 213.3588409423828, 'l1_loss': 376.0811767578125, 'l2_loss': 198.3155975341797}
Step 4100: {'loss': 213.68338012695312, 'l1_loss': 376.8278503417969, 'l2_loss': 198.61026000976562}
Step 4200: {'loss': 213.2888946533203, 'l1_loss': 398.7471923828125, 'l2_loss': 197.33900451660156}
Step 4300: {'loss': 213.56411743164062, 'l1_loss': 394.05462646484375, 'l2_loss': 197.8019256591797}
Step 4400: {'loss': 212.266357421875, 'l1_loss': 385.938232421875, 'l2_loss': 196.82882690429688}
Step 4500: {'loss': 212.64559936523438, 'l1_loss': 394.13818359375, 'l2_loss': 196.88006591796875}
Step 4600: {'loss': 212.2814178466797, 'l1_loss': 391.9147033691406, 'l2_loss': 196.60482788085938}
Step 4700: {'loss': 211.34786987304688, 'l1_loss': 410.26611328125, 'l2_loss': 194.93722534179688}
Step 4800: {'loss': 211.72157287597656, 'l1_loss': 398.9112243652344, 'l2_loss': 195.76512145996094}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_9/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_9/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.89000004529953, scr_score: -0.11940327718929286
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8770000338554382, scr_score: -0.31343338021640943
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8800000548362732, scr_score: -0.26865692886494025
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8670000433921814, scr_score: -0.46268703189205684
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8640000224113464, scr_score: -0.507463483243526
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8680000305175781, scr_score: -0.4477618446488796
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6070000529289246, scr_score: 0.027777869760247858
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6110000014305115, scr_score: 0.03787874910931702
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6110000014305115, scr_score: 0.03787874910931702
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.612000048160553, scr_score: 0.04040411946335353
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6270000338554382, scr_score: 0.07828286857267056
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6260000467300415, scr_score: 0.07575764873540326
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6260000467300415, scr_score: 0.07575764873540326
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8510000109672546, scr_score: 0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8420000672340393, scr_score: -0.02678537216718253
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8470000624656677, scr_score: 0.017857446962504415
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8510000109672546, scr_score: 0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8400000333786011, scr_score: -0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8460000157356262, scr_score: 0.008928457389060843
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6540000438690186, scr_score: 0.020408244832903703
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6540000438690186, scr_score: 0.020408244832903703
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6520000100135803, scr_score: 0.01457724326302535
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6550000309944153, scr_score: 0.02332365873061937
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6510000228881836, scr_score: 0.011661829365309686
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6470000147819519, scr_score: 0.0
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6460000276565552, scr_score: -0.002915413897715665
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9220000505447388, scr_score: 0.057692660382202984
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9220000505447388, scr_score: 0.057692660382202984
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.07692316509555075
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.23076834904449253
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9080000519752502, scr_score: -0.21153784433114478
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.23076834904449253
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5850000381469727, scr_score: 0.019464759008712428
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5879999995231628, scr_score: 0.02676393486938317
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5870000123977661, scr_score: 0.02433087624915959
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8340000510215759, scr_score: -0.007692208939046454
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8320000171661377, scr_score: -0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8400000333786011, scr_score: 0.038461503192516594
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8370000123977661, scr_score: 0.015384417878092908
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8360000252723694, scr_score: 0.007692208939046454
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6650000214576721, scr_score: 0.0
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6520000100135803, scr_score: -0.03939396894995775
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.659000039100647, scr_score: -0.018181762353783503
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6600000262260437, scr_score: -0.015151498731504951
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6550000309944153, scr_score: -0.030302997463009903
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6630000472068787, scr_score: -0.006060527244557103
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6640000343322754, scr_score: -0.0030302636222785517
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8320000171661377, scr_score: 0.08670510074388647
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8370000123977661, scr_score: 0.11560680099184864
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8370000123977661, scr_score: 0.11560680099184864
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8320000171661377, scr_score: 0.08670510074388647
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8420000672340393, scr_score: 0.14450884577525472
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8400000333786011, scr_score: 0.1329479589548035
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8330000638961792, scr_score: 0.09248571642183404
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6820000410079956, scr_score: 0.003921518212141116
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6830000281333923, scr_score: 0.007843036424282232
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6860000491142273, scr_score: 0.01960782480441495
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.687000036239624, scr_score: 0.023529343016556067
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6770000457763672, scr_score: -0.015686306592273835
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6670000553131104, scr_score: -0.05490195620110374
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6650000214576721, scr_score: -0.06274522636909534
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7970000505447388, scr_score: 0.010752867351179899
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8020000457763672, scr_score: 0.037634555046637416
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8090000152587891, scr_score: 0.07526878963828001
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.800000011920929, scr_score: 0.02688168769545752
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7870000600814819, scr_score: -0.043010508039735135
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7970000505447388, scr_score: 0.010752867351179899
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.800000011920929, scr_score: 0.02688168769545752
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7110000252723694, scr_score: 0.031746091816212414
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7090000510215759, scr_score: 0.0238096871253275
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: 0.07142858832330974
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.703000009059906, scr_score: 0.0
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7070000171661377, scr_score: 0.015873045908106207
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7020000219345093, scr_score: -0.003968202345442456
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.6980000138282776, scr_score: -0.019841248253548664
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8580000400543213, scr_score: 0.08888903606079672
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8520000576972961, scr_score: 0.044444738788260094
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8470000624656677, scr_score: 0.0074077508085256686
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8350000381469727, scr_score: -0.08148128525227105
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8440000414848328, scr_score: -0.014814618585604379
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: -0.1555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8220000267028809, scr_score: -0.17777763060586996
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6600000262260437, scr_score: 0.026229560737229315
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6640000343322754, scr_score: 0.03934434110584397
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6730000376701355, scr_score: 0.06885254807896189
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6670000553131104, scr_score: 0.049180475238570026
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6780000329017639, scr_score: 0.08524597468346515
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6760000586509705, scr_score: 0.07868868221168794
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6720000505447388, scr_score: 0.06557390184307328
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.784000039100647, scr_score: 0.05405400180747055
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7360000610351562, scr_score: 0.05042026277129501
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.734000027179718, scr_score: 0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7200000286102295, scr_score: -0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7190000414848328, scr_score: -0.02100838021146122
Step 0: {'loss': 764.490966796875, 'l1_loss': 4697.18115234375, 'l2_loss': 576.6036987304688}
Enabled 5 new features. Total active: 581
Enabled 5 new features. Total active: 586
Enabled 5 new features. Total active: 591
Enabled 5 new features. Total active: 596
Enabled 5 new features. Total active: 601
Enabled 5 new features. Total active: 606
Enabled 5 new features. Total active: 611
Enabled 5 new features. Total active: 616
Enabled 5 new features. Total active: 621
Enabled 5 new features. Total active: 626
Enabled 5 new features. Total active: 631
Enabled 5 new features. Total active: 636
Enabled 5 new features. Total active: 641
Enabled 5 new features. Total active: 646
Enabled 5 new features. Total active: 651
Enabled 5 new features. Total active: 656
Enabled 5 new features. Total active: 661
Enabled 5 new features. Total active: 666
Enabled 5 new features. Total active: 671
Enabled 5 new features. Total active: 676
Enabled 5 new features. Total active: 681
Enabled 5 new features. Total active: 686
Enabled 5 new features. Total active: 691
Enabled 5 new features. Total active: 696
Enabled 5 new features. Total active: 701
Enabled 5 new features. Total active: 706
Enabled 5 new features. Total active: 711
Enabled 5 new features. Total active: 716
Enabled 5 new features. Total active: 721
Enabled 5 new features. Total active: 726
Enabled 5 new features. Total active: 731
Enabled 5 new features. Total active: 736
Enabled 5 new features. Total active: 741
Enabled 5 new features. Total active: 746
Enabled 5 new features. Total active: 751
Enabled 5 new features. Total active: 756
Enabled 5 new features. Total active: 761
Enabled 5 new features. Total active: 766
Enabled 5 new features. Total active: 771
Enabled 5 new features. Total active: 776
Enabled 5 new features. Total active: 781
Enabled 5 new features. Total active: 786
Enabled 5 new features. Total active: 791
Enabled 5 new features. Total active: 796
Enabled 5 new features. Total active: 801
Enabled 5 new features. Total active: 806
Enabled 5 new features. Total active: 811
Enabled 5 new features. Total active: 816
Enabled 5 new features. Total active: 821
Enabled 5 new features. Total active: 826
Enabled 5 new features. Total active: 831
Enabled 5 new features. Total active: 836
Enabled 5 new features. Total active: 841
Enabled 5 new features. Total active: 846
Enabled 5 new features. Total active: 851
Enabled 5 new features. Total active: 856
Enabled 5 new features. Total active: 861
Enabled 5 new features. Total active: 866
Enabled 5 new features. Total active: 871
Enabled 5 new features. Total active: 876
Enabled 5 new features. Total active: 881
Enabled 5 new features. Total active: 886
Enabled 5 new features. Total active: 891
Enabled 5 new features. Total active: 896
Enabled 5 new features. Total active: 901
Enabled 5 new features. Total active: 906
Enabled 5 new features. Total active: 911
Enabled 5 new features. Total active: 916
Enabled 5 new features. Total active: 921
Enabled 5 new features. Total active: 926
Enabled 5 new features. Total active: 931
Enabled 5 new features. Total active: 936
Enabled 5 new features. Total active: 941
Enabled 5 new features. Total active: 946
Enabled 5 new features. Total active: 951
Enabled 5 new features. Total active: 956
Enabled 5 new features. Total active: 961
Enabled 5 new features. Total active: 966
Step 100: {'loss': 620.5950317382812, 'l1_loss': 5621.6884765625, 'l2_loss': 395.7275085449219}
Enabled 5 new features. Total active: 971
Enabled 5 new features. Total active: 976
Enabled 5 new features. Total active: 981
Enabled 5 new features. Total active: 986
Enabled 5 new features. Total active: 991
Enabled 5 new features. Total active: 996
Enabled 5 new features. Total active: 1001
Enabled 5 new features. Total active: 1006
Enabled 5 new features. Total active: 1011
Enabled 5 new features. Total active: 1016
Enabled 5 new features. Total active: 1021
Enabled 5 new features. Total active: 1026
Enabled 5 new features. Total active: 1031
Enabled 5 new features. Total active: 1036
Enabled 5 new features. Total active: 1041
Enabled 5 new features. Total active: 1046
Enabled 5 new features. Total active: 1051
Enabled 5 new features. Total active: 1056
Enabled 5 new features. Total active: 1061
Enabled 5 new features. Total active: 1066
Enabled 5 new features. Total active: 1071
Enabled 5 new features. Total active: 1076
Enabled 5 new features. Total active: 1081
Enabled 5 new features. Total active: 1086
Enabled 5 new features. Total active: 1091
Enabled 5 new features. Total active: 1096
Enabled 5 new features. Total active: 1101
Enabled 5 new features. Total active: 1106
Enabled 5 new features. Total active: 1111
Enabled 5 new features. Total active: 1116
Enabled 5 new features. Total active: 1121
Enabled 5 new features. Total active: 1126
Enabled 5 new features. Total active: 1131
Enabled 5 new features. Total active: 1136
Enabled 5 new features. Total active: 1141
Enabled 5 new features. Total active: 1146
Enabled 5 new features. Total active: 1151
Enabled 5 new features. Total active: 1156
Enabled 5 new features. Total active: 1161
Enabled 5 new features. Total active: 1166
Enabled 5 new features. Total active: 1171
Enabled 5 new features. Total active: 1176
Enabled 5 new features. Total active: 1181
Enabled 5 new features. Total active: 1186
Enabled 5 new features. Total active: 1191
Enabled 5 new features. Total active: 1196
Enabled 5 new features. Total active: 1201
Enabled 5 new features. Total active: 1206
Enabled 5 new features. Total active: 1211
Enabled 5 new features. Total active: 1216
Enabled 5 new features. Total active: 1221
Enabled 5 new features. Total active: 1226
Enabled 5 new features. Total active: 1231
Enabled 5 new features. Total active: 1236
Enabled 5 new features. Total active: 1241
Enabled 5 new features. Total active: 1246
Step 200: {'loss': 450.95123291015625, 'l1_loss': 3012.580078125, 'l2_loss': 330.4480285644531}
Step 300: {'loss': 358.84234619140625, 'l1_loss': 1292.1107177734375, 'l2_loss': 307.1579284667969}
Step 400: {'loss': 329.0159606933594, 'l1_loss': 923.0557250976562, 'l2_loss': 292.0937194824219}
Step 500: {'loss': 297.5426025390625, 'l1_loss': 1029.015625, 'l2_loss': 256.3819885253906}
Step 600: {'loss': 277.7048645019531, 'l1_loss': 967.36328125, 'l2_loss': 239.01034545898438}
Step 700: {'loss': 260.67572021484375, 'l1_loss': 872.28369140625, 'l2_loss': 225.78436279296875}
Step 800: {'loss': 253.41041564941406, 'l1_loss': 790.52734375, 'l2_loss': 221.78932189941406}
Step 900: {'loss': 244.75770568847656, 'l1_loss': 639.5426635742188, 'l2_loss': 219.17599487304688}
Step 1000: {'loss': 242.8085174560547, 'l1_loss': 652.822509765625, 'l2_loss': 216.69561767578125}
Step 1100: {'loss': 237.02969360351562, 'l1_loss': 541.3903198242188, 'l2_loss': 215.37408447265625}
Step 1200: {'loss': 231.49356079101562, 'l1_loss': 451.82171630859375, 'l2_loss': 213.42068481445312}
Step 1300: {'loss': 233.14292907714844, 'l1_loss': 481.7913513183594, 'l2_loss': 213.87127685546875}
Step 1400: {'loss': 230.11659240722656, 'l1_loss': 486.27374267578125, 'l2_loss': 210.6656494140625}
Step 1500: {'loss': 228.56785583496094, 'l1_loss': 443.9303894042969, 'l2_loss': 210.81063842773438}
Step 1600: {'loss': 227.81271362304688, 'l1_loss': 442.8467102050781, 'l2_loss': 210.09884643554688}
Step 1700: {'loss': 225.86209106445312, 'l1_loss': 422.3501281738281, 'l2_loss': 208.96807861328125}
Step 1800: {'loss': 226.05221557617188, 'l1_loss': 444.537841796875, 'l2_loss': 208.2707061767578}
Step 1900: {'loss': 224.7721710205078, 'l1_loss': 426.5970764160156, 'l2_loss': 207.70828247070312}
Step 2000: {'loss': 223.48291015625, 'l1_loss': 424.3153076171875, 'l2_loss': 206.5102996826172}
Step 2100: {'loss': 223.84124755859375, 'l1_loss': 409.407470703125, 'l2_loss': 207.46495056152344}
Step 2200: {'loss': 220.6785430908203, 'l1_loss': 375.8022766113281, 'l2_loss': 205.64645385742188}
Step 2300: {'loss': 221.04800415039062, 'l1_loss': 415.175048828125, 'l2_loss': 204.44100952148438}
Step 2400: {'loss': 221.41732788085938, 'l1_loss': 406.0655517578125, 'l2_loss': 205.17471313476562}
Step 2500: {'loss': 219.2967071533203, 'l1_loss': 411.88311767578125, 'l2_loss': 202.82138061523438}Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 622.8250732421875, 'l1_loss': 6244.1435546875, 'l2_loss': 355.8350830078125}
Step 200: {'loss': 442.5502014160156, 'l1_loss': 2759.38623046875, 'l2_loss': 318.8096618652344}
Step 300: {'loss': 364.8193054199219, 'l1_loss': 1213.5875244140625, 'l2_loss': 309.97314453125}
Step 400: {'loss': 335.6042175292969, 'l1_loss': 801.322265625, 'l2_loss': 301.65618896484375}
Step 500: {'loss': 307.5481872558594, 'l1_loss': 871.8662109375, 'l2_loss': 272.48095703125}
Step 600: {'loss': 285.3959655761719, 'l1_loss': 901.422119140625, 'l2_loss': 249.3350830078125}
Step 700: {'loss': 265.7890930175781, 'l1_loss': 889.9216918945312, 'l2_loss': 230.1918487548828}
Step 800: {'loss': 256.7901611328125, 'l1_loss': 872.9219360351562, 'l2_loss': 221.8731231689453}
Step 900: {'loss': 246.69029235839844, 'l1_loss': 710.0457153320312, 'l2_loss': 218.2882080078125}
Step 1000: {'loss': 245.0933380126953, 'l1_loss': 744.2099609375, 'l2_loss': 215.32470703125}
Step 1100: {'loss': 237.6471710205078, 'l1_loss': 613.95947265625, 'l2_loss': 213.0887451171875}
Step 1200: {'loss': 230.5098876953125, 'l1_loss': 508.29010009765625, 'l2_loss': 210.17825317382812}
Step 1300: {'loss': 232.17764282226562, 'l1_loss': 551.0537109375, 'l2_loss': 210.13522338867188}
Step 1400: {'loss': 228.55526733398438, 'l1_loss': 558.0118408203125, 'l2_loss': 206.23452758789062}
Step 1500: {'loss': 226.49708557128906, 'l1_loss': 508.35736083984375, 'l2_loss': 206.16241455078125}
Step 1600: {'loss': 225.8092803955078, 'l1_loss': 508.0850524902344, 'l2_loss': 205.4853057861328}
Step 1700: {'loss': 223.3820343017578, 'l1_loss': 487.4073486328125, 'l2_loss': 203.88514709472656}
Step 1800: {'loss': 223.19827270507812, 'l1_loss': 517.2998657226562, 'l2_loss': 202.50494384765625}
Step 1900: {'loss': 221.42230224609375, 'l1_loss': 501.87762451171875, 'l2_loss': 201.34695434570312}
Step 2000: {'loss': 220.36233520507812, 'l1_loss': 501.6819152832031, 'l2_loss': 200.2937774658203}
Step 2100: {'loss': 220.3744659423828, 'l1_loss': 486.5117492675781, 'l2_loss': 200.91261291503906}
Step 2200: {'loss': 216.24342346191406, 'l1_loss': 447.9612121582031, 'l2_loss': 198.32342529296875}
Step 2300: {'loss': 216.7855682373047, 'l1_loss': 506.4610900878906, 'l2_loss': 196.52516174316406}
Step 2400: {'loss': 216.50567626953125, 'l1_loss': 501.85345458984375, 'l2_loss': 196.42837524414062}
Step 2500: {'loss': 214.1877899169922, 'l1_loss': 514.0272216796875, 'l2_loss': 193.6214599609375}
Step 2600: {'loss': 214.71258544921875, 'l1_loss': 543.7763671875, 'l2_loss': 192.95497131347656}
Step 2700: {'loss': 214.1226806640625, 'l1_loss': 516.1276245117188, 'l2_loss': 193.4702606201172}
Step 2800: {'loss': 212.82733154296875, 'l1_loss': 494.43560791015625, 'l2_loss': 193.04428100585938}
Step 2900: {'loss': 210.61778259277344, 'l1_loss': 477.91864013671875, 'l2_loss': 191.49636840820312}
Step 3000: {'loss': 212.4323272705078, 'l1_loss': 521.8007202148438, 'l2_loss': 191.5524139404297}
Step 3100: {'loss': 210.40892028808594, 'l1_loss': 511.73516845703125, 'l2_loss': 189.93331909179688}
Step 3200: {'loss': 209.85870361328125, 'l1_loss': 488.31793212890625, 'l2_loss': 190.313720703125}
Step 3300: {'loss': 208.7799530029297, 'l1_loss': 488.6858825683594, 'l2_loss': 189.2191619873047}
Step 3400: {'loss': 209.6642303466797, 'l1_loss': 516.4910888671875, 'l2_loss': 188.9928741455078}
Step 3500: {'loss': 206.04978942871094, 'l1_loss': 479.3744201660156, 'l2_loss': 186.85943603515625}
Step 3600: {'loss': 208.45770263671875, 'l1_loss': 507.98876953125, 'l2_loss': 188.1217498779297}
Step 3700: {'loss': 207.54762268066406, 'l1_loss': 491.6065979003906, 'l2_loss': 187.8673095703125}
Step 3800: {'loss': 205.5189208984375, 'l1_loss': 504.18402099609375, 'l2_loss': 185.3186492919922}
Step 3900: {'loss': 205.87521362304688, 'l1_loss': 496.51141357421875, 'l2_loss': 185.994873046875}
Step 4000: {'loss': 205.05616760253906, 'l1_loss': 480.17755126953125, 'l2_loss': 185.82357788085938}
Step 4100: {'loss': 204.86207580566406, 'l1_loss': 482.163330078125, 'l2_loss': 185.54852294921875}
Step 4200: {'loss': 204.5190887451172, 'l1_loss': 507.74749755859375, 'l2_loss': 184.1710205078125}
Step 4300: {'loss': 204.6338653564453, 'l1_loss': 502.6726989746094, 'l2_loss': 184.48207092285156}
Step 4400: {'loss': 202.78060913085938, 'l1_loss': 492.9920654296875, 'l2_loss': 183.00901794433594}
Step 4500: {'loss': 203.4093780517578, 'l1_loss': 502.8629455566406, 'l2_loss': 183.23794555664062}
Step 4600: {'loss': 202.72190856933594, 'l1_loss': 502.0948791503906, 'l2_loss': 182.58984375}
Step 4700: {'loss': 201.69923400878906, 'l1_loss': 521.1862182617188, 'l2_loss': 180.81005859375}
Step 4800: {'loss': 201.737548828125, 'l1_loss': 508.1014404296875, 'l2_loss': 181.3701629638672}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131

Step 2600: {'loss': 219.04002380371094, 'l1_loss': 430.4140625, 'l2_loss': 201.82345581054688}
Step 2700: {'loss': 218.8960723876953, 'l1_loss': 409.3546447753906, 'l2_loss': 202.52188110351562}
Step 2800: {'loss': 218.8064727783203, 'l1_loss': 392.6961669921875, 'l2_loss': 203.0986328125}
Step 2900: {'loss': 217.0283966064453, 'l1_loss': 379.1009826660156, 'l2_loss': 201.86436462402344}
Step 3000: {'loss': 218.28489685058594, 'l1_loss': 409.16558837890625, 'l2_loss': 201.91827392578125}
Step 3100: {'loss': 217.038330078125, 'l1_loss': 399.659912109375, 'l2_loss': 201.05194091796875}
Step 3200: {'loss': 216.92906188964844, 'l1_loss': 381.6346435546875, 'l2_loss': 201.66368103027344}
Step 3300: {'loss': 216.3424072265625, 'l1_loss': 381.7891845703125, 'l2_loss': 201.0708465576172}
Step 3400: {'loss': 216.77874755859375, 'l1_loss': 400.76947021484375, 'l2_loss': 200.7479705810547}
Step 3500: {'loss': 214.1649169921875, 'l1_loss': 374.33868408203125, 'l2_loss': 199.19137573242188}
Step 3600: {'loss': 216.15731811523438, 'l1_loss': 395.29302978515625, 'l2_loss': 200.34559631347656}
Step 3700: {'loss': 215.07362365722656, 'l1_loss': 385.0608215332031, 'l2_loss': 199.6711883544922}
Step 3800: {'loss': 213.69363403320312, 'l1_loss': 394.29388427734375, 'l2_loss': 197.921875}
Step 3900: {'loss': 214.09716796875, 'l1_loss': 387.4491271972656, 'l2_loss': 198.5991973876953}
Step 4000: {'loss': 213.3656005859375, 'l1_loss': 374.77264404296875, 'l2_loss': 198.37469482421875}
Step 4100: {'loss': 213.68771362304688, 'l1_loss': 376.6044616699219, 'l2_loss': 198.62353515625}
Step 4200: {'loss': 213.28414916992188, 'l1_loss': 398.1248779296875, 'l2_loss': 197.35916137695312}
Step 4300: {'loss': 213.5869903564453, 'l1_loss': 392.98779296875, 'l2_loss': 197.8674774169922}
Step 4400: {'loss': 212.246826171875, 'l1_loss': 385.1096496582031, 'l2_loss': 196.84243774414062}
Step 4500: {'loss': 212.71621704101562, 'l1_loss': 393.9525146484375, 'l2_loss': 196.95811462402344}
Step 4600: {'loss': 212.27963256835938, 'l1_loss': 391.9889221191406, 'l2_loss': 196.60008239746094}
Step 4700: {'loss': 211.42340087890625, 'l1_loss': 409.91400146484375, 'l2_loss': 195.02684020996094}
Step 4800: {'loss': 211.74571228027344, 'l1_loss': 399.1106872558594, 'l2_loss': 195.78128051757812}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_141038_step_progressive_sae/run_10/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_10/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8870000243186951, scr_score: -0.16417972854076204
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8820000290870667, scr_score: -0.23880655437858572
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8770000338554382, scr_score: -0.31343338021640943
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8810000419616699, scr_score: -0.253731741621763
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8660000562667847, scr_score: -0.4776122191352341
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6100000143051147, scr_score: 0.03535352927204973
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6130000352859497, scr_score: 0.042929339300620824
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6130000352859497, scr_score: 0.042929339300620824
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6160000562667847, scr_score: 0.050505149329191916
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6270000338554382, scr_score: 0.07828286857267056
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6340000033378601, scr_score: 0.0959595579503108
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.628000020980835, scr_score: 0.08080808840993783
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8490000367164612, scr_score: 0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8390000462532043, scr_score: -0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8390000462532043, scr_score: -0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8500000238418579, scr_score: 0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8410000205039978, scr_score: -0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8440000414848328, scr_score: -0.008928457389060843
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6530000567436218, scr_score: 0.01749283093518804
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6570000052452087, scr_score: 0.0291544865260507
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6500000357627869, scr_score: 0.00874641546759402
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6510000228881836, scr_score: 0.011661829365309686
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6550000309944153, scr_score: 0.02332365873061937
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6480000019073486, scr_score: 0.002915413897715665
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6480000019073486, scr_score: 0.002915413897715665
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9200000166893005, scr_score: 0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.192307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9100000262260437, scr_score: -0.17307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9080000519752502, scr_score: -0.21153784433114478
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5830000042915344, scr_score: 0.014598496744803374
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5850000381469727, scr_score: 0.019464759008712428
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5900000333786011, scr_score: 0.03163019713329222
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5860000252723694, scr_score: 0.02189781762893601
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8350000381469727, scr_score: 0.0
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8390000462532043, scr_score: 0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8350000381469727, scr_score: 0.0
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8340000510215759, scr_score: -0.007692208939046454
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6650000214576721, scr_score: 0.0
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6520000100135803, scr_score: -0.03939396894995775
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6600000262260437, scr_score: -0.015151498731504951
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6710000038146973, scr_score: 0.018181762353783503
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6760000586509705, scr_score: 0.03333344170540065
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6640000343322754, scr_score: -0.0030302636222785517
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6660000085830688, scr_score: 0.0030302636222785517
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8360000252723694, scr_score: 0.10982652984934499
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8420000672340393, scr_score: 0.14450884577525472
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8380000591278076, scr_score: 0.1213874166697962
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8220000267028809, scr_score: 0.02890170024796216
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.831000030040741, scr_score: 0.08092482960138282
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8300000429153442, scr_score: 0.07514455845887918
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8300000429153442, scr_score: 0.07514455845887918
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6820000410079956, scr_score: 0.003921518212141116
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6840000152587891, scr_score: 0.011764554636423348
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6860000491142273, scr_score: 0.01960782480441495
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6830000281333923, scr_score: 0.007843036424282232
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6820000410079956, scr_score: 0.003921518212141116
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6790000200271606, scr_score: -0.007843270167991604
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6670000553131104, scr_score: -0.05490195620110374
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7990000247955322, scr_score: 0.02150541424736498
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8020000457763672, scr_score: 0.037634555046637416
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8100000619888306, scr_score: 0.08064538354136737
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7990000247955322, scr_score: 0.02150541424736498
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7880000472068787, scr_score: -0.0376342345916426
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7990000247955322, scr_score: 0.02150541424736498
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.03571429416165487
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.03571429416165487
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.718000054359436, scr_score: 0.05952398128698237
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.703000009059906, scr_score: 0.0
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7020000219345093, scr_score: -0.003968202345442456
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: -0.03968249650709733
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.6970000267028809, scr_score: -0.02380945059899112
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8550000190734863, scr_score: 0.06666666666666667
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8480000495910645, scr_score: 0.014815060101327859
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8420000672340393, scr_score: -0.029629237171208758
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8330000638961792, scr_score: -0.09629590383787542
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8350000381469727, scr_score: -0.08148128525227105
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: -0.1481479519189377
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8210000395774841, scr_score: -0.18518493989867213
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6600000262260437, scr_score: 0.026229560737229315
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6630000472068787, scr_score: 0.036065694869955374
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6740000247955322, scr_score: 0.07213119431485049
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6620000600814819, scr_score: 0.03278704863406677
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6620000600814819, scr_score: 0.03278704863406677
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6670000553131104, scr_score: 0.049180475238570026
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6730000376701355, scr_score: 0.06885254807896189
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.784000039100647, scr_score: 0.05405400180747055
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7830000519752502, scr_score: 0.048648666064176484
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7830000519752502, scr_score: 0.048648666064176484
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7800000309944153, scr_score: 0.03243233664702935
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.737000048160553, scr_score: 0.054621888725657895
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7380000352859497, scr_score: 0.058823514680020776
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7330000400543213, scr_score: 0.03781513446855956
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7190000414848328, scr_score: -0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7190000414848328, scr_score: -0.02100838021146122
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 622.8250732421875, 'l1_loss': 6244.1435546875, 'l2_loss': 355.8350830078125}
Step 200: {'loss': 442.5502014160156, 'l1_loss': 2759.38623046875, 'l2_loss': 318.8096618652344}
Step 300: {'loss': 364.8193054199219, 'l1_loss': 1213.5875244140625, 'l2_loss': 309.97314453125}
Step 400: {'loss': 335.6042175292969, 'l1_loss': 801.322265625, 'l2_loss': 301.65618896484375}
Step 500: {'loss': 307.5481872558594, 'l1_loss': 871.8662109375, 'l2_loss': 272.48095703125}
Step 600: {'loss': 285.3959655761719, 'l1_loss': 901.422119140625, 'l2_loss': 249.3350830078125}
Step 700: {'loss': 265.7890930175781, 'l1_loss': 889.9216918945312, 'l2_loss': 230.1918487548828}
Step 800: {'loss': 256.7901611328125, 'l1_loss': 872.9219360351562, 'l2_loss': 221.8731231689453}
Step 900: {'loss': 246.69029235839844, 'l1_loss': 710.0457153320312, 'l2_loss': 218.2882080078125}
Step 1000: {'loss': 245.0933380126953, 'l1_loss': 744.2099609375, 'l2_loss': 215.32470703125}
Step 1100: {'loss': 237.6471710205078, 'l1_loss': 613.95947265625, 'l2_loss': 213.0887451171875}
Step 1200: {'loss': 230.5098876953125, 'l1_loss': 508.29010009765625, 'l2_loss': 210.17825317382812}
Step 1300: {'loss': 232.17764282226562, 'l1_loss': 551.0537109375, 'l2_loss': 210.13522338867188}
Step 1400: {'loss': 228.55526733398438, 'l1_loss': 558.0118408203125, 'l2_loss': 206.23452758789062}
Step 1500: {'loss': 226.49708557128906, 'l1_loss': 508.35736083984375, 'l2_loss': 206.16241455078125}
Step 1600: {'loss': 225.8092803955078, 'l1_loss': 508.0850524902344, 'l2_loss': 205.4853057861328}
Step 1700: {'loss': 223.3820343017578, 'l1_loss': 487.4073486328125, 'l2_loss': 203.88514709472656}
Step 1800: {'loss': 223.19827270507812, 'l1_loss': 517.2998657226562, 'l2_loss': 202.50494384765625}
Step 1900: {'loss': 221.42230224609375, 'l1_loss': 501.87762451171875, 'l2_loss': 201.34695434570312}
Step 2000: {'loss': 220.36233520507812, 'l1_loss': 501.6819152832031, 'l2_loss': 200.2937774658203}
Step 2100: {'loss': 220.3744659423828, 'l1_loss': 486.5117492675781, 'l2_loss': 200.91261291503906}
Step 2200: {'loss': 216.24342346191406, 'l1_loss': 447.9612121582031, 'l2_loss': 198.32342529296875}
Step 2300: {'loss': 216.7855682373047, 'l1_loss': 506.4610900878906, 'l2_loss': 196.52516174316406}
Step 2400: {'loss': 216.50567626953125, 'l1_loss': 501.85345458984375, 'l2_loss': 196.42837524414062}
Step 2500: {'loss': 214.1877899169922, 'l1_loss': 514.0272216796875, 'l2_loss': 193.6214599609375}
Step 2600: {'loss': 214.71258544921875, 'l1_loss': 543.7763671875, 'l2_loss': 192.95497131347656}
Step 2700: {'loss': 214.1226806640625, 'l1_loss': 516.1276245117188, 'l2_loss': 193.4702606201172}
Step 2800: {'loss': 212.82733154296875, 'l1_loss': 494.43560791015625, 'l2_loss': 193.04428100585938}
Step 2900: {'loss': 210.61778259277344, 'l1_loss': 477.91864013671875, 'l2_loss': 191.49636840820312}
Step 3000: {'loss': 212.4323272705078, 'l1_loss': 521.8007202148438, 'l2_loss': 191.5524139404297}
Step 3100: {'loss': 210.40892028808594, 'l1_loss': 511.73516845703125, 'l2_loss': 189.93331909179688}
Step 3200: {'loss': 209.85870361328125, 'l1_loss': 488.31793212890625, 'l2_loss': 190.313720703125}
Step 3300: {'loss': 208.7799530029297, 'l1_loss': 488.6858825683594, 'l2_loss': 189.2191619873047}
Step 3400: {'loss': 209.6642303466797, 'l1_loss': 516.4910888671875, 'l2_loss': 188.9928741455078}
Step 3500: {'loss': 206.04978942871094, 'l1_loss': 479.3744201660156, 'l2_loss': 186.85943603515625}
Step 3600: {'loss': 208.45770263671875, 'l1_loss': 507.98876953125, 'l2_loss': 188.1217498779297}
Step 3700: {'loss': 207.54762268066406, 'l1_loss': 491.6065979003906, 'l2_loss': 187.8673095703125}
Step 3800: {'loss': 205.5189208984375, 'l1_loss': 504.18402099609375, 'l2_loss': 185.3186492919922}
Step 3900: {'loss': 205.87521362304688, 'l1_loss': 496.51141357421875, 'l2_loss': 185.994873046875}
Step 4000: {'loss': 205.05616760253906, 'l1_loss': 480.17755126953125, 'l2_loss': 185.82357788085938}
Step 4100: {'loss': 204.86207580566406, 'l1_loss': 482.163330078125, 'l2_loss': 185.54852294921875}
Step 4200: {'loss': 204.5190887451172, 'l1_loss': 507.74749755859375, 'l2_loss': 184.1710205078125}
Step 4300: {'loss': 204.6338653564453, 'l1_loss': 502.6726989746094, 'l2_loss': 184.48207092285156}
Step 4400: {'loss': 202.78060913085938, 'l1_loss': 492.9920654296875, 'l2_loss': 183.00901794433594}
Step 4500: {'loss': 203.4093780517578, 'l1_loss': 502.8629455566406, 'l2_loss': 183.23794555664062}
Step 4600: {'loss': 202.72190856933594, 'l1_loss': 502.0948791503906, 'l2_loss': 182.58984375}
Step 4700: {'loss': 201.69923400878906, 'l1_loss': 521.1862182617188, 'l2_loss': 180.81005859375}
Step 4800: {'loss': 201.737548828125, 'l1_loss': 508.1014404296875, 'l2_loss': 181.3701629638672}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 622.8250732421875, 'l1_loss': 6244.1435546875, 'l2_loss': 355.8350830078125}
Step 200: {'loss': 442.5502014160156, 'l1_loss': 2759.38623046875, 'l2_loss': 318.8096618652344}
Step 300: {'loss': 364.8193054199219, 'l1_loss': 1213.5875244140625, 'l2_loss': 309.97314453125}
Step 400: {'loss': 335.6042175292969, 'l1_loss': 801.322265625, 'l2_loss': 301.65618896484375}
Step 500: {'loss': 307.5481872558594, 'l1_loss': 871.8662109375, 'l2_loss': 272.48095703125}
Step 600: {'loss': 285.3959655761719, 'l1_loss': 901.422119140625, 'l2_loss': 249.3350830078125}
Step 700: {'loss': 265.7890930175781, 'l1_loss': 889.9216918945312, 'l2_loss': 230.1918487548828}
Step 800: {'loss': 256.7901611328125, 'l1_loss': 872.9219360351562, 'l2_loss': 221.8731231689453}
Step 900: {'loss': 246.69029235839844, 'l1_loss': 710.0457153320312, 'l2_loss': 218.2882080078125}
Step 1000: {'loss': 245.0933380126953, 'l1_loss': 744.2099609375, 'l2_loss': 215.32470703125}
Step 1100: {'loss': 237.6471710205078, 'l1_loss': 613.95947265625, 'l2_loss': 213.0887451171875}
Step 1200: {'loss': 230.5098876953125, 'l1_loss': 508.29010009765625, 'l2_loss': 210.17825317382812}
Step 1300: {'loss': 232.17764282226562, 'l1_loss': 551.0537109375, 'l2_loss': 210.13522338867188}
Step 1400: {'loss': 228.55526733398438, 'l1_loss': 558.0118408203125, 'l2_loss': 206.23452758789062}
Step 1500: {'loss': 226.49708557128906, 'l1_loss': 508.35736083984375, 'l2_loss': 206.16241455078125}
Step 1600: {'loss': 225.8092803955078, 'l1_loss': 508.0850524902344, 'l2_loss': 205.4853057861328}
Step 1700: {'loss': 223.3820343017578, 'l1_loss': 487.4073486328125, 'l2_loss': 203.88514709472656}
Step 1800: {'loss': 223.19827270507812, 'l1_loss': 517.2998657226562, 'l2_loss': 202.50494384765625}
Step 1900: {'loss': 221.42230224609375, 'l1_loss': 501.87762451171875, 'l2_loss': 201.34695434570312}
Step 2000: {'loss': 220.36233520507812, 'l1_loss': 501.6819152832031, 'l2_loss': 200.2937774658203}
Step 2100: {'loss': 220.3744659423828, 'l1_loss': 486.5117492675781, 'l2_loss': 200.91261291503906}
Step 2200: {'loss': 216.24342346191406, 'l1_loss': 447.9612121582031, 'l2_loss': 198.32342529296875}
Step 2300: {'loss': 216.7855682373047, 'l1_loss': 506.4610900878906, 'l2_loss': 196.52516174316406}
Step 2400: {'loss': 216.50567626953125, 'l1_loss': 501.85345458984375, 'l2_loss': 196.42837524414062}
Step 2500: {'loss': 214.1877899169922, 'l1_loss': 514.0272216796875, 'l2_loss': 193.6214599609375}
Step 2600: {'loss': 214.71258544921875, 'l1_loss': 543.7763671875, 'l2_loss': 192.95497131347656}
Step 2700: {'loss': 214.1226806640625, 'l1_loss': 516.1276245117188, 'l2_loss': 193.4702606201172}
Step 2800: {'loss': 212.82733154296875, 'l1_loss': 494.43560791015625, 'l2_loss': 193.04428100585938}
Step 2900: {'loss': 210.61778259277344, 'l1_loss': 477.91864013671875, 'l2_loss': 191.49636840820312}
Step 3000: {'loss': 212.4323272705078, 'l1_loss': 521.8007202148438, 'l2_loss': 191.5524139404297}
Step 3100: {'loss': 210.40892028808594, 'l1_loss': 511.73516845703125, 'l2_loss': 189.93331909179688}
Step 3200: {'loss': 209.85870361328125, 'l1_loss': 488.31793212890625, 'l2_loss': 190.313720703125}
Step 3300: {'loss': 208.7799530029297, 'l1_loss': 488.6858825683594, 'l2_loss': 189.2191619873047}
Step 3400: {'loss': 209.6642303466797, 'l1_loss': 516.4910888671875, 'l2_loss': 188.9928741455078}
Step 3500: {'loss': 206.04978942871094, 'l1_loss': 479.3744201660156, 'l2_loss': 186.85943603515625}
Step 3600: {'loss': 208.45770263671875, 'l1_loss': 507.98876953125, 'l2_loss': 188.1217498779297}
Step 3700: {'loss': 207.54762268066406, 'l1_loss': 491.6065979003906, 'l2_loss': 187.8673095703125}
Step 3800: {'loss': 205.5189208984375, 'l1_loss': 504.18402099609375, 'l2_loss': 185.3186492919922}
Step 3900: {'loss': 205.87521362304688, 'l1_loss': 496.51141357421875, 'l2_loss': 185.994873046875}
Step 4000: {'loss': 205.05616760253906, 'l1_loss': 480.17755126953125, 'l2_loss': 185.82357788085938}
Step 4100: {'loss': 204.86207580566406, 'l1_loss': 482.163330078125, 'l2_loss': 185.54852294921875}
Step 4200: {'loss': 204.5190887451172, 'l1_loss': 507.74749755859375, 'l2_loss': 184.1710205078125}
Step 4300: {'loss': 204.6338653564453, 'l1_loss': 502.6726989746094, 'l2_loss': 184.48207092285156}
Step 4400: {'loss': 202.78060913085938, 'l1_loss': 492.9920654296875, 'l2_loss': 183.00901794433594}
Step 4500: {'loss': 203.4093780517578, 'l1_loss': 502.8629455566406, 'l2_loss': 183.23794555664062}
Step 4600: {'loss': 202.72190856933594, 'l1_loss': 502.0948791503906, 'l2_loss': 182.58984375}
Step 4700: {'loss': 201.69923400878906, 'l1_loss': 521.1862182617188, 'l2_loss': 180.81005859375}
Step 4800: {'loss': 201.737548828125, 'l1_loss': 508.1014404296875, 'l2_loss': 181.3701629638672}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 622.8250732421875, 'l1_loss': 6244.1435546875, 'l2_loss': 355.8350830078125}
Step 200: {'loss': 442.5502014160156, 'l1_loss': 2759.38623046875, 'l2_loss': 318.8096618652344}
Step 300: {'loss': 364.8193054199219, 'l1_loss': 1213.5875244140625, 'l2_loss': 309.97314453125}
Step 400: {'loss': 335.6042175292969, 'l1_loss': 801.322265625, 'l2_loss': 301.65618896484375}
Step 500: {'loss': 307.5481872558594, 'l1_loss': 871.8662109375, 'l2_loss': 272.48095703125}
Step 600: {'loss': 285.3959655761719, 'l1_loss': 901.422119140625, 'l2_loss': 249.3350830078125}
Step 700: {'loss': 265.7890930175781, 'l1_loss': 889.9216918945312, 'l2_loss': 230.1918487548828}
Step 800: {'loss': 256.7901611328125, 'l1_loss': 872.9219360351562, 'l2_loss': 221.8731231689453}
Step 900: {'loss': 246.69029235839844, 'l1_loss': 710.0457153320312, 'l2_loss': 218.2882080078125}
Step 1000: {'loss': 245.0933380126953, 'l1_loss': 744.2099609375, 'l2_loss': 215.32470703125}
Step 1100: {'loss': 237.6471710205078, 'l1_loss': 613.95947265625, 'l2_loss': 213.0887451171875}
Step 1200: {'loss': 230.5098876953125, 'l1_loss': 508.29010009765625, 'l2_loss': 210.17825317382812}
Step 1300: {'loss': 232.17764282226562, 'l1_loss': 551.0537109375, 'l2_loss': 210.13522338867188}
Step 1400: {'loss': 228.55526733398438, 'l1_loss': 558.0118408203125, 'l2_loss': 206.23452758789062}
Step 1500: {'loss': 226.49708557128906, 'l1_loss': 508.35736083984375, 'l2_loss': 206.16241455078125}
Step 1600: {'loss': 225.8092803955078, 'l1_loss': 508.0850524902344, 'l2_loss': 205.4853057861328}
Step 1700: {'loss': 223.3820343017578, 'l1_loss': 487.4073486328125, 'l2_loss': 203.88514709472656}
Step 1800: {'loss': 223.19827270507812, 'l1_loss': 517.2998657226562, 'l2_loss': 202.50494384765625}
Step 1900: {'loss': 221.42230224609375, 'l1_loss': 501.87762451171875, 'l2_loss': 201.34695434570312}
Step 2000: {'loss': 220.36233520507812, 'l1_loss': 501.6819152832031, 'l2_loss': 200.2937774658203}
Step 2100: {'loss': 220.3744659423828, 'l1_loss': 486.5117492675781, 'l2_loss': 200.91261291503906}
Step 2200: {'loss': 216.24342346191406, 'l1_loss': 447.9612121582031, 'l2_loss': 198.32342529296875}
Step 2300: {'loss': 216.7855682373047, 'l1_loss': 506.4610900878906, 'l2_loss': 196.52516174316406}
Step 2400: {'loss': 216.50567626953125, 'l1_loss': 501.85345458984375, 'l2_loss': 196.42837524414062}
Step 2500: {'loss': 214.1877899169922, 'l1_loss': 514.0272216796875, 'l2_loss': 193.6214599609375}
Step 2600: {'loss': 214.71258544921875, 'l1_loss': 543.7763671875, 'l2_loss': 192.95497131347656}
Step 2700: {'loss': 214.1226806640625, 'l1_loss': 516.1276245117188, 'l2_loss': 193.4702606201172}
Step 2800: {'loss': 212.82733154296875, 'l1_loss': 494.43560791015625, 'l2_loss': 193.04428100585938}
Step 2900: {'loss': 210.61778259277344, 'l1_loss': 477.91864013671875, 'l2_loss': 191.49636840820312}
Step 3000: {'loss': 212.4323272705078, 'l1_loss': 521.8007202148438, 'l2_loss': 191.5524139404297}
Step 3100: {'loss': 210.40892028808594, 'l1_loss': 511.73516845703125, 'l2_loss': 189.93331909179688}
Step 3200: {'loss': 209.85870361328125, 'l1_loss': 488.31793212890625, 'l2_loss': 190.313720703125}
Step 3300: {'loss': 208.7799530029297, 'l1_loss': 488.6858825683594, 'l2_loss': 189.2191619873047}
Step 3400: {'loss': 209.6642303466797, 'l1_loss': 516.4910888671875, 'l2_loss': 188.9928741455078}
Step 3500: {'loss': 206.04978942871094, 'l1_loss': 479.3744201660156, 'l2_loss': 186.85943603515625}
Step 3600: {'loss': 208.45770263671875, 'l1_loss': 507.98876953125, 'l2_loss': 188.1217498779297}
Step 3700: {'loss': 207.54762268066406, 'l1_loss': 491.6065979003906, 'l2_loss': 187.8673095703125}
Step 3800: {'loss': 205.5189208984375, 'l1_loss': 504.18402099609375, 'l2_loss': 185.3186492919922}
Step 3900: {'loss': 205.87521362304688, 'l1_loss': 496.51141357421875, 'l2_loss': 185.994873046875}
Step 4000: {'loss': 205.05616760253906, 'l1_loss': 480.17755126953125, 'l2_loss': 185.82357788085938}
Step 4100: {'loss': 204.86207580566406, 'l1_loss': 482.163330078125, 'l2_loss': 185.54852294921875}
Step 4200: {'loss': 204.5190887451172, 'l1_loss': 507.74749755859375, 'l2_loss': 184.1710205078125}
Step 4300: {'loss': 204.6338653564453, 'l1_loss': 502.6726989746094, 'l2_loss': 184.48207092285156}
Step 4400: {'loss': 202.78060913085938, 'l1_loss': 492.9920654296875, 'l2_loss': 183.00901794433594}
Step 4500: {'loss': 203.4093780517578, 'l1_loss': 502.8629455566406, 'l2_loss': 183.23794555664062}
Step 4600: {'loss': 202.72190856933594, 'l1_loss': 502.0948791503906, 'l2_loss': 182.58984375}
Step 4700: {'loss': 201.69923400878906, 'l1_loss': 521.1862182617188, 'l2_loss': 180.81005859375}
Step 4800: {'loss': 201.737548828125, 'l1_loss': 508.1014404296875, 'l2_loss': 181.3701629638672}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8480000495910645, scr_score: -0.7462691480001744
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -1.074627715459761
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8100000619888306, scr_score: -1.3134333802164095
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8010000586509705, scr_score: -1.4477618446488796
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6080000400543213, scr_score: 0.03030308959751515
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.656000018119812, scr_score: 0.15151514695403728
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6270000338554382, scr_score: 0.07828286857267056
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6430000066757202, scr_score: 0.11868683751925485
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6850000619888306, scr_score: 0.22474757585217325
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6910000443458557, scr_score: 0.23989904539254622
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9570000171661377
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9940000176429749
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.843000054359436, scr_score: -0.017856914778121687
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8250000476837158, scr_score: -0.1785712765187478
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8170000314712524, scr_score: -0.25
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8220000267028809, scr_score: -0.20535718087031304
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8270000219345093, scr_score: -0.16071436174062612
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8020000457763672, scr_score: -0.38392845738906084
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6720000505447388, scr_score: 0.07288639008957377
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.690000057220459, scr_score: 0.12536453534624384
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6950000524520874, scr_score: 0.1399417786092692
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.7040000557899475, scr_score: 0.16618085123760423
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6970000267028809, scr_score: 0.14577260640470052
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.690000057220459, scr_score: 0.12536453534624384
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.690000057220459, scr_score: 0.12536453534624384
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.8076915141400433
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8870000243186951, scr_score: -0.6153841745222463
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8720000386238098, scr_score: -0.9038451839489418
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8670000433921814, scr_score: -0.9999988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8630000352859497, scr_score: -1.076922018853391
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8480000495910645, scr_score: -1.3653830282800865
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8210000395774841, scr_score: -1.8846135329934344
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6220000386238098, scr_score: 0.10948908814468004
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6310000419616699, scr_score: 0.13138690577361606
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6170000433921814, scr_score: 0.09732365002010025
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6190000176429749, scr_score: 0.10218976726054742
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6330000162124634, scr_score: 0.13625302301406322
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6390000581741333, scr_score: 0.15085166478232848
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6710000038146973, scr_score: 0.22871041077025442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9940000176429749
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9660000205039978
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8220000267028809, scr_score: -0.10000009169945687
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8200000524520874, scr_score: -0.11538450957754977
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8200000524520874, scr_score: -0.11538450957754977
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8270000219345093, scr_score: -0.06153858850694027
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8280000686645508, scr_score: -0.053845921070609495
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8370000123977661, scr_score: 0.015384417878092908
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7470000386238098, scr_score: -0.67692309808449
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6920000314712524, scr_score: 0.08181820152219406
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7080000042915344, scr_score: 0.13030296133898747
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7240000367164612, scr_score: 0.17878790177589307
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7440000176429749, scr_score: 0.23939389670191288
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7350000143051147, scr_score: 0.21212116286118152
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7460000514984131, scr_score: 0.24545460456658216
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7470000386238098, scr_score: 0.24848486818886073
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9360000491142273
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9340000152587891
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.831000030040741, scr_score: 0.08092482960138282
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8360000252723694, scr_score: 0.10982652984934499
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7570000290870667, scr_score: -0.34682074751098985
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7700000405311584, scr_score: -0.27167618905211066
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7870000600814819, scr_score: -0.17341020148777295
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7860000133514404, scr_score: -0.17919081716572052
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7880000472068787, scr_score: -0.1676299303452693
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7560000419616699, scr_score: 0.2941176058099336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7600000500679016, scr_score: 0.30980391240220745
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7610000371932983, scr_score: 0.3137254306143486
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7570000290870667, scr_score: 0.2980391240220748
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7630000114440918, scr_score: 0.32156846703863085
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7490000128746033, scr_score: 0.26666651083752707
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7010000348091125, scr_score: 0.0784312992176598
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9770000576972961
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9970000386238098
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8110000491142273, scr_score: 0.08602165698945992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8300000429153442, scr_score: 0.18817213432319746
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8220000267028809, scr_score: 0.14516130582846748
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.831000030040741, scr_score: 0.19354840777128998
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8290000557899475, scr_score: 0.1827958608751049
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7900000214576721, scr_score: -0.02688168769545752
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7260000109672546, scr_score: 0.0912698365768584
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7310000061988831, scr_score: 0.11111108483040708
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7690000534057617, scr_score: 0.2619049026942478
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7710000276565552, scr_score: 0.26984130738513273
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7360000610351562, scr_score: 0.1309525696102921
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.08333343188597349
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7080000042915344, scr_score: 0.019841248253548664
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9570000171661377
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9800000190734863
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8770000338554382, scr_score: 0.22962967868693224
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.89000004529953, scr_score: 0.32592602404053117
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8630000352859497, scr_score: 0.12592602404053113
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8550000190734863, scr_score: 0.06666666666666667
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8370000123977661, scr_score: -0.06666666666666667
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8180000185966492, scr_score: -0.20740730929280218
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7950000166893005, scr_score: -0.37777763060586994
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6610000133514404, scr_score: 0.029508206973117915
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6730000376701355, scr_score: 0.06885254807896189
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6810000538825989, scr_score: 0.0950821088161912
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6910000443458557, scr_score: 0.1278689620251977
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.675000011920929, scr_score: 0.07540984055073909
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7120000123977661, scr_score: 0.19672131467909934
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6970000267028809, scr_score: 0.1475410348655896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9620000720024109
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9620000720024109
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9580000638961792
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7830000519752502, scr_score: 0.048648666064176484
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7950000166893005, scr_score: 0.11351333935823518
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.8030000329017639, scr_score: 0.15675666967911758
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7870000600814819, scr_score: 0.07027033122461769
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7200000286102295, scr_score: -0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7170000076293945, scr_score: -0.029411882559833787
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7150000333786011, scr_score: -0.03781513446855956
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7080000042915344, scr_score: -0.06722701702839334
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 686.972900390625, 'l1_loss': 6243.33056640625, 'l2_loss': 356.2686767578125}
Step 200: {'loss': 480.45611572265625, 'l1_loss': 2753.01171875, 'l2_loss': 319.4789733886719}
Step 300: {'loss': 375.7550964355469, 'l1_loss': 1205.7596435546875, 'l2_loss': 310.51690673828125}
Step 400: {'loss': 337.02764892578125, 'l1_loss': 789.4542236328125, 'l2_loss': 302.805908203125}
Step 500: {'loss': 308.61865234375, 'l1_loss': 852.528076171875, 'l2_loss': 274.43023681640625}
Step 600: {'loss': 286.5560302734375, 'l1_loss': 885.6654052734375, 'l2_loss': 251.12901306152344}
Step 700: {'loss': 267.3284912109375, 'l1_loss': 873.4923706054688, 'l2_loss': 232.38876342773438}
Step 800: {'loss': 258.49346923828125, 'l1_loss': 864.1677856445312, 'l2_loss': 223.92662048339844}
Step 900: {'loss': 248.01390075683594, 'l1_loss': 703.0728759765625, 'l2_loss': 219.8909912109375}
Step 1000: {'loss': 246.63446044921875, 'l1_loss': 741.4940185546875, 'l2_loss': 216.9746856689453}
Step 1100: {'loss': 239.36431884765625, 'l1_loss': 607.6380615234375, 'l2_loss': 215.05877685546875}
Step 1200: {'loss': 232.10008239746094, 'l1_loss': 498.5189208984375, 'l2_loss': 212.15927124023438}
Step 1300: {'loss': 233.86453247070312, 'l1_loss': 541.3057861328125, 'l2_loss': 212.2122802734375}
Step 1400: {'loss': 230.42550659179688, 'l1_loss': 546.3812255859375, 'l2_loss': 208.5702362060547}
Step 1500: {'loss': 228.5347900390625, 'l1_loss': 494.5269775390625, 'l2_loss': 208.75369262695312}
Step 1600: {'loss': 227.81524658203125, 'l1_loss': 492.2726745605469, 'l2_loss': 208.1236572265625}
Step 1700: {'loss': 225.497802734375, 'l1_loss': 471.26226806640625, 'l2_loss': 206.6472625732422}
Step 1800: {'loss': 225.8040771484375, 'l1_loss': 501.0902099609375, 'l2_loss': 205.75994873046875}
Step 1900: {'loss': 223.90716552734375, 'l1_loss': 484.2774353027344, 'l2_loss': 204.53604125976562}
Step 2000: {'loss': 222.81454467773438, 'l1_loss': 483.9730529785156, 'l2_loss': 203.45559692382812}
Step 2100: {'loss': 223.1005859375, 'l1_loss': 466.84100341796875, 'l2_loss': 204.42684936523438}
Step 2200: {'loss': 218.96334838867188, 'l1_loss': 428.1998291015625, 'l2_loss': 201.83523559570312}
Step 2300: {'loss': 219.71267700195312, 'l1_loss': 484.53314208984375, 'l2_loss': 200.33126831054688}
Step 2400: {'loss': 219.9736328125, 'l1_loss': 476.13970947265625, 'l2_loss': 200.92776489257812}
Step 2500: {'loss': 217.37254333496094, 'l1_loss': 485.4300842285156, 'l2_loss': 197.95510864257812}
Step 2600: {'loss': 217.58924865722656, 'l1_loss': 512.210205078125, 'l2_loss': 197.09588623046875}
Step 2700: {'loss': 216.94290161132812, 'l1_loss': 484.91436767578125, 'l2_loss': 197.54495239257812}
Step 2800: {'loss': 215.9457244873047, 'l1_loss': 463.954345703125, 'l2_loss': 197.37893676757812}
Step 2900: {'loss': 213.77548217773438, 'l1_loss': 448.75152587890625, 'l2_loss': 195.8232421875}
Step 3000: {'loss': 215.39047241210938, 'l1_loss': 490.0321044921875, 'l2_loss': 195.78817749023438}
Step 3100: {'loss': 213.22315979003906, 'l1_loss': 481.21923828125, 'l2_loss': 193.96694946289062}
Step 3200: {'loss': 212.59695434570312, 'l1_loss': 459.1700134277344, 'l2_loss': 194.21701049804688}
Step 3300: {'loss': 211.71023559570312, 'l1_loss': 461.03094482421875, 'l2_loss': 193.26673889160156}
Step 3400: {'loss': 212.73257446289062, 'l1_loss': 487.052001953125, 'l2_loss': 193.2469482421875}
Step 3500: {'loss': 208.9605712890625, 'l1_loss': 452.00604248046875, 'l2_loss': 190.87806701660156}
Step 3600: {'loss': 211.39443969726562, 'l1_loss': 479.54290771484375, 'l2_loss': 192.20541381835938}
Step 3700: {'loss': 210.30584716796875, 'l1_loss': 463.7123718261719, 'l2_loss': 191.75045776367188}
Step 3800: {'loss': 208.66778564453125, 'l1_loss': 474.62615966796875, 'l2_loss': 189.6439666748047}
Step 3900: {'loss': 208.77610778808594, 'l1_loss': 467.44232177734375, 'l2_loss': 190.05210876464844}
Step 4000: {'loss': 207.9316864013672, 'l1_loss': 451.63006591796875, 'l2_loss': 189.8304443359375}
Step 4100: {'loss': 207.9199676513672, 'l1_loss': 453.9993591308594, 'l2_loss': 189.72055053710938}
Step 4200: {'loss': 207.64193725585938, 'l1_loss': 478.8993225097656, 'l2_loss': 188.47146606445312}
Step 4300: {'loss': 207.30934143066406, 'l1_loss': 473.9902038574219, 'l2_loss': 188.3412628173828}
Step 4400: {'loss': 205.7394256591797, 'l1_loss': 462.638427734375, 'l2_loss': 187.22509765625}
Step 4500: {'loss': 206.18618774414062, 'l1_loss': 471.74237060546875, 'l2_loss': 187.3050537109375}
Step 4600: {'loss': 205.5085906982422, 'l1_loss': 470.5675048828125, 'l2_loss': 186.65283203125}
Step 4700: {'loss': 204.86976623535156, 'l1_loss': 489.2868347167969, 'l2_loss': 185.26416015625}
Step 4800: {'loss': 204.6875457763672, 'l1_loss': 476.705322265625, 'l2_loss': 185.59915161132812}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_2/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_2/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8560000658035278, scr_score: -0.6268658708108815
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8210000395774841, scr_score: -1.1492545412975848
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8050000667572021, scr_score: -1.388060206054233
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7990000247955322, scr_score: -1.4776131087571716
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6390000581741333, scr_score: 0.1085859581701857
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6460000276565552, scr_score: 0.12626264754782596
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.675000011920929, scr_score: 0.19949492592919268
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6520000100135803, scr_score: 0.14141411708819893
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6950000524520874, scr_score: 0.25000007525838464
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6990000605583191, scr_score: 0.260101105124223
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6950000524520874, scr_score: 0.25000007525838464
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8510000109672546, scr_score: 0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8390000462532043, scr_score: -0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8270000219345093, scr_score: -0.16071436174062612
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8410000205039978, scr_score: -0.0357143617406261
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8230000138282776, scr_score: -0.1964287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8200000524520874, scr_score: -0.22321409564843475
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6610000133514404, scr_score: 0.04081631589136039
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6620000600814819, scr_score: 0.04373190356352308
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6760000586509705, scr_score: 0.08454821945488346
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08746363335259913
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08746363335259913
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06997080241741109
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6660000085830688, scr_score: 0.055393559154385735
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8970000147819519, scr_score: -0.42307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.893000066280365, scr_score: -0.4999988537578403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.3076915141400433
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9010000228881836, scr_score: -0.3461536698088985
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.23076834904449253
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.192307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8980000615119934, scr_score: -0.4038451839489418
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5920000076293945, scr_score: 0.03649631437373938
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6020000576972961, scr_score: 0.06082733564636087
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.597000002861023, scr_score: 0.04866175249831918
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5960000157356262, scr_score: 0.0462286938780956
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5980000495910645, scr_score: 0.051094956142004655
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8250000476837158, scr_score: -0.07692300638503319
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8250000476837158, scr_score: -0.07692300638503319
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -0.09230788276041041
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8200000524520874, scr_score: -0.11538450957754977
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8160000443458557, scr_score: -0.14615380383101992
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7970000505447388, scr_score: -0.29230760766203984
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6930000185966492, scr_score: 0.08484846514447261
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7080000042915344, scr_score: 0.13030296133898747
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7230000495910645, scr_score: 0.1757576381536145
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.718000054359436, scr_score: 0.16060613942210955
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7090000510215759, scr_score: 0.1333334055813782
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7050000429153442, scr_score: 0.12121217047215181
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7010000348091125, scr_score: 0.10909093536292541
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8340000510215759, scr_score: 0.09826598756433769
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.831000030040741, scr_score: 0.08092482960138282
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7660000324249268, scr_score: -0.29479761815756916
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7750000357627869, scr_score: -0.24277448880414848
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7760000228881836, scr_score: -0.23699421766164483
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7740000486373901, scr_score: -0.24855475994665213
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7970000505447388, scr_score: -0.11560680099184864
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7490000128746033, scr_score: 0.26666651083752707
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7520000338554382, scr_score: 0.2784312992176598
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7500000596046448, scr_score: 0.27058826279337755
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7630000114440918, scr_score: 0.32156846703863085
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7480000257492065, scr_score: 0.262744992625386
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7200000286102295, scr_score: 0.15294108022317848
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6950000524520874, scr_score: 0.05490195620110374
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8070000410079956, scr_score: 0.06451624274209494
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8200000524520874, scr_score: 0.1344087589322824
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: 0.16129044662773992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: 0.16129044662773992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8180000185966492, scr_score: 0.12365589158110252
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: 0.03225828159854488
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8030000329017639, scr_score: 0.04301082849472996
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7130000591278076, scr_score: 0.039682733033433705
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: 0.07142858832330974
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7360000610351562, scr_score: 0.1309525696102921
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7200000286102295, scr_score: 0.06746038597786728
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7390000224113464, scr_score: 0.14285717664661948
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7150000333786011, scr_score: 0.04761913772431862
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.6960000395774841, scr_score: -0.027777652944433576
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8770000338554382, scr_score: 0.22962967868693224
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8440000414848328, scr_score: -0.014814618585604379
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8480000495910645, scr_score: 0.014815060101327859
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8690000176429749, scr_score: 0.17037032131306776
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8170000314712524, scr_score: -0.21481461858560438
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7910000085830688, scr_score: -0.40740730929280217
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7970000505447388, scr_score: -0.3629625705045421
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6640000343322754, scr_score: 0.03934434110584397
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6700000166893005, scr_score: 0.05901641394623583
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6700000166893005, scr_score: 0.05901641394623583
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7110000252723694, scr_score: 0.19344266844321076
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6850000619888306, scr_score: 0.10819688918480586
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6930000185966492, scr_score: 0.13442625449697493
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7990000247955322, scr_score: 0.13513500451867638
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7990000247955322, scr_score: 0.13513500451867638
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7910000085830688, scr_score: 0.09189167419779398
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7910000085830688, scr_score: 0.09189167419779398
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7270000576972961, scr_score: 0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7350000143051147, scr_score: 0.04621838637728532
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7150000333786011, scr_score: -0.03781513446855956
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7050000429153442, scr_score: -0.079831894891482
Step 0: {'loss': 592.9337768554688, 'l1_loss': 807.9572143554688, 'l2_loss': 560.615478515625}
Step 100: {'loss': 374.4619140625, 'l1_loss': 665.8123168945312, 'l2_loss': 347.8294372558594}
Step 200: {'loss': 304.9625244140625, 'l1_loss': 516.9395751953125, 'l2_loss': 284.2849426269531}
Step 300: {'loss': 256.4898681640625, 'l1_loss': 382.99517822265625, 'l2_loss': 241.17007446289062}
Step 400: {'loss': 223.1628875732422, 'l1_loss': 323.2347412109375, 'l2_loss': 210.23350524902344}
Step 500: {'loss': 205.97555541992188, 'l1_loss': 301.7506103515625, 'l2_loss': 193.90553283691406}
Step 600: {'loss': 194.38746643066406, 'l1_loss': 301.8435363769531, 'l2_loss': 182.313720703125}
Step 700: {'loss': 180.69464111328125, 'l1_loss': 309.5435485839844, 'l2_loss': 168.31289672851562}
Step 800: {'loss': 171.84622192382812, 'l1_loss': 335.2774353027344, 'l2_loss': 158.43511962890625}
Step 900: {'loss': 161.63504028320312, 'l1_loss': 350.53997802734375, 'l2_loss': 147.61343383789062}
Step 1000: {'loss': 154.15643310546875, 'l1_loss': 403.3084716796875, 'l2_loss': 138.0240936279297}
Step 1100: {'loss': 144.7806854248047, 'l1_loss': 417.5899353027344, 'l2_loss': 128.07708740234375}
Step 1200: {'loss': 136.33778381347656, 'l1_loss': 426.4534912109375, 'l2_loss': 119.27964782714844}
Step 1300: {'loss': 133.56362915039062, 'l1_loss': 462.2497863769531, 'l2_loss': 115.07363891601562}
Step 1400: {'loss': 126.90682983398438, 'l1_loss': 484.0710144042969, 'l2_loss': 107.54399108886719}
Step 1500: {'loss': 124.0749282836914, 'l1_loss': 493.671630859375, 'l2_loss': 104.32806396484375}
Step 1600: {'loss': 120.36831665039062, 'l1_loss': 513.9019775390625, 'l2_loss': 99.81224060058594}
Step 1700: {'loss': 117.1510009765625, 'l1_loss': 522.7591552734375, 'l2_loss': 96.24063110351562}
Step 1800: {'loss': 114.37433624267578, 'l1_loss': 543.4942016601562, 'l2_loss': 92.63456726074219}
Step 1900: {'loss': 112.08549499511719, 'l1_loss': 551.2261352539062, 'l2_loss': 90.03645324707031}
Step 2000: {'loss': 109.9367446899414, 'l1_loss': 560.8278198242188, 'l2_loss': 87.50363159179688}
Step 2100: {'loss': 108.52883911132812, 'l1_loss': 569.1994018554688, 'l2_loss': 85.7608642578125}
Step 2200: {'loss': 106.0535659790039, 'l1_loss': 567.7522583007812, 'l2_loss': 83.34347534179688}
Step 2300: {'loss': 104.4866714477539, 'l1_loss': 583.5927734375, 'l2_loss': 81.14295959472656}
Step 2400: {'loss': 103.5901107788086, 'l1_loss': 592.328857421875, 'l2_loss': 79.89695739746094}
Step 2500: {'loss': 101.84353637695312, 'l1_loss': 597.6416625976562, 'l2_loss': 77.9378662109375}
Step 2600: {'loss': 100.74706268310547, 'l1_loss': 608.037353515625, 'l2_loss': 76.42556762695312}
Step 2700: {'loss': 99.78997039794922, 'l1_loss': 611.4707641601562, 'l2_loss': 75.33113861083984}
Step 2800: {'loss': 99.308349609375, 'l1_loss': 614.84033203125, 'l2_loss': 74.71473693847656}
Step 2900: {'loss': 97.86936950683594, 'l1_loss': 615.6104125976562, 'l2_loss': 73.24494934082031}
Step 3000: {'loss': 97.92668914794922, 'l1_loss': 627.665283203125, 'l2_loss': 72.82007598876953}
Step 3100: {'loss': 96.21394348144531, 'l1_loss': 631.5369873046875, 'l2_loss': 70.95246887207031}
Step 3200: {'loss': 96.36175537109375, 'l1_loss': 634.9320068359375, 'l2_loss': 70.9644775390625}
Step 3300: {'loss': 94.9679183959961, 'l1_loss': 642.2694091796875, 'l2_loss': 69.27714538574219}
Step 3400: {'loss': 94.51664733886719, 'l1_loss': 652.4022216796875, 'l2_loss': 68.42056274414062}
Step 3500: {'loss': 92.7206039428711, 'l1_loss': 649.5695190429688, 'l2_loss': 66.73782348632812}
Step 3600: {'loss': 93.31874084472656, 'l1_loss': 661.3385009765625, 'l2_loss': 66.86520385742188}
Step 3700: {'loss': 92.74652099609375, 'l1_loss': 666.5457763671875, 'l2_loss': 66.0846939086914}
Step 3800: {'loss': 91.4326171875, 'l1_loss': 668.0802001953125, 'l2_loss': 64.70941162109375}
Step 3900: {'loss': 91.61221313476562, 'l1_loss': 673.855712890625, 'l2_loss': 64.65798950195312}
Step 4000: {'loss': 90.9683837890625, 'l1_loss': 678.1351318359375, 'l2_loss': 63.84298324584961}
Step 4100: {'loss': 90.1524429321289, 'l1_loss': 686.828857421875, 'l2_loss': 62.67928695678711}
Step 4200: {'loss': 89.23329162597656, 'l1_loss': 697.0214233398438, 'l2_loss': 61.35243225097656}
Step 4300: {'loss': 89.19548034667969, 'l1_loss': 701.792236328125, 'l2_loss': 61.12379455566406}
Step 4400: {'loss': 87.81129455566406, 'l1_loss': 705.0614624023438, 'l2_loss': 59.60884094238281}
Step 4500: {'loss': 87.59038543701172, 'l1_loss': 711.2454833984375, 'l2_loss': 59.14056396484375}
Step 4600: {'loss': 87.23847961425781, 'l1_loss': 715.157470703125, 'l2_loss': 58.63218307495117}
Step 4700: {'loss': 86.8974609375, 'l1_loss': 718.7430419921875, 'l2_loss': 58.147743225097656}
Step 4800: {'loss': 86.28060913085938, 'l1_loss': 721.8001708984375, 'l2_loss': 57.408607482910156}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_164702_grouped_sparse_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 648.958984375, 'l1_loss': 6243.86181640625, 'l2_loss': 356.0167541503906}
Step 200: {'loss': 453.7168273925781, 'l1_loss': 2756.275146484375, 'l2_loss': 319.1379089355469}
Step 300: {'loss': 363.69354248046875, 'l1_loss': 1209.8046875, 'l2_loss': 310.2304382324219}
Step 400: {'loss': 334.2003173828125, 'l1_loss': 795.7052612304688, 'l2_loss': 302.216552734375}
Step 500: {'loss': 307.9451904296875, 'l1_loss': 859.9107666015625, 'l2_loss': 273.5482482910156}
Step 600: {'loss': 286.13031005859375, 'l1_loss': 890.7115478515625, 'l2_loss': 250.5}
Step 700: {'loss': 266.76043701171875, 'l1_loss': 880.4578857421875, 'l2_loss': 231.54190063476562}
Step 800: {'loss': 257.72796630859375, 'l1_loss': 870.6801147460938, 'l2_loss': 222.90077209472656}
Step 900: {'loss': 247.15365600585938, 'l1_loss': 708.4495849609375, 'l2_loss': 218.81565856933594}
Step 1000: {'loss': 245.67190551757812, 'l1_loss': 746.6568603515625, 'l2_loss': 215.80563354492188}
Step 1100: {'loss': 238.09988403320312, 'l1_loss': 612.4073486328125, 'l2_loss': 213.6035919189453}
Step 1200: {'loss': 231.05148315429688, 'l1_loss': 503.6303405761719, 'l2_loss': 210.90625}
Step 1300: {'loss': 232.7899627685547, 'l1_loss': 546.4017333984375, 'l2_loss': 210.9338836669922}
Step 1400: {'loss': 229.11801147460938, 'l1_loss': 551.0086669921875, 'l2_loss': 207.07765197753906}
Step 1500: {'loss': 227.1787567138672, 'l1_loss': 501.02618408203125, 'l2_loss': 207.13717651367188}
Step 1600: {'loss': 226.57272338867188, 'l1_loss': 499.3513488769531, 'l2_loss': 206.59857177734375}
Step 1700: {'loss': 224.2220916748047, 'l1_loss': 477.5545654296875, 'l2_loss': 205.119873046875}
Step 1800: {'loss': 224.43898010253906, 'l1_loss': 507.4410705566406, 'l2_loss': 204.14129638671875}
Step 1900: {'loss': 222.61639404296875, 'l1_loss': 490.9642639160156, 'l2_loss': 202.97772216796875}
Step 2000: {'loss': 221.33294677734375, 'l1_loss': 492.1885986328125, 'l2_loss': 201.6448516845703}
Step 2100: {'loss': 221.30996704101562, 'l1_loss': 477.4259033203125, 'l2_loss': 202.21279907226562}
Step 2200: {'loss': 217.40452575683594, 'l1_loss': 438.2254943847656, 'l2_loss': 199.87109375}
Step 2300: {'loss': 217.98556518554688, 'l1_loss': 495.1167907714844, 'l2_loss': 198.18060302734375}
Step 2400: {'loss': 218.1540985107422, 'l1_loss': 487.3978271484375, 'l2_loss': 198.6580810546875}
Step 2500: {'loss': 215.5335693359375, 'l1_loss': 497.6669921875, 'l2_loss': 195.62539672851562}
Step 2600: {'loss': 215.85507202148438, 'l1_loss': 525.4777221679688, 'l2_loss': 194.83364868164062}
Step 2700: {'loss': 215.48333740234375, 'l1_loss': 497.8242492675781, 'l2_loss': 195.56982421875}
Step 2800: {'loss': 214.41969299316406, 'l1_loss': 476.92242431640625, 'l2_loss': 195.3422088623047}
Step 2900: {'loss': 212.29049682617188, 'l1_loss': 461.1178283691406, 'l2_loss': 193.8385772705078}
Step 3000: {'loss': 213.92019653320312, 'l1_loss': 502.6482238769531, 'l2_loss': 193.81300354003906}
Step 3100: {'loss': 211.6779022216797, 'l1_loss': 494.45489501953125, 'l2_loss': 191.89859008789062}
Step 3200: {'loss': 211.26507568359375, 'l1_loss': 471.9258728027344, 'l2_loss': 192.35464477539062}
Step 3300: {'loss': 210.16738891601562, 'l1_loss': 473.89385986328125, 'l2_loss': 191.18777465820312}
Step 3400: {'loss': 211.25726318359375, 'l1_loss': 499.98980712890625, 'l2_loss': 191.25241088867188}
Step 3500: {'loss': 207.55975341796875, 'l1_loss': 465.94586181640625, 'l2_loss': 188.9154052734375}
Step 3600: {'loss': 209.95509338378906, 'l1_loss': 493.022705078125, 'l2_loss': 190.2207794189453}
Step 3700: {'loss': 208.7952880859375, 'l1_loss': 478.1637268066406, 'l2_loss': 189.66366577148438}
Step 3800: {'loss': 206.9347381591797, 'l1_loss': 489.4975891113281, 'l2_loss': 187.34942626953125}
Step 3900: {'loss': 207.21080017089844, 'l1_loss': 481.2608337402344, 'l2_loss': 187.93434143066406}
Step 4000: {'loss': 206.3347625732422, 'l1_loss': 465.0615234375, 'l2_loss': 187.71987915039062}
Step 4100: {'loss': 206.52133178710938, 'l1_loss': 465.3670349121094, 'l2_loss': 187.89312744140625}
Step 4200: {'loss': 206.1288604736328, 'l1_loss': 490.6705322265625, 'l2_loss': 186.49627685546875}
Step 4300: {'loss': 206.0915985107422, 'l1_loss': 485.5190124511719, 'l2_loss': 186.6199951171875}
Step 4400: {'loss': 204.33413696289062, 'l1_loss': 475.447509765625, 'l2_loss': 185.302001953125}
Step 4500: {'loss': 204.77224731445312, 'l1_loss': 484.5613708496094, 'l2_loss': 185.38284301757812}
Step 4600: {'loss': 204.3213653564453, 'l1_loss': 482.9867858886719, 'l2_loss': 184.98291015625}
Step 4700: {'loss': 203.3502197265625, 'l1_loss': 502.5021667480469, 'l2_loss': 183.18414306640625}
Step 4800: {'loss': 203.5143280029297, 'l1_loss': 488.4384460449219, 'l2_loss': 183.96278381347656}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_3/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9920000433921814
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9630000591278076
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9960000514984131
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8820000290870667, scr_score: -0.23880655437858572
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8770000338554382, scr_score: -0.31343338021640943
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.737000048160553, scr_score: -2.402986282919348
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.6000000238418579, scr_score: -4.4477636238927545
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.5, scr_score: -5.940301919893103
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6070000529289246, scr_score: 0.027777869760247858
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6170000433921814, scr_score: 0.0530303691664592
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.675000011920929, scr_score: 0.19949492592919268
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6520000100135803, scr_score: 0.14141411708819893
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.5, scr_score: -0.24242426522981353
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9540000557899475
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9940000176429749
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8270000219345093, scr_score: -0.16363640304438812
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8260000348091125, scr_score: -0.17272719391122376
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8180000185966492, scr_score: -0.24545460456658216
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8100000619888306, scr_score: -0.318181473361604
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8070000410079956, scr_score: -0.34545438782244753
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7410000562667847, scr_score: -0.9454541710783129
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.89000004529953, scr_score: 0.4090910076109703
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6510000228881836, scr_score: 0.005865026775768214
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6570000052452087, scr_score: 0.023460281896742038
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.64000004529953, scr_score: -0.026392970078295327
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6550000309944153, scr_score: 0.017595255120973825
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6890000104904175, scr_score: 0.11730193386471774
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06451599370812708
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.5800000429153442, scr_score: -0.20234604566904113
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9880000352859497
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.968000054359436
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9970000386238098
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: 0.0
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.1272732395770455
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.16363640304438812
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9190000295639038, scr_score: 0.054544745201013926
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.7090914410992396
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.6950000524520874, scr_score: -4.018181581733671
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.7260000109672546, scr_score: -3.454546045665822
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5800000429153442, scr_score: 0.0
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.581000030040741, scr_score: 0.0024569707924063976
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6240000128746033, scr_score: 0.10810803290469247
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6270000338554382, scr_score: 0.11547909173066843
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6420000195503235, scr_score: 0.1523340929630347
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6480000019073486, scr_score: 0.1670760641662299
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6530000567436218, scr_score: 0.17936121102577543
Saved evaluation results to: run_3/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8990000486373901, scr_score: 0.014925187243177268
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8990000486373901, scr_score: 0.014925187243177268
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9000000357627869, scr_score: 0.029850374486354535
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8650000691413879, scr_score: -0.49253740637841137
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8660000562667847, scr_score: -0.4776122191352341
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8180000185966492, scr_score: -1.194030992649054
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7990000247955322, scr_score: -1.4776131087571716
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6510000228881836, scr_score: 0.13888889725093162
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.656000018119812, scr_score: 0.15151514695403728
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6820000410079956, scr_score: 0.21717176582360218
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.659000039100647, scr_score: 0.1590909569826084
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6910000443458557, scr_score: 0.23989904539254622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6970000267028809, scr_score: 0.2550505149329192
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6950000524520874, scr_score: 0.25000007525838464
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8510000109672546, scr_score: 0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8350000381469727, scr_score: -0.0892856382593739
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8060000538825989, scr_score: -0.34821409564843475
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: 0.04956273135895441
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6680000424385071, scr_score: 0.06122456072426409
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.675000011920929, scr_score: 0.08163263178272077
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6700000166893005, scr_score: 0.06705538851969542
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6690000295639038, scr_score: 0.06413997462197976
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6780000329017639, scr_score: 0.09037904725031479
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6820000410079956, scr_score: 0.10204087661562448
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8990000486373901, scr_score: -0.384614679235594
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8980000615119934, scr_score: -0.4038451839489418
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9030000567436218, scr_score: -0.3076915141400433
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.192307339617797
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.23076834904449253
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9100000262260437, scr_score: -0.17307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8830000162124634, scr_score: -0.692307339617797
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5900000333786011, scr_score: 0.03163019713329222
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5980000495910645, scr_score: 0.051094956142004655
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6010000109672546, scr_score: 0.058394132002675396
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5960000157356262, scr_score: 0.0462286938780956
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6110000014305115, scr_score: 0.08272500825183499
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8200000524520874, scr_score: -0.11538450957754977
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8220000267028809, scr_score: -0.10000009169945687
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8090000152587891, scr_score: -0.20000018339891373
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8180000185966492, scr_score: -0.130769385952927
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -0.16153868020639714
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7890000343322754, scr_score: -0.3538461961689801
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7880000472068787, scr_score: -0.36153840510802654
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.687000036239624, scr_score: 0.0666667027906891
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.706000030040741, scr_score: 0.12424243409443037
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7190000414848328, scr_score: 0.16363640304438812
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7250000238418579, scr_score: 0.1818181653981716
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7110000252723694, scr_score: 0.13939393282593532
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8470000624656677, scr_score: 0.17341054602321687
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8570000529289246, scr_score: 0.23121394651914118
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8560000658035278, scr_score: 0.22543367537663753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8020000457763672, scr_score: -0.08670510074388647
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8080000281333923, scr_score: -0.05202312935342067
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8080000281333923, scr_score: -0.05202312935342067
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.815000057220459, scr_score: -0.011560542285007299
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7390000224113464, scr_score: 0.22745086122869718
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7450000643730164, scr_score: 0.25098043798896263
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7470000386238098, scr_score: 0.2588234744132449
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7520000338554382, scr_score: 0.2784312992176598
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7510000467300415, scr_score: 0.2745097810055187
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6970000267028809, scr_score: 0.06274499262538596
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6770000457763672, scr_score: -0.015686306592273835
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8130000233650208, scr_score: 0.09677420388564499
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.815000057220459, scr_score: 0.1075270712368249
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: 0.16129044662773992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8290000557899475, scr_score: 0.1827958608751049
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8160000443458557, scr_score: 0.11290334468491743
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7970000505447388, scr_score: 0.010752867351179899
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.718000054359436, scr_score: 0.05952398128698237
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7220000624656677, scr_score: 0.07539702719508858
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.09523827544863724
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7230000495910645, scr_score: 0.07936522954053103
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.08333343188597349
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: 0.07142858832330974
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7010000348091125, scr_score: -0.007936404690884913
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8670000433921814, scr_score: 0.15555570272746339
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8280000686645508, scr_score: -0.13333289181760985
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8360000252723694, scr_score: -0.07407397595946885
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8170000314712524, scr_score: -0.21481461858560438
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: -0.1555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7850000262260437, scr_score: -0.4518516065653388
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7890000343322754, scr_score: -0.42222192787840657
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6690000295639038, scr_score: 0.055737767710347226
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6680000424385071, scr_score: 0.05245912147445863
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6740000247955322, scr_score: 0.07213119431485049
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6610000133514404, scr_score: 0.029508206973117915
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7120000123977661, scr_score: 0.19672131467909934
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7100000381469727, scr_score: 0.19016402220732215
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6990000605583191, scr_score: 0.15409852276242703
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7660000324249268, scr_score: -0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7680000066757202, scr_score: -0.032432658834294255
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7900000214576721, scr_score: 0.0864863384544999
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7760000228881836, scr_score: 0.010810671486588149
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7830000519752502, scr_score: 0.048648666064176484
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7260000109672546, scr_score: 0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7240000367164612, scr_score: 0.0
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7210000157356262, scr_score: -0.012605128302735451
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7120000123977661, scr_score: -0.05042026277129501
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9690000414848328
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9640000462532043
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8280000686645508, scr_score: -0.09302297643398832
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8370000123977661, scr_score: -0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8410000205039978, scr_score: 0.007751837694252218
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8060000538825989, scr_score: -0.2635657159649414
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7330000400543213, scr_score: -0.8294572604690469
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.6500000357627869, scr_score: -1.4728681060186364
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.5, scr_score: -2.6356590078553372
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6850000619888306, scr_score: 0.07462696392834063
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6990000605583191, scr_score: 0.1164179996754661
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7010000348091125, scr_score: 0.12238807138607294
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.17910446433400803
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7700000405311584, scr_score: 0.3283582142663968
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7950000166893005, scr_score: 0.4029850002704449
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6910000443458557, scr_score: 0.09253735698445369
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9310000538825989
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9930000305175781
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9980000257492065
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.815000057220459, scr_score: -0.05421688477417381
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8020000457763672, scr_score: -0.1325302026773256
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.812000036239624, scr_score: -0.07228929938693394
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7770000100135803, scr_score: -0.28313281996741135
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7510000467300415, scr_score: -0.43975909670960833
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: -0.9638555298385864
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.5, scr_score: -1.9518074928061506
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6720000505447388, scr_score: 0.0
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7010000348091125, scr_score: 0.10583936286547818
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.718000054359436, scr_score: 0.16788323232085922
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7240000367164612, scr_score: 0.1897809790259078
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7270000576972961, scr_score: 0.2007299611460364
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7740000486373901, scr_score: 0.37226278166186894
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8100000619888306, scr_score: 0.5036496969625777
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9790000319480896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9790000319480896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9960000514984131
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8110000491142273, scr_score: 0.01176490387700127
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.812000036239624, scr_score: 0.017647180507740362
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8250000476837158, scr_score: 0.09411782855391783
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8380000591278076, scr_score: 0.1705884766000953
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8320000171661377, scr_score: 0.13529411558461457
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8610000610351562, scr_score: 0.30588259218470987
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.906000018119812, scr_score: 0.5705881961076769
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6910000443458557, scr_score: 0.007518923352021404
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7140000462532043, scr_score: 0.0939850853959675
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7110000252723694, scr_score: 0.08270681240672771
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.14285730291256044
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7490000128746033, scr_score: 0.22556389124170353
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7360000610351562, scr_score: 0.1766918978026952
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6420000195503235, scr_score: -0.17669167372511058
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9580000638961792
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9820000529289246
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.999000072479248
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8760000467300415, scr_score: 0.06250023283054354
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.878000020980835, scr_score: 0.08035713810549912
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8770000338554382, scr_score: 0.07142868546802132
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9010000228881836, scr_score: 0.2857142096879858
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.008928984821577298
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9290000200271606, scr_score: 0.5357140766419609
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6540000438690186, scr_score: 0.07598793776116658
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6630000472068787, scr_score: 0.10334356636813018
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6710000038146973, scr_score: 0.12765953977612934
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6830000281333923, scr_score: 0.16413377164178158
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7050000429153442, scr_score: 0.23100310614425953
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7440000176429749, scr_score: 0.3495440426617002
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.8370000123977661, scr_score: 0.6322187508209225
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male / female: 0.9610000252723694
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_data_only: 0.9600000381469727
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for professor / nurse: 0.9610000252723694
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for nurse_data_only: 0.9590000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for male_professor / female_nurse: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
Test accuracy for female_nurse_data_only: 0.9980000257492065
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7500000596046448, scr_score: -0.014422890934105061
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7540000081062317, scr_score: 0.004807630311368353
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7320000529289246, scr_score: -0.10096138278188053
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7410000562667847, scr_score: -0.0576921368579928
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7740000486373901, scr_score: 0.10096166934266682
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7680000066757202, scr_score: 0.07211531435288414
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.5690000057220459, scr_score: -0.8846154397232281
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7480000257492065, scr_score: 0.02293575849509752
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7480000257492065, scr_score: 0.02293575849509752
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7600000500679016, scr_score: 0.07798174293281654
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7590000629425049, scr_score: 0.0733946459169587
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7890000343322754, scr_score: 0.2110091968875438
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7950000166893005, scr_score: 0.23853205239849917
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.8010000586509705, scr_score: 0.26605518132526285
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 605.5480346679688, 'l1_loss': 6244.248046875, 'l2_loss': 355.77813720703125}
Step 200: {'loss': 429.0907287597656, 'l1_loss': 2760.68212890625, 'l2_loss': 318.6634521484375}
Step 300: {'loss': 358.45660400390625, 'l1_loss': 1215.6123046875, 'l2_loss': 309.8318786621094}
Step 400: {'loss': 333.53778076171875, 'l1_loss': 803.9285888671875, 'l2_loss': 301.3792724609375}
Step 500: {'loss': 307.12872314453125, 'l1_loss': 876.64501953125, 'l2_loss': 272.06060791015625}
Step 600: {'loss': 285.1254577636719, 'l1_loss': 905.6136474609375, 'l2_loss': 248.9008026123047}
Step 700: {'loss': 265.2477111816406, 'l1_loss': 892.3057861328125, 'l2_loss': 229.55528259277344}
Step 800: {'loss': 256.6397705078125, 'l1_loss': 874.8211669921875, 'l2_loss': 221.6469268798828}
Step 900: {'loss': 246.29196166992188, 'l1_loss': 713.045654296875, 'l2_loss': 217.7701416015625}
Step 1000: {'loss': 244.5673065185547, 'l1_loss': 747.566162109375, 'l2_loss': 214.66465759277344}
Step 1100: {'loss': 237.0288848876953, 'l1_loss': 617.46533203125, 'l2_loss': 212.3302764892578}
Step 1200: {'loss': 229.916748046875, 'l1_loss': 511.9620361328125, 'l2_loss': 209.43826293945312}
Step 1300: {'loss': 231.44371032714844, 'l1_loss': 554.7470703125, 'l2_loss': 209.25381469726562}
Step 1400: {'loss': 227.79959106445312, 'l1_loss': 561.323486328125, 'l2_loss': 205.3466339111328}
Step 1500: {'loss': 225.99923706054688, 'l1_loss': 511.88214111328125, 'l2_loss': 205.52392578125}
Step 1600: {'loss': 224.96353149414062, 'l1_loss': 513.6851806640625, 'l2_loss': 204.41519165039062}
Step 1700: {'loss': 222.34495544433594, 'l1_loss': 493.8476867675781, 'l2_loss': 202.59103393554688}
Step 1800: {'loss': 222.2341766357422, 'l1_loss': 524.06298828125, 'l2_loss': 201.27163696289062}
Step 1900: {'loss': 220.46511840820312, 'l1_loss': 508.19873046875, 'l2_loss': 200.13710021972656}
Step 2000: {'loss': 219.19383239746094, 'l1_loss': 510.45751953125, 'l2_loss': 198.77548217773438}
Step 2100: {'loss': 219.13352966308594, 'l1_loss': 495.43389892578125, 'l2_loss': 199.31613159179688}
Step 2200: {'loss': 214.87991333007812, 'l1_loss': 456.92376708984375, 'l2_loss': 196.60281372070312}
Step 2300: {'loss': 215.56887817382812, 'l1_loss': 514.3973388671875, 'l2_loss': 194.99290466308594}
Step 2400: {'loss': 215.6605682373047, 'l1_loss': 508.1925354003906, 'l2_loss': 195.332763671875}
Step 2500: {'loss': 213.26795959472656, 'l1_loss': 520.8536376953125, 'l2_loss': 192.42645263671875}
Step 2600: {'loss': 213.49925231933594, 'l1_loss': 552.5823974609375, 'l2_loss': 191.3946533203125}
Step 2700: {'loss': 212.9375, 'l1_loss': 524.5093994140625, 'l2_loss': 191.95675659179688}
Step 2800: {'loss': 211.5879364013672, 'l1_loss': 502.73162841796875, 'l2_loss': 191.47837829589844}
Step 2900: {'loss': 209.37266540527344, 'l1_loss': 487.212158203125, 'l2_loss': 189.8837890625}
Step 3000: {'loss': 211.31756591796875, 'l1_loss': 532.1027221679688, 'l2_loss': 190.02908325195312}
Step 3100: {'loss': 208.8206024169922, 'l1_loss': 522.3722534179688, 'l2_loss': 187.89886474609375}
Step 3200: {'loss': 208.64239501953125, 'l1_loss': 499.28582763671875, 'l2_loss': 188.66427612304688}
Step 3300: {'loss': 207.50479125976562, 'l1_loss': 500.71502685546875, 'l2_loss': 187.45846557617188}
Step 3400: {'loss': 208.56439208984375, 'l1_loss': 529.3704833984375, 'l2_loss': 187.38076782226562}
Step 3500: {'loss': 204.9283447265625, 'l1_loss': 493.17071533203125, 'l2_loss': 185.19032287597656}
Step 3600: {'loss': 207.2086181640625, 'l1_loss': 522.5402221679688, 'l2_loss': 186.3012237548828}
Step 3700: {'loss': 206.13198852539062, 'l1_loss': 506.2645263671875, 'l2_loss': 185.87759399414062}
Step 3800: {'loss': 204.23011779785156, 'l1_loss': 519.0894775390625, 'l2_loss': 183.45578002929688}
Step 3900: {'loss': 204.69334411621094, 'l1_loss': 510.3206481933594, 'l2_loss': 184.27645874023438}
Step 4000: {'loss': 203.88612365722656, 'l1_loss': 494.65167236328125, 'l2_loss': 184.09458923339844}
Step 4100: {'loss': 203.5714111328125, 'l1_loss': 495.54522705078125, 'l2_loss': 183.72413635253906}
Step 4200: {'loss': 203.5684051513672, 'l1_loss': 520.44580078125, 'l2_loss': 182.7401580810547}
Step 4300: {'loss': 203.6455078125, 'l1_loss': 515.055908203125, 'l2_loss': 183.03575134277344}
Step 4400: {'loss': 201.72933959960938, 'l1_loss': 504.0842590332031, 'l2_loss': 181.5533447265625}
Step 4500: {'loss': 202.55201721191406, 'l1_loss': 512.4132080078125, 'l2_loss': 182.03750610351562}
Step 4600: {'loss': 201.78016662597656, 'l1_loss': 511.60650634765625, 'l2_loss': 181.2932586669922}
Step 4700: {'loss': 200.8853759765625, 'l1_loss': 530.5300903320312, 'l2_loss': 179.6295928955078}
Step 4800: {'loss': 201.07579040527344, 'l1_loss': 517.8969116210938, 'l2_loss': 180.3491973876953}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_4/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_4/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.893000066280365, scr_score: -0.0746268258378237
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8860000371932983, scr_score: -0.1791049157839393
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -1.2537326312437003
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8040000200271606, scr_score: -1.4029862829193478
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7900000214576721, scr_score: -1.6119415731896416
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6430000066757202, scr_score: 0.11868683751925485
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6350000500679016, scr_score: 0.09848492830434731
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6360000371932983, scr_score: 0.1010101481416146
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6860000491142273, scr_score: 0.22727279568944056
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7000000476837158, scr_score: 0.2626263249614903
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8370000123977661, scr_score: -0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8380000591278076, scr_score: -0.062499733907808636
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8370000123977661, scr_score: -0.0714287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8320000171661377, scr_score: -0.11607154261093916
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8270000219345093, scr_score: -0.16071436174062612
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8260000348091125, scr_score: -0.16964281912968696
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8040000200271606, scr_score: -0.36607154261093916
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6520000100135803, scr_score: 0.01457724326302535
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: 0.04956273135895441
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6740000247955322, scr_score: 0.0787172178850051
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.687000036239624, scr_score: 0.11661811987864983
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6810000538825989, scr_score: 0.09912546271790881
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6830000281333923, scr_score: 0.10495629051334014
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6820000410079956, scr_score: 0.10204087661562448
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: -0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8970000147819519, scr_score: -0.42307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8920000195503235, scr_score: -0.5192305047133478
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8560000658035278, scr_score: -1.211536698088985
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5840000510215759, scr_score: 0.01703170038848885
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5980000495910645, scr_score: 0.051094956142004655
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5960000157356262, scr_score: 0.0462286938780956
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.612000048160553, scr_score: 0.08515821189552046
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.612000048160553, scr_score: 0.08515821189552046
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6180000305175781, scr_score: 0.09975670864032384
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6340000033378601, scr_score: 0.1386860816342868
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8270000219345093, scr_score: -0.06153858850694027
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8240000605583191, scr_score: -0.08461521532407963
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8460000157356262, scr_score: 0.08461521532407963
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8540000319480896, scr_score: 0.14615380383101992
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8360000252723694, scr_score: 0.007692208939046454
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7770000100135803, scr_score: -0.4461540789293905
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6850000619888306, scr_score: 0.060606175546132
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6910000443458557, scr_score: 0.0787879378999155
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7010000348091125, scr_score: 0.10909093536292541
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7310000061988831, scr_score: 0.19999992775195513
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7330000400543213, scr_score: 0.2060606356166244
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7210000157356262, scr_score: 0.16969693028894522
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8520000576972961, scr_score: 0.20231224627117902
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8540000319480896, scr_score: 0.21387278855618633
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7860000133514404, scr_score: -0.17919081716572052
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7870000600814819, scr_score: -0.17341020148777295
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.796000063419342, scr_score: -0.12138707213435229
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7950000166893005, scr_score: -0.12716768781229984
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8070000410079956, scr_score: -0.05780340049592432
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7500000596046448, scr_score: 0.27058826279337755
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.76500004529953, scr_score: 0.3294117372066224
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7740000486373901, scr_score: 0.3647058686033112
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7720000147819519, scr_score: 0.3568625984353196
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7400000095367432, scr_score: 0.2313723794408383
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7330000400543213, scr_score: 0.2039215182121411
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6850000619888306, scr_score: 0.015686306592273835
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8190000653266907, scr_score: 0.12903248548418988
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: 0.1559141731796474
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8200000524520874, scr_score: 0.1344087589322824
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8300000429153442, scr_score: 0.18817213432319746
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8050000667572021, scr_score: 0.053763695845909856
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7830000519752502, scr_score: -0.06451592228710011
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7280000448226929, scr_score: 0.09920647779407969
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7350000143051147, scr_score: 0.12698413073851328
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.734000027179718, scr_score: 0.12301592839307082
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7640000581741333, scr_score: 0.2420636544406992
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7690000534057617, scr_score: 0.2619049026942478
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7490000128746033, scr_score: 0.1825396731537168
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.051587340069761074
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8760000467300415, scr_score: 0.22222236939413004
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8450000286102295, scr_score: -0.0074073092928021896
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8420000672340393, scr_score: -0.029629237171208758
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8400000333786011, scr_score: -0.04444429727253662
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8300000429153442, scr_score: -0.11851827323200548
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7900000214576721, scr_score: -0.41481461858560437
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7860000133514404, scr_score: -0.4444442972725366
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6650000214576721, scr_score: 0.042622987341732574
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6700000166893005, scr_score: 0.05901641394623583
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6770000457763672, scr_score: 0.08196732844757654
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.703000009059906, scr_score: 0.16721310770598144
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7020000219345093, scr_score: 0.16393446147009283
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6780000329017639, scr_score: 0.08524597468346515
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7020000219345093, scr_score: 0.16393446147009283
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7900000214576721, scr_score: 0.0864863384544999
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7750000357627869, scr_score: 0.0054053357432940746
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7780000567436218, scr_score: 0.021621665160441204
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7880000472068787, scr_score: 0.07567566696791175
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.781000018119812, scr_score: 0.03783767239032343
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7990000247955322, scr_score: 0.13513500451867638
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7880000472068787, scr_score: 0.07567566696791175
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7290000319480896, scr_score: 0.02100838021146122
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7320000529289246, scr_score: 0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7320000529289246, scr_score: 0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.734000027179718, scr_score: 0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7200000286102295, scr_score: -0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7080000042915344, scr_score: -0.06722701702839334
Step 0: {'loss': 645.93359375, 'l1_loss': 2132.95361328125, 'l2_loss': 560.615478515625}
Step 100: {'loss': 413.7883605957031, 'l1_loss': 1603.276611328125, 'l2_loss': 349.65728759765625}
Step 200: {'loss': 323.93609619140625, 'l1_loss': 928.767578125, 'l2_loss': 286.785400390625}
Step 300: {'loss': 266.10009765625, 'l1_loss': 554.3193359375, 'l2_loss': 243.92733764648438}
Step 400: {'loss': 230.66229248046875, 'l1_loss': 421.1033935546875, 'l2_loss': 213.8181610107422}
Step 500: {'loss': 212.12167358398438, 'l1_loss': 365.827880859375, 'l2_loss': 197.48855590820312}
Step 600: {'loss': 200.64878845214844, 'l1_loss': 331.8508605957031, 'l2_loss': 187.374755859375}
Step 700: {'loss': 187.4137420654297, 'l1_loss': 307.1131896972656, 'l2_loss': 175.12921142578125}
Step 800: {'loss': 179.08448791503906, 'l1_loss': 311.2341613769531, 'l2_loss': 166.63511657714844}
Step 900: {'loss': 170.1921844482422, 'l1_loss': 307.9134521484375, 'l2_loss': 157.87564086914062}
Step 1000: {'loss': 163.86203002929688, 'l1_loss': 347.6180114746094, 'l2_loss': 149.95730590820312}
Step 1100: {'loss': 156.47630310058594, 'l1_loss': 352.8339538574219, 'l2_loss': 142.36294555664062}
Step 1200: {'loss': 149.72991943359375, 'l1_loss': 353.2548522949219, 'l2_loss': 135.5997314453125}
Step 1300: {'loss': 148.20729064941406, 'l1_loss': 381.79510498046875, 'l2_loss': 132.93548583984375}
Step 1400: {'loss': 142.55923461914062, 'l1_loss': 396.5006103515625, 'l2_loss': 126.69921112060547}
Step 1500: {'loss': 140.73463439941406, 'l1_loss': 400.15960693359375, 'l2_loss': 124.72825622558594}
Step 1600: {'loss': 137.85182189941406, 'l1_loss': 413.80029296875, 'l2_loss': 121.2998046875}
Step 1700: {'loss': 135.35498046875, 'l1_loss': 417.2143859863281, 'l2_loss': 118.66639709472656}
Step 1800: {'loss': 132.9994354248047, 'l1_loss': 432.8236999511719, 'l2_loss': 115.68649291992188}
Step 1900: {'loss': 131.15853881835938, 'l1_loss': 436.4681396484375, 'l2_loss': 113.69981384277344}
Step 2000: {'loss': 129.38644409179688, 'l1_loss': 442.1815490722656, 'l2_loss': 111.69918823242188}
Step 2100: {'loss': 128.3626708984375, 'l1_loss': 447.2505798339844, 'l2_loss': 110.47264099121094}
Step 2200: {'loss': 126.09049987792969, 'l1_loss': 442.64678955078125, 'l2_loss': 108.38462829589844}
Step 2300: {'loss': 124.73127746582031, 'l1_loss': 455.36053466796875, 'l2_loss': 106.51686096191406}
Step 2400: {'loss': 124.35122680664062, 'l1_loss': 460.9576416015625, 'l2_loss': 105.91291809082031}
Step 2500: {'loss': 122.61106872558594, 'l1_loss': 463.1470642089844, 'l2_loss': 104.0851821899414}
Step 2600: {'loss': 121.41230773925781, 'l1_loss': 471.2654113769531, 'l2_loss': 102.56169128417969}
Step 2700: {'loss': 120.8072280883789, 'l1_loss': 472.83746337890625, 'l2_loss': 101.89373016357422}
Step 2800: {'loss': 120.51728820800781, 'l1_loss': 476.9767150878906, 'l2_loss': 101.43821716308594}
Step 2900: {'loss': 118.94075012207031, 'l1_loss': 476.9371032714844, 'l2_loss': 99.86326599121094}
Step 3000: {'loss': 119.18646240234375, 'l1_loss': 485.9298095703125, 'l2_loss': 99.749267578125}
Step 3100: {'loss': 117.4886703491211, 'l1_loss': 487.6253356933594, 'l2_loss': 97.98365783691406}
Step 3200: {'loss': 118.04098510742188, 'l1_loss': 488.8526306152344, 'l2_loss': 98.48688507080078}
Step 3300: {'loss': 116.62242126464844, 'l1_loss': 494.77606201171875, 'l2_loss': 96.83138275146484}
Step 3400: {'loss': 116.33305358886719, 'l1_loss': 501.7977600097656, 'l2_loss': 96.26113891601562}
Step 3500: {'loss': 114.67816925048828, 'l1_loss': 498.9154357910156, 'l2_loss': 94.72154998779297}
Step 3600: {'loss': 115.48865509033203, 'l1_loss': 507.5472412109375, 'l2_loss': 95.186767578125}
Step 3700: {'loss': 115.04325103759766, 'l1_loss': 509.3985290527344, 'l2_loss': 94.66731262207031}
Step 3800: {'loss': 113.65594482421875, 'l1_loss': 508.4472351074219, 'l2_loss': 93.31805419921875}
Step 3900: {'loss': 114.19676971435547, 'l1_loss': 513.0449829101562, 'l2_loss': 93.67497253417969}
Step 4000: {'loss': 113.82869720458984, 'l1_loss': 515.1807861328125, 'l2_loss': 93.22146606445312}
Step 4100: {'loss': 113.31464385986328, 'l1_loss': 519.039306640625, 'l2_loss': 92.55307006835938}
Step 4200: {'loss': 112.50275421142578, 'l1_loss': 526.318603515625, 'l2_loss': 91.45001220703125}
Step 4300: {'loss': 112.79856872558594, 'l1_loss': 529.381103515625, 'l2_loss': 91.62332916259766}
Step 4400: {'loss': 111.40895080566406, 'l1_loss': 531.8927001953125, 'l2_loss': 90.13324737548828}
Step 4500: {'loss': 111.46580505371094, 'l1_loss': 536.4765625, 'l2_loss': 90.00674438476562}
Step 4600: {'loss': 111.25952911376953, 'l1_loss': 538.8906860351562, 'l2_loss': 89.70390319824219}
Step 4700: {'loss': 110.91148376464844, 'l1_loss': 539.4584350585938, 'l2_loss': 89.33314514160156}
Step 4800: {'loss': 110.32770538330078, 'l1_loss': 542.533447265625, 'l2_loss': 88.6263656616211}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_164702_grouped_sparse_sae/run_2/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_2/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8960000276565552, scr_score: -0.0298512641082919
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -0.20895529027029383
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8650000691413879, scr_score: -0.49253740637841137
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.862000048160553, scr_score: -0.5373138577298805
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8830000162124634, scr_score: -0.22388136713540846
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.859000027179718, scr_score: -0.5820903090813497
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6190000176429749, scr_score: 0.058080808840993786
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6370000243186951, scr_score: 0.1035353679788819
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6700000166893005, scr_score: 0.18686867622608702
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7070000171661377, scr_score: 0.2803030143391305
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7090000510215759, scr_score: 0.28535360453043435
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7000000476837158, scr_score: 0.2626263249614903
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7200000286102295, scr_score: 0.31313132377391295
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8330000638961792, scr_score: -0.10909057412270103
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.843000054359436, scr_score: -0.01818158173367131
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8290000557899475, scr_score: -0.14545427945038022
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8380000591278076, scr_score: -0.06363607792818617
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8090000152587891, scr_score: -0.32727280608877624
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7800000309944153, scr_score: -0.5909089923890297
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.5190000534057617, scr_score: -2.9636357528119843
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.64000004529953, scr_score: -0.026392970078295327
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6420000195503235, scr_score: -0.020527943302527114
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6430000066757202, scr_score: -0.017595429914643006
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6490000486373901, scr_score: 0.0
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6880000233650208, scr_score: 0.11436942047683363
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6370000243186951, scr_score: -0.035190685035616835
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.503000020980835, scr_score: -0.42815254802849345
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: 0.03636316346734262
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9040000438690186, scr_score: -0.2181822319660752
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8990000486373901, scr_score: -0.30909122435510494
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.906000018119812, scr_score: -0.18181906849873258
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8890000581741333, scr_score: -0.4909092091331643
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9070000648498535, scr_score: -0.16363640304438812
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9220000505447388, scr_score: 0.10909057412270103
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5860000252723694, scr_score: 0.01474197120319516
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5950000286102295, scr_score: 0.03685500123236629
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5990000367164612, scr_score: 0.04668303085074865
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6050000190734863, scr_score: 0.061425002053943814
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6020000576972961, scr_score: 0.05405408967672462
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6180000305175781, scr_score: 0.0933660617014973
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6640000343322754, scr_score: 0.20638818263975933
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8180000185966492, scr_score: -0.17054273953095306
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8260000348091125, scr_score: -0.10852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8450000286102295, scr_score: 0.03875965052274194
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.800000011920929, scr_score: -0.31007766623341637
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7140000462532043, scr_score: -0.9767440248657625
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.6000000238418579, scr_score: -1.8604650732975367
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.51500004529953, scr_score: -2.5193795942356307
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6620000600814819, scr_score: 0.005970249634899351
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6670000553131104, scr_score: 0.02089560683570898
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6770000457763672, scr_score: 0.05074632123732823
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7140000462532043, scr_score: 0.16119407127789498
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7400000095367432, scr_score: 0.23880589313724654
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7780000567436218, scr_score: 0.3522388569574092
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.690000057220459, scr_score: 0.08955232112915026
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8140000104904175, scr_score: -0.06024126235449825
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7980000376701355, scr_score: -0.1566266358063036
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7760000228881836, scr_score: -0.28915683848362916
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.815000057220459, scr_score: -0.05421688477417381
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8090000152587891, scr_score: -0.09036171399969407
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7590000629425049, scr_score: -0.3915662304516524
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7980000376701355, scr_score: -0.1566266358063036
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6720000505447388, scr_score: 0.0
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6640000343322754, scr_score: -0.02919714063020385
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7010000348091125, scr_score: 0.10583936286547818
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6930000185966492, scr_score: 0.07664222223527434
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7330000400543213, scr_score: 0.22262770785108496
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7630000114440918, scr_score: 0.33211665891153647
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8320000171661377, scr_score: 0.5839415073928252
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8240000605583191, scr_score: 0.08823555192317874
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8300000429153442, scr_score: 0.12352956232313639
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8030000329017639, scr_score: -0.035294010399957644
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8070000410079956, scr_score: -0.011764553261478189
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8130000233650208, scr_score: 0.023529457138479457
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.7520000338554382, scr_score: -0.3352939753384053
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.893000066280365, scr_score: 0.49411789867702244
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.687000036239624, scr_score: -0.007518699274436781
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6860000491142273, scr_score: -0.011278048911655172
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6920000314712524, scr_score: 0.011278272989239795
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.13157902992332066
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7580000162124634, scr_score: 0.2593984861318383
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7660000324249268, scr_score: 0.2894737313847547
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6450000405311584, scr_score: -0.1654134007358708
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.878000020980835, scr_score: 0.08035713810549912
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8820000290870667, scr_score: 0.11607148083950977
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8690000176429749, scr_score: 0.0
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.7890000343322754, scr_score: -0.7142852581279147
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8190000653266907, scr_score: -0.4464279537148845
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8720000386238098, scr_score: 0.026785890096532875
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6470000147819519, scr_score: 0.054711257213927195
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6790000200271606, scr_score: 0.15197569435323083
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7130000591278076, scr_score: 0.255319260721361
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7020000219345093, scr_score: 0.22188450288557088
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7210000157356262, scr_score: 0.2796352341293602
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.737000048160553, scr_score: 0.32826754328356317
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7540000081062317, scr_score: 0.3799391452985259
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.024038438117628045
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7600000500679016, scr_score: 0.03365398530115103
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7690000534057617, scr_score: 0.07692323122503876
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7450000643730164, scr_score: -0.038461329051733106
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7220000624656677, scr_score: -0.14903825901713663
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7560000419616699, scr_score: 0.014423177494891337
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7620000243186951, scr_score: 0.043269245923887735
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7460000514984131, scr_score: 0.013761564463381832
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7510000467300415, scr_score: 0.03669732295847935
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.06880727548529256
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7430000305175781, scr_score: 0.0
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7610000371932983, scr_score: 0.08256883994867438
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7770000100135803, scr_score: 0.15596321244982478
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7280000448226929, scr_score: -0.06880727548529256
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 605.5480346679688, 'l1_loss': 6244.248046875, 'l2_loss': 355.77813720703125}
Step 200: {'loss': 429.0907287597656, 'l1_loss': 2760.68212890625, 'l2_loss': 318.6634521484375}
Step 300: {'loss': 358.4563903808594, 'l1_loss': 1215.6123046875, 'l2_loss': 309.8319091796875}
Step 400: {'loss': 333.53631591796875, 'l1_loss': 803.9303588867188, 'l2_loss': 301.37908935546875}
Step 500: {'loss': 307.1258850097656, 'l1_loss': 876.6446533203125, 'l2_loss': 272.0600891113281}
Step 600: {'loss': 285.1235656738281, 'l1_loss': 905.626953125, 'l2_loss': 248.89849853515625}
Step 700: {'loss': 265.2445983886719, 'l1_loss': 892.3431396484375, 'l2_loss': 229.55078125}
Step 800: {'loss': 256.6255798339844, 'l1_loss': 874.810302734375, 'l2_loss': 221.6331787109375}
Step 900: {'loss': 246.28753662109375, 'l1_loss': 713.0626220703125, 'l2_loss': 217.76498413085938}
Step 1000: {'loss': 244.6793212890625, 'l1_loss': 747.3875732421875, 'l2_loss': 214.7838134765625}
Step 1100: {'loss': 236.995849609375, 'l1_loss': 617.3843383789062, 'l2_loss': 212.30047607421875}
Step 1200: {'loss': 229.90310668945312, 'l1_loss': 511.9677429199219, 'l2_loss': 209.42431640625}
Step 1300: {'loss': 231.4391326904297, 'l1_loss': 554.5811767578125, 'l2_loss': 209.25587463378906}
Step 1400: {'loss': 227.829345703125, 'l1_loss': 561.5989990234375, 'l2_loss': 205.36538696289062}
Step 1500: {'loss': 226.03256225585938, 'l1_loss': 512.3970947265625, 'l2_loss': 205.53668212890625}
Step 1600: {'loss': 224.94564819335938, 'l1_loss': 514.1094970703125, 'l2_loss': 204.38125610351562}
Step 1700: {'loss': 222.36734008789062, 'l1_loss': 493.70733642578125, 'l2_loss': 202.61903381347656}
Step 1800: {'loss': 222.09500122070312, 'l1_loss': 524.0841674804688, 'l2_loss': 201.1316375732422}
Step 1900: {'loss': 220.48414611816406, 'l1_loss': 508.0865783691406, 'l2_loss': 200.1605987548828}
Step 2000: {'loss': 219.19834899902344, 'l1_loss': 510.5494689941406, 'l2_loss': 198.77635192871094}
Step 2100: {'loss': 219.13998413085938, 'l1_loss': 495.8056640625, 'l2_loss': 199.3077392578125}
Step 2200: {'loss': 214.83352661132812, 'l1_loss': 457.3599853515625, 'l2_loss': 196.53897094726562}
Step 2300: {'loss': 215.61843872070312, 'l1_loss': 514.8944702148438, 'l2_loss': 195.0208740234375}
Step 2400: {'loss': 215.76568603515625, 'l1_loss': 508.437255859375, 'l2_loss': 195.42816162109375}
Step 2500: {'loss': 213.3447265625, 'l1_loss': 521.1517333984375, 'l2_loss': 192.49862670898438}
Step 2600: {'loss': 213.59823608398438, 'l1_loss': 553.5255737304688, 'l2_loss': 191.45716857910156}
Step 2700: {'loss': 212.9928436279297, 'l1_loss': 525.1671752929688, 'l2_loss': 191.9829559326172}
Step 2800: {'loss': 211.62110900878906, 'l1_loss': 503.5516052246094, 'l2_loss': 191.47677612304688}
Step 2900: {'loss': 209.44630432128906, 'l1_loss': 487.76934814453125, 'l2_loss': 189.92935180664062}
Step 3000: {'loss': 211.21531677246094, 'l1_loss': 532.156494140625, 'l2_loss': 189.92807006835938}
Step 3100: {'loss': 208.8734588623047, 'l1_loss': 522.1390380859375, 'l2_loss': 187.98670959472656}
Step 3200: {'loss': 208.55096435546875, 'l1_loss': 499.8977966308594, 'l2_loss': 188.5545654296875}
Step 3300: {'loss': 207.48745727539062, 'l1_loss': 500.90570068359375, 'l2_loss': 187.45040893554688}
Step 3400: {'loss': 208.49435424804688, 'l1_loss': 529.451171875, 'l2_loss': 187.31460571289062}
Step 3500: {'loss': 204.8288116455078, 'l1_loss': 493.1787109375, 'l2_loss': 185.10125732421875}
Step 3600: {'loss': 207.3512725830078, 'l1_loss': 521.8228759765625, 'l2_loss': 186.46820068359375}
Step 3700: {'loss': 206.0731964111328, 'l1_loss': 506.2624816894531, 'l2_loss': 185.82168579101562}
Step 3800: {'loss': 204.19143676757812, 'l1_loss': 517.9906005859375, 'l2_loss': 183.4647674560547}
Step 3900: {'loss': 204.6878662109375, 'l1_loss': 509.7762451171875, 'l2_loss': 184.29498291015625}
Step 4000: {'loss': 203.90225219726562, 'l1_loss': 493.2729797363281, 'l2_loss': 184.162841796875}
Step 4100: {'loss': 203.70217895507812, 'l1_loss': 494.01287841796875, 'l2_loss': 183.94012451171875}
Step 4200: {'loss': 203.50804138183594, 'l1_loss': 519.1160888671875, 'l2_loss': 182.74136352539062}
Step 4300: {'loss': 203.5869598388672, 'l1_loss': 513.2554321289062, 'l2_loss': 183.05526733398438}
Step 4400: {'loss': 201.74917602539062, 'l1_loss': 502.893798828125, 'l2_loss': 181.62030029296875}
Step 4500: {'loss': 202.46250915527344, 'l1_loss': 510.9028625488281, 'l2_loss': 182.01974487304688}
Step 4600: {'loss': 201.68338012695312, 'l1_loss': 510.3809814453125, 'l2_loss': 181.25143432617188}
Step 4700: {'loss': 200.86001586914062, 'l1_loss': 528.6785888671875, 'l2_loss': 179.70733642578125}
Step 4800: {'loss': 201.01693725585938, 'l1_loss': 515.7923583984375, 'l2_loss': 180.38186645507812}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_5/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_5/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8960000276565552, scr_score: -0.0298512641082919
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -1.0000008896219374
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8080000281333923, scr_score: -1.3432846443247013
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7890000343322754, scr_score: -1.6268667604328189
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6440000534057617, scr_score: 0.12121220787329137
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6380000114440918, scr_score: 0.10606058781614919
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6380000114440918, scr_score: 0.10606058781614919
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6820000410079956, scr_score: 0.21717176582360218
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6910000443458557, scr_score: 0.23989904539254622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6990000605583191, scr_score: 0.260101105124223
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8350000381469727, scr_score: -0.0892856382593739
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.843000054359436, scr_score: -0.017856914778121687
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8390000462532043, scr_score: -0.05357127651874779
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8330000638961792, scr_score: -0.10714255303749558
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8240000605583191, scr_score: -0.18749973390780864
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8200000524520874, scr_score: -0.22321409564843475
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8050000667572021, scr_score: -0.3571425530374956
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6520000100135803, scr_score: 0.01457724326302535
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: 0.04956273135895441
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.687000036239624, scr_score: 0.11661811987864983
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6820000410079956, scr_score: 0.10204087661562448
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6760000586509705, scr_score: 0.08454821945488346
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08746363335259913
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6810000538825989, scr_score: 0.09912546271790881
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: -0.019230504713347762
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.921000063419342, scr_score: 0.038462155668855226
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9190000295639038, scr_score: 0.0
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.89000004529953, scr_score: -0.5576915141400433
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8980000615119934, scr_score: -0.4038451839489418
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8480000495910645, scr_score: -1.3653830282800865
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5960000157356262, scr_score: 0.0462286938780956
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6030000448226929, scr_score: 0.06326039426658445
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6040000319480896, scr_score: 0.06569345288680803
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6160000562667847, scr_score: 0.09489059139987667
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6130000352859497, scr_score: 0.08759127051574404
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6420000195503235, scr_score: 0.15815084064299922
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8210000395774841, scr_score: -0.10769230063850332
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8190000653266907, scr_score: -0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8420000672340393, scr_score: 0.05384637956789382
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.831000030040741, scr_score: -0.030769294253470136
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8190000653266907, scr_score: -0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7760000228881836, scr_score: -0.45384628786843695
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6790000200271606, scr_score: 0.042424232572236306
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.690000057220459, scr_score: 0.07575767427763695
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7000000476837158, scr_score: 0.10606067174064686
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7150000333786011, scr_score: 0.1515151679351617
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7330000400543213, scr_score: 0.2060606356166244
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7300000190734863, scr_score: 0.19696966412967656
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7200000286102295, scr_score: 0.16666666666666666
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.843000054359436, scr_score: 0.15028911691775837
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8470000624656677, scr_score: 0.17341054602321687
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.784000039100647, scr_score: -0.19075135945072783
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7820000648498535, scr_score: -0.2023119017357351
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7940000295639038, scr_score: -0.1329479589548035
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8020000457763672, scr_score: -0.08670510074388647
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7990000247955322, scr_score: -0.10404625870684134
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7560000419616699, scr_score: 0.2941176058099336
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7710000276565552, scr_score: 0.3529410802231785
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7740000486373901, scr_score: 0.3647058686033112
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7770000100135803, scr_score: 0.37647042323973456
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7820000648498535, scr_score: 0.39607848178785887
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7550000548362732, scr_score: 0.29019608759779253
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6950000524520874, scr_score: 0.05490195620110374
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8270000219345093, scr_score: 0.17204299352392502
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8230000138282776, scr_score: 0.15053757927656003
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8240000605583191, scr_score: 0.1559141731796474
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8280000686645508, scr_score: 0.17741958742701236
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8220000267028809, scr_score: 0.14516130582846748
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8060000538825989, scr_score: 0.0591399692940024
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.7850000262260437, scr_score: -0.05376337539091504
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7230000495910645, scr_score: 0.07936522954053103
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7350000143051147, scr_score: 0.12698413073851328
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7330000400543213, scr_score: 0.11904772604762837
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7420000433921814, scr_score: 0.15476202020928323
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7400000095367432, scr_score: 0.14682537899206194
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7280000448226929, scr_score: 0.09920647779407969
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7120000123977661, scr_score: 0.03571429416165487
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8670000433921814, scr_score: 0.15555570272746339
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8410000205039978, scr_score: -0.037036987979734426
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8360000252723694, scr_score: -0.07407397595946885
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8470000624656677, scr_score: 0.0074077508085256686
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8280000686645508, scr_score: -0.13333289181760985
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7890000343322754, scr_score: -0.42222192787840657
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7870000600814819, scr_score: -0.43703654646401097
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.659000039100647, scr_score: 0.022950914501340715
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6650000214576721, scr_score: 0.042622987341732574
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6800000071525574, scr_score: 0.09180326715524234
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6840000152587891, scr_score: 0.104918047523857
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.675000011920929, scr_score: 0.07540984055073909
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6790000200271606, scr_score: 0.08852462091935374
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7010000348091125, scr_score: 0.16065581523420422
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7910000085830688, scr_score: 0.09189167419779398
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7730000615119934, scr_score: -0.0054053357432940746
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7750000357627869, scr_score: 0.0054053357432940746
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7820000648498535, scr_score: 0.04324333032088241
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7870000600814819, scr_score: 0.07027033122461769
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7920000553131104, scr_score: 0.09729733212835297
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7850000262260437, scr_score: 0.05945933755076463
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7320000529289246, scr_score: 0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7320000529289246, scr_score: 0.03361350851419667
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7330000400543213, scr_score: 0.03781513446855956
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7170000076293945, scr_score: -0.029411882559833787
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7140000462532043, scr_score: -0.04201676042292244
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7110000252723694, scr_score: -0.054621888725657895
Step 0: {'loss': 597.160888671875, 'l1_loss': 913.6357421875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 377.8468933105469, 'l1_loss': 746.327392578125, 'l2_loss': 347.9938049316406}
Step 200: {'loss': 307.076416015625, 'l1_loss': 563.2680053710938, 'l2_loss': 284.5456848144531}
Step 300: {'loss': 258.0362548828125, 'l1_loss': 411.30169677734375, 'l2_loss': 241.5841827392578}
Step 400: {'loss': 224.9102020263672, 'l1_loss': 345.99169921875, 'l2_loss': 211.07054138183594}
Step 500: {'loss': 207.55010986328125, 'l1_loss': 320.83477783203125, 'l2_loss': 194.7167205810547}
Step 600: {'loss': 196.16014099121094, 'l1_loss': 317.9692687988281, 'l2_loss': 183.44137573242188}
Step 700: {'loss': 182.84193420410156, 'l1_loss': 322.3741455078125, 'l2_loss': 169.9469757080078}
Step 800: {'loss': 174.23361206054688, 'l1_loss': 345.2910461425781, 'l2_loss': 160.42196655273438}
Step 900: {'loss': 164.30447387695312, 'l1_loss': 356.7428894042969, 'l2_loss': 150.03475952148438}
Step 1000: {'loss': 157.2150421142578, 'l1_loss': 408.74981689453125, 'l2_loss': 140.86505126953125}
Step 1100: {'loss': 148.13812255859375, 'l1_loss': 421.5376281738281, 'l2_loss': 131.276611328125}
Step 1200: {'loss': 140.03875732421875, 'l1_loss': 429.8335266113281, 'l2_loss': 122.84541320800781}
Step 1300: {'loss': 137.4354705810547, 'l1_loss': 467.31640625, 'l2_loss': 118.74281311035156}
Step 1400: {'loss': 130.82101440429688, 'l1_loss': 491.75799560546875, 'l2_loss': 111.15069580078125}
Step 1500: {'loss': 127.9563980102539, 'l1_loss': 501.9186096191406, 'l2_loss': 107.87965393066406}
Step 1600: {'loss': 124.3241958618164, 'l1_loss': 522.1914672851562, 'l2_loss': 103.43653869628906}
Step 1700: {'loss': 121.18775939941406, 'l1_loss': 530.361328125, 'l2_loss': 99.97330474853516}
Step 1800: {'loss': 118.27037811279297, 'l1_loss': 553.72412109375, 'l2_loss': 96.12141418457031}
Step 1900: {'loss': 115.98384094238281, 'l1_loss': 561.5540161132812, 'l2_loss': 93.52168273925781}
Step 2000: {'loss': 113.59794616699219, 'l1_loss': 573.78369140625, 'l2_loss': 90.64659881591797}
Step 2100: {'loss': 112.1446533203125, 'l1_loss': 583.357177734375, 'l2_loss': 88.81037139892578}
Step 2200: {'loss': 109.63749694824219, 'l1_loss': 581.3701171875, 'l2_loss': 86.3826904296875}
Step 2300: {'loss': 108.2988052368164, 'l1_loss': 594.81103515625, 'l2_loss': 84.50636291503906}
Step 2400: {'loss': 107.67127990722656, 'l1_loss': 600.615234375, 'l2_loss': 83.64666748046875}
Step 2500: {'loss': 106.03536224365234, 'l1_loss': 603.7525024414062, 'l2_loss': 81.88526153564453}
Step 2600: {'loss': 104.85446166992188, 'l1_loss': 614.2295532226562, 'l2_loss': 80.2852783203125}
Step 2700: {'loss': 103.93006896972656, 'l1_loss': 619.1268310546875, 'l2_loss': 79.16500091552734}
Step 2800: {'loss': 103.41029357910156, 'l1_loss': 624.3048095703125, 'l2_loss': 78.43810272216797}
Step 2900: {'loss': 101.9081039428711, 'l1_loss': 624.9337768554688, 'l2_loss': 76.91075134277344}
Step 3000: {'loss': 102.06578063964844, 'l1_loss': 634.4749755859375, 'l2_loss': 76.68678283691406}
Step 3100: {'loss': 100.40158081054688, 'l1_loss': 635.7836303710938, 'l2_loss': 74.9702377319336}
Step 3200: {'loss': 100.66848754882812, 'l1_loss': 638.5234375, 'l2_loss': 75.12754821777344}
Step 3300: {'loss': 99.32019805908203, 'l1_loss': 644.521484375, 'l2_loss': 73.53933715820312}
Step 3400: {'loss': 98.95404815673828, 'l1_loss': 653.49560546875, 'l2_loss': 72.81422424316406}
Step 3500: {'loss': 97.19058227539062, 'l1_loss': 649.7689208984375, 'l2_loss': 71.1998291015625}
Step 3600: {'loss': 97.80684661865234, 'l1_loss': 664.0097045898438, 'l2_loss': 71.2464599609375}
Step 3700: {'loss': 97.18849182128906, 'l1_loss': 669.2398681640625, 'l2_loss': 70.41889953613281}
Step 3800: {'loss': 95.7933120727539, 'l1_loss': 669.620361328125, 'l2_loss': 69.00849914550781}
Step 3900: {'loss': 95.92601013183594, 'l1_loss': 675.5804443359375, 'l2_loss': 68.90279388427734}
Step 4000: {'loss': 95.36917114257812, 'l1_loss': 679.5821533203125, 'l2_loss': 68.18588256835938}
Step 4100: {'loss': 94.79115295410156, 'l1_loss': 685.7485961914062, 'l2_loss': 67.3612060546875}
Step 4200: {'loss': 93.90491485595703, 'l1_loss': 694.2946166992188, 'l2_loss': 66.13313293457031}
Step 4300: {'loss': 93.95408630371094, 'l1_loss': 699.0673828125, 'l2_loss': 65.99139404296875}
Step 4400: {'loss': 92.6187515258789, 'l1_loss': 702.4705810546875, 'l2_loss': 64.51992797851562}
Step 4500: {'loss': 92.35694122314453, 'l1_loss': 708.6847534179688, 'l2_loss': 64.00955200195312}
Step 4600: {'loss': 92.04512786865234, 'l1_loss': 713.029296875, 'l2_loss': 63.523956298828125}
Step 4700: {'loss': 91.7179183959961, 'l1_loss': 714.92333984375, 'l2_loss': 63.12098693847656}
Step 4800: {'loss': 91.10625457763672, 'l1_loss': 720.598388671875, 'l2_loss': 62.282318115234375}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_164702_grouped_sparse_sae/run_3/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_3/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8960000276565552, scr_score: -0.0298512641082919
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.9000000357627869, scr_score: 0.029850374486354535
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.878000020980835, scr_score: -0.29850819297323217
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8520000576972961, scr_score: -0.6865675094055279
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.6910000443458557, scr_score: -3.0895537923248755
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.6240000128746033, scr_score: -4.089554681946813
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6160000562667847, scr_score: 0.050505149329191916
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6290000081062317, scr_score: 0.08333330824720513
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7050000429153442, scr_score: 0.27525257466459596
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6450000405311584, scr_score: 0.12373742771055866
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.5610000491142273, scr_score: -0.08838374792173971
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.5130000114440918, scr_score: -0.20959595579503107
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8320000171661377, scr_score: -0.11818190684987326
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8320000171661377, scr_score: -0.11818190684987326
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.800000011920929, scr_score: -0.4090910076109703
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7850000262260437, scr_score: -0.5454544961945148
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7890000343322754, scr_score: -0.5090907908668356
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.527999997138977, scr_score: -2.8818180931501267
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.64000004529953, scr_score: -0.026392970078295327
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6380000114440918, scr_score: -0.032258171647732724
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6540000438690186, scr_score: 0.014662741733089717
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6530000567436218, scr_score: 0.011730228345205611
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6800000071525574, scr_score: 0.09090896378642241
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7010000348091125, scr_score: 0.15249261890033458
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6380000114440918, scr_score: -0.032258171647732724
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9150000214576721, scr_score: -0.018182665454344472
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.909000039100647, scr_score: -0.1272732395770455
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9180000424385071, scr_score: 0.03636316346734262
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9110000729560852, scr_score: -0.09090899238902972
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8250000476837158, scr_score: -1.6545456121775524
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.7640000581741333, scr_score: -2.7636361863002534
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8010000586509705, scr_score: -2.09090899238903
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.581000030040741, scr_score: 0.0024569707924063976
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5870000123977661, scr_score: 0.01719894199560156
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5940000414848328, scr_score: 0.03439803043995989
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6160000562667847, scr_score: 0.08845212011668452
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6160000562667847, scr_score: 0.08845212011668452
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6570000052452087, scr_score: 0.189189094195401
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6200000047683716, scr_score: 0.0982800032863101
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8290000557899475, scr_score: -0.0852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8520000576972961, scr_score: 0.09302343848546918
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.843000054359436, scr_score: 0.023255975134237508
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7760000228881836, scr_score: -0.4961240811528739
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7290000319480896, scr_score: -0.8604650732975366
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.6660000085830688, scr_score: -1.3488373167561583
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.5, scr_score: -2.6356590078553372
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7040000557899475, scr_score: 0.1313433568762757
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7240000367164612, scr_score: 0.19104478567951422
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7290000319480896, scr_score: 0.20597014288032386
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7410000562667847, scr_score: 0.24179110691684247
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.76500004529953, scr_score: 0.31343285706558716
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8070000410079956, scr_score: 0.4388059643069635
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8530000448226929, scr_score: 0.5761193928938461
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8210000395774841, scr_score: -0.018072414612760133
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.800000011920929, scr_score: -0.1445785987738679
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7540000081062317, scr_score: -0.4216870411609548
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7380000352859497, scr_score: -0.5180724146127601
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.6580000519752502, scr_score: -1.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.6350000500679016, scr_score: -1.1385542211935435
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6730000376701355, scr_score: 0.003649588194973327
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6830000281333923, scr_score: 0.04014590521512383
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6990000605583191, scr_score: 0.09854018647553153
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7100000381469727, scr_score: 0.13868609169065535
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.753000020980835, scr_score: 0.295620341891386
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8220000267028809, scr_score: 0.5474451903726748
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.843000054359436, scr_score: 0.6240876301431577
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.812000036239624, scr_score: 0.017647180507740362
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8230000138282776, scr_score: 0.08235292467691656
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8330000638961792, scr_score: 0.14117674283087675
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8070000410079956, scr_score: -0.011764553261478189
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8280000686645508, scr_score: 0.11176500906165819
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.7610000371932983, scr_score: -0.2823527844307073
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8300000429153442, scr_score: 0.12352956232313639
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6980000138282776, scr_score: 0.03383459489013476
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6950000524520874, scr_score: 0.02255654597847959
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7160000205039978, scr_score: 0.10150378467040429
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.14285730291256044
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7630000114440918, scr_score: 0.27819545839551485
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7880000472068787, scr_score: 0.37218054379148235
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6080000400543213, scr_score: -0.3045111299336282
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8750000596046448, scr_score: 0.05357178019306575
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8750000596046448, scr_score: 0.05357178019306575
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8800000548362732, scr_score: 0.0982145755645542
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8700000643730164, scr_score: 0.008928984821577298
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8980000615119934, scr_score: 0.2589288517755524
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.7000000476837158, scr_score: -1.5089276543613286
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6550000309944153, scr_score: 0.07902741179102869
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6670000553131104, scr_score: 0.11550164365668093
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6940000057220459, scr_score: 0.1975683483084694
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6960000395774841, scr_score: 0.20364747753729592
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.737000048160553, scr_score: 0.32826754328356317
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7610000371932983, scr_score: 0.4012158258457653
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.737000048160553, scr_score: 0.32826754328356317
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.753000020980835, scr_score: 0.0
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7570000290870667, scr_score: 0.01923080780625969
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7500000596046448, scr_score: -0.014422890934105061
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7640000581741333, scr_score: 0.05288479310741072
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7690000534057617, scr_score: 0.07692323122503876
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7550000548362732, scr_score: 0.009615547183522984
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.5180000066757202, scr_score: -1.129807737771663
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7490000128746033, scr_score: 0.02752285551095536
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7540000081062317, scr_score: 0.05045861400605288
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7600000500679016, scr_score: 0.07798174293281654
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7570000290870667, scr_score: 0.06422017846943472
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7450000643730164, scr_score: 0.009174467447523987
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7360000610351562, scr_score: -0.0321099525268132
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.8700000643730164, scr_score: 0.5825689766565786
Step 0: {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 605.5480346679688, 'l1_loss': 6244.248046875, 'l2_loss': 355.77813720703125}
Step 200: {'loss': 429.0907287597656, 'l1_loss': 2760.68212890625, 'l2_loss': 318.6634521484375}
Step 300: {'loss': 358.4563903808594, 'l1_loss': 1215.6123046875, 'l2_loss': 309.8319091796875}
Step 400: {'loss': 333.53631591796875, 'l1_loss': 803.9303588867188, 'l2_loss': 301.37908935546875}
Step 500: {'loss': 307.1258850097656, 'l1_loss': 876.6446533203125, 'l2_loss': 272.0600891113281}
Step 600: {'loss': 285.12353515625, 'l1_loss': 905.6270751953125, 'l2_loss': 248.89846801757812}
Step 700: {'loss': 265.24456787109375, 'l1_loss': 892.343017578125, 'l2_loss': 229.55078125}
Step 800: {'loss': 256.65087890625, 'l1_loss': 874.7777709960938, 'l2_loss': 221.65977478027344}
Step 900: {'loss': 246.30950927734375, 'l1_loss': 713.0436401367188, 'l2_loss': 217.7877197265625}
Step 1000: {'loss': 244.75120544433594, 'l1_loss': 747.5176391601562, 'l2_loss': 214.85049438476562}
Step 1100: {'loss': 237.0272979736328, 'l1_loss': 617.312744140625, 'l2_loss': 212.3347930908203}
Step 1200: {'loss': 229.90902709960938, 'l1_loss': 511.9720458984375, 'l2_loss': 209.43011474609375}
Step 1300: {'loss': 231.42291259765625, 'l1_loss': 554.5096435546875, 'l2_loss': 209.24252319335938}
Step 1400: {'loss': 227.8118438720703, 'l1_loss': 561.5390625, 'l2_loss': 205.35028076171875}
Step 1500: {'loss': 226.0447235107422, 'l1_loss': 512.2708740234375, 'l2_loss': 205.55389404296875}
Step 1600: {'loss': 224.96054077148438, 'l1_loss': 514.0197143554688, 'l2_loss': 204.39974975585938}
Step 1700: {'loss': 222.37603759765625, 'l1_loss': 493.6068420410156, 'l2_loss': 202.63174438476562}
Step 1800: {'loss': 222.13818359375, 'l1_loss': 523.8748779296875, 'l2_loss': 201.18319702148438}
Step 1900: {'loss': 220.47201538085938, 'l1_loss': 507.98492431640625, 'l2_loss': 200.15260314941406}
Step 2000: {'loss': 219.24545288085938, 'l1_loss': 510.42291259765625, 'l2_loss': 198.82852172851562}
Step 2100: {'loss': 219.15286254882812, 'l1_loss': 495.96087646484375, 'l2_loss': 199.3144073486328}
Step 2200: {'loss': 214.84751892089844, 'l1_loss': 457.4938049316406, 'l2_loss': 196.54769897460938}
Step 2300: {'loss': 215.5487823486328, 'l1_loss': 514.7645263671875, 'l2_loss': 194.9571533203125}
Step 2400: {'loss': 215.6943817138672, 'l1_loss': 508.2388610839844, 'l2_loss': 195.36480712890625}
Step 2500: {'loss': 213.1907958984375, 'l1_loss': 521.1251831054688, 'l2_loss': 192.34576416015625}
Step 2600: {'loss': 213.45042419433594, 'l1_loss': 552.80322265625, 'l2_loss': 191.33824157714844}
Step 2700: {'loss': 212.9765625, 'l1_loss': 524.4588623046875, 'l2_loss': 191.99818420410156}
Step 2800: {'loss': 211.5472412109375, 'l1_loss': 502.95477294921875, 'l2_loss': 191.4269561767578}
Step 2900: {'loss': 209.34930419921875, 'l1_loss': 487.54486083984375, 'l2_loss': 189.84622192382812}
Step 3000: {'loss': 211.1261749267578, 'l1_loss': 531.4461669921875, 'l2_loss': 189.86825561523438}
Step 3100: {'loss': 208.69346618652344, 'l1_loss': 521.8773193359375, 'l2_loss': 187.81417846679688}
Step 3200: {'loss': 208.46095275878906, 'l1_loss': 499.2996826171875, 'l2_loss': 188.48776245117188}
Step 3300: {'loss': 207.49168395996094, 'l1_loss': 500.25469970703125, 'l2_loss': 187.479736328125}
Step 3400: {'loss': 208.4571075439453, 'l1_loss': 528.9713745117188, 'l2_loss': 187.29600524902344}
Step 3500: {'loss': 204.78387451171875, 'l1_loss': 493.07427978515625, 'l2_loss': 185.0606689453125}
Step 3600: {'loss': 207.27725219726562, 'l1_loss': 521.8899536132812, 'l2_loss': 186.36647033691406}
Step 3700: {'loss': 205.98167419433594, 'l1_loss': 506.65277099609375, 'l2_loss': 185.70877075195312}
Step 3800: {'loss': 204.0693817138672, 'l1_loss': 518.7972412109375, 'l2_loss': 183.3170166015625}
Step 3900: {'loss': 204.47677612304688, 'l1_loss': 510.5101318359375, 'l2_loss': 184.05352783203125}
Step 4000: {'loss': 203.72610473632812, 'l1_loss': 494.6702880859375, 'l2_loss': 183.9350128173828}
Step 4100: {'loss': 203.519287109375, 'l1_loss': 495.521728515625, 'l2_loss': 183.69772338867188}
Step 4200: {'loss': 203.2821807861328, 'l1_loss': 520.64013671875, 'l2_loss': 182.4243927001953}
Step 4300: {'loss': 203.27886962890625, 'l1_loss': 514.65185546875, 'l2_loss': 182.69187927246094}
Step 4400: {'loss': 201.5397186279297, 'l1_loss': 504.06903076171875, 'l2_loss': 181.33737182617188}
Step 4500: {'loss': 202.25128173828125, 'l1_loss': 512.0927734375, 'l2_loss': 181.763916015625}
Step 4600: {'loss': 201.48756408691406, 'l1_loss': 511.41485595703125, 'l2_loss': 181.02691650390625}
Step 4700: {'loss': 200.66104125976562, 'l1_loss': 529.978515625, 'l2_loss': 179.43646240234375}
Step 4800: {'loss': 200.89512634277344, 'l1_loss': 517.0694580078125, 'l2_loss': 180.1936492919922}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_6/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_6/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.893000066280365, scr_score: -0.0746268258378237
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -1.2537326312437003
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7900000214576721, scr_score: -1.6119415731896416
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7770000100135803, scr_score: -1.8059716762167581
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6390000581741333, scr_score: 0.1085859581701857
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6320000290870667, scr_score: 0.09090911827577622
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6530000567436218, scr_score: 0.14393948744223542
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6850000619888306, scr_score: 0.22474757585217325
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6950000524520874, scr_score: 0.25000007525838464
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.703000009059906, scr_score: 0.27020198447329213
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7080000042915344, scr_score: 0.28282823417639785
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8350000381469727, scr_score: -0.0892856382593739
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8350000381469727, scr_score: -0.0892856382593739
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8280000686645508, scr_score: -0.15178537216718252
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8230000138282776, scr_score: -0.1964287234812522
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8290000557899475, scr_score: -0.14285691477812168
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8040000200271606, scr_score: -0.36607154261093916
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6520000100135803, scr_score: 0.01457724326302535
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6640000343322754, scr_score: 0.04956273135895441
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6770000457763672, scr_score: 0.08746363335259913
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6970000267028809, scr_score: 0.14577260640470052
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6860000491142273, scr_score: 0.11370270598093417
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6850000619888306, scr_score: 0.1107872920832185
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6820000410079956, scr_score: 0.10204087661562448
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9190000295639038, scr_score: 0.0
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9000000357627869, scr_score: -0.36538417452224625
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9000000357627869, scr_score: -0.36538417452224625
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8970000147819519, scr_score: -0.42307683490444925
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.878000020980835, scr_score: -0.7884610094266955
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.593000054359436, scr_score: 0.038929518017424856
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5990000367164612, scr_score: 0.05352801476222823
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6000000238418579, scr_score: 0.05596107338245181
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6100000143051147, scr_score: 0.0802919496316114
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6100000143051147, scr_score: 0.0802919496316114
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6110000014305115, scr_score: 0.08272500825183499
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6310000419616699, scr_score: 0.13138690577361606
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8290000557899475, scr_score: -0.046153712131563045
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.815000057220459, scr_score: -0.15384601277006638
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8370000123977661, scr_score: 0.015384417878092908
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8510000109672546, scr_score: 0.12307671851659623
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8380000591278076, scr_score: 0.023077085314423682
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8230000138282776, scr_score: -0.09230788276041041
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7770000100135803, scr_score: -0.4461540789293905
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6830000281333923, scr_score: 0.054545467681462706
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6910000443458557, scr_score: 0.0787879378999155
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7020000219345093, scr_score: 0.11212119898520397Step 0: {'loss': 615.43359375, 'l1_loss': 1370.45361328125, 'l2_loss': 560.615478515625}
Step 100: {'loss': 392.25018310546875, 'l1_loss': 1087.0364990234375, 'l2_loss': 348.76873779296875}
Step 200: {'loss': 316.3326416015625, 'l1_loss': 755.8297729492188, 'l2_loss': 286.0994567871094}
Step 300: {'loss': 264.4372863769531, 'l1_loss': 530.2420043945312, 'l2_loss': 243.22760009765625}
Step 400: {'loss': 231.1903533935547, 'l1_loss': 437.2378234863281, 'l2_loss': 213.70083618164062}
Step 500: {'loss': 214.07728576660156, 'l1_loss': 396.2789001464844, 'l2_loss': 198.22613525390625}
Step 600: {'loss': 203.40194702148438, 'l1_loss': 378.74591064453125, 'l2_loss': 188.25210571289062}
Step 700: {'loss': 190.72238159179688, 'l1_loss': 368.8122863769531, 'l2_loss': 175.9698944091797}
Step 800: {'loss': 183.31979370117188, 'l1_loss': 382.30267333984375, 'l2_loss': 168.02769470214844}
Step 900: {'loss': 174.2516632080078, 'l1_loss': 378.390869140625, 'l2_loss': 159.11602783203125}
Step 1000: {'loss': 168.3650360107422, 'l1_loss': 424.95703125, 'l2_loss': 151.36676025390625}
Step 1100: {'loss': 160.09129333496094, 'l1_loss': 430.4610595703125, 'l2_loss': 142.8728485107422}
Step 1200: {'loss': 152.49681091308594, 'l1_loss': 432.01104736328125, 'l2_loss': 135.21636962890625}
Step 1300: {'loss': 150.75257873535156, 'l1_loss': 468.5841064453125, 'l2_loss': 132.00921630859375}
Step 1400: {'loss': 144.68540954589844, 'l1_loss': 491.1799011230469, 'l2_loss': 125.03821563720703}
Step 1500: {'loss': 142.28111267089844, 'l1_loss': 499.19256591796875, 'l2_loss': 122.31341552734375}
Step 1600: {'loss': 138.98541259765625, 'l1_loss': 519.943359375, 'l2_loss': 118.18768310546875}
Step 1700: {'loss': 135.85440063476562, 'l1_loss': 531.0908203125, 'l2_loss': 114.61076354980469}
Step 1800: {'loss': 133.14588928222656, 'l1_loss': 555.359619140625, 'l2_loss': 110.93151092529297}
Step 1900: {'loss': 130.92596435546875, 'l1_loss': 561.5213012695312, 'l2_loss': 108.46511840820312}
Step 2000: {'loss': 128.84280395507812, 'l1_loss': 570.2357788085938, 'l2_loss': 106.03337097167969}
Step 2100: {'loss': 127.6501235961914, 'l1_loss': 579.09375, 'l2_loss': 104.48637390136719}
Step 2200: {'loss': 124.86038970947266, 'l1_loss': 577.250244140625, 'l2_loss': 101.77037811279297}
Step 2300: {'loss': 123.5218505859375, 'l1_loss': 595.2889404296875, 'l2_loss': 99.71029663085938}
Step 2400: {'loss': 122.81529235839844, 'l1_loss': 602.7987670898438, 'l2_loss': 98.70333862304688}
Step 2500: {'loss': 120.88209533691406, 'l1_loss': 607.6122436523438, 'l2_loss': 96.57760620117188}
Step 2600: {'loss': 119.67469024658203, 'l1_loss': 619.4580078125, 'l2_loss': 94.89636993408203}
Step 2700: {'loss': 118.82674407958984, 'l1_loss': 625.6024169921875, 'l2_loss': 93.80265045166016}
Step 2800: {'loss': 118.28820037841797, 'l1_loss': 631.5477294921875, 'l2_loss': 93.02629089355469}
Step 2900: {'loss': 116.51492309570312, 'l1_loss': 633.83642578125, 'l2_loss': 91.16146850585938}
Step 3000: {'loss': 116.58122253417969, 'l1_loss': 648.2689208984375, 'l2_loss': 90.65046691894531}
Step 3100: {'loss': 114.833984375, 'l1_loss': 651.8302001953125, 'l2_loss': 88.76077270507812}
Step 3200: {'loss': 114.9444351196289, 'l1_loss': 657.8286743164062, 'l2_loss': 88.63128662109375}
Step 3300: {'loss': 113.65830993652344, 'l1_loss': 664.0972900390625, 'l2_loss': 87.09442138671875}
Step 3400: {'loss': 113.35246276855469, 'l1_loss': 673.0546264648438, 'l2_loss': 86.43028259277344}
Step 3500: {'loss': 111.39855194091797, 'l1_loss': 670.4053955078125, 'l2_loss': 84.58233642578125}
Step 3600: {'loss': 112.29045867919922, 'l1_loss': 682.9175415039062, 'l2_loss': 84.9737548828125}
Step 3700: {'loss': 111.64894104003906, 'l1_loss': 687.7463989257812, 'l2_loss': 84.13908386230469}
Step 3800: {'loss': 110.1197509765625, 'l1_loss': 687.9979858398438, 'l2_loss': 82.5998306274414}
Step 3900: {'loss': 110.4918212890625, 'l1_loss': 696.2981567382812, 'l2_loss': 82.63990020751953}
Step 4000: {'loss': 110.00532531738281, 'l1_loss': 700.8639526367188, 'l2_loss': 81.97076416015625}
Step 4100: {'loss': 109.35750579833984, 'l1_loss': 707.0808715820312, 'l2_loss': 81.07427215576172}
Step 4200: {'loss': 108.55156707763672, 'l1_loss': 715.7968139648438, 'l2_loss': 79.91969299316406}
Step 4300: {'loss': 108.71139526367188, 'l1_loss': 719.866455078125, 'l2_loss': 79.91674041748047}
Step 4400: {'loss': 107.34503936767578, 'l1_loss': 720.63232421875, 'l2_loss': 78.51974487304688}
Step 4500: {'loss': 107.25286102294922, 'l1_loss': 725.5665283203125, 'l2_loss': 78.2302017211914}
Step 4600: {'loss': 107.07138061523438, 'l1_loss': 730.0866088867188, 'l2_loss': 77.867919921875}
Step 4700: {'loss': 106.62873077392578, 'l1_loss': 729.530029296875, 'l2_loss': 77.4475326538086}
Step 4800: {'loss': 106.146484375, 'l1_loss': 734.6829223632812, 'l2_loss': 76.75917053222656}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_164702_grouped_sparse_sae/run_4/final_info.json

Running absorption evaluation...

dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7260000109672546, scr_score: 0.18484842902045018
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7220000624656677, scr_score: 0.17272737453133596
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7360000610351562, scr_score: 0.21515160710357226
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7210000157356262, scr_score: 0.16969693028894522
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8550000190734863, scr_score: 0.21965305969868998
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8560000658035278, scr_score: 0.22543367537663753
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7880000472068787, scr_score: -0.1676299303452693
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7920000553131104, scr_score: -0.1445085012398108
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8020000457763672, scr_score: -0.08670510074388647
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.7950000166893005, scr_score: -0.12716768781229984
dir: 1, original_acc: 0.8170000314712524, clean_acc: 0.9900000691413879, changed_acc: 0.8090000152587891, scr_score: -0.04624285821091702
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7500000596046448, scr_score: 0.27058826279337755
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7640000581741333, scr_score: 0.32549021899448133
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7680000066757202, scr_score: 0.34117629184304576
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7730000615119934, scr_score: 0.36078435039117007
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7880000472068787, scr_score: 0.41960782480441494
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.7760000228881836, scr_score: 0.37254890502759347
dir: 2, original_acc: 0.6810000538825989, clean_acc: 0.9360000491142273, changed_acc: 0.6840000152587891, scr_score: 0.011764554636423348
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8220000267028809, scr_score: 0.14516130582846748
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8280000686645508, scr_score: 0.17741958742701236
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8260000348091125, scr_score: 0.16666672007583247
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8200000524520874, scr_score: 0.1344087589322824
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8250000476837158, scr_score: 0.16129044662773992
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.8020000457763672, scr_score: 0.037634555046637416
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.9810000658035278, changed_acc: 0.784000039100647, scr_score: -0.059139648839007575
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7270000576972961, scr_score: 0.09523827544863724
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7350000143051147, scr_score: 0.12698413073851328
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7450000643730164, scr_score: 0.16666686377194698
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7640000581741333, scr_score: 0.2420636544406992
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7510000467300415, scr_score: 0.1904763143709381
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7280000448226929, scr_score: 0.09920647779407969
dir: 2, original_acc: 0.703000009059906, clean_acc: 0.9550000429153442, changed_acc: 0.7140000462532043, scr_score: 0.04365093537887616
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8750000596046448, scr_score: 0.21481506010132786
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8460000157356262, scr_score: 0.0
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8440000414848328, scr_score: -0.014814618585604379
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8520000576972961, scr_score: 0.044444738788260094
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.8380000591278076, scr_score: -0.059258915858141
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.7980000376701355, scr_score: -0.3555552612117399
dir: 1, original_acc: 0.8460000157356262, clean_acc: 0.9810000658035278, changed_acc: 0.784000039100647, scr_score: -0.459258915858141
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.659000039100647, scr_score: 0.022950914501340715
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6760000586509705, scr_score: 0.07868868221168794
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6810000538825989, scr_score: 0.0950821088161912
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6950000524520874, scr_score: 0.14098374239381237
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.706000030040741, scr_score: 0.1770492418387075
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.6790000200271606, scr_score: 0.08852462091935374
dir: 2, original_acc: 0.6520000100135803, clean_acc: 0.9570000171661377, changed_acc: 0.7040000557899475, scr_score: 0.1704919493669303
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7930000424385071, scr_score: 0.10270266787164704
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7750000357627869, scr_score: 0.0054053357432940746
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7790000438690186, scr_score: 0.027027000903735277
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7860000133514404, scr_score: 0.0648646732940587
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7910000085830688, scr_score: 0.09189167419779398
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7890000343322754, scr_score: 0.08108100271120583
dir: 1, original_acc: 0.7740000486373901, clean_acc: 0.9590000510215759, changed_acc: 0.7880000472068787, scr_score: 0.07567566696791175
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7280000448226929, scr_score: 0.016806754257098336
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7310000061988831, scr_score: 0.02941163212018699
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7350000143051147, scr_score: 0.04621838637728532
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7220000624656677, scr_score: -0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7300000190734863, scr_score: 0.025210006165824105
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7220000624656677, scr_score: -0.008403251908725769
dir: 2, original_acc: 0.7240000367164612, clean_acc: 0.9620000720024109, changed_acc: 0.7080000042915344, scr_score: -0.06722701702839334
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_4/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8990000486373901, scr_score: 0.014925187243177268
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.89000004529953, scr_score: -0.11940327718929286
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8730000257492065, scr_score: -0.3731350188110559
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8550000190734863, scr_score: -0.6417919476759961
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7240000367164612, scr_score: -2.597016385946464
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8420000672340393, scr_score: -0.8358211610811753
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6070000529289246, scr_score: 0.027777869760247858
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6130000352859497, scr_score: 0.042929339300620824
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6240000128746033, scr_score: 0.07070705854409946
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.687000036239624, scr_score: 0.22979801552670784
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6650000214576721, scr_score: 0.17424242652298136
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.5980000495910645, scr_score: 0.005050590191303803
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.5040000081062317, scr_score: -0.23232323536397514
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: 0.0363637053276792
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8280000686645508, scr_score: -0.15454507031721587
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8370000123977661, scr_score: -0.0727274106553584
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7750000357627869, scr_score: -0.6363634885835445
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7490000128746033, scr_score: -0.8727273022832911
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7940000295639038, scr_score: -0.4636362946723208
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7400000095367432, scr_score: -0.9545455038054852
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6340000033378601, scr_score: -0.04398839999293833
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6360000371932983, scr_score: -0.03812319842350094
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.659000039100647, scr_score: 0.029325483466179435
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6600000262260437, scr_score: 0.03225799685406354
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7110000252723694, scr_score: 0.181818102366514
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7300000190734863, scr_score: 0.23753655591098877
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.6050000190734863, scr_score: -0.12903233700359254
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9170000553131104, scr_score: 0.01818158173367131
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9140000343322754, scr_score: -0.036364247188015786
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.893000066280365, scr_score: -0.41818179847780595
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.878000020980835, scr_score: -0.6909098593655683
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8770000338554382, scr_score: -0.7090914410992396
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8840000629425049, scr_score: -0.5818182015221941
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8630000352859497, scr_score: -0.9636368365326574
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.5900000333786011, scr_score: 0.024570000821577526
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.581000030040741, scr_score: 0.0024569707924063976
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6010000109672546, scr_score: 0.051596972435561446
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6070000529289246, scr_score: 0.06633909008751339
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6130000352859497, scr_score: 0.08108106129070855
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6430000066757202, scr_score: 0.15479106375544113
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6910000443458557, scr_score: 0.2727272727272727
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8300000429153442, scr_score: -0.07751930104548388
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8510000109672546, scr_score: 0.0852711387397361
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8270000219345093, scr_score: -0.10077527617972139
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8020000457763672, scr_score: -0.2945735287934311
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7970000505447388, scr_score: -0.33333317931617307
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7390000224113464, scr_score: -0.7829457722520528
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.5020000338554382, scr_score: -2.620154870415352
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7110000252723694, scr_score: 0.1522387857876922
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7210000157356262, scr_score: 0.18208950018931144
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7330000400543213, scr_score: 0.21791046422583005
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7600000500679016, scr_score: 0.29850749986477754
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8070000410079956, scr_score: 0.4388059643069635
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8210000395774841, scr_score: 0.480597000054089
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8800000548362732, scr_score: 0.6567164285327936
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8230000138282776, scr_score: -0.006024377580324436
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8300000429153442, scr_score: 0.03614447016141368
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7830000519752502, scr_score: -0.24698799074189107
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7440000176429749, scr_score: -0.48192794445134646
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7590000629425049, scr_score: -0.3915662304516524
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7600000500679016, scr_score: -0.38554221193543453
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.6740000247955322, scr_score: 0.007299176389946654
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7000000476837158, scr_score: 0.10218977467050486
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.718000054359436, scr_score: 0.16788323232085922
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7310000061988831, scr_score: 0.2153283139259297
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7760000228881836, scr_score: 0.3795619580518156
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.815000057220459, scr_score: 0.5218978554726529
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.7910000085830688, scr_score: 0.43430643358204135
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8210000395774841, scr_score: 0.07058837141543838
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8300000429153442, scr_score: 0.12352956232313639
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8220000267028809, scr_score: 0.07647064804617747
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8350000381469727, scr_score: 0.15294129609235493
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8490000367164612, scr_score: 0.2352942207692715
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8010000586509705, scr_score: -0.047058563661435834
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.9100000262260437, scr_score: 0.5941176532461563
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.687000036239624, scr_score: -0.007518699274436781
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6910000443458557, scr_score: 0.007518923352021404
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6930000185966492, scr_score: 0.015037622626458184
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7300000190734863, scr_score: 0.15413535182421562
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7620000243186951, scr_score: 0.2744361087582965
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.8020000457763672, scr_score: 0.4248121109452937
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.6800000071525574, scr_score: -0.03383459489013476
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8710000514984131, scr_score: 0.017857437459055086
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.878000020980835, scr_score: 0.08035713810549912
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8860000371932983, scr_score: 0.15178582357352044
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8320000171661377, scr_score: -0.33035700505947424
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8850000500679016, scr_score: 0.14285737093604264
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9130000472068787, scr_score: 0.39285723789001775
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9280000329017639, scr_score: 0.5267856240044831
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6540000438690186, scr_score: 0.07598793776116658
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6460000276565552, scr_score: 0.05167178318406509
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6730000376701355, scr_score: 0.13373866900495587
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7020000219345093, scr_score: 0.22188450288557088
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.718000054359436, scr_score: 0.27051681203977385
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7110000252723694, scr_score: 0.24924013149253446
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.8050000667572021, scr_score: 0.5349544948507212
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7560000419616699, scr_score: 0.014423177494891337
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7640000581741333, scr_score: 0.05288479310741072
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7680000066757202, scr_score: 0.07211531435288414
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7730000615119934, scr_score: 0.09615403903129846
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7860000133514404, scr_score: 0.1586538062006596
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7930000424385071, scr_score: 0.19230779150181063
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.5360000133514404, scr_score: -1.0432692459238877
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7460000514984131, scr_score: 0.013761564463381832
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7450000643730164, scr_score: 0.009174467447523987
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7610000371932983, scr_score: 0.08256883994867438
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7730000615119934, scr_score: 0.13761482438639341
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7580000162124634, scr_score: 0.06880727548529256
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7750000357627869, scr_score: 0.1467890184181091
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.8690000176429749, scr_score: 0.5779816062249123
Plots have been saved to the 'plots' directory
Step 0: {'loss': 962.2271118164062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}
Step 100: {'loss': 642.6691284179688, 'l1_loss': 6243.6376953125, 'l2_loss': 356.106201171875}
Step 200: {'loss': 445.5507507324219, 'l1_loss': 2756.267822265625, 'l2_loss': 319.12127685546875}
Step 300: {'loss': 363.9036560058594, 'l1_loss': 1210.3465576171875, 'l2_loss': 310.1884765625}
Step 400: {'loss': 335.0495300292969, 'l1_loss': 796.766845703125, 'l2_loss': 302.12750244140625}
Step 500: {'loss': 307.9459228515625, 'l1_loss': 862.17919921875, 'l2_loss': 273.3529052734375}
Step 600: {'loss': 286.0335388183594, 'l1_loss': 892.2207641601562, 'l2_loss': 250.33856201171875}
Step 700: {'loss': 266.6194152832031, 'l1_loss': 883.0731201171875, 'l2_loss': 231.29608154296875}
Step 800: {'loss': 257.6353759765625, 'l1_loss': 872.291015625, 'l2_loss': 222.74371337890625}
Step 900: {'loss': 247.1330108642578, 'l1_loss': 709.89501953125, 'l2_loss': 218.73719787597656}
Step 1000: {'loss': 245.4971466064453, 'l1_loss': 748.6680297851562, 'l2_loss': 215.5504150390625}
Step 1100: {'loss': 238.0408935546875, 'l1_loss': 614.6963500976562, 'l2_loss': 213.45303344726562}
Step 1200: {'loss': 230.97225952148438, 'l1_loss': 505.4617919921875, 'l2_loss': 210.75364685058594}
Step 1300: {'loss': 232.80360412597656, 'l1_loss': 548.6661987304688, 'l2_loss': 210.85693359375}
Step 1400: {'loss': 229.04888916015625, 'l1_loss': 553.8463134765625, 'l2_loss': 206.89503479003906}
Step 1500: {'loss': 227.3150177001953, 'l1_loss': 502.6466064453125, 'l2_loss': 207.2090301513672}
Step 1600: {'loss': 226.66473388671875, 'l1_loss': 501.9544677734375, 'l2_loss': 206.58633422851562}
Step 1700: {'loss': 224.18414306640625, 'l1_loss': 480.403564453125, 'l2_loss': 204.96798706054688}
Step 1800: {'loss': 224.18421936035156, 'l1_loss': 510.59735107421875, 'l2_loss': 203.76031494140625}
Step 1900: {'loss': 222.2464141845703, 'l1_loss': 494.06756591796875, 'l2_loss': 202.48370361328125}
Step 2000: {'loss': 221.06024169921875, 'l1_loss': 495.50689697265625, 'l2_loss': 201.2399444580078}
Step 2100: {'loss': 221.02439880371094, 'l1_loss': 480.4700622558594, 'l2_loss': 201.80557250976562}
Step 2200: {'loss': 217.1396026611328, 'l1_loss': 441.48004150390625, 'l2_loss': 199.4803924560547}
Step 2300: {'loss': 217.68380737304688, 'l1_loss': 497.51715087890625, 'l2_loss': 197.78309631347656}
Step 2400: {'loss': 217.79983520507812, 'l1_loss': 489.5353088378906, 'l2_loss': 198.21823120117188}
Step 2500: {'loss': 215.209716796875, 'l1_loss': 499.7786865234375, 'l2_loss': 195.21792602539062}
Step 2600: {'loss': 215.65621948242188, 'l1_loss': 526.8222045898438, 'l2_loss': 194.58323669433594}
Step 2700: {'loss': 215.14166259765625, 'l1_loss': 499.48980712890625, 'l2_loss': 195.1614990234375}
Step 2800: {'loss': 214.0491180419922, 'l1_loss': 478.7393798828125, 'l2_loss': 194.89927673339844}
Step 2900: {'loss': 211.87611389160156, 'l1_loss': 463.29974365234375, 'l2_loss': 193.34335327148438}
Step 3000: {'loss': 213.53982543945312, 'l1_loss': 504.583984375, 'l2_loss': 193.35427856445312}
Step 3100: {'loss': 211.28421020507812, 'l1_loss': 496.2826232910156, 'l2_loss': 191.42947387695312}
Step 3200: {'loss': 210.92259216308594, 'l1_loss': 473.44415283203125, 'l2_loss': 191.98193359375}
Step 3300: {'loss': 209.8727264404297, 'l1_loss': 475.82611083984375, 'l2_loss': 190.83836364746094}
Step 3400: {'loss': 210.69361877441406, 'l1_loss': 502.02813720703125, 'l2_loss': 190.61135864257812}
Step 3500: {'loss': 207.13133239746094, 'l1_loss': 466.4176025390625, 'l2_loss': 188.4739990234375}
Step 3600: {'loss': 209.57579040527344, 'l1_loss': 493.9742431640625, 'l2_loss': 189.81494140625}
Step 3700: {'loss': 208.48727416992188, 'l1_loss': 478.4759521484375, 'l2_loss': 189.33938598632812}
Step 3800: {'loss': 206.49038696289062, 'l1_loss': 490.00653076171875, 'l2_loss': 186.88784790039062}
Step 3900: {'loss': 206.90811157226562, 'l1_loss': 482.3589782714844, 'l2_loss': 187.60009765625}
Step 4000: {'loss': 206.0227508544922, 'l1_loss': 466.1164855957031, 'l2_loss': 187.3748321533203}
Step 4100: {'loss': 205.89352416992188, 'l1_loss': 466.5392150878906, 'l2_loss': 187.2235107421875}
Step 4200: {'loss': 205.74398803710938, 'l1_loss': 492.0869445800781, 'l2_loss': 186.05807495117188}
Step 4300: {'loss': 205.67576599121094, 'l1_loss': 487.1521911621094, 'l2_loss': 186.18115234375}
Step 4400: {'loss': 203.85006713867188, 'l1_loss': 476.2586669921875, 'l2_loss': 184.79574584960938}
Step 4500: {'loss': 204.54739379882812, 'l1_loss': 485.2534484863281, 'l2_loss': 185.12696838378906}
Step 4600: {'loss': 203.8086395263672, 'l1_loss': 483.85882568359375, 'l2_loss': 184.44876098632812}
Step 4700: {'loss': 203.01588439941406, 'l1_loss': 503.31219482421875, 'l2_loss': 182.869384765625}
Step 4800: {'loss': 202.97244262695312, 'l1_loss': 489.4422912597656, 'l2_loss': 183.3897705078125}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/free_PhD_labor/results/sae_variants/20250127_160327_threshold_orthogonal_sae/run_7/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_7/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8950000405311584, scr_score: -0.04477645135146917
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8960000276565552, scr_score: -0.0298512641082919
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -1.2537326312437003
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7980000376701355, scr_score: -1.4925382960003488
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7910000085830688, scr_score: -1.5970163859464643
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6530000567436218, scr_score: 0.14393948744223542
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6730000376701355, scr_score: 0.19444448625465813
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6850000619888306, scr_score: 0.22474757585217325
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6940000057220459, scr_score: 0.2474747049043481
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6850000619888306, scr_score: 0.22474757585217325
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7010000348091125, scr_score: 0.2651515447987576
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7040000557899475, scr_score: 0.2727273548273287
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8470000624656677, scr_score: 0.017857446962504415
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8320000171661377, scr_score: -0.11607154261093916
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8360000252723694, scr_score: -0.08035718087031306
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8400000333786011, scr_score: -0.04464281912968695
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9570000171661377, changed_acc: 0.8040000200271606, scr_score: -0.36607154261093916
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6670000553131104, scr_score: 0.058309146826548426
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6680000424385071, scr_score: 0.06122456072426409
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6700000166893005, scr_score: 0.06705538851969542
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6810000538825989, scr_score: 0.09912546271790881
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6710000038146973, scr_score: 0.06997080241741109
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6780000329017639, scr_score: 0.09037904725031479
dir: 2, original_acc: 0.6470000147819519, clean_acc: 0.9900000691413879, changed_acc: 0.6820000410079956, scr_score: 0.10204087661562448
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9190000295639038, scr_score: 0.0
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9160000681877136, scr_score: -0.05769151414004328
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9120000600814819, scr_score: -0.13461467923559403
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.906000018119812, scr_score: -0.25
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.9020000696182251, scr_score: -0.32692201885339106
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8760000467300415, scr_score: -0.826922018853391
dir: 1, original_acc: 0.9190000295639038, clean_acc: 0.971000075340271, changed_acc: 0.8670000433921814, scr_score: -0.9999988537578403
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5860000252723694, scr_score: 0.02189781762893601
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5950000286102295, scr_score: 0.04379563525787202
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5910000205039978, scr_score: 0.034063255753515806
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.5990000367164612, scr_score: 0.05352801476222823
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.597000002861023, scr_score: 0.04866175249831918
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6200000047683716, scr_score: 0.10462282588077099
dir: 2, original_acc: 0.5770000219345093, clean_acc: 0.9880000352859497, changed_acc: 0.6290000081062317, scr_score: 0.126520643509707
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8280000686645508, scr_score: -0.053845921070609495
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8260000348091125, scr_score: -0.06923079744598673
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8170000314712524, scr_score: -0.13846159489197346
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8240000605583191, scr_score: -0.08461521532407963
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.8130000233650208, scr_score: -0.1692308891454436
dir: 1, original_acc: 0.8350000381469727, clean_acc: 0.9650000333786011, changed_acc: 0.7830000519752502, scr_score: -0.39999990830054316
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6950000524520874, scr_score: 0.0909091730091419
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.6990000605583191, scr_score: 0.1030304081183683
dir: 2, original_acc: 0.6650000214576721, clean_acc: 0.9950000643730164, changed_acc: 0.7120000123977661, scr_score: 0.14242419644821386