Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.

Generating idea 1/1
Iteration 1/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The prototype idea introduces sparsity-weighted orthogonality constraints, but we can refine this further by making the orthogonality constraints adaptive during training. The key insight is that early in training, features are less differentiated and need stronger orthogonality pressure, while later they become more naturally separated and can benefit from relaxed constraints. This allows for more flexible feature learning while still preventing feature absorption.

The main modifications are:
1. Add a curriculum schedule for the orthogonality weight based on training progress
2. Make the competition coefficients temperature-dependent to gradually sharpen the focus on strongly competing features
3. Maintain the core sparsity-based mechanism but make it more dynamic

This refinement preserves the elegant connection between sparsity and orthogonality while adding principled adaptation, without introducing complex new components. The adaptive weighting should help prevent both under-separation (early) and over-constraining (late) of features.

For mechanistic interpretability, this should lead to:
- Cleaner initial feature separation when it matters most
- More natural evolution of feature relationships as training progresses
- Better balance between independence and meaningful feature relationships
- Reduced likelihood of artificial feature splitting

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation",
    "Experiment": "1. Implement temperature-dependent competition coefficients\n2. Add curriculum schedule for orthogonality weight\n3. Train on google/gemma-2-2b with standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature competition patterns across training\n6. Evaluate impact of adaptation schedules",
    "Technical_Details": "The method extends sparsity-based orthogonality with adaptive weighting: L = L_recon + λ_1 * L_sparse + λ_2(t) * Σ_(i,j) c_(ij,T) * |f_i^T f_j| where λ_2(t) = λ_2_max * (1 - t/t_max)^p follows a curriculum schedule and c_(ij,T) = softmax(n_ij/T) uses temperature T to control competition sharpness. n_ij is the normalized intersection size of sparsity masks. Temperature T starts low (sharp competition) and increases gradually, while λ_2(t) decreases to reduce overall orthogonality pressure.",
    "Implementation_Plan": "1. Add temperature parameter to mask intersection function\n2. Implement curriculum scheduler for orthogonality weight\n3. Modify competition coefficient computation\n4. Add configuration for adaptation parameters\n5. Update training loop to use schedules\n6. Add logging for adaptation metrics",
    "Interestingness_Evaluation": "Making orthogonality constraints adapt to training dynamics provides an elegant way to balance feature separation and natural relationships.",
    "Interestingness": 8,
    "Feasibility_Evaluation": "Implementation requires only simple scheduling functions and modified coefficient computation; no complex architecture changes; training time similar to baseline; straightforward parameter tuning process.",
    "Feasibility": 9,
    "Novelty_Evaluation": "While building on existing orthogonality ideas, the adaptive approach offers a novel perspective on managing feature relationships through training.",
    "Novelty": 7,
    "Expected_Research_Impact": "The adaptive constraints should lead to cleaner initial feature separation while allowing more natural relationships to develop, improving interpretability scores on sparse_probing and core benchmarks.",
    "Research_Impact": 8,
    "Overall_Score": 8.3,
    "Abstract": "One of the key challenges in training sparse autoencoders (SAEs) is balancing feature independence with meaningful relationships between features. We propose an adaptive approach to sparsity-guided orthogonality constraints that evolves with training progress. Early in training, strong orthogonality pressure helps establish distinct features, while gradual relaxation allows natural relationships to emerge. We implement this through a curriculum-scheduled orthogonality weight and temperature-dependent competition coefficients derived from activation sparsity patterns. This approach maintains the computational efficiency of standard SAEs while providing more principled control over feature relationships throughout training."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Adaptive Sparsity-Guided Orthogonality for Improved Feature Separation', 'Experiment': '1. Implement temperature-dependent competition coefficients\n2. Add curriculum schedule for orthogonality weight\n3. Train on google/gemma-2-2b with standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature competition patterns across training\n6. Evaluate impact of adaptation schedules', 'Technical_Details': 'The method extends sparsity-based orthogonality with adaptive weighting: L = L_recon + λ_1 * L_sparse + λ_2(t) * Σ_(i,j) c_(ij,T) * |f_i^T f_j| where λ_2(t) = λ_2_max * (1 - t/t_max)^p follows a curriculum schedule and c_(ij,T) = softmax(n_ij/T) uses temperature T to control competition sharpness. n_ij is the normalized intersection size of sparsity masks. Temperature T starts low (sharp competition) and increases gradually, while λ_2(t) decreases to reduce overall orthogonality pressure.', 'Implementation_Plan': '1. Add temperature parameter to mask intersection function\n2. Implement curriculum scheduler for orthogonality weight\n3. Modify competition coefficient computation\n4. Add configuration for adaptation parameters\n5. Update training loop to use schedules\n6. Add logging for adaptation metrics', 'Interestingness_Evaluation': 'Making orthogonality constraints adapt to training dynamics provides an elegant way to balance feature separation and natural relationships.', 'Interestingness': 8, 'Feasibility_Evaluation': 'Implementation requires only simple scheduling functions and modified coefficient computation; no complex architecture changes; training time similar to baseline; straightforward parameter tuning process.', 'Feasibility': 9, 'Novelty_Evaluation': 'While building on existing orthogonality ideas, the adaptive approach offers a novel perspective on managing feature relationships through training.', 'Novelty': 7, 'Expected_Research_Impact': 'The adaptive constraints should lead to cleaner initial feature separation while allowing more natural relationships to develop, improving interpretability scores on sparse_probing and core benchmarks.', 'Research_Impact': 8, 'Overall_Score': 8.3, 'Abstract': 'One of the key challenges in training sparse autoencoders (SAEs) is balancing feature independence with meaningful relationships between features. We propose an adaptive approach to sparsity-guided orthogonality constraints that evolves with training progress. Early in training, strong orthogonality pressure helps establish distinct features, while gradual relaxation allows natural relationships to emerge. We implement this through a curriculum-scheduled orthogonality weight and temperature-dependent competition coefficients derived from activation sparsity patterns. This approach maintains the computational efficiency of standard SAEs while providing more principled control over feature relationships throughout training.'}
Iteration 2/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
Looking at my previous idea, I notice several areas for improvement:

1. The ratings were too conservative given the strong theoretical foundation and practical simplicity
2. The technical implementation could be more precise and focused
3. The adaptation mechanism could be simplified while maintaining effectiveness

Key refinements:
- Remove the separate temperature parameter and instead tie adaptation directly to training progress
- Make the competition coefficients binary based on a dynamic threshold
- Strengthen the connection to the original sparsity-guided approach

These changes make the idea even more feasible while potentially increasing effectiveness:
- Simpler implementation with fewer hyperparameters
- More direct relationship to original sparsity patterns
- Clearer theoretical justification
- Faster training due to simplified computations

The core insight remains the same - adapting orthogonality constraints based on training progress - but the implementation becomes more elegant and practical.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Progressive Sparsity-Guided Orthogonality for Feature Separation",
    "Experiment": "1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule",
    "Technical_Details": "The method uses a simplified adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) is binary: 1 if normalized mask intersection > threshold(p) = base_threshold * (1-p), 0 otherwise. This creates strong initial orthogonality that gradually relaxes, while maintaining computational efficiency through binary competition coefficients.",
    "Implementation_Plan": "1. Add training progress tracking\n2. Implement dynamic threshold computation\n3. Modify competition coefficient function for binary output\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression",
    "Interestingness_Evaluation": "The progression from strict to relaxed orthogonality provides an elegant solution to the feature separation-relationship trade-off while maintaining simplicity.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple progress tracking and threshold computation; binary coefficients reduce computation; no complex architecture changes; easily within 30-minute training limit.",
    "Feasibility": 10,
    "Novelty_Evaluation": "The progress-based adaptation of sparsity-guided orthogonality offers a novel and principled approach to managing feature relationships.",
    "Novelty": 8,
    "Expected_Research_Impact": "The simplified yet principled approach should significantly improve feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.",
    "Research_Impact": 9,
    "Overall_Score": 9.2,
    "Abstract": "We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a simple yet principled mechanism that adapts orthogonality constraints based on training progress, using binary competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more systematic control over feature relationships throughout the training process."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Progressive Sparsity-Guided Orthogonality for Feature Separation', 'Experiment': '1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule', 'Technical_Details': 'The method uses a simplified adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) is binary: 1 if normalized mask intersection > threshold(p) = base_threshold * (1-p), 0 otherwise. This creates strong initial orthogonality that gradually relaxes, while maintaining computational efficiency through binary competition coefficients.', 'Implementation_Plan': '1. Add training progress tracking\n2. Implement dynamic threshold computation\n3. Modify competition coefficient function for binary output\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression', 'Interestingness_Evaluation': 'The progression from strict to relaxed orthogonality provides an elegant solution to the feature separation-relationship trade-off while maintaining simplicity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple progress tracking and threshold computation; binary coefficients reduce computation; no complex architecture changes; easily within 30-minute training limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'The progress-based adaptation of sparsity-guided orthogonality offers a novel and principled approach to managing feature relationships.', 'Novelty': 8, 'Expected_Research_Impact': 'The simplified yet principled approach should significantly improve feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.', 'Research_Impact': 9, 'Overall_Score': 9.2, 'Abstract': 'We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a simple yet principled mechanism that adapts orthogonality constraints based on training progress, using binary competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more systematic control over feature relationships throughout the training process.'}
Iteration 3/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The current idea is already quite refined, with strong ratings across all metrics and a clear, implementable approach. However, I notice one potential area for further improvement: the binary threshold for competition coefficients might be too abrupt. 

Instead of a hard threshold, we can use a simple linear interpolation based on the intersection size relative to the dynamic threshold. This maintains the computational efficiency while providing smoother transitions in feature relationships. The change is minimal but potentially important:

- Smoother adaptation of feature relationships
- More stable training behavior
- Still maintains simplicity and efficiency
- Better handles edge cases near the threshold

This small refinement makes the approach more robust while keeping all the benefits of the previous version. The implementation remains straightforward, with only a minor modification to the coefficient computation.

NEW IDEA JSON:
```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Progressive Sparsity-Guided Orthogonality for Feature Separation",
    "Experiment": "1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule",
    "Technical_Details": "The method uses a smoothly adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) = max(0, min(1, (n_ij - threshold(p))/threshold(p))) with n_ij as normalized mask intersection and threshold(p) = base_threshold * (1-p). This creates smooth transitions in feature relationships while maintaining computational efficiency through simple linear interpolation.",
    "Implementation_Plan": "1. Add training progress tracking\n2. Implement smooth threshold computation\n3. Modify competition coefficient function for interpolation\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression",
    "Interestingness_Evaluation": "The smooth progression from strict to relaxed orthogonality provides an elegant and stable solution to the feature separation-relationship trade-off.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple progress tracking and linear interpolation; efficient computation; no complex architecture changes; easily within 30-minute training limit.",
    "Feasibility": 10,
    "Novelty_Evaluation": "The smoothly progressive adaptation of sparsity-guided orthogonality offers a novel and robust approach to managing feature relationships.",
    "Novelty": 8,
    "Expected_Research_Impact": "The refined approach should provide more stable and consistent improvements in feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.",
    "Research_Impact": 9,
    "Overall_Score": 9.2,
    "Abstract": "We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a smooth adaptation mechanism that continuously adjusts orthogonality constraints based on training progress, using interpolated competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more stable and systematic control over feature relationships throughout the training process."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Progressive Sparsity-Guided Orthogonality for Feature Separation', 'Experiment': '1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule', 'Technical_Details': 'The method uses a smoothly adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) = max(0, min(1, (n_ij - threshold(p))/threshold(p))) with n_ij as normalized mask intersection and threshold(p) = base_threshold * (1-p). This creates smooth transitions in feature relationships while maintaining computational efficiency through simple linear interpolation.', 'Implementation_Plan': '1. Add training progress tracking\n2. Implement smooth threshold computation\n3. Modify competition coefficient function for interpolation\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression', 'Interestingness_Evaluation': 'The smooth progression from strict to relaxed orthogonality provides an elegant and stable solution to the feature separation-relationship trade-off.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple progress tracking and linear interpolation; efficient computation; no complex architecture changes; easily within 30-minute training limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'The smoothly progressive adaptation of sparsity-guided orthogonality offers a novel and robust approach to managing feature relationships.', 'Novelty': 8, 'Expected_Research_Impact': 'The refined approach should provide more stable and consistent improvements in feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.', 'Research_Impact': 9, 'Overall_Score': 9.2, 'Abstract': 'We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a smooth adaptation mechanism that continuously adjusts orthogonality constraints based on training progress, using interpolated competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more stable and systematic control over feature relationships throughout the training process.'}
Iteration 4/5
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
After careful review, I believe the current idea has reached an optimal balance of effectiveness, simplicity, and feasibility. The smooth adaptation mechanism provides a principled approach while maintaining computational efficiency. The ratings accurately reflect:

1. Interestingness (9): The smooth progression mechanism is elegant and intuitive
2. Feasibility (10): Implementation is straightforward with minimal computational overhead
3. Novelty (8): The approach offers a fresh perspective on feature relationship management
4. Research Impact (9): The smooth adaptation should provide stable and consistent improvements

Further modifications would likely either:
- Add unnecessary complexity
- Reduce computational efficiency
- Deviate from the core sparsity-guided principle
- Make implementation more challenging

The current version maintains the spirit of the prototype while providing meaningful improvements through smooth adaptation. 

I am done.

```json
{
    "Name": "adaptive_sparse_orthogonal_sae",
    "Title": "Progressive Sparsity-Guided Orthogonality for Feature Separation",
    "Experiment": "1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule",
    "Technical_Details": "The method uses a smoothly adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) = max(0, min(1, (n_ij - threshold(p))/threshold(p))) with n_ij as normalized mask intersection and threshold(p) = base_threshold * (1-p). This creates smooth transitions in feature relationships while maintaining computational efficiency through simple linear interpolation.",
    "Implementation_Plan": "1. Add training progress tracking\n2. Implement smooth threshold computation\n3. Modify competition coefficient function for interpolation\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression",
    "Interestingness_Evaluation": "The smooth progression from strict to relaxed orthogonality provides an elegant and stable solution to the feature separation-relationship trade-off.",
    "Interestingness": 9,
    "Feasibility_Evaluation": "Implementation requires only simple progress tracking and linear interpolation; efficient computation; no complex architecture changes; easily within 30-minute training limit.",
    "Feasibility": 10,
    "Novelty_Evaluation": "The smoothly progressive adaptation of sparsity-guided orthogonality offers a novel and robust approach to managing feature relationships.",
    "Novelty": 8,
    "Expected_Research_Impact": "The refined approach should provide more stable and consistent improvements in feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.",
    "Research_Impact": 9,
    "Overall_Score": 9.2,
    "Abstract": "We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a smooth adaptation mechanism that continuously adjusts orthogonality constraints based on training progress, using interpolated competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more stable and systematic control over feature relationships throughout the training process."
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

{'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Progressive Sparsity-Guided Orthogonality for Feature Separation', 'Experiment': '1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule', 'Technical_Details': 'The method uses a smoothly adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) = max(0, min(1, (n_ij - threshold(p))/threshold(p))) with n_ij as normalized mask intersection and threshold(p) = base_threshold * (1-p). This creates smooth transitions in feature relationships while maintaining computational efficiency through simple linear interpolation.', 'Implementation_Plan': '1. Add training progress tracking\n2. Implement smooth threshold computation\n3. Modify competition coefficient function for interpolation\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression', 'Interestingness_Evaluation': 'The smooth progression from strict to relaxed orthogonality provides an elegant and stable solution to the feature separation-relationship trade-off.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple progress tracking and linear interpolation; efficient computation; no complex architecture changes; easily within 30-minute training limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'The smoothly progressive adaptation of sparsity-guided orthogonality offers a novel and robust approach to managing feature relationships.', 'Novelty': 8, 'Expected_Research_Impact': 'The refined approach should provide more stable and consistent improvements in feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.', 'Research_Impact': 9, 'Overall_Score': 9.2, 'Abstract': 'We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a smooth adaptation mechanism that continuously adjusts orthogonality constraints based on training progress, using interpolated competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more stable and systematic control over feature relationships throughout the training process.'}
Idea generation converged after 4 iterations.
Skipping seed idea 0
Skipping seed idea 1
Skipping seed idea 2
Skipping seed idea 3
Skipping seed idea 4
Skipping seed idea 5
Skipping seed idea 6
Skipping seed idea 7

Checking novelty of idea 8: adaptive_sparse_orthogonal_sae
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The idea proposes a novel approach to managing feature relationships in sparse autoencoders through progressive orthogonality constraints. The key innovation appears to be the smooth adaptation of orthogonality requirements based on training progress. Before making a decision, I should search for existing work on:
1. Progressive or adaptive orthogonality in autoencoders
2. Dynamic feature separation techniques in sparse autoencoders
3. Training progression-based constraints in neural networks

Let me first search for papers specifically about adaptive orthogonality in autoencoders.

RESPONSE:
```json
{
    "Query": "adaptive orthogonality autoencoder progressive feature separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 92, "offset": 0, "next": 10, "data": [{"paperId": "aad6d0a3d37a338c82168da13fc17f766ccf1774", "title": "Memory-Augmented Autoencoder With Adaptive Reconstruction and Sample Attribution Mining for Hyperspectral Anomaly Detection", "abstract": "Hyperspectral anomaly detection (HAD) aims to identify targets that are significantly different from their surrounding background, employing an unsupervised paradigm. Recently, detectors based on autoencoder (AE) have become predominant methods an
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
While the search results show various applications of adaptive and progressive techniques in different contexts (like domain adaptation, mirror detection, etc.), none directly addresses progressive orthogonality in sparse autoencoders. The closest is paper #5 "Deep Autoencoder With Orthogonal Features for Process Monitoring" which uses orthogonal feature separation, but it focuses on quality-relevant vs quality-irrelevant features rather than progressive adaptation of orthogonality constraints.

Let me search specifically for papers about sparse autoencoders with dynamic or progressive constraints, as this might reveal more directly relevant work.

RESPONSE:
```json
{
    "Query": "sparse autoencoder progressive constraint dynamic orthogonality training",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 16, "offset": 0, "next": 10, "data": [{"paperId": "87159f81aebcdb29e1f9c40ce7e3d574651ff38b", "title": "Sparse Representations for Object- and Ego-Motion Estimations in Dynamic Scenes", "abstract": "Disentangling the sources of visual motion in a dynamic scene during self-movement or ego motion is important for autonomous navigation and tracking. In the dynamic image segments of a video frame containing independently moving objects, optic flow relative to the next frame is the sum of t
!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

!!!

THOUGHT:
The search results reveal some related concepts but nothing that directly matches our proposed approach. Paper #7 "Sparsity-Aware Orthogonal Initialization" deals with orthogonal initialization for sparse networks, but doesn't address progressive adaptation during training. Paper #4 discusses self-supervised autoencoder training with regularization, but doesn't use progressive orthogonality constraints.

Let me make one final search specifically for papers about sparse autoencoders that use dynamic or adaptive constraints during training, particularly in the context of interpretability or feature separation.

RESPONSE:
```json
{
    "Query": "interpretable sparse autoencoder dynamic constraints feature separation",
    "Decision": "undecided"
}
```
@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

@@@

Response Status Code: 200
Response Content: {"total": 56, "offset": 0, "next": 10, "data": [{"paperId": "49c0d8062b29937c531d5d193386863e5b07375e", "title": "Efficient Blind Source Separation Method for fMRI Using Autoencoder and Spatiotemporal Sparsity Constraints", "abstract": "Diversity measures exploited by blind source separation (BSS) methods are usually based on either statistical attributes/geometrical structures or sparse/overcomplete (underdetermined) representations of the signals. This leads to some inefficient BSS methods tha
Decision made: novel after round 3
saved
Processing idea: adaptive_sparse_orthogonal_sae

[2025-02-06 15:12:34]  
*Starting idea: 20250206_151232_adaptive_sparse_orthogonal_sae*

[2025-02-06 15:12:34] Starting experiments execution
Starting experiment with prompt for coder: Your goal is to implement the following idea: Progressive Sparsity-Guided Orthogonality for Feature Separation. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Implement progress-based competition thresholds
2. Add dynamic orthogonality weighting
3. Train on google/gemma-2-2b using standard datasets
4. Compare against baseline and static orthogonal SAEs
5. Analyze feature separation across training phases
6. Evaluate impact of progression schedule.
The implementation plan is as follows: 1. Add training progress tracking
2. Implement smooth threshold computation
3. Modify competition coefficient function for interpolation
4. Add configuration for base threshold and decay rate
5. Update training loop with progress-based adaptation
6. Add visualization for feature separation progression.

You can also refer to other information in the idea: {'Name': 'adaptive_sparse_orthogonal_sae', 'Title': 'Progressive Sparsity-Guided Orthogonality for Feature Separation', 'Experiment': '1. Implement progress-based competition thresholds\n2. Add dynamic orthogonality weighting\n3. Train on google/gemma-2-2b using standard datasets\n4. Compare against baseline and static orthogonal SAEs\n5. Analyze feature separation across training phases\n6. Evaluate impact of progression schedule', 'Technical_Details': 'The method uses a smoothly adaptive orthogonality loss: L = L_recon + λ_1 * L_sparse + λ_2(p) * Σ_(i,j) c_(ij,p) * |f_i^T f_j| where p = step/total_steps is training progress, λ_2(p) = λ_2_max * (1-p)^2, and c_(ij,p) = max(0, min(1, (n_ij - threshold(p))/threshold(p))) with n_ij as normalized mask intersection and threshold(p) = base_threshold * (1-p). This creates smooth transitions in feature relationships while maintaining computational efficiency through simple linear interpolation.', 'Implementation_Plan': '1. Add training progress tracking\n2. Implement smooth threshold computation\n3. Modify competition coefficient function for interpolation\n4. Add configuration for base threshold and decay rate\n5. Update training loop with progress-based adaptation\n6. Add visualization for feature separation progression', 'Interestingness_Evaluation': 'The smooth progression from strict to relaxed orthogonality provides an elegant and stable solution to the feature separation-relationship trade-off.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Implementation requires only simple progress tracking and linear interpolation; efficient computation; no complex architecture changes; easily within 30-minute training limit.', 'Feasibility': 10, 'Novelty_Evaluation': 'The smoothly progressive adaptation of sparsity-guided orthogonality offers a novel and robust approach to managing feature relationships.', 'Novelty': 8, 'Expected_Research_Impact': 'The refined approach should provide more stable and consistent improvements in feature interpretability on sparse_probing and core benchmarks while maintaining computational efficiency.', 'Research_Impact': 9, 'Overall_Score': 9.2, 'Abstract': 'We present a progressive approach to sparsity-guided orthogonality constraints in sparse autoencoders (SAEs). The key insight is that feature separation requirements evolve during training - early stages benefit from strict orthogonality to establish distinct features, while later stages can allow more natural relationships to emerge. We implement this through a smooth adaptation mechanism that continuously adjusts orthogonality constraints based on training progress, using interpolated competition coefficients derived from activation patterns. This maintains the computational efficiency of standard SAEs while providing more stable and systematic control over feature relationships throughout the training process.', 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

If the experiments in the idea is already implemented in 'experiment.py' you are given with, you should try to improve its result by further enhancing the implementation.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run (you can try to run with different hyperparameters in the same run across different iterations.).

Note that we already provide the baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

Baseline results from TopK SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0002, 'steps': 2441, 'seed': 42, 'activation_dim': 2304, 'dict_size': 16384, 'k': 320, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'AutoEncoderTopK', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 2897.302734375, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '969c6142-0386-42f4-ae7f-d4a8c9cc59af', 'datetime_epoch_millis': 1738867729005, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.02959233779334607, 'mean_num_split_features': 1.36}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.002259036144578313, 'num_absorption': 6, 'num_probe_true_positives': 2656, 'num_split_features': 3}, {'first_letter': 'b', 'absorption_rate': 0.001910828025477707, 'num_absorption': 3, 'num_probe_true_positives': 1570, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.10699740836727138, 'num_absorption': 289, 'num_probe_true_positives': 2701, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.02460984393757503, 'num_absorption': 41, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.0006242197253433209, 'num_absorption': 1, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.03104026845637584, 'num_absorption': 37, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0008340283569641367, 'num_absorption': 1, 'num_probe_true_positives': 1199, 'num_split_features': 3}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.059844404548174746, 'num_absorption': 100, 'num_probe_true_positives': 1671, 'num_split_features': 1}, {'first_letter': 'j', 'absorption_rate': 0.0071090047393364926, 'num_absorption': 3, 'num_probe_true_positives': 422, 'num_split_features': 2}, {'first_letter': 'k', 'absorption_rate': 0.009022556390977444, 'num_absorption': 6, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.012315270935960592, 'num_absorption': 15, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.003474232773595831, 'num_absorption': 6, 'num_probe_true_positives': 1727, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.056074766355140186, 'num_absorption': 60, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.00042643923240938164, 'num_absorption': 1, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0018404907975460123, 'num_absorption': 3, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.00815313718539525, 'num_absorption': 23, 'num_probe_true_positives': 2821, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.029805352798053526, 'num_absorption': 49, 'num_probe_true_positives': 1644, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.09689922480620156, 'num_absorption': 75, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.03333333333333333, 'num_absorption': 28, 'num_probe_true_positives': 840, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.11894273127753303, 'num_absorption': 81, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.06315789473684211, 'num_absorption': 12, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.05952380952380952, 'num_absorption': 15, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': '5305f3c1b8d29cb4db6bd2dbbdf587e40053f2ba', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9877717391304348, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.123046875}, 'model_performance_preservation': {'ce_loss_score': 0.9884868421052632, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 3.046875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.74609375, 'mse': 1.6015625, 'cossim': 0.921875}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 138.0, 'l2_ratio': 0.921875, 'relative_reconstruction_bias': 1.0}, 'sparsity': {'l0': 320.0, 'l1': 956.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'be76fe50-7f26-41a7-b566-1bc46ba5f543', 'datetime_epoch_millis': 1738868607285, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.951, 'llm_top_1_test_accuracy': 0.65251875, 'llm_top_2_test_accuracy': 0.7218625, 'llm_top_5_test_accuracy': 0.78204375, 'llm_top_10_test_accuracy': 0.8321, 'llm_top_20_test_accuracy': 0.87753125, 'llm_top_50_test_accuracy': 0.9227312500000001, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9585937943309545, 'sae_top_1_test_accuracy': 0.7449374999999999, 'sae_top_2_test_accuracy': 0.8111062499999999, 'sae_top_5_test_accuracy': 0.8505875, 'sae_top_10_test_accuracy': 0.8858562499999999, 'sae_top_20_test_accuracy': 0.9124625, 'sae_top_50_test_accuracy': 0.9326124999999998, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9700000405311584, 'sae_top_1_test_accuracy': 0.7086, 'sae_top_2_test_accuracy': 0.8136000000000001, 'sae_top_5_test_accuracy': 0.8534, 'sae_top_10_test_accuracy': 0.8917999999999999, 'sae_top_20_test_accuracy': 0.9358000000000001, 'sae_top_50_test_accuracy': 0.9491999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9494, 'llm_top_1_test_accuracy': 0.6652, 'llm_top_2_test_accuracy': 0.7081999999999999, 'llm_top_5_test_accuracy': 0.7691999999999999, 'llm_top_10_test_accuracy': 0.8027999999999998, 'llm_top_20_test_accuracy': 0.8672000000000001, 'llm_top_50_test_accuracy': 0.9120000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9550000548362731, 'sae_top_1_test_accuracy': 0.8068000000000002, 'sae_top_2_test_accuracy': 0.82, 'sae_top_5_test_accuracy': 0.8507999999999999, 'sae_top_10_test_accuracy': 0.9066000000000001, 'sae_top_20_test_accuracy': 0.9141999999999999, 'sae_top_50_test_accuracy': 0.9301999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9100000000000001, 'llm_top_1_test_accuracy': 0.6876, 'llm_top_2_test_accuracy': 0.7344, 'llm_top_5_test_accuracy': 0.7704000000000001, 'llm_top_10_test_accuracy': 0.7972, 'llm_top_20_test_accuracy': 0.8528, 'llm_top_50_test_accuracy': 0.8926000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9280000448226928, 'sae_top_1_test_accuracy': 0.7862, 'sae_top_2_test_accuracy': 0.8192, 'sae_top_5_test_accuracy': 0.8493999999999999, 'sae_top_10_test_accuracy': 0.8757999999999999, 'sae_top_20_test_accuracy': 0.8892, 'sae_top_50_test_accuracy': 0.9074, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8969999999999999, 'llm_top_1_test_accuracy': 0.6, 'llm_top_2_test_accuracy': 0.6486000000000001, 'llm_top_5_test_accuracy': 0.6706, 'llm_top_10_test_accuracy': 0.7644, 'llm_top_20_test_accuracy': 0.8072000000000001, 'llm_top_50_test_accuracy': 0.8624, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9160000443458557, 'sae_top_1_test_accuracy': 0.6874, 'sae_top_2_test_accuracy': 0.751, 'sae_top_5_test_accuracy': 0.7891999999999999, 'sae_top_10_test_accuracy': 0.8240000000000001, 'sae_top_20_test_accuracy': 0.8458, 'sae_top_50_test_accuracy': 0.8775999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.9815, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9770000576972961, 'sae_top_1_test_accuracy': 0.674, 'sae_top_2_test_accuracy': 0.886, 'sae_top_5_test_accuracy': 0.913, 'sae_top_10_test_accuracy': 0.929, 'sae_top_20_test_accuracy': 0.95, 'sae_top_50_test_accuracy': 0.965, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9682000000000001, 'llm_top_1_test_accuracy': 0.6454, 'llm_top_2_test_accuracy': 0.693, 'llm_top_5_test_accuracy': 0.7554000000000001, 'llm_top_10_test_accuracy': 0.8022, 'llm_top_20_test_accuracy': 0.8657999999999999, 'llm_top_50_test_accuracy': 0.9238, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9726000308990479, 'sae_top_1_test_accuracy': 0.6602, 'sae_top_2_test_accuracy': 0.6724, 'sae_top_5_test_accuracy': 0.7438, 'sae_top_10_test_accuracy': 0.806, 'sae_top_20_test_accuracy': 0.8793999999999998, 'sae_top_50_test_accuracy': 0.9186, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9425, 'llm_top_1_test_accuracy': 0.65875, 'llm_top_2_test_accuracy': 0.7795, 'llm_top_5_test_accuracy': 0.82375, 'llm_top_10_test_accuracy': 0.867, 'llm_top_20_test_accuracy': 0.89525, 'llm_top_50_test_accuracy': 0.9232500000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9507500380277634, 'sae_top_1_test_accuracy': 0.8165, 'sae_top_2_test_accuracy': 0.8492500000000001, 'sae_top_5_test_accuracy': 0.8895, 'sae_top_10_test_accuracy': 0.9022499999999999, 'sae_top_20_test_accuracy': 0.9165, 'sae_top_50_test_accuracy': 0.9325000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6466000000000001, 'llm_top_2_test_accuracy': 0.796, 'llm_top_5_test_accuracy': 0.909, 'llm_top_10_test_accuracy': 0.9638, 'llm_top_20_test_accuracy': 0.9870000000000001, 'llm_top_50_test_accuracy': 0.998, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9994000434875489, 'sae_top_1_test_accuracy': 0.8198000000000001, 'sae_top_2_test_accuracy': 0.8774000000000001, 'sae_top_5_test_accuracy': 0.9156000000000001, 'sae_top_10_test_accuracy': 0.9513999999999999, 'sae_top_20_test_accuracy': 0.9687999999999999, 'sae_top_50_test_accuracy': 0.9803999999999998, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': '5305f3c1b8d29cb4db6bd2dbbdf587e40053f2ba', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': '9374e38e-f925-4488-afdf-59c6d236dafa', 'datetime_epoch_millis': 1738869563655, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.06367039680480957}}, 'eval_result_details': [], 'sae_bench_commit_hash': '5305f3c1b8d29cb4db6bd2dbbdf587e40053f2ba', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'TopK', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'TopK', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Baseline results from standard SAE: 
{'training results for layer 12': {'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 16384, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 2441, 'final_loss': 701.0477905273438, 'layer': 12, 'dict_size': 16384, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'b5f3e18f-7844-4d8a-ac26-9b99d357cf9a', 'datetime_epoch_millis': 1738804062497, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.016138604243358595, 'mean_num_split_features': 1.4166666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2656, 'num_split_features': 2}, {'first_letter': 'b', 'absorption_rate': 0.0006369426751592356, 'num_absorption': 1, 'num_probe_true_positives': 1570, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2701, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.009603841536614645, 'num_absorption': 16, 'num_probe_true_positives': 1666, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.05305867665418227, 'num_absorption': 85, 'num_probe_true_positives': 1602, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.013422818791946308, 'num_absorption': 16, 'num_probe_true_positives': 1192, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.021684737281067557, 'num_absorption': 26, 'num_probe_true_positives': 1199, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.0062047569803516025, 'num_absorption': 6, 'num_probe_true_positives': 967, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.006582884500299222, 'num_absorption': 11, 'num_probe_true_positives': 1671, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 422, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 665, 'num_split_features': 2}, {'first_letter': 'l', 'absorption_rate': 0.09770114942528736, 'num_absorption': 119, 'num_probe_true_positives': 1218, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.0005790387955993051, 'num_absorption': 1, 'num_probe_true_positives': 1727, 'num_split_features': 4}, {'first_letter': 'n', 'absorption_rate': 0.013173652694610778, 'num_absorption': 11, 'num_probe_true_positives': 835, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.022429906542056073, 'num_absorption': 24, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.008955223880597015, 'num_absorption': 21, 'num_probe_true_positives': 2345, 'num_split_features': 1}, {'first_letter': 'q', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.0006134969325153375, 'num_absorption': 1, 'num_probe_true_positives': 1630, 'num_split_features': 2}, {'first_letter': 's', 'absorption_rate': 0.0857851825593761, 'num_absorption': 242, 'num_probe_true_positives': 2821, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.011557177615571776, 'num_absorption': 19, 'num_probe_true_positives': 1644, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.021963824289405683, 'num_absorption': 17, 'num_probe_true_positives': 774, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.0014684287812041115, 'num_absorption': 1, 'num_probe_true_positives': 681, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 190, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.011904761904761904, 'num_absorption': 3, 'num_probe_true_positives': 252, 'num_split_features': 1}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_12_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_12_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.9995723748059007, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 0.004302978515625}, 'model_performance_preservation': {'ce_loss_score': 1.0, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 2.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.984375, 'mse': 0.0888671875, 'cossim': 0.99609375}, 'shrinkage': {'l2_norm_in': 149.0, 'l2_norm_out': 147.0, 'l2_ratio': 0.984375, 'relative_reconstruction_bias': 0.98828125}, 'sparsity': {'l0': 8895.810546875, 'l1': 12608.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'fc35f1d1-c27c-402b-abd2-fe38f12c05aa', 'datetime_epoch_millis': 1738804969188, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.95075625, 'llm_top_1_test_accuracy': 0.65444375, 'llm_top_2_test_accuracy': 0.71966875, 'llm_top_5_test_accuracy': 0.7860875, 'llm_top_10_test_accuracy': 0.83056875, 'llm_top_20_test_accuracy': 0.8764437500000001, 'llm_top_50_test_accuracy': 0.9228625, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9578625451773405, 'sae_top_1_test_accuracy': 0.6448624999999999, 'sae_top_2_test_accuracy': 0.6875375000000001, 'sae_top_5_test_accuracy': 0.7562874999999999, 'sae_top_10_test_accuracy': 0.8066500000000001, 'sae_top_20_test_accuracy': 0.8664625000000001, 'sae_top_50_test_accuracy': 0.9122750000000001, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.96, 'llm_top_1_test_accuracy': 0.6436, 'llm_top_2_test_accuracy': 0.6911999999999999, 'llm_top_5_test_accuracy': 0.792, 'llm_top_10_test_accuracy': 0.8333999999999999, 'llm_top_20_test_accuracy': 0.897, 'llm_top_50_test_accuracy': 0.9378, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6862, 'sae_top_2_test_accuracy': 0.6885999999999999, 'sae_top_5_test_accuracy': 0.7842, 'sae_top_10_test_accuracy': 0.8386000000000001, 'sae_top_20_test_accuracy': 0.8806, 'sae_top_50_test_accuracy': 0.9252, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.9492, 'llm_top_1_test_accuracy': 0.6794, 'llm_top_2_test_accuracy': 0.717, 'llm_top_5_test_accuracy': 0.767, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8614, 'llm_top_50_test_accuracy': 0.9086000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9546000480651855, 'sae_top_1_test_accuracy': 0.6844, 'sae_top_2_test_accuracy': 0.6834, 'sae_top_5_test_accuracy': 0.7402000000000001, 'sae_top_10_test_accuracy': 0.7884, 'sae_top_20_test_accuracy': 0.8503999999999999, 'sae_top_50_test_accuracy': 0.8998000000000002, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.9181999999999999, 'llm_top_1_test_accuracy': 0.6862, 'llm_top_2_test_accuracy': 0.732, 'llm_top_5_test_accuracy': 0.7722, 'llm_top_10_test_accuracy': 0.8008, 'llm_top_20_test_accuracy': 0.8532, 'llm_top_50_test_accuracy': 0.8922000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9368000507354737, 'sae_top_1_test_accuracy': 0.5932000000000001, 'sae_top_2_test_accuracy': 0.725, 'sae_top_5_test_accuracy': 0.7638, 'sae_top_10_test_accuracy': 0.8056000000000001, 'sae_top_20_test_accuracy': 0.8465999999999999, 'sae_top_50_test_accuracy': 0.8838000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.8948, 'llm_top_1_test_accuracy': 0.6039999999999999, 'llm_top_2_test_accuracy': 0.6508, 'llm_top_5_test_accuracy': 0.6878, 'llm_top_10_test_accuracy': 0.745, 'llm_top_20_test_accuracy': 0.8051999999999999, 'llm_top_50_test_accuracy': 0.8628, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9118000507354737, 'sae_top_1_test_accuracy': 0.5828, 'sae_top_2_test_accuracy': 0.6594, 'sae_top_5_test_accuracy': 0.7343999999999999, 'sae_top_10_test_accuracy': 0.768, 'sae_top_20_test_accuracy': 0.8114000000000001, 'sae_top_50_test_accuracy': 0.8452, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.982, 'llm_top_1_test_accuracy': 0.673, 'llm_top_2_test_accuracy': 0.724, 'llm_top_5_test_accuracy': 0.766, 'llm_top_10_test_accuracy': 0.826, 'llm_top_20_test_accuracy': 0.848, 'llm_top_50_test_accuracy': 0.932, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9765000343322754, 'sae_top_1_test_accuracy': 0.699, 'sae_top_2_test_accuracy': 0.701, 'sae_top_5_test_accuracy': 0.792, 'sae_top_10_test_accuracy': 0.814, 'sae_top_20_test_accuracy': 0.908, 'sae_top_50_test_accuracy': 0.932, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.9642, 'llm_top_1_test_accuracy': 0.6634, 'llm_top_2_test_accuracy': 0.7002, 'llm_top_5_test_accuracy': 0.7648, 'llm_top_10_test_accuracy': 0.8024000000000001, 'llm_top_20_test_accuracy': 0.8608, 'llm_top_50_test_accuracy': 0.9258, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.965600049495697, 'sae_top_1_test_accuracy': 0.6342000000000001, 'sae_top_2_test_accuracy': 0.6708000000000001, 'sae_top_5_test_accuracy': 0.7096000000000001, 'sae_top_10_test_accuracy': 0.7712, 'sae_top_20_test_accuracy': 0.8295999999999999, 'sae_top_50_test_accuracy': 0.9026, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.93825, 'llm_top_1_test_accuracy': 0.63375, 'llm_top_2_test_accuracy': 0.7607499999999999, 'llm_top_5_test_accuracy': 0.8265, 'llm_top_10_test_accuracy': 0.8727499999999999, 'llm_top_20_test_accuracy': 0.8977499999999999, 'llm_top_50_test_accuracy': 0.9265, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9530000537633896, 'sae_top_1_test_accuracy': 0.6285, 'sae_top_2_test_accuracy': 0.7015, 'sae_top_5_test_accuracy': 0.7515000000000001, 'sae_top_10_test_accuracy': 0.856, 'sae_top_20_test_accuracy': 0.8875, 'sae_top_50_test_accuracy': 0.9199999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.9994, 'llm_top_1_test_accuracy': 0.6522, 'llm_top_2_test_accuracy': 0.7814, 'llm_top_5_test_accuracy': 0.9123999999999999, 'llm_top_10_test_accuracy': 0.9642, 'llm_top_20_test_accuracy': 0.9882, 'llm_top_50_test_accuracy': 0.9972000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9990000247955322, 'sae_top_1_test_accuracy': 0.6506000000000001, 'sae_top_2_test_accuracy': 0.6706, 'sae_top_5_test_accuracy': 0.7745999999999998, 'sae_top_10_test_accuracy': 0.8114000000000001, 'sae_top_20_test_accuracy': 0.9176, 'sae_top_50_test_accuracy': 0.9895999999999999, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'unlearning evaluation results': {'eval_type_id': 'unlearning', 'eval_config': {'random_seed': 42, 'dataset_names': ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging'], 'intervention_method': 'clamp_feature_activation', 'retain_thresholds': [0.001, 0.01], 'n_features_list': [10, 20], 'multipliers': [25, 50, 100, 200], 'dataset_size': 1024, 'seq_len': 1024, 'n_batch_loss_added': 50, 'target_metric': 'correct', 'save_metrics': True, 'model_name': 'google/gemma-2-2b-it', 'llm_batch_size': 32, 'llm_dtype': 'bfloat16'}, 'eval_id': 'b48744e1-4f43-4d25-af92-4f1a46abab60', 'datetime_epoch_millis': 1738805225729, 'eval_result_metrics': {'unlearning': {'unlearning_score': 0.0}}, 'eval_result_details': [], 'sae_bench_commit_hash': 'c600ccd8de71ccb5cbbab0eb1c6e8cc361ee3481', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_12_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 16384, 'hook_layer': 12, 'hook_name': 'blocks.12.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the Progressive Sparsity-Guided Orthogonality idea. Let me plan the experiments:                                                     

Planned Runs:                                                                                                                                            

 1 Run 1: Implement basic progress tracking and orthogonality loss                                                                                       
 2 Run 2: Add dynamic threshold computation (base_threshold=0.1)                                                                                         
 3 Run 3: Test different base_threshold (0.2)                                                                                                            
 4 Run 4: Test different base_threshold (0.05)                                                                                                           
 5 Run 5: Add competition coefficient interpolation                                                                                                      
 6 Run 6: Optimize final parameters based on previous results                                                                                            

Let's start with Run 1 by implementing progress tracking and basic orthogonality loss. We need to modify the TrainerTopK class in experiment.py.         

Here are the changes:                                                                                                                                    

experiment.py                                                                                                                                            

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def __init__(                                                                                                                                       
         self,                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                     
         activation_dim=512,                                                                                                                             
         dict_size=64 * 512,                                                                                                                             
         k=100,                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                          
         decay_start=24000,  # when does the lr decay start                                                                                              
         steps=30000,  # when when does training end                                                                                                     
         seed=None,                                                                                                                                      
         device=None,                                                                                                                                    
         layer=None,                                                                                                                                     
         lm_name=None,                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                   
         submodule_name=None,                                                                                                                            
     ):                                                                                                                                                  
 =======                                                                                                                                                 
     def __init__(                                                                                                                                       
         self,                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                     
         activation_dim=512,                                                                                                                             
         dict_size=64 * 512,                                                                                                                             
         k=100,                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                          
         ortho_lambda=0.1,  # orthogonality loss weight                                                                                                  
         decay_start=24000,  # when does the lr decay start                                                                                              
         steps=30000,  # when when does training end                                                                                                     
         seed=None,                                                                                                                                      
         device=None,                                                                                                                                    
         layer=None,                                                                                                                                     
         lm_name=None,                                                                                                                                   
         wandb_name="AutoEncoderTopK",                                                                                                                   
         submodule_name=None,                                                                                                                            
     ):                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features"]                                                                                     
         self.effective_l0 = -1                                                                                                                          
         self.dead_features = -1                                                                                                                         
 =======                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss"]                                                           
         self.effective_l0 = -1                                                                                                                          
         self.dead_features = -1                                                                                                                         
         self.progress = 0.0                                                                                                                             
         self.ortho_loss = 0.0                                                                                                                           
         self.ortho_lambda = ortho_lambda                                                                                                                
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def loss(self, x, step=None, logging=False):                                                                                                        
 =======                                                                                                                                                 
     def compute_orthogonality_loss(self, f):                                                                                                            
         # Compute feature correlations                                                                                                                  
         f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)                                                                                        
         feature_correlations = torch.abs(torch.mm(f_norm.T, f_norm))                                                                                    
                                                                                                                                                         
         # Zero out diagonal elements (self-correlation)                                                                                                 
         mask = 1 - torch.eye(feature_correlations.shape[0], device=f.device)                                                                            
         feature_correlations = feature_correlations * mask                                                                                              
                                                                                                                                                         
         # Compute mean correlation                                                                                                                      
         ortho_loss = feature_correlations.mean()                                                                                                        
         return ortho_loss                                                                                                                               
                                                                                                                                                         
     def loss(self, x, step=None, logging=False):                                                                                                        
         # Update progress                                                                                                                               
         self.progress = step / self.steps if step is not None else 0.0                                                                                  
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         loss = l2_loss + self.auxk_alpha * auxk_loss                                                                                                    
 =======                                                                                                                                                 
         # Compute orthogonality loss with progress-based weighting                                                                                      
         ortho_loss = self.compute_orthogonality_loss(f)                                                                                                 
         self.ortho_loss = ortho_loss.item()                                                                                                             
                                                                                                                                                         
         # Total loss with orthogonality term weighted by (1-progress)^2                                                                                 
         ortho_weight = self.ortho_lambda * (1 - self.progress) ** 2                                                                                     
         loss = l2_loss + self.auxk_alpha * auxk_loss + ortho_weight * ortho_loss                                                                        
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

Let's also create a plotting script to visualize the training progression:                                                                               

plot.py                                                                                                                                                  

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 # write your code here                                                                                                                                  
 =======                                                                                                                                                 
 import matplotlib.pyplot as plt                                                                                                                         
 import json                                                                                                                                             
 import numpy as np                                                                                                                                      
                                                                                                                                                         
 def plot_training_progression(run_dir):                                                                                                                 
     # Load results                                                                                                                                      
     with open(f"{run_dir}/final_info.json", 'r') as f:                                                                                                  
         results = json.load(f)                                                                                                                          
                                                                                                                                                         
     # Extract metrics                                                                                                                                   
     training_results = results["training results for layer 12"]                                                                                         
                                                                                                                                                         
     # Create figure                                                                                                                                     
     plt.figure(figsize=(12, 8))                                                                                                                         
                                                                                                                                                         
     # Plot metrics                                                                                                                                      
     metrics = training_results.get("metrics", {})                                                                                                       
     plt.plot(metrics.get("progress", []), label="Training Progress")                                                                                    
     plt.plot(metrics.get("ortho_loss", []), label="Orthogonality Loss")                                                                                 
                                                                                                                                                         
     plt.xlabel("Training Steps")                                                                                                                        
     plt.ylabel("Value")                                                                                                                                 
     plt.title("Training Progression")                                                                                                                   
     plt.legend()                                                                                                                                        
     plt.grid(True)                                                                                                                                      
                                                                                                                                                         
     # Save plot                                                                                                                                         
     plt.savefig(f"{run_dir}/training_progression.png")                                                                                                  
     plt.close()                                                                                                                                         
                                                                                                                                                         
 if __name__ == "__main__":                                                                                                                              
     import sys                                                                                                                                          
     run_dir = sys.argv[1] if len(sys.argv) > 1 else "run_1"                                                                                             
     plot_training_progression(run_dir)                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

These changes:                                                                                                                                           

 1 Add orthogonality loss tracking                                                                                                                       
 2 Implement progress-based weighting                                                                                                                    
 3 Add visualization capabilities                                                                                                                        

To run the first experiment:                                                                                                                             

                                                                                                                                                         
 python experiment.py --out_dir=run_1                                                                                                                    
 python plot.py run_1                                                                                                                                    
                                                                                                                                                         

This implements the basic progressive orthogonality loss where:                                                                                          

 • Early in training (progress ≈ 0), orthogonality loss has maximum weight                                                                               
 • As training progresses, orthogonality constraint smoothly relaxes                                                                                     
 • Final stages (progress ≈ 1) have minimal orthogonality constraints                                                                                    

The key metrics to watch are:                                                                                                                            

 • Absorption score (lower is better)                                                                                                                    
 • Feature separation in sparse probing results                                                                                                          
 • Orthogonality loss progression                                                                                                                        

After seeing the results from this run, we can proceed with implementing the dynamic threshold computation in Run 2.                                     

Tokens: 42k sent, 1.6k received. Cost: $0.15 message, $0.15 session.
Applied edit to experiment.py
Applied edit to plot.py

python experiment.py --out_dir=run_1
python plot.py run_1                
coder_out: I'll help implement the Progressive Sparsity-Guided Orthogonality idea. Let me plan the experiments:

Planned Runs:
1. Run 1: Implement basic progress tracking and orthogonality loss
2. Run 2: Add dynamic threshold computation (base_threshold=0.1)
3. Run 3: Test different base_threshold (0.2)
4. Run 4: Test different base_threshold (0.05)
5. Run 5: Add competition coefficient interpolation
6. Run 6: Optimize final parameters based on previous results

Let's start with Run 1 by implementing progress tracking and basic orthogonality loss. We need to modify the TrainerTopK class in experiment.py.

Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
    ):
=======
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        ortho_lambda=0.1,  # orthogonality loss weight
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
    ):
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.logging_parameters = ["effective_l0", "dead_features"]
        self.effective_l0 = -1
        self.dead_features = -1
=======
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss"]
        self.effective_l0 = -1
        self.dead_features = -1
        self.progress = 0.0
        self.ortho_loss = 0.0
        self.ortho_lambda = ortho_lambda
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def loss(self, x, step=None, logging=False):
=======
    def compute_orthogonality_loss(self, f):
        # Compute feature correlations
        f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)
        feature_correlations = torch.abs(torch.mm(f_norm.T, f_norm))
        
        # Zero out diagonal elements (self-correlation)
        mask = 1 - torch.eye(feature_correlations.shape[0], device=f.device)
        feature_correlations = feature_correlations * mask
        
        # Compute mean correlation
        ortho_loss = feature_correlations.mean()
        return ortho_loss

    def loss(self, x, step=None, logging=False):
        # Update progress
        self.progress = step / self.steps if step is not None else 0.0
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        loss = l2_loss + self.auxk_alpha * auxk_loss
=======
        # Compute orthogonality loss with progress-based weighting
        ortho_loss = self.compute_orthogonality_loss(f)
        self.ortho_loss = ortho_loss.item()
        
        # Total loss with orthogonality term weighted by (1-progress)^2
        ortho_weight = self.ortho_lambda * (1 - self.progress) ** 2
        loss = l2_loss + self.auxk_alpha * auxk_loss + ortho_weight * ortho_loss
>>>>>>> REPLACE
```

Let's also create a plotting script to visualize the training progression:

plot.py
```python
<<<<<<< SEARCH
# write your code here
=======
import matplotlib.pyplot as plt
import json
import numpy as np

def plot_training_progression(run_dir):
    # Load results
    with open(f"{run_dir}/final_info.json", 'r') as f:
        results = json.load(f)
    
    # Extract metrics
    training_results = results["training results for layer 12"]
    
    # Create figure
    plt.figure(figsize=(12, 8))
    
    # Plot metrics
    metrics = training_results.get("metrics", {})
    plt.plot(metrics.get("progress", []), label="Training Progress")
    plt.plot(metrics.get("ortho_loss", []), label="Orthogonality Loss")
    
    plt.xlabel("Training Steps")
    plt.ylabel("Value")
    plt.title("Training Progression")
    plt.legend()
    plt.grid(True)
    
    # Save plot
    plt.savefig(f"{run_dir}/training_progression.png")
    plt.close()

if __name__ == "__main__":
    import sys
    run_dir = sys.argv[1] if len(sys.argv) > 1 else "run_1"
    plot_training_progression(run_dir)
>>>>>>> REPLACE
```

These changes:
1. Add orthogonality loss tracking
2. Implement progress-based weighting
3. Add visualization capabilities

To run the first experiment:

```bash
python experiment.py --out_dir=run_1
python plot.py run_1
```

This implements the basic progressive orthogonality loss where:
- Early in training (progress ≈ 0), orthogonality loss has maximum weight
- As training progresses, orthogonality constraint smoothly relaxes
- Final stages (progress ≈ 1) have minimal orthogonality constraints

The key metrics to watch are:
- Absorption score (lower is better)
- Feature separation in sparse probing results
- Orthogonality loss progression

After seeing the results from this run, we can proceed with implementing the dynamic threshold computation in Run 2., type: <class 'str'>
Step 0: 31236.619140625
Step 100: 4581.2177734375
Step 200: 4063.962890625
Step 300: 3815.761962890625
Step 400: 3596.413330078125
Step 500: 3448.873046875
Step 600: 3422.193359375
Step 700: 3241.78076171875
Step 800: 3210.460205078125
Step 900: 3109.399658203125
Step 1000: 3120.27978515625
Step 1100: 3111.6083984375
Step 1200: 3009.05029296875
Step 1300: 3049.201904296875
Step 1400: 3053.856689453125
Step 1500: 3065.5888671875
Step 1600: 2995.72216796875
Step 1700: 3016.8671875
Step 1800: 2993.15380859375
Step 1900: 2998.55322265625
Step 2000: 2993.687744140625
Step 2100: 3018.085693359375
Step 2200: 2892.82421875
Step 2300: 2897.036865234375
Step 2400: 2871.2705078125

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_1/final_info.json

[2025-02-06 15:25:17]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 738.59s

[2025-02-06 15:37:36]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 347.74s

[2025-02-06 15:43:23]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 0: 0.956000030040741
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 1: 0.9670000672340393
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 2: 0.9590000510215759
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 6: 0.9930000305175781
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 9: 0.9790000319480896
Num non-zero elements: 1
Test accuracy for 0: 0.646
Num non-zero elements: 1
Test accuracy for 1: 0.55
Num non-zero elements: 1
Test accuracy for 2: 0.668
Num non-zero elements: 1
Test accuracy for 6: 0.758
Num non-zero elements: 1
Test accuracy for 9: 0.942
Num non-zero elements: 2
Test accuracy for 0: 0.695
Num non-zero elements: 2
Test accuracy for 1: 0.654
Num non-zero elements: 2
Test accuracy for 2: 0.797
Num non-zero elements: 2
Test accuracy for 6: 0.96
Num non-zero elements: 2
Test accuracy for 9: 0.941
Num non-zero elements: 5
Test accuracy for 0: 0.751
Num non-zero elements: 5
Test accuracy for 1: 0.777
Num non-zero elements: 5
Test accuracy for 2: 0.856
Num non-zero elements: 5
Test accuracy for 6: 0.969
Num non-zero elements: 5
Test accuracy for 9: 0.956
Num non-zero elements: 10
Test accuracy for 0: 0.821
Num non-zero elements: 10
Test accuracy for 1: 0.871
Num non-zero elements: 10
Test accuracy for 2: 0.896
Num non-zero elements: 10
Test accuracy for 6: 0.975
Num non-zero elements: 10
Test accuracy for 9: 0.955
Num non-zero elements: 20
Test accuracy for 0: 0.883
Num non-zero elements: 20
Test accuracy for 1: 0.936
Num non-zero elements: 20
Test accuracy for 2: 0.905
Num non-zero elements: 20
Test accuracy for 6: 0.983
Num non-zero elements: 20
Test accuracy for 9: 0.953
Num non-zero elements: 50
Test accuracy for 0: 0.911
Num non-zero elements: 50
Test accuracy for 1: 0.936
Num non-zero elements: 50
Test accuracy for 2: 0.936
Num non-zero elements: 50
Test accuracy for 6: 0.987
Num non-zero elements: 50
Test accuracy for 9: 0.968
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.957
Num non-zero elements: 2304
Test accuracy for 13: 0.95
Num non-zero elements: 2304
Test accuracy for 14: 0.951
Num non-zero elements: 2304
Test accuracy for 18: 0.919
Num non-zero elements: 2304
Test accuracy for 19: 0.956
Num non-zero elements: 1
Test accuracy for 11: 0.574
Num non-zero elements: 1
Test accuracy for 13: 0.676
Num non-zero elements: 1
Test accuracy for 14: 0.645
Num non-zero elements: 1
Test accuracy for 18: 0.711
Num non-zero elements: 1
Test accuracy for 19: 0.792
Num non-zero elements: 2
Test accuracy for 11: 0.78
Num non-zero elements: 2
Test accuracy for 13: 0.722
Num non-zero elements: 2
Test accuracy for 14: 0.664
Num non-zero elements: 2
Test accuracy for 18: 0.708
Num non-zero elements: 2
Test accuracy for 19: 0.778
Num non-zero elements: 5
Test accuracy for 11: 0.781
Num non-zero elements: 5
Test accuracy for 13: 0.753
Num non-zero elements: 5
Test accuracy for 14: 0.721
Num non-zero elements: 5
Test accuracy for 18: 0.737
Num non-zero elements: 5
Test accuracy for 19: 0.836
Num non-zero elements: 10
Test accuracy for 11: 0.862
Num non-zero elements: 10
Test accuracy for 13: 0.784
Num non-zero elements: 10
Test accuracy for 14: 0.785
Num non-zero elements: 10
Test accuracy for 18: 0.744
Num non-zero elements: 10
Test accuracy for 19: 0.862
Num non-zero elements: 20
Test accuracy for 11: 0.914
Num non-zero elements: 20
Test accuracy for 13: 0.867
Num non-zero elements: 20
Test accuracy for 14: 0.878
Num non-zero elements: 20
Test accuracy for 18: 0.756
Num non-zero elements: 20
Test accuracy for 19: 0.901
Num non-zero elements: 50
Test accuracy for 11: 0.938
Num non-zero elements: 50
Test accuracy for 13: 0.919
Num non-zero elements: 50
Test accuracy for 14: 0.893
Num non-zero elements: 50
Test accuracy for 18: 0.811
Num non-zero elements: 50
Test accuracy for 19: 0.926
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 11: 0.9590000510215759
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 13: 0.9480000734329224
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 14: 0.9510000348091125
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 18: 0.9150000214576721
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 19: 0.9780000448226929
Num non-zero elements: 1
Test accuracy for 11: 0.846
Num non-zero elements: 1
Test accuracy for 13: 0.798
Num non-zero elements: 1
Test accuracy for 14: 0.785
Num non-zero elements: 1
Test accuracy for 18: 0.739
Num non-zero elements: 1
Test accuracy for 19: 0.83
Num non-zero elements: 2
Test accuracy for 11: 0.862
Num non-zero elements: 2
Test accuracy for 13: 0.811
Num non-zero elements: 2
Test accuracy for 14: 0.789
Num non-zero elements: 2
Test accuracy for 18: 0.747
Num non-zero elements: 2
Test accuracy for 19: 0.847
Num non-zero elements: 5
Test accuracy for 11: 0.862
Num non-zero elements: 5
Test accuracy for 13: 0.857
Num non-zero elements: 5
Test accuracy for 14: 0.852
Num non-zero elements: 5
Test accuracy for 18: 0.747
Num non-zero elements: 5
Test accuracy for 19: 0.897
Num non-zero elements: 10
Test accuracy for 11: 0.875
Num non-zero elements: 10
Test accuracy for 13: 0.921
Num non-zero elements: 10
Test accuracy for 14: 0.867
Num non-zero elements: 10
Test accuracy for 18: 0.748
Num non-zero elements: 10
Test accuracy for 19: 0.911
Num non-zero elements: 20
Test accuracy for 11: 0.94
Num non-zero elements: 20
Test accuracy for 13: 0.915
Num non-zero elements: 20
Test accuracy for 14: 0.895
Num non-zero elements: 20
Test accuracy for 18: 0.865
Num non-zero elements: 20
Test accuracy for 19: 0.922
Num non-zero elements: 50
Test accuracy for 11: 0.955
Num non-zero elements: 50
Test accuracy for 13: 0.923
Num non-zero elements: 50
Test accuracy for 14: 0.935
Num non-zero elements: 50
Test accuracy for 18: 0.887
Num non-zero elements: 50
Test accuracy for 19: 0.938
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.947
Num non-zero elements: 2304
Test accuracy for 21: 0.888
Num non-zero elements: 2304
Test accuracy for 22: 0.903
Num non-zero elements: 2304
Test accuracy for 25: 0.951
Num non-zero elements: 2304
Test accuracy for 26: 0.861
Num non-zero elements: 1
Test accuracy for 20: 0.722
Num non-zero elements: 1
Test accuracy for 21: 0.774
Num non-zero elements: 1
Test accuracy for 22: 0.647
Num non-zero elements: 1
Test accuracy for 25: 0.703
Num non-zero elements: 1
Test accuracy for 26: 0.625
Num non-zero elements: 2
Test accuracy for 20: 0.81
Num non-zero elements: 2
Test accuracy for 21: 0.758
Num non-zero elements: 2
Test accuracy for 22: 0.682
Num non-zero elements: 2
Test accuracy for 25: 0.769
Num non-zero elements: 2
Test accuracy for 26: 0.682
Num non-zero elements: 5
Test accuracy for 20: 0.856
Num non-zero elements: 5
Test accuracy for 21: 0.804
Num non-zero elements: 5
Test accuracy for 22: 0.695
Num non-zero elements: 5
Test accuracy for 25: 0.796
Num non-zero elements: 5
Test accuracy for 26: 0.672
Num non-zero elements: 10
Test accuracy for 20: 0.875
Num non-zero elements: 10
Test accuracy for 21: 0.811
Num non-zero elements: 10
Test accuracy for 22: 0.749
Num non-zero elements: 10
Test accuracy for 25: 0.849
Num non-zero elements: 10
Test accuracy for 26: 0.712
Num non-zero elements: 20
Test accuracy for 20: 0.911
Num non-zero elements: 20
Test accuracy for 21: 0.826
Num non-zero elements: 20
Test accuracy for 22: 0.787
Num non-zero elements: 20
Test accuracy for 25: 0.891
Num non-zero elements: 20
Test accuracy for 26: 0.819
Num non-zero elements: 50
Test accuracy for 20: 0.921
Num non-zero elements: 50
Test accuracy for 21: 0.88
Num non-zero elements: 50
Test accuracy for 22: 0.868
Num non-zero elements: 50
Test accuracy for 25: 0.932
Num non-zero elements: 50
Test accuracy for 26: 0.843
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 20: 0.9640000462532043
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 21: 0.9270000457763672
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 22: 0.9140000343322754
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for 25: 0.9600000381469727
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 26: 0.8980000615119934
Num non-zero elements: 1
Test accuracy for 20: 0.862
Num non-zero elements: 1
Test accuracy for 21: 0.781
Num non-zero elements: 1
Test accuracy for 22: 0.742
Num non-zero elements: 1
Test accuracy for 25: 0.814
Num non-zero elements: 1
Test accuracy for 26: 0.731
Num non-zero elements: 2
Test accuracy for 20: 0.905
Num non-zero elements: 2
Test accuracy for 21: 0.786
Num non-zero elements: 2
Test accuracy for 22: 0.778
Num non-zero elements: 2
Test accuracy for 25: 0.85
Num non-zero elements: 2
Test accuracy for 26: 0.804
Num non-zero elements: 5
Test accuracy for 20: 0.916
Num non-zero elements: 5
Test accuracy for 21: 0.85
Num non-zero elements: 5
Test accuracy for 22: 0.836
Num non-zero elements: 5
Test accuracy for 25: 0.908
Num non-zero elements: 5
Test accuracy for 26: 0.796
Num non-zero elements: 10
Test accuracy for 20: 0.918
Num non-zero elements: 10
Test accuracy for 21: 0.856
Num non-zero elements: 10
Test accuracy for 22: 0.849
Num non-zero elements: 10
Test accuracy for 25: 0.913
Num non-zero elements: 10
Test accuracy for 26: 0.811
Num non-zero elements: 20
Test accuracy for 20: 0.924
Num non-zero elements: 20
Test accuracy for 21: 0.875
Num non-zero elements: 20
Test accuracy for 22: 0.856
Num non-zero elements: 20
Test accuracy for 25: 0.923
Num non-zero elements: 20
Test accuracy for 26: 0.839
Num non-zero elements: 50
Test accuracy for 20: 0.94
Num non-zero elements: 50
Test accuracy for 21: 0.883
Num non-zero elements: 50
Test accuracy for 22: 0.875
Num non-zero elements: 50
Test accuracy for 25: 0.954
Num non-zero elements: 50
Test accuracy for 26: 0.845
Removing 28 from train set
Removing 0 from train set
Removing 7 from train set
Removing 31 from train set
Removing 29 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.938
Num non-zero elements: 2304
Test accuracy for 2: 0.901
Num non-zero elements: 2304
Test accuracy for 3: 0.917
Num non-zero elements: 2304
Test accuracy for 5: 0.9
Num non-zero elements: 2304
Test accuracy for 6: 0.832
Num non-zero elements: 1
Test accuracy for 1: 0.685
Num non-zero elements: 1
Test accuracy for 2: 0.579
Num non-zero elements: 1
Test accuracy for 3: 0.594
Num non-zero elements: 1
Test accuracy for 5: 0.567
Num non-zero elements: 1
Test accuracy for 6: 0.578
Num non-zero elements: 2
Test accuracy for 1: 0.747
Num non-zero elements: 2
Test accuracy for 2: 0.636
Num non-zero elements: 2
Test accuracy for 3: 0.61
Num non-zero elements: 2
Test accuracy for 5: 0.622
Num non-zero elements: 2
Test accuracy for 6: 0.627
Num non-zero elements: 5
Test accuracy for 1: 0.773
Num non-zero elements: 5
Test accuracy for 2: 0.633
Num non-zero elements: 5
Test accuracy for 3: 0.649
Num non-zero elements: 5
Test accuracy for 5: 0.628
Num non-zero elements: 5
Test accuracy for 6: 0.699
Num non-zero elements: 10
Test accuracy for 1: 0.795
Num non-zero elements: 10
Test accuracy for 2: 0.764
Num non-zero elements: 10
Test accuracy for 3: 0.751
Num non-zero elements: 10
Test accuracy for 5: 0.747
Num non-zero elements: 10
Test accuracy for 6: 0.743
Num non-zero elements: 20
Test accuracy for 1: 0.862
Num non-zero elements: 20
Test accuracy for 2: 0.851
Num non-zero elements: 20
Test accuracy for 3: 0.796
Num non-zero elements: 20
Test accuracy for 5: 0.791
Num non-zero elements: 20
Test accuracy for 6: 0.746
Num non-zero elements: 50
Test accuracy for 1: 0.924
Num non-zero elements: 50
Test accuracy for 2: 0.877
Num non-zero elements: 50
Test accuracy for 3: 0.855
Num non-zero elements: 50
Test accuracy for 5: 0.858
Num non-zero elements: 50
Test accuracy for 6: 0.805
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 1: 0.9450000524520874
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 2: 0.9360000491142273
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 3: 0.9270000457763672
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 5: 0.9240000247955322
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 6: 0.8710000514984131
Num non-zero elements: 1
Test accuracy for 1: 0.836
Num non-zero elements: 1
Test accuracy for 2: 0.639
Num non-zero elements: 1
Test accuracy for 3: 0.614
Num non-zero elements: 1
Test accuracy for 5: 0.609
Num non-zero elements: 1
Test accuracy for 6: 0.732
Num non-zero elements: 2
Test accuracy for 1: 0.886
Num non-zero elements: 2
Test accuracy for 2: 0.663
Num non-zero elements: 2
Test accuracy for 3: 0.661
Num non-zero elements: 2
Test accuracy for 5: 0.822
Num non-zero elements: 2
Test accuracy for 6: 0.754
Num non-zero elements: 5
Test accuracy for 1: 0.914
Num non-zero elements: 5
Test accuracy for 2: 0.713
Num non-zero elements: 5
Test accuracy for 3: 0.741
Num non-zero elements: 5
Test accuracy for 5: 0.846
Num non-zero elements: 5
Test accuracy for 6: 0.767
Num non-zero elements: 10
Test accuracy for 1: 0.926
Num non-zero elements: 10
Test accuracy for 2: 0.779
Num non-zero elements: 10
Test accuracy for 3: 0.782
Num non-zero elements: 10
Test accuracy for 5: 0.87
Num non-zero elements: 10
Test accuracy for 6: 0.78
Num non-zero elements: 20
Test accuracy for 1: 0.924
Num non-zero elements: 20
Test accuracy for 2: 0.862
Num non-zero elements: 20
Test accuracy for 3: 0.819
Num non-zero elements: 20
Test accuracy for 5: 0.883
Num non-zero elements: 20
Test accuracy for 6: 0.777
Num non-zero elements: 50
Test accuracy for 1: 0.936
Num non-zero elements: 50
Test accuracy for 2: 0.896
Num non-zero elements: 50
Test accuracy for 3: 0.888
Num non-zero elements: 50
Test accuracy for 5: 0.9
Num non-zero elements: 50
Test accuracy for 6: 0.81
Num non-zero elements: 2304
Test accuracy for 1.0: 0.981
Num non-zero elements: 2304
Test accuracy for 5.0: 0.982
Num non-zero elements: 1
Test accuracy for 1.0: 0.671
Num non-zero elements: 1
Test accuracy for 5.0: 0.671
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.847
Num non-zero elements: 20
Test accuracy for 5.0: 0.847
Num non-zero elements: 50
Test accuracy for 1.0: 0.933
Num non-zero elements: 50
Test accuracy for 5.0: 0.933
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 1.0: 0.9820000529289246
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 5.0: 0.9800000190734863
Num non-zero elements: 1
Test accuracy for 1.0: 0.889
Num non-zero elements: 1
Test accuracy for 5.0: 0.889
Num non-zero elements: 2
Test accuracy for 1.0: 0.908
Num non-zero elements: 2
Test accuracy for 5.0: 0.908
Num non-zero elements: 5
Test accuracy for 1.0: 0.919
Num non-zero elements: 5
Test accuracy for 5.0: 0.919
Num non-zero elements: 10
Test accuracy for 1.0: 0.935
Num non-zero elements: 10
Test accuracy for 5.0: 0.935
Num non-zero elements: 20
Test accuracy for 1.0: 0.949
Num non-zero elements: 20
Test accuracy for 5.0: 0.949
Num non-zero elements: 50
Test accuracy for 1.0: 0.952
Num non-zero elements: 50
Test accuracy for 5.0: 0.952
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.956
Num non-zero elements: 2304
Test accuracy for Python: 0.982
Num non-zero elements: 2304
Test accuracy for HTML: 0.988
Num non-zero elements: 2304
Test accuracy for Java: 0.962
Num non-zero elements: 2304
Test accuracy for PHP: 0.955
Num non-zero elements: 1
Test accuracy for C: 0.663
Num non-zero elements: 1
Test accuracy for Python: 0.647
Num non-zero elements: 1
Test accuracy for HTML: 0.711
Num non-zero elements: 1
Test accuracy for Java: 0.617
Num non-zero elements: 1
Test accuracy for PHP: 0.589
Num non-zero elements: 2
Test accuracy for C: 0.676
Num non-zero elements: 2
Test accuracy for Python: 0.677
Num non-zero elements: 2
Test accuracy for HTML: 0.784
Num non-zero elements: 2
Test accuracy for Java: 0.69
Num non-zero elements: 2
Test accuracy for PHP: 0.638
Num non-zero elements: 5
Test accuracy for C: 0.756
Num non-zero elements: 5
Test accuracy for Python: 0.716
Num non-zero elements: 5
Test accuracy for HTML: 0.907
Num non-zero elements: 5
Test accuracy for Java: 0.741
Num non-zero elements: 5
Test accuracy for PHP: 0.66
Num non-zero elements: 10
Test accuracy for C: 0.798
Num non-zero elements: 10
Test accuracy for Python: 0.741
Num non-zero elements: 10
Test accuracy for HTML: 0.947
Num non-zero elements: 10
Test accuracy for Java: 0.79
Num non-zero elements: 10
Test accuracy for PHP: 0.739
Num non-zero elements: 20
Test accuracy for C: 0.841
Num non-zero elements: 20
Test accuracy for Python: 0.856
Num non-zero elements: 20
Test accuracy for HTML: 0.949
Num non-zero elements: 20
Test accuracy for Java: 0.854
Num non-zero elements: 20
Test accuracy for PHP: 0.831
Num non-zero elements: 50
Test accuracy for C: 0.911
Num non-zero elements: 50
Test accuracy for Python: 0.943
Num non-zero elements: 50
Test accuracy for HTML: 0.969
Num non-zero elements: 50
Test accuracy for Java: 0.892
Num non-zero elements: 50
Test accuracy for PHP: 0.903
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for C: 0.9550000429153442
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for Python: 0.9880000352859497
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for HTML: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for Java: 0.9630000591278076
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for PHP: 0.9570000171661377
Num non-zero elements: 1
Test accuracy for C: 0.562
Num non-zero elements: 1
Test accuracy for Python: 0.56
Num non-zero elements: 1
Test accuracy for HTML: 0.843
Num non-zero elements: 1
Test accuracy for Java: 0.649
Num non-zero elements: 1
Test accuracy for PHP: 0.606
Num non-zero elements: 2
Test accuracy for C: 0.635
Num non-zero elements: 2
Test accuracy for Python: 0.554
Num non-zero elements: 2
Test accuracy for HTML: 0.864
Num non-zero elements: 2
Test accuracy for Java: 0.657
Num non-zero elements: 2
Test accuracy for PHP: 0.612
Num non-zero elements: 5
Test accuracy for C: 0.705
Num non-zero elements: 5
Test accuracy for Python: 0.713
Num non-zero elements: 5
Test accuracy for HTML: 0.927
Num non-zero elements: 5
Test accuracy for Java: 0.658
Num non-zero elements: 5
Test accuracy for PHP: 0.7
Num non-zero elements: 10
Test accuracy for C: 0.827
Num non-zero elements: 10
Test accuracy for Python: 0.719
Num non-zero elements: 10
Test accuracy for HTML: 0.946
Num non-zero elements: 10
Test accuracy for Java: 0.715
Num non-zero elements: 10
Test accuracy for PHP: 0.908
Num non-zero elements: 20
Test accuracy for C: 0.856
Num non-zero elements: 20
Test accuracy for Python: 0.774
Num non-zero elements: 20
Test accuracy for HTML: 0.959
Num non-zero elements: 20
Test accuracy for Java: 0.826
Num non-zero elements: 20
Test accuracy for PHP: 0.909
Num non-zero elements: 50
Test accuracy for C: 0.898
Num non-zero elements: 50
Test accuracy for Python: 0.859
Num non-zero elements: 50
Test accuracy for HTML: 0.967
Num non-zero elements: 50
Test accuracy for Java: 0.896
Num non-zero elements: 50
Test accuracy for PHP: 0.911
Num non-zero elements: 2304
Test accuracy for 0: 0.933
Num non-zero elements: 2304
Test accuracy for 1: 0.987
Num non-zero elements: 2304
Test accuracy for 2: 0.913
Num non-zero elements: 2304
Test accuracy for 3: 0.929
Num non-zero elements: 1
Test accuracy for 0: 0.567
Num non-zero elements: 1
Test accuracy for 1: 0.671
Num non-zero elements: 1
Test accuracy for 2: 0.662
Num non-zero elements: 1
Test accuracy for 3: 0.631
Num non-zero elements: 2
Test accuracy for 0: 0.7
Num non-zero elements: 2
Test accuracy for 1: 0.79
Num non-zero elements: 2
Test accuracy for 2: 0.684
Num non-zero elements: 2
Test accuracy for 3: 0.809
Num non-zero elements: 5
Test accuracy for 0: 0.825
Num non-zero elements: 5
Test accuracy for 1: 0.886
Num non-zero elements: 5
Test accuracy for 2: 0.753
Num non-zero elements: 5
Test accuracy for 3: 0.842
Num non-zero elements: 10
Test accuracy for 0: 0.882
Num non-zero elements: 10
Test accuracy for 1: 0.934
Num non-zero elements: 10
Test accuracy for 2: 0.799
Num non-zero elements: 10
Test accuracy for 3: 0.889
Num non-zero elements: 20
Test accuracy for 0: 0.892
Num non-zero elements: 20
Test accuracy for 1: 0.955
Num non-zero elements: 20
Test accuracy for 2: 0.836
Num non-zero elements: 20
Test accuracy for 3: 0.904
Num non-zero elements: 50
Test accuracy for 0: 0.906
Num non-zero elements: 50
Test accuracy for 1: 0.971
Num non-zero elements: 50
Test accuracy for 2: 0.889
Num non-zero elements: 50
Test accuracy for 3: 0.931
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.9440000653266907
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 1: 0.987000048160553
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 2: 0.9300000667572021
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 3: 0.9500000476837158
Num non-zero elements: 1
Test accuracy for 0: 0.853
Num non-zero elements: 1
Test accuracy for 1: 0.979
Num non-zero elements: 1
Test accuracy for 2: 0.821
Num non-zero elements: 1
Test accuracy for 3: 0.701
Num non-zero elements: 2
Test accuracy for 0: 0.876
Num non-zero elements: 2
Test accuracy for 1: 0.972
Num non-zero elements: 2
Test accuracy for 2: 0.822
Num non-zero elements: 2
Test accuracy for 3: 0.727
Num non-zero elements: 5
Test accuracy for 0: 0.893
Num non-zero elements: 5
Test accuracy for 1: 0.978
Num non-zero elements: 5
Test accuracy for 2: 0.836
Num non-zero elements: 5
Test accuracy for 3: 0.848
Num non-zero elements: 10
Test accuracy for 0: 0.895
Num non-zero elements: 10
Test accuracy for 1: 0.977
Num non-zero elements: 10
Test accuracy for 2: 0.875
Num non-zero elements: 10
Test accuracy for 3: 0.875
Num non-zero elements: 20
Test accuracy for 0: 0.908
Num non-zero elements: 20
Test accuracy for 1: 0.98
Num non-zero elements: 20
Test accuracy for 2: 0.876
Num non-zero elements: 20
Test accuracy for 3: 0.901
Num non-zero elements: 50
Test accuracy for 0: 0.929
Num non-zero elements: 50
Test accuracy for 1: 0.981
Num non-zero elements: 50
Test accuracy for 2: 0.892
Num non-zero elements: 50
Test accuracy for 3: 0.943
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.999
Num non-zero elements: 2304
Test accuracy for fr: 1.0
Num non-zero elements: 2304
Test accuracy for de: 0.999
Num non-zero elements: 2304
Test accuracy for es: 1.0
Num non-zero elements: 2304
Test accuracy for nl: 0.999
Num non-zero elements: 1
Test accuracy for en: 0.732
Num non-zero elements: 1
Test accuracy for fr: 0.597
Num non-zero elements: 1
Test accuracy for de: 0.753
Num non-zero elements: 1
Test accuracy for es: 0.492
Num non-zero elements: 1
Test accuracy for nl: 0.65
Num non-zero elements: 2
Test accuracy for en: 0.821
Num non-zero elements: 2
Test accuracy for fr: 0.595
Num non-zero elements: 2
Test accuracy for de: 0.83
Num non-zero elements: 2
Test accuracy for es: 0.914
Num non-zero elements: 2
Test accuracy for nl: 0.747
Num non-zero elements: 5
Test accuracy for en: 0.887
Num non-zero elements: 5
Test accuracy for fr: 0.925
Num non-zero elements: 5
Test accuracy for de: 0.869
Num non-zero elements: 5
Test accuracy for es: 0.979
Num non-zero elements: 5
Test accuracy for nl: 0.867
Num non-zero elements: 10
Test accuracy for en: 0.978
Num non-zero elements: 10
Test accuracy for fr: 0.974
Num non-zero elements: 10
Test accuracy for de: 0.96
Num non-zero elements: 10
Test accuracy for es: 0.989
Num non-zero elements: 10
Test accuracy for nl: 0.94
Num non-zero elements: 20
Test accuracy for en: 0.991
Num non-zero elements: 20
Test accuracy for fr: 0.986
Num non-zero elements: 20
Test accuracy for de: 0.981
Num non-zero elements: 20
Test accuracy for es: 0.996
Num non-zero elements: 20
Test accuracy for nl: 0.984
Num non-zero elements: 50
Test accuracy for en: 1.0
Num non-zero elements: 50
Test accuracy for fr: 1.0
Num non-zero elements: 50
Test accuracy for de: 0.995
Num non-zero elements: 50
Test accuracy for es: 1.0
Num non-zero elements: 50
Test accuracy for nl: 0.994
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for en: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for fr: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for de: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for es: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for nl: 0.999000072479248
Num non-zero elements: 1
Test accuracy for en: 0.999
Num non-zero elements: 1
Test accuracy for fr: 0.819
Num non-zero elements: 1
Test accuracy for de: 0.81
Num non-zero elements: 1
Test accuracy for es: 0.859
Num non-zero elements: 1
Test accuracy for nl: 0.756
Num non-zero elements: 2
Test accuracy for en: 0.998
Num non-zero elements: 2
Test accuracy for fr: 0.826
Num non-zero elements: 2
Test accuracy for de: 0.874
Num non-zero elements: 2
Test accuracy for es: 0.872
Num non-zero elements: 2
Test accuracy for nl: 0.851
Num non-zero elements: 5
Test accuracy for en: 0.999
Num non-zero elements: 5
Test accuracy for fr: 0.856
Num non-zero elements: 5
Test accuracy for de: 0.892
Num non-zero elements: 5
Test accuracy for es: 0.885
Num non-zero elements: 5
Test accuracy for nl: 0.857
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.887
Num non-zero elements: 10
Test accuracy for de: 0.966
Num non-zero elements: 10
Test accuracy for es: 0.902
Num non-zero elements: 10
Test accuracy for nl: 0.929
Num non-zero elements: 20
Test accuracy for en: 1.0
Num non-zero elements: 20
Test accuracy for fr: 0.897
Num non-zero elements: 20
Test accuracy for de: 0.971
Num non-zero elements: 20
Test accuracy for es: 0.969
Num non-zero elements: 20
Test accuracy for nl: 0.97
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 0.953
Num non-zero elements: 50
Test accuracy for de: 0.984
Num non-zero elements: 50
Test accuracy for es: 0.984
Num non-zero elements: 50
Test accuracy for nl: 0.973
Time spent on sparse_probing: 627.73s

[2025-02-06 15:53:51]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
24432 16027
1962 859
torch.Size([6378, 1024]) torch.Size([275, 1024])
Forget sparsity: [0.001841 0.004826 0.027409 ... 0.017248 0.033188 0.002655]
(16384,)
Retain sparsity: [0.002255 0.00668  0.02557  ... 0.018389 0.045116 0.00262 ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15494 12897  6853  4299  7792   735  7492 14419  6532 12036 15408 16303
  3314  5592  9963  5447 13558  5235 14021  4321  5102  5787 16312  9376
  4335  3109 14359  3158 11204 12264 11447  4582   788   976   888  4625
  1833  9704  1140  5829  9234 10138 11844  6346  4614  5742 12057  8940
  1647 11787  8948  2895 12208  5917 12650  8911   889 13492 14471 13944
  8097  4013  1398  6074 14189  3667 10259 15811  4566  4350 13088 10688
 15916  2945 10581 12598  9102  5585 10332  7225  1259  5403  2916  7987
  9364  2270  5145  9564  9060  7753 11197   666   223 12816  9594 13211
 13048 15790   195  1557 12827 15053 10532  4410   106   364 12108 14395
 14496 15306 15211  9511  9129 15923  9430 13450  2659 14885  7528   846
  3341  3036  7769  2705  1188 15688 10961  9863 16076 11982   932  2042
  9209  4821 16197 14796 11918 14192 15504  8671  6073  3233 11669  7385
  4789 15820  4991  7328 10449  9637  6300  1579 10744  6823  3028  3465
    84  2874  3772 13991 12260  1207 14532   638  4849  6399  7700 10024
 15816 15374 14707   711 15846 16304  8749  9689  5484  3873  4458  8631
  6066   456 14204  1462 12861  9000  6940 10277  1871 15322  2655 10807
  1119  1634 13476 12173  7141 12933 11432 13265  8680  8545 10083  6060
  3809  4488 16174 11625 14949  1500 11996  2709  4361 12205 13930 15509
 11529 10901  3707  5119  7907 13058 10289  8039  1360  2528  7609  6039
  5417   689  3380 13094  5620  2789  4874 11578  4125   168  8549 13463
 15436  2755  7988  3622   399 12747   263  7968    79  5993 11034 10302
  8327  8959  6662  1455  4595   987  8817  9475  1201  1975  4223  6079
  9230  1896  2132 11902  5079  1444 10455  4662  7309  5761  6159  5706
  4143  9175   639 12244  4178 11929  4855  1182 15735   883 12418 15577
 15928  7756   499  5749  5921 10895 15113  7556 11221 15955 11788  6264
 10993 12600  9537 11163 14505 11828  4445  6178  3760  9504 10681 10387
  7144  7249  6784  9237 15222 15814  1535  3907  5897  6061 11514  2046
  7455  6115  8821  5571 12610   588  3941 15741 11451  7918 13403  8571
  9635  6737 13685  2847 16176 11454  9201  1571 15808 13371 13513 10313
 10866  9212 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036]), array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036, 15408, 16303,  3314,  5592,  9963,  5447, 13558,  5235,
       14021,  4321])], 'multiplier': [25, 50, 100, 200]}
Saving target question ids for gemma-2-2b-it on wmdp-bio...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct.csv saved, with 267 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct.csv saved, with 267 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct.csv saved, with 534 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct-iff-question.csv saved, with 243 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct-iff-question.csv saved, with 244 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct-iff-question.csv saved, with 487 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/wmdp-bio_correct-no-tricks.csv saved, with 262 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/wmdp-bio_correct-no-tricks.csv saved, with 263 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/wmdp-bio_correct-no-tricks.csv saved, with 525 questions
Saving target question ids for gemma-2-2b-it on high_school_us_history...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct.csv saved, with 110 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct-iff-question.csv saved, with 54 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct-iff-question.csv saved, with 54 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct-iff-question.csv saved, with 108 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 55 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-us-history_correct-no-tricks.csv saved, with 110 questions
Saving target question ids for gemma-2-2b-it on college_computer_science...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct.csv saved, with 9 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct-iff-question.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct-iff-question.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct-iff-question.csv saved, with 9 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-college-computer-science_correct-no-tricks.csv saved, with 4 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-college-computer-science_correct-no-tricks.csv saved, with 5 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-college-computer-science_correct-no-tricks.csv saved, with 9 questions
Saving target question ids for gemma-2-2b-it on high_school_geography...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct.csv saved, with 51 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct.csv saved, with 52 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct.csv saved, with 103 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct-iff-question.csv saved, with 49 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct-iff-question.csv saved, with 50 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct-iff-question.csv saved, with 99 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-high-school-geography_correct-no-tricks.csv saved, with 51 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-high-school-geography_correct-no-tricks.csv saved, with 52 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-high-school-geography_correct-no-tricks.csv saved, with 103 questions
Saving target question ids for gemma-2-2b-it on human_aging...
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct.csv saved, with 84 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct-iff-question.csv saved, with 40 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct-iff-question.csv saved, with 41 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct-iff-question.csv saved, with 81 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/train/mmlu-human-aging_correct-no-tricks.csv saved, with 41 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/test/mmlu-human-aging_correct-no-tricks.csv saved, with 42 questions
artifacts/unlearning/google/gemma-2-2b-it/data/question_ids/all/mmlu-human-aging_correct-no-tricks.csv saved, with 83 questions
Calculating metrics for retain threshold: 0.01
Top features for ablation: [12681 11085 10405 ... 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343]), array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343, 14751,   276, 13183, 10253,  4801, 14016,    14,  8289,
        1329, 13470])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   1.000000                1.000000  ...          25              1.000000
1   0.842697                0.945455  ...         200              0.931373
2   0.973783                1.000000  ...         100              0.996732
3   0.743446                0.736364  ...         200              0.669935
4   0.803371                0.918182  ...         200              0.888889
5   0.992509                1.000000  ...          50              0.996732
6   0.992509                0.990909  ...         100              0.986928
7   0.973783                0.963636  ...         100              0.967320
8   1.000000                0.990909  ...          50              0.993464
9   0.998127                1.000000  ...          50              1.000000
10  1.000000                1.000000  ...          25              1.000000
11  0.958802                1.000000  ...         100              0.986928
12  1.000000                0.990909  ...          25              0.996732
13  1.000000                1.000000  ...          25              1.000000
14  0.977528                0.981818  ...         200              0.970588
15  0.998127                0.981818  ...          50              0.983660

[16 rows x 10 columns]
Currently on iteration 1 of run 2
Let me first add the Run 1 results to notes.txt, then proceed with Run 2 implementation.                                                                 

notes.txt                                                                                                                                                

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 # Title: Progressive Sparsity-Guided Orthogonality for Feature Separation                                                                               
 # Experiment description: 1. Implement progress-based competition thresholds                                                                            
 2. Add dynamic orthogonality weighting                                                                                                                  
 3. Train on google/gemma-2-2b using standard datasets                                                                                                   
 4. Compare against baseline and static orthogonal SAEs                                                                                                  
 5. Analyze feature separation across training phases                                                                                                    
 6. Evaluate impact of progression schedule                                                                                                              
 ## Run 0: Baseline                                                                                                                                      
 =======                                                                                                                                                 
 # Title: Progressive Sparsity-Guided Orthogonality for Feature Separation                                                                               
 # Experiment description: 1. Implement progress-based competition thresholds                                                                            
 2. Add dynamic orthogonality weighting                                                                                                                  
 3. Train on google/gemma-2-2b using standard datasets                                                                                                   
 4. Compare against baseline and static orthogonal SAEs                                                                                                  
 5. Analyze feature separation across training phases                                                                                                    
 6. Evaluate impact of progression schedule                                                                                                              
                                                                                                                                                         
 ## Run 0: Baseline                                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 Description: Baseline results.                                                                                                                          
 =======                                                                                                                                                 
 Description: Baseline results.                                                                                                                          
                                                                                                                                                         
 ## Run 1: Basic Progressive Orthogonality                                                                                                               
 Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress       
 ((1-p)^2).                                                                                                                                              
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Added progress tracking (step/total_steps)                                                                                                            
 - Implemented orthogonality loss between feature vectors                                                                                                
 - Applied progressive weighting scheme                                                                                                                  
 - Base orthogonality weight λ=0.1                                                                                                                       
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.04128 (target <0.01)                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                     
    - Suggests orthogonality may be too strong initially                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7756 (target >0.74)                                                                                                                
    - Better than baseline (0.7449)                                                                                                                      
    - Shows improved feature separation                                                                                                                  
                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                               
    - Lower than baseline (0.0637)                                                                                                                       
    - May need to adjust feature competition                                                                                                             
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375                                                                                                                           
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. While feature separation improved (sparse probing), the absorption score increased                                                                   
 2. Initial orthogonality constraint may be too strict                                                                                                   
 3. Need to balance feature separation with reconstruction quality                                                                                       
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Implement dynamic threshold computation                                                                                                              
 2. Reduce initial orthogonality weight                                                                                                                  
 3. Adjust progression schedule                                                                                                                          
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

For Run 2, let's implement dynamic threshold computation and reduce the initial orthogonality weight. Here are the changes:                              

experiment.py                                                                                                                                            

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def __init__(                                                                                                                                       
         self,                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                     
         activation_dim=512,                                                                                                                             
         dict_size=64 * 512,                                                                                                                             
         k=100,                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                          
         ortho_lambda=0.1,  # orthogonality loss weight                                                                                                  
         decay_start=24000,  # when does the lr decay start                                                                                              
 =======                                                                                                                                                 
     def __init__(                                                                                                                                       
         self,                                                                                                                                           
         dict_class=AutoEncoderTopK,                                                                                                                     
         activation_dim=512,                                                                                                                             
         dict_size=64 * 512,                                                                                                                             
         k=100,                                                                                                                                          
         auxk_alpha=1 / 32,  # see Appendix A.2                                                                                                          
         ortho_lambda=0.05,  # reduced orthogonality loss weight                                                                                         
         base_threshold=0.1,  # base threshold for competition                                                                                           
         decay_start=24000,  # when does the lr decay start                                                                                              
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss"]                                                           
         self.effective_l0 = -1                                                                                                                          
         self.dead_features = -1                                                                                                                         
         self.progress = 0.0                                                                                                                             
         self.ortho_loss = 0.0                                                                                                                           
         self.ortho_lambda = ortho_lambda                                                                                                                
 =======                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold"]                                              
         self.effective_l0 = -1                                                                                                                          
         self.dead_features = -1                                                                                                                         
         self.progress = 0.0                                                                                                                             
         self.ortho_loss = 0.0                                                                                                                           
         self.threshold = 0.0                                                                                                                            
         self.ortho_lambda = ortho_lambda                                                                                                                
         self.base_threshold = base_threshold                                                                                                            
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def compute_orthogonality_loss(self, f):                                                                                                            
 =======                                                                                                                                                 
     def compute_threshold(self, progress):                                                                                                              
         """Compute dynamic threshold based on training progress"""                                                                                      
         return self.base_threshold * (1 - progress)                                                                                                     
                                                                                                                                                         
     def compute_competition_coefficient(self, correlation, threshold):                                                                                  
         """Compute smooth competition coefficient"""                                                                                                    
         return torch.clamp((correlation - threshold) / threshold, min=0.0, max=1.0)                                                                     
                                                                                                                                                         
     def compute_orthogonality_loss(self, f):                                                                                                            
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         # Compute feature correlations                                                                                                                  
         f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)                                                                                        
         feature_correlations = torch.abs(torch.mm(f_norm.T, f_norm))                                                                                    
                                                                                                                                                         
         # Zero out diagonal elements (self-correlation)                                                                                                 
         mask = 1 - torch.eye(feature_correlations.shape[0], device=f.device)                                                                            
         feature_correlations = feature_correlations * mask                                                                                              
                                                                                                                                                         
         # Compute mean correlation                                                                                                                      
         ortho_loss = feature_correlations.mean()                                                                                                        
         return ortho_loss                                                                                                                               
 =======                                                                                                                                                 
         # Normalize features                                                                                                                            
         f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)                                                                                        
                                                                                                                                                         
         # Compute correlations                                                                                                                          
         correlations = torch.abs(torch.mm(f_norm.T, f_norm))                                                                                            
                                                                                                                                                         
         # Zero out diagonal                                                                                                                             
         mask = 1 - torch.eye(correlations.shape[0], device=f.device)                                                                                    
         correlations = correlations * mask                                                                                                              
                                                                                                                                                         
         # Get current threshold                                                                                                                         
         threshold = self.compute_threshold(self.progress)                                                                                               
         self.threshold = threshold                                                                                                                      
                                                                                                                                                         
         # Compute competition coefficients                                                                                                              
         coefficients = self.compute_competition_coefficient(correlations, threshold)                                                                    
                                                                                                                                                         
         # Weight correlations by coefficients                                                                                                           
         weighted_correlations = correlations * coefficients                                                                                             
                                                                                                                                                         
         return weighted_correlations.mean()                                                                                                             
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

These changes:                                                                                                                                           

 1 Reduce initial orthogonality weight from 0.1 to 0.05                                                                                                  
 2 Add dynamic threshold computation based on training progress                                                                                          
 3 Implement smooth competition coefficients                                                                                                             
 4 Add threshold tracking to logging parameters                                                                                                          

The key differences from Run 1:                                                                                                                          

 • Lower initial orthogonality weight to reduce early constraint strength                                                                                
 • Dynamic threshold that decreases linearly with training progress                                                                                      
 • Smooth competition coefficients that only penalize correlations above threshold                                                                       
 • More granular control over feature relationships                                                                                                      

Let's run this with:                                                                                                                                     

                                                                                                                                                         
 python experiment.py --out_dir=run_2                                                                                                                    
                                                                                                                                                         

This should provide better balance between feature separation and reconstruction quality while maintaining the progressive nature of the constraints.    

Tokens: 53k sent, 1.8k received. Cost: $0.19 message, $0.34 session.
Applied edit to notes.txt
Applied edit to experiment.py

python experiment.py --out_dir=run_2
coder_out: Let me first add the Run 1 results to notes.txt, then proceed with Run 2 implementation.

notes.txt
```python
<<<<<<< SEARCH
# Title: Progressive Sparsity-Guided Orthogonality for Feature Separation
# Experiment description: 1. Implement progress-based competition thresholds
2. Add dynamic orthogonality weighting
3. Train on google/gemma-2-2b using standard datasets
4. Compare against baseline and static orthogonal SAEs
5. Analyze feature separation across training phases
6. Evaluate impact of progression schedule
## Run 0: Baseline
=======
# Title: Progressive Sparsity-Guided Orthogonality for Feature Separation
# Experiment description: 1. Implement progress-based competition thresholds
2. Add dynamic orthogonality weighting
3. Train on google/gemma-2-2b using standard datasets
4. Compare against baseline and static orthogonal SAEs
5. Analyze feature separation across training phases
6. Evaluate impact of progression schedule

## Run 0: Baseline
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
Description: Baseline results.
=======
Description: Baseline results.

## Run 1: Basic Progressive Orthogonality
Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress ((1-p)^2).

Key Implementation Details:
- Added progress tracking (step/total_steps)
- Implemented orthogonality loss between feature vectors
- Applied progressive weighting scheme
- Base orthogonality weight λ=0.1

Results Analysis:
1. Absorption Score: 0.04128 (target <0.01)
   - Higher than baseline (0.02959)
   - Suggests orthogonality may be too strong initially

2. Sparse Probing: 0.7756 (target >0.74)
   - Better than baseline (0.7449)
   - Shows improved feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Lower than baseline (0.0637)
   - May need to adjust feature competition

Core Metrics:
- Reconstruction MSE: 1.59375
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. While feature separation improved (sparse probing), the absorption score increased
2. Initial orthogonality constraint may be too strict
3. Need to balance feature separation with reconstruction quality

Next Steps:
1. Implement dynamic threshold computation
2. Reduce initial orthogonality weight
3. Adjust progression schedule
>>>>>>> REPLACE
```

For Run 2, let's implement dynamic threshold computation and reduce the initial orthogonality weight. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        ortho_lambda=0.1,  # orthogonality loss weight
        decay_start=24000,  # when does the lr decay start
=======
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        ortho_lambda=0.05,  # reduced orthogonality loss weight
        base_threshold=0.1,  # base threshold for competition
        decay_start=24000,  # when does the lr decay start
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss"]
        self.effective_l0 = -1
        self.dead_features = -1
        self.progress = 0.0
        self.ortho_loss = 0.0
        self.ortho_lambda = ortho_lambda
=======
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold"]
        self.effective_l0 = -1
        self.dead_features = -1
        self.progress = 0.0
        self.ortho_loss = 0.0
        self.threshold = 0.0
        self.ortho_lambda = ortho_lambda
        self.base_threshold = base_threshold
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def compute_orthogonality_loss(self, f):
=======
    def compute_threshold(self, progress):
        """Compute dynamic threshold based on training progress"""
        return self.base_threshold * (1 - progress)

    def compute_competition_coefficient(self, correlation, threshold):
        """Compute smooth competition coefficient"""
        return torch.clamp((correlation - threshold) / threshold, min=0.0, max=1.0)

    def compute_orthogonality_loss(self, f):
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        # Compute feature correlations
        f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)
        feature_correlations = torch.abs(torch.mm(f_norm.T, f_norm))
        
        # Zero out diagonal elements (self-correlation)
        mask = 1 - torch.eye(feature_correlations.shape[0], device=f.device)
        feature_correlations = feature_correlations * mask
        
        # Compute mean correlation
        ortho_loss = feature_correlations.mean()
        return ortho_loss
=======
        # Normalize features
        f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)
        
        # Compute correlations
        correlations = torch.abs(torch.mm(f_norm.T, f_norm))
        
        # Zero out diagonal
        mask = 1 - torch.eye(correlations.shape[0], device=f.device)
        correlations = correlations * mask
        
        # Get current threshold
        threshold = self.compute_threshold(self.progress)
        self.threshold = threshold
        
        # Compute competition coefficients
        coefficients = self.compute_competition_coefficient(correlations, threshold)
        
        # Weight correlations by coefficients
        weighted_correlations = correlations * coefficients
        
        return weighted_correlations.mean()
>>>>>>> REPLACE
```

These changes:
1. Reduce initial orthogonality weight from 0.1 to 0.05
2. Add dynamic threshold computation based on training progress
3. Implement smooth competition coefficients
4. Add threshold tracking to logging parameters

The key differences from Run 1:
- Lower initial orthogonality weight to reduce early constraint strength
- Dynamic threshold that decreases linearly with training progress
- Smooth competition coefficients that only penalize correlations above threshold
- More granular control over feature relationships

Let's run this with:

```bash
python experiment.py --out_dir=run_2
```

This should provide better balance between feature separation and reconstruction quality while maintaining the progressive nature of the constraints., type: <class 'str'>
Step 0: 31236.6171875
Step 100: 4581.09375
Step 200: 4064.0703125
Step 300: 3817.74951171875
Step 400: 3594.90283203125
Step 500: 3449.5078125
Step 600: 3422.958251953125
Step 700: 3239.967529296875
Step 800: 3211.20703125
Step 900: 3109.689208984375
Step 1000: 3123.35009765625
Step 1100: 3109.25390625
Step 1200: 3004.5029296875
Step 1300: 3046.258056640625
Step 1400: 3045.80908203125
Step 1500: 3055.24072265625
Step 1600: 3018.970947265625
Step 1700: 3023.4404296875
Step 1800: 3028.1630859375
Step 1900: 2987.734619140625
Step 2000: 2990.28857421875
Step 2100: 3012.593994140625
Step 2200: 2900.52685546875
Step 2300: 2893.518798828125
Step 2400: 2868.0478515625

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_2/final_info.json

[2025-02-06 16:21:32]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 368.01s

[2025-02-06 16:27:40]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_2/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 190.99s

[2025-02-06 16:30:51]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.9550000429153442
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 1: 0.9660000205039978
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 2: 0.9590000510215759
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 6: 0.9880000352859497
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 9: 0.9720000624656677
Num non-zero elements: 1
Test accuracy for 0: 0.646
Num non-zero elements: 1
Test accuracy for 1: 0.553
Num non-zero elements: 1
Test accuracy for 2: 0.648
Num non-zero elements: 1
Test accuracy for 6: 0.749
Num non-zero elements: 1
Test accuracy for 9: 0.892
Num non-zero elements: 2
Test accuracy for 0: 0.697
Num non-zero elements: 2
Test accuracy for 1: 0.651
Num non-zero elements: 2
Test accuracy for 2: 0.767
Num non-zero elements: 2
Test accuracy for 6: 0.97
Num non-zero elements: 2
Test accuracy for 9: 0.939
Num non-zero elements: 5
Test accuracy for 0: 0.801
Num non-zero elements: 5
Test accuracy for 1: 0.741
Num non-zero elements: 5
Test accuracy for 2: 0.852
Num non-zero elements: 5
Test accuracy for 6: 0.971
Num non-zero elements: 5
Test accuracy for 9: 0.945
Num non-zero elements: 10
Test accuracy for 0: 0.853
Num non-zero elements: 10
Test accuracy for 1: 0.791
Num non-zero elements: 10
Test accuracy for 2: 0.891
Num non-zero elements: 10
Test accuracy for 6: 0.98
Num non-zero elements: 10
Test accuracy for 9: 0.953
Num non-zero elements: 20
Test accuracy for 0: 0.886
Num non-zero elements: 20
Test accuracy for 1: 0.899
Num non-zero elements: 20
Test accuracy for 2: 0.924
Num non-zero elements: 20
Test accuracy for 6: 0.979
Num non-zero elements: 20
Test accuracy for 9: 0.959
Num non-zero elements: 50
Test accuracy for 0: 0.92
Num non-zero elements: 50
Test accuracy for 1: 0.949
Num non-zero elements: 50
Test accuracy for 2: 0.924
Num non-zero elements: 50
Test accuracy for 6: 0.99
Num non-zero elements: 50
Test accuracy for 9: 0.965
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.954
Num non-zero elements: 2304
Test accuracy for 13: 0.956
Num non-zero elements: 2304
Test accuracy for 14: 0.948
Num non-zero elements: 2304
Test accuracy for 18: 0.915
Num non-zero elements: 2304
Test accuracy for 19: 0.954
Num non-zero elements: 1
Test accuracy for 11: 0.562
Num non-zero elements: 1
Test accuracy for 13: 0.675
Num non-zero elements: 1
Test accuracy for 14: 0.636
Num non-zero elements: 1
Test accuracy for 18: 0.711
Num non-zero elements: 1
Test accuracy for 19: 0.802
Num non-zero elements: 2
Test accuracy for 11: 0.684
Num non-zero elements: 2
Test accuracy for 13: 0.7
Num non-zero elements: 2
Test accuracy for 14: 0.682
Num non-zero elements: 2
Test accuracy for 18: 0.725
Num non-zero elements: 2
Test accuracy for 19: 0.764
Num non-zero elements: 5
Test accuracy for 11: 0.781
Num non-zero elements: 5
Test accuracy for 13: 0.75
Num non-zero elements: 5
Test accuracy for 14: 0.741
Num non-zero elements: 5
Test accuracy for 18: 0.726
Num non-zero elements: 5
Test accuracy for 19: 0.83
Num non-zero elements: 10
Test accuracy for 11: 0.815
Num non-zero elements: 10
Test accuracy for 13: 0.776
Num non-zero elements: 10
Test accuracy for 14: 0.791
Num non-zero elements: 10
Test accuracy for 18: 0.745
Num non-zero elements: 10
Test accuracy for 19: 0.861
Num non-zero elements: 20
Test accuracy for 11: 0.906
Num non-zero elements: 20
Test accuracy for 13: 0.879
Num non-zero elements: 20
Test accuracy for 14: 0.867
Num non-zero elements: 20
Test accuracy for 18: 0.763
Num non-zero elements: 20
Test accuracy for 19: 0.899
Num non-zero elements: 50
Test accuracy for 11: 0.939
Num non-zero elements: 50
Test accuracy for 13: 0.914
Num non-zero elements: 50
Test accuracy for 14: 0.909
Num non-zero elements: 50
Test accuracy for 18: 0.84
Num non-zero elements: 50
Test accuracy for 19: 0.928
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 11: 0.9640000462532043
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 13: 0.9580000638961792
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 14: 0.9570000171661377
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for 18: 0.937000036239624
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 19: 0.9720000624656677
Num non-zero elements: 1
Test accuracy for 11: 0.835
Num non-zero elements: 1
Test accuracy for 13: 0.801
Num non-zero elements: 1
Test accuracy for 14: 0.802
Num non-zero elements: 1
Test accuracy for 18: 0.742
Num non-zero elements: 1
Test accuracy for 19: 0.83
Num non-zero elements: 2
Test accuracy for 11: 0.847
Num non-zero elements: 2
Test accuracy for 13: 0.806
Num non-zero elements: 2
Test accuracy for 14: 0.85
Num non-zero elements: 2
Test accuracy for 18: 0.742
Num non-zero elements: 2
Test accuracy for 19: 0.856
Num non-zero elements: 5
Test accuracy for 11: 0.864
Num non-zero elements: 5
Test accuracy for 13: 0.855
Num non-zero elements: 5
Test accuracy for 14: 0.857
Num non-zero elements: 5
Test accuracy for 18: 0.859
Num non-zero elements: 5
Test accuracy for 19: 0.899
Num non-zero elements: 10
Test accuracy for 11: 0.95
Num non-zero elements: 10
Test accuracy for 13: 0.885
Num non-zero elements: 10
Test accuracy for 14: 0.852
Num non-zero elements: 10
Test accuracy for 18: 0.864
Num non-zero elements: 10
Test accuracy for 19: 0.914
Num non-zero elements: 20
Test accuracy for 11: 0.949
Num non-zero elements: 20
Test accuracy for 13: 0.917
Num non-zero elements: 20
Test accuracy for 14: 0.906
Num non-zero elements: 20
Test accuracy for 18: 0.868
Num non-zero elements: 20
Test accuracy for 19: 0.916
Num non-zero elements: 50
Test accuracy for 11: 0.955
Num non-zero elements: 50
Test accuracy for 13: 0.916
Num non-zero elements: 50
Test accuracy for 14: 0.928
Num non-zero elements: 50
Test accuracy for 18: 0.898
Num non-zero elements: 50
Test accuracy for 19: 0.938
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.946
Num non-zero elements: 2304
Test accuracy for 21: 0.907
Num non-zero elements: 2304
Test accuracy for 22: 0.902
Num non-zero elements: 2304
Test accuracy for 25: 0.964
Num non-zero elements: 2304
Test accuracy for 26: 0.867
Num non-zero elements: 1
Test accuracy for 20: 0.719
Num non-zero elements: 1
Test accuracy for 21: 0.77
Num non-zero elements: 1
Test accuracy for 22: 0.645
Num non-zero elements: 1
Test accuracy for 25: 0.693
Num non-zero elements: 1
Test accuracy for 26: 0.616
Num non-zero elements: 2
Test accuracy for 20: 0.804
Num non-zero elements: 2
Test accuracy for 21: 0.756
Num non-zero elements: 2
Test accuracy for 22: 0.687
Num non-zero elements: 2
Test accuracy for 25: 0.757
Num non-zero elements: 2
Test accuracy for 26: 0.679
Num non-zero elements: 5
Test accuracy for 20: 0.861
Num non-zero elements: 5
Test accuracy for 21: 0.792
Num non-zero elements: 5
Test accuracy for 22: 0.713
Num non-zero elements: 5
Test accuracy for 25: 0.799
Num non-zero elements: 5
Test accuracy for 26: 0.687
Num non-zero elements: 10
Test accuracy for 20: 0.878
Num non-zero elements: 10
Test accuracy for 21: 0.811
Num non-zero elements: 10
Test accuracy for 22: 0.734
Num non-zero elements: 10
Test accuracy for 25: 0.842
Num non-zero elements: 10
Test accuracy for 26: 0.714
Num non-zero elements: 20
Test accuracy for 20: 0.916
Num non-zero elements: 20
Test accuracy for 21: 0.84
Num non-zero elements: 20
Test accuracy for 22: 0.812
Num non-zero elements: 20
Test accuracy for 25: 0.91
Num non-zero elements: 20
Test accuracy for 26: 0.814
Num non-zero elements: 50
Test accuracy for 20: 0.928
Num non-zero elements: 50
Test accuracy for 21: 0.878
Num non-zero elements: 50
Test accuracy for 22: 0.871
Num non-zero elements: 50
Test accuracy for 25: 0.918
Num non-zero elements: 50
Test accuracy for 26: 0.832
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 20: 0.9530000686645508
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 21: 0.9230000376701355
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 22: 0.9190000295639038
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 25: 0.9650000333786011
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 26: 0.893000066280365
Num non-zero elements: 1
Test accuracy for 20: 0.879
Num non-zero elements: 1
Test accuracy for 21: 0.766
Num non-zero elements: 1
Test accuracy for 22: 0.749
Num non-zero elements: 1
Test accuracy for 25: 0.813
Num non-zero elements: 1
Test accuracy for 26: 0.723
Num non-zero elements: 2
Test accuracy for 20: 0.889
Num non-zero elements: 2
Test accuracy for 21: 0.836
Num non-zero elements: 2
Test accuracy for 22: 0.814
Num non-zero elements: 2
Test accuracy for 25: 0.851
Num non-zero elements: 2
Test accuracy for 26: 0.794
Num non-zero elements: 5
Test accuracy for 20: 0.913
Num non-zero elements: 5
Test accuracy for 21: 0.839
Num non-zero elements: 5
Test accuracy for 22: 0.847
Num non-zero elements: 5
Test accuracy for 25: 0.913
Num non-zero elements: 5
Test accuracy for 26: 0.795
Num non-zero elements: 10
Test accuracy for 20: 0.918
Num non-zero elements: 10
Test accuracy for 21: 0.864
Num non-zero elements: 10
Test accuracy for 22: 0.846
Num non-zero elements: 10
Test accuracy for 25: 0.925
Num non-zero elements: 10
Test accuracy for 26: 0.81
Num non-zero elements: 20
Test accuracy for 20: 0.924
Num non-zero elements: 20
Test accuracy for 21: 0.867
Num non-zero elements: 20
Test accuracy for 22: 0.866
Num non-zero elements: 20
Test accuracy for 25: 0.929
Num non-zero elements: 20
Test accuracy for 26: 0.846
Num non-zero elements: 50
Test accuracy for 20: 0.939
Num non-zero elements: 50
Test accuracy for 21: 0.881
Num non-zero elements: 50
Test accuracy for 22: 0.887
Num non-zero elements: 50
Test accuracy for 25: 0.958
Num non-zero elements: 50
Test accuracy for 26: 0.871
Removing 29 from train set
Removing 31 from train set
Removing 7 from train set
Removing 0 from train set
Removing 28 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.936
Num non-zero elements: 2304
Test accuracy for 2: 0.919
Num non-zero elements: 2304
Test accuracy for 3: 0.911
Num non-zero elements: 2304
Test accuracy for 5: 0.905
Num non-zero elements: 2304
Test accuracy for 6: 0.831
Num non-zero elements: 1
Test accuracy for 1: 0.642
Num non-zero elements: 1
Test accuracy for 2: 0.602
Num non-zero elements: 1
Test accuracy for 3: 0.573
Num non-zero elements: 1
Test accuracy for 5: 0.554
Num non-zero elements: 1
Test accuracy for 6: 0.597
Num non-zero elements: 2
Test accuracy for 1: 0.741
Num non-zero elements: 2
Test accuracy for 2: 0.636
Num non-zero elements: 2
Test accuracy for 3: 0.608
Num non-zero elements: 2
Test accuracy for 5: 0.56
Num non-zero elements: 2
Test accuracy for 6: 0.621
Num non-zero elements: 5
Test accuracy for 1: 0.767
Num non-zero elements: 5
Test accuracy for 2: 0.642
Num non-zero elements: 5
Test accuracy for 3: 0.647
Num non-zero elements: 5
Test accuracy for 5: 0.641
Num non-zero elements: 5
Test accuracy for 6: 0.667
Num non-zero elements: 10
Test accuracy for 1: 0.809
Num non-zero elements: 10
Test accuracy for 2: 0.757
Num non-zero elements: 10
Test accuracy for 3: 0.737
Num non-zero elements: 10
Test accuracy for 5: 0.693
Num non-zero elements: 10
Test accuracy for 6: 0.735
Num non-zero elements: 20
Test accuracy for 1: 0.867
Num non-zero elements: 20
Test accuracy for 2: 0.847
Num non-zero elements: 20
Test accuracy for 3: 0.782
Num non-zero elements: 20
Test accuracy for 5: 0.788
Num non-zero elements: 20
Test accuracy for 6: 0.756
Num non-zero elements: 50
Test accuracy for 1: 0.923
Num non-zero elements: 50
Test accuracy for 2: 0.879
Num non-zero elements: 50
Test accuracy for 3: 0.857
Num non-zero elements: 50
Test accuracy for 5: 0.861
Num non-zero elements: 50
Test accuracy for 6: 0.783
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 1: 0.9520000219345093
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 2: 0.9340000152587891
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 3: 0.9100000262260437
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 5: 0.921000063419342
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 6: 0.8790000677108765
Num non-zero elements: 1
Test accuracy for 1: 0.838
Num non-zero elements: 1
Test accuracy for 2: 0.643
Num non-zero elements: 1
Test accuracy for 3: 0.624
Num non-zero elements: 1
Test accuracy for 5: 0.61
Num non-zero elements: 1
Test accuracy for 6: 0.739
Num non-zero elements: 2
Test accuracy for 1: 0.89
Num non-zero elements: 2
Test accuracy for 2: 0.682
Num non-zero elements: 2
Test accuracy for 3: 0.659
Num non-zero elements: 2
Test accuracy for 5: 0.831
Num non-zero elements: 2
Test accuracy for 6: 0.747
Num non-zero elements: 5
Test accuracy for 1: 0.915
Num non-zero elements: 5
Test accuracy for 2: 0.728
Num non-zero elements: 5
Test accuracy for 3: 0.742
Num non-zero elements: 5
Test accuracy for 5: 0.845
Num non-zero elements: 5
Test accuracy for 6: 0.757
Num non-zero elements: 10
Test accuracy for 1: 0.917
Num non-zero elements: 10
Test accuracy for 2: 0.786
Num non-zero elements: 10
Test accuracy for 3: 0.773
Num non-zero elements: 10
Test accuracy for 5: 0.875
Num non-zero elements: 10
Test accuracy for 6: 0.77
Num non-zero elements: 20
Test accuracy for 1: 0.92
Num non-zero elements: 20
Test accuracy for 2: 0.845
Num non-zero elements: 20
Test accuracy for 3: 0.806
Num non-zero elements: 20
Test accuracy for 5: 0.891
Num non-zero elements: 20
Test accuracy for 6: 0.795
Num non-zero elements: 50
Test accuracy for 1: 0.933
Num non-zero elements: 50
Test accuracy for 2: 0.89
Num non-zero elements: 50
Test accuracy for 3: 0.875
Num non-zero elements: 50
Test accuracy for 5: 0.91
Num non-zero elements: 50
Test accuracy for 6: 0.792
Num non-zero elements: 2304
Test accuracy for 1.0: 0.982
Num non-zero elements: 2304
Test accuracy for 5.0: 0.981
Num non-zero elements: 1
Test accuracy for 1.0: 0.671
Num non-zero elements: 1
Test accuracy for 5.0: 0.671
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.848
Num non-zero elements: 20
Test accuracy for 5.0: 0.848
Num non-zero elements: 50
Test accuracy for 1.0: 0.932
Num non-zero elements: 50
Test accuracy for 5.0: 0.933
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 1.0: 0.9800000190734863
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 5.0: 0.9800000190734863
Num non-zero elements: 1
Test accuracy for 1.0: 0.899
Num non-zero elements: 1
Test accuracy for 5.0: 0.899
Num non-zero elements: 2
Test accuracy for 1.0: 0.92
Num non-zero elements: 2
Test accuracy for 5.0: 0.92
Num non-zero elements: 5
Test accuracy for 1.0: 0.919
Num non-zero elements: 5
Test accuracy for 5.0: 0.919
Num non-zero elements: 10
Test accuracy for 1.0: 0.936
Num non-zero elements: 10
Test accuracy for 5.0: 0.936
Num non-zero elements: 20
Test accuracy for 1.0: 0.951
Num non-zero elements: 20
Test accuracy for 5.0: 0.951
Num non-zero elements: 50
Test accuracy for 1.0: 0.96
Num non-zero elements: 50
Test accuracy for 5.0: 0.96
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.949
Num non-zero elements: 2304
Test accuracy for Python: 0.983
Num non-zero elements: 2304
Test accuracy for HTML: 0.981
Num non-zero elements: 2304
Test accuracy for Java: 0.96
Num non-zero elements: 2304
Test accuracy for PHP: 0.949
Num non-zero elements: 1
Test accuracy for C: 0.653
Num non-zero elements: 1
Test accuracy for Python: 0.633
Num non-zero elements: 1
Test accuracy for HTML: 0.776
Num non-zero elements: 1
Test accuracy for Java: 0.629
Num non-zero elements: 1
Test accuracy for PHP: 0.59
Num non-zero elements: 2
Test accuracy for C: 0.749
Num non-zero elements: 2
Test accuracy for Python: 0.681
Num non-zero elements: 2
Test accuracy for HTML: 0.796
Num non-zero elements: 2
Test accuracy for Java: 0.689
Num non-zero elements: 2
Test accuracy for PHP: 0.653
Num non-zero elements: 5
Test accuracy for C: 0.769
Num non-zero elements: 5
Test accuracy for Python: 0.717
Num non-zero elements: 5
Test accuracy for HTML: 0.9
Num non-zero elements: 5
Test accuracy for Java: 0.726
Num non-zero elements: 5
Test accuracy for PHP: 0.71
Num non-zero elements: 10
Test accuracy for C: 0.805
Num non-zero elements: 10
Test accuracy for Python: 0.747
Num non-zero elements: 10
Test accuracy for HTML: 0.948
Num non-zero elements: 10
Test accuracy for Java: 0.786
Num non-zero elements: 10
Test accuracy for PHP: 0.73
Num non-zero elements: 20
Test accuracy for C: 0.866
Num non-zero elements: 20
Test accuracy for Python: 0.862
Num non-zero elements: 20
Test accuracy for HTML: 0.957
Num non-zero elements: 20
Test accuracy for Java: 0.841
Num non-zero elements: 20
Test accuracy for PHP: 0.834
Num non-zero elements: 50
Test accuracy for C: 0.905
Num non-zero elements: 50
Test accuracy for Python: 0.954
Num non-zero elements: 50
Test accuracy for HTML: 0.971
Num non-zero elements: 50
Test accuracy for Java: 0.894
Num non-zero elements: 50
Test accuracy for PHP: 0.903
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for C: 0.9430000185966492
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for Python: 0.984000027179718
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for HTML: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 39 epochs
Test accuracy for Java: 0.9690000414848328
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for PHP: 0.9580000638961792
Num non-zero elements: 1
Test accuracy for C: 0.58
Num non-zero elements: 1
Test accuracy for Python: 0.536
Num non-zero elements: 1
Test accuracy for HTML: 0.856
Num non-zero elements: 1
Test accuracy for Java: 0.627
Num non-zero elements: 1
Test accuracy for PHP: 0.596
Num non-zero elements: 2
Test accuracy for C: 0.636
Num non-zero elements: 2
Test accuracy for Python: 0.562
Num non-zero elements: 2
Test accuracy for HTML: 0.886
Num non-zero elements: 2
Test accuracy for Java: 0.664
Num non-zero elements: 2
Test accuracy for PHP: 0.622
Num non-zero elements: 5
Test accuracy for C: 0.702
Num non-zero elements: 5
Test accuracy for Python: 0.709
Num non-zero elements: 5
Test accuracy for HTML: 0.912
Num non-zero elements: 5
Test accuracy for Java: 0.669
Num non-zero elements: 5
Test accuracy for PHP: 0.694
Num non-zero elements: 10
Test accuracy for C: 0.833
Num non-zero elements: 10
Test accuracy for Python: 0.723
Num non-zero elements: 10
Test accuracy for HTML: 0.943
Num non-zero elements: 10
Test accuracy for Java: 0.713
Num non-zero elements: 10
Test accuracy for PHP: 0.778
Num non-zero elements: 20
Test accuracy for C: 0.833
Num non-zero elements: 20
Test accuracy for Python: 0.784
Num non-zero elements: 20
Test accuracy for HTML: 0.96
Num non-zero elements: 20
Test accuracy for Java: 0.819
Num non-zero elements: 20
Test accuracy for PHP: 0.906
Num non-zero elements: 50
Test accuracy for C: 0.902
Num non-zero elements: 50
Test accuracy for Python: 0.856
Num non-zero elements: 50
Test accuracy for HTML: 0.97
Num non-zero elements: 50
Test accuracy for Java: 0.894
Num non-zero elements: 50
Test accuracy for PHP: 0.914
Num non-zero elements: 2304
Test accuracy for 0: 0.929
Num non-zero elements: 2304
Test accuracy for 1: 0.985
Num non-zero elements: 2304
Test accuracy for 2: 0.922
Num non-zero elements: 2304
Test accuracy for 3: 0.944
Num non-zero elements: 1
Test accuracy for 0: 0.572
Num non-zero elements: 1
Test accuracy for 1: 0.667
Num non-zero elements: 1
Test accuracy for 2: 0.659
Num non-zero elements: 1
Test accuracy for 3: 0.648
Num non-zero elements: 2
Test accuracy for 0: 0.805
Num non-zero elements: 2
Test accuracy for 1: 0.81
Num non-zero elements: 2
Test accuracy for 2: 0.695
Num non-zero elements: 2
Test accuracy for 3: 0.739
Num non-zero elements: 5
Test accuracy for 0: 0.81
Num non-zero elements: 5
Test accuracy for 1: 0.878
Num non-zero elements: 5
Test accuracy for 2: 0.748
Num non-zero elements: 5
Test accuracy for 3: 0.853
Num non-zero elements: 10
Test accuracy for 0: 0.852
Num non-zero elements: 10
Test accuracy for 1: 0.942
Num non-zero elements: 10
Test accuracy for 2: 0.791
Num non-zero elements: 10
Test accuracy for 3: 0.884
Num non-zero elements: 20
Test accuracy for 0: 0.882
Num non-zero elements: 20
Test accuracy for 1: 0.958
Num non-zero elements: 20
Test accuracy for 2: 0.843
Num non-zero elements: 20
Test accuracy for 3: 0.889
Num non-zero elements: 50
Test accuracy for 0: 0.9
Num non-zero elements: 50
Test accuracy for 1: 0.969
Num non-zero elements: 50
Test accuracy for 2: 0.897
Num non-zero elements: 50
Test accuracy for 3: 0.929
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 0: 0.9470000267028809
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 1: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 2: 0.9460000395774841
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 3: 0.9500000476837158
Num non-zero elements: 1
Test accuracy for 0: 0.852
Num non-zero elements: 1
Test accuracy for 1: 0.978
Num non-zero elements: 1
Test accuracy for 2: 0.789
Num non-zero elements: 1
Test accuracy for 3: 0.708
Num non-zero elements: 2
Test accuracy for 0: 0.881
Num non-zero elements: 2
Test accuracy for 1: 0.976
Num non-zero elements: 2
Test accuracy for 2: 0.826
Num non-zero elements: 2
Test accuracy for 3: 0.725
Num non-zero elements: 5
Test accuracy for 0: 0.891
Num non-zero elements: 5
Test accuracy for 1: 0.982
Num non-zero elements: 5
Test accuracy for 2: 0.834
Num non-zero elements: 5
Test accuracy for 3: 0.847
Num non-zero elements: 10
Test accuracy for 0: 0.901
Num non-zero elements: 10
Test accuracy for 1: 0.973
Num non-zero elements: 10
Test accuracy for 2: 0.876
Num non-zero elements: 10
Test accuracy for 3: 0.872
Num non-zero elements: 20
Test accuracy for 0: 0.907
Num non-zero elements: 20
Test accuracy for 1: 0.977
Num non-zero elements: 20
Test accuracy for 2: 0.864
Num non-zero elements: 20
Test accuracy for 3: 0.886
Num non-zero elements: 50
Test accuracy for 0: 0.933
Num non-zero elements: 50
Test accuracy for 1: 0.982
Num non-zero elements: 50
Test accuracy for 2: 0.896
Num non-zero elements: 50
Test accuracy for 3: 0.923
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.999
Num non-zero elements: 2304
Test accuracy for fr: 1.0
Num non-zero elements: 2304
Test accuracy for de: 0.999
Num non-zero elements: 2304
Test accuracy for es: 0.999
Num non-zero elements: 2304
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.757
Num non-zero elements: 1
Test accuracy for fr: 0.604
Num non-zero elements: 1
Test accuracy for de: 0.734
Num non-zero elements: 1
Test accuracy for es: 0.499
Num non-zero elements: 1
Test accuracy for nl: 0.63
Num non-zero elements: 2
Test accuracy for en: 0.831
Num non-zero elements: 2
Test accuracy for fr: 0.587
Num non-zero elements: 2
Test accuracy for de: 0.845
Num non-zero elements: 2
Test accuracy for es: 0.91
Num non-zero elements: 2
Test accuracy for nl: 0.736
Num non-zero elements: 5
Test accuracy for en: 0.889
Num non-zero elements: 5
Test accuracy for fr: 0.896
Num non-zero elements: 5
Test accuracy for de: 0.868
Num non-zero elements: 5
Test accuracy for es: 0.983
Num non-zero elements: 5
Test accuracy for nl: 0.869
Num non-zero elements: 10
Test accuracy for en: 0.976
Num non-zero elements: 10
Test accuracy for fr: 0.974
Num non-zero elements: 10
Test accuracy for de: 0.96
Num non-zero elements: 10
Test accuracy for es: 0.991
Num non-zero elements: 10
Test accuracy for nl: 0.912
Num non-zero elements: 20
Test accuracy for en: 0.995
Num non-zero elements: 20
Test accuracy for fr: 0.991
Num non-zero elements: 20
Test accuracy for de: 0.986
Num non-zero elements: 20
Test accuracy for es: 0.999
Num non-zero elements: 20
Test accuracy for nl: 0.981
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 1.0
Num non-zero elements: 50
Test accuracy for de: 0.998
Num non-zero elements: 50
Test accuracy for es: 0.999
Num non-zero elements: 50
Test accuracy for nl: 0.994
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for en: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for fr: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for de: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for es: 0.9980000257492065
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for nl: 0.9980000257492065
Num non-zero elements: 1
Test accuracy for en: 0.999
Num non-zero elements: 1
Test accuracy for fr: 0.818
Num non-zero elements: 1
Test accuracy for de: 0.829
Num non-zero elements: 1
Test accuracy for es: 0.86
Num non-zero elements: 1
Test accuracy for nl: 0.754
Num non-zero elements: 2
Test accuracy for en: 0.998
Num non-zero elements: 2
Test accuracy for fr: 0.824
Num non-zero elements: 2
Test accuracy for de: 0.877
Num non-zero elements: 2
Test accuracy for es: 0.87
Num non-zero elements: 2
Test accuracy for nl: 0.847
Num non-zero elements: 5
Test accuracy for en: 0.999
Num non-zero elements: 5
Test accuracy for fr: 0.852
Num non-zero elements: 5
Test accuracy for de: 0.951
Num non-zero elements: 5
Test accuracy for es: 0.882
Num non-zero elements: 5
Test accuracy for nl: 0.917
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.94
Num non-zero elements: 10
Test accuracy for de: 0.972
Num non-zero elements: 10
Test accuracy for es: 0.893
Num non-zero elements: 10
Test accuracy for nl: 0.96
Num non-zero elements: 20
Test accuracy for en: 1.0
Num non-zero elements: 20
Test accuracy for fr: 0.95
Num non-zero elements: 20
Test accuracy for de: 0.97
Num non-zero elements: 20
Test accuracy for es: 0.947
Num non-zero elements: 20
Test accuracy for nl: 0.969
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 0.973
Num non-zero elements: 50
Test accuracy for de: 0.98
Num non-zero elements: 50
Test accuracy for es: 0.985
Num non-zero elements: 50
Test accuracy for nl: 0.979
Time spent on sparse_probing: 487.46s

[2025-02-06 16:38:59]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.001841 0.004826 0.027409 ... 0.017248 0.033188 0.002655]
(16384,)
Retain sparsity: [0.002255 0.00668  0.02557  ... 0.018389 0.045116 0.00262 ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15494 12897  6853  4299  7792   735  7492 14419  6532 12036 15408 16303
  3314  5592  9963  5447 13558  5235 14021  4321  5102  5787 16312  9376
  4335  3109 14359  3158 11204 12264 11447  4582   788   976   888  4625
  1833  9704  1140  5829  9234 10138 11844  6346  4614  5742 12057  8940
  1647 11787  8948  2895 12208  5917 12650  8911   889 13492 14471 13944
  8097  4013  1398  6074 14189  3667 10259 15811  4566  4350 13088 10688
 15916  2945 10581 12598  9102  5585 10332  7225  1259  5403  2916  7987
  9364  2270  5145  9564  9060  7753 11197   666   223 12816  9594 13211
 13048 15790   195  1557 12827 15053 10532  4410   106   364 12108 14395
 14496 15306 15211  9511  9129 15923  9430 13450  2659 14885  7528   846
  3341  3036  7769  2705  1188 15688 10961  9863 16076 11982   932  2042
  9209  4821 16197 14796 11918 14192 15504  8671  6073  3233 11669  7385
  4789 15820  4991  7328 10449  9637  6300  1579 10744  6823  3028  3465
    84  2874  3772 13991 12260  1207 14532   638  4849  6399  7700 10024
 15816 15374 14707   711 15846 16304  8749  9689  5484  3873  4458  8631
  6066   456 14204  1462 12861  9000  6940 10277  1871 15322  2655 10807
  1119  1634 13476 12173  7141 12933 11432 13265  8680  8545 10083  6060
  3809  4488 16174 11625 14949  1500 11996  2709  4361 12205 13930 15509
 11529 10901  3707  5119  7907 13058 10289  8039  1360  2528  7609  6039
  5417   689  3380 13094  5620  2789  4874 11578  4125   168  8549 13463
 15436  2755  7988  3622   399 12747   263  7968    79  5993 11034 10302
  8327  8959  6662  1455  4595   987  8817  9475  1201  1975  4223  6079
  9230  1896  2132 11902  5079  1444 10455  4662  7309  5761  6159  5706
  4143  9175   639 12244  4178 11929  4855  1182 15735   883 12418 15577
 15928  7756   499  5749  5921 10895 15113  7556 11221 15955 11788  6264
 10993 12600  9537 11163 14505 11828  4445  6178  3760  9504 10681 10387
  7144  7249  6784  9237 15222 15814  1535  3907  5897  6061 11514  2046
  7455  6115  8821  5571 12610   588  3941 15741 11451  7918 13403  8571
  9635  6737 13685  2847 16176 11454  9201  1571 15808 13371 13513 10313
 10866  9212 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036]), array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036, 15408, 16303,  3314,  5592,  9963,  5447, 13558,  5235,
       14021,  4321])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [12681 11085 10405 ... 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343]), array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343, 14751,   276, 13183, 10253,  4801, 14016,    14,  8289,
        1329, 13470])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   1.000000                1.000000  ...          25              1.000000
1   0.801498                0.954545  ...         200              0.934641
2   0.970037                1.000000  ...         100              0.993464
3   0.801498                0.809091  ...         200              0.777778
4   0.764045                0.936364  ...         200              0.898693
5   0.992509                1.000000  ...          50              1.000000
6   0.996255                0.990909  ...         100              0.993464
7   0.964419                0.981818  ...         100              0.986928
8   1.000000                0.990909  ...          50              0.996732
9   1.000000                1.000000  ...          50              1.000000
10  1.000000                1.000000  ...          25              1.000000
11  0.951311                0.990909  ...         100              0.977124
12  1.000000                1.000000  ...          25              1.000000
13  1.000000                1.000000  ...          25              1.000000
14  0.977528                0.990909  ...         200              0.973856
15  0.996255                1.000000  ...          50              1.000000

[16 rows x 10 columns]
Currently on iteration 1 of run 3
Let me first add the Run 2 results to notes.txt, then proceed with Run 3 implementation.                                                                 

notes.txt                                                                                                                                                

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 ## Run 1: Basic Progressive Orthogonality                                                                                                               
 Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress       
 ((1-p)^2).                                                                                                                                              
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Added progress tracking (step/total_steps)                                                                                                            
 - Implemented orthogonality loss between feature vectors                                                                                                
 - Applied progressive weighting scheme                                                                                                                  
 - Base orthogonality weight λ=0.1                                                                                                                       
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.04128 (target <0.01)                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                     
    - Suggests orthogonality may be too strong initially                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7756 (target >0.74)                                                                                                                
    - Better than baseline (0.7449)                                                                                                                      
    - Shows improved feature separation                                                                                                                  
                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                               
    - Lower than baseline (0.0637)                                                                                                                       
    - May need to adjust feature competition                                                                                                             
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375                                                                                                                           
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. While feature separation improved (sparse probing), the absorption score increased                                                                   
 2. Initial orthogonality constraint may be too strict                                                                                                   
 3. Need to balance feature separation with reconstruction quality                                                                                       
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Implement dynamic threshold computation                                                                                                              
 2. Reduce initial orthogonality weight                                                                                                                  
 3. Adjust progression schedule                                                                                                                          
 =======                                                                                                                                                 
 ## Run 1: Basic Progressive Orthogonality                                                                                                               
 Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress       
 ((1-p)^2).                                                                                                                                              
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Added progress tracking (step/total_steps)                                                                                                            
 - Implemented orthogonality loss between feature vectors                                                                                                
 - Applied progressive weighting scheme                                                                                                                  
 - Base orthogonality weight λ=0.1                                                                                                                       
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.04128 (target <0.01)                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                     
    - Suggests orthogonality may be too strong initially                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7756 (target >0.74)                                                                                                                
    - Better than baseline (0.7449)                                                                                                                      
    - Shows improved feature separation                                                                                                                  
                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                               
    - Lower than baseline (0.0637)                                                                                                                       
    - May need to adjust feature competition                                                                                                             
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375                                                                                                                           
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. While feature separation improved (sparse probing), the absorption score increased                                                                   
 2. Initial orthogonality constraint may be too strict                                                                                                   
 3. Need to balance feature separation with reconstruction quality                                                                                       
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Implement dynamic threshold computation                                                                                                              
 2. Reduce initial orthogonality weight                                                                                                                  
 3. Adjust progression schedule                                                                                                                          
                                                                                                                                                         
 ## Run 2: Dynamic Threshold Implementation                                                                                                              
 Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation  
 thresholds.                                                                                                                                             
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Reduced orthogonality weight from 0.1 to 0.05                                                                                                         
 - Added dynamic threshold computation: threshold = base_threshold * (1-progress)                                                                        
 - Implemented smooth competition coefficients                                                                                                           
 - Base threshold = 0.1                                                                                                                                  
 - Added threshold tracking                                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03676 (target <0.01)                                                                                                             
    - Improved from Run 1 (0.04128)                                                                                                                      
    - Still higher than baseline (0.02959)                                                                                                               
    - Dynamic thresholding shows promise                                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7748 (target >0.74)                                                                                                                
    - Similar to Run 1 (0.7756)                                                                                                                          
    - Maintains good feature separation                                                                                                                  
    - Consistent performance across datasets                                                                                                             
                                                                                                                                                         
 3. Unlearning Score: 0.0300 (target >0.1)                                                                                                               
    - Slight improvement from Run 1 (0.0262)                                                                                                             
    - Still below baseline (0.0637)                                                                                                                      
    - May need stronger competition                                                                                                                      
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375 (unchanged)                                                                                                               
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Dynamic thresholding improved absorption without sacrificing probing                                                                                 
 2. Lower orthogonality weight helped balance constraints                                                                                                
 3. Competition coefficients provide smoother transitions                                                                                                
 4. Need stronger feature competition for unlearning                                                                                                     
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Increase base threshold to 0.2 for stronger competition                                                                                              
 2. Keep reduced orthogonality weight                                                                                                                    
 3. Maintain dynamic threshold computation                                                                                                               
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

For Run 3, let's increase the base threshold to encourage stronger feature competition. Here are the changes:                                            

experiment.py                                                                                                                                            

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         ortho_lambda=0.05,  # reduced orthogonality loss weight                                                                                         
         base_threshold=0.1,  # base threshold for competition                                                                                           
 =======                                                                                                                                                 
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                   
         base_threshold=0.2,  # increased threshold for stronger competition                                                                             
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

This change will:                                                                                                                                        

 1 Keep the reduced orthogonality weight that showed improvement                                                                                         
 2 Double the base threshold to encourage stronger feature competition                                                                                   
 3 Maintain the dynamic threshold computation mechanism                                                                                                  

The higher base threshold should:                                                                                                                        

 • Increase competition between features early in training                                                                                               
 • Lead to better feature separation                                                                                                                     
 • Potentially improve unlearning performance                                                                                                            
 • Help reduce absorption score further                                                                                                                  

Let's run this with:                                                                                                                                     

                                                                                                                                                         
 python experiment.py --out_dir=run_3                                                                                                                    
                                                                                                                                                         

The key metrics to watch will be:                                                                                                                        

 • Absorption score (hoping to see further reduction)                                                                                                    
 • Unlearning score (aiming for improvement)                                                                                                             
 • Sparse probing accuracy (monitoring for any impact)                                                                                                   

Tokens: 63k sent, 1.5k received. Cost: $0.21 message, $0.55 session.
Applied edit to notes.txt
Applied edit to experiment.py

python experiment.py --out_dir=run_3
coder_out: Let me first add the Run 2 results to notes.txt, then proceed with Run 3 implementation.

notes.txt
```python
<<<<<<< SEARCH
## Run 1: Basic Progressive Orthogonality
Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress ((1-p)^2).

Key Implementation Details:
- Added progress tracking (step/total_steps)
- Implemented orthogonality loss between feature vectors
- Applied progressive weighting scheme
- Base orthogonality weight λ=0.1

Results Analysis:
1. Absorption Score: 0.04128 (target <0.01)
   - Higher than baseline (0.02959)
   - Suggests orthogonality may be too strong initially

2. Sparse Probing: 0.7756 (target >0.74)
   - Better than baseline (0.7449)
   - Shows improved feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Lower than baseline (0.0637)
   - May need to adjust feature competition

Core Metrics:
- Reconstruction MSE: 1.59375
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. While feature separation improved (sparse probing), the absorption score increased
2. Initial orthogonality constraint may be too strict
3. Need to balance feature separation with reconstruction quality

Next Steps:
1. Implement dynamic threshold computation
2. Reduce initial orthogonality weight
3. Adjust progression schedule
=======
## Run 1: Basic Progressive Orthogonality
Description: Implemented basic progress-based orthogonality loss with λ=0.1. The orthogonality weight decays quadratically with training progress ((1-p)^2).

Key Implementation Details:
- Added progress tracking (step/total_steps)
- Implemented orthogonality loss between feature vectors
- Applied progressive weighting scheme
- Base orthogonality weight λ=0.1

Results Analysis:
1. Absorption Score: 0.04128 (target <0.01)
   - Higher than baseline (0.02959)
   - Suggests orthogonality may be too strong initially

2. Sparse Probing: 0.7756 (target >0.74)
   - Better than baseline (0.7449)
   - Shows improved feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Lower than baseline (0.0637)
   - May need to adjust feature competition

Core Metrics:
- Reconstruction MSE: 1.59375
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. While feature separation improved (sparse probing), the absorption score increased
2. Initial orthogonality constraint may be too strict
3. Need to balance feature separation with reconstruction quality

Next Steps:
1. Implement dynamic threshold computation
2. Reduce initial orthogonality weight
3. Adjust progression schedule

## Run 2: Dynamic Threshold Implementation
Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation thresholds.

Key Implementation Details:
- Reduced orthogonality weight from 0.1 to 0.05
- Added dynamic threshold computation: threshold = base_threshold * (1-progress)
- Implemented smooth competition coefficients
- Base threshold = 0.1
- Added threshold tracking

Results Analysis:
1. Absorption Score: 0.03676 (target <0.01)
   - Improved from Run 1 (0.04128)
   - Still higher than baseline (0.02959)
   - Dynamic thresholding shows promise

2. Sparse Probing: 0.7748 (target >0.74)
   - Similar to Run 1 (0.7756)
   - Maintains good feature separation
   - Consistent performance across datasets

3. Unlearning Score: 0.0300 (target >0.1)
   - Slight improvement from Run 1 (0.0262)
   - Still below baseline (0.0637)
   - May need stronger competition

Core Metrics:
- Reconstruction MSE: 1.59375 (unchanged)
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. Dynamic thresholding improved absorption without sacrificing probing
2. Lower orthogonality weight helped balance constraints
3. Competition coefficients provide smoother transitions
4. Need stronger feature competition for unlearning

Next Steps:
1. Increase base threshold to 0.2 for stronger competition
2. Keep reduced orthogonality weight
3. Maintain dynamic threshold computation
>>>>>>> REPLACE
```

For Run 3, let's increase the base threshold to encourage stronger feature competition. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
        ortho_lambda=0.05,  # reduced orthogonality loss weight
        base_threshold=0.1,  # base threshold for competition
=======
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.2,  # increased threshold for stronger competition
>>>>>>> REPLACE
```

This change will:
1. Keep the reduced orthogonality weight that showed improvement
2. Double the base threshold to encourage stronger feature competition
3. Maintain the dynamic threshold computation mechanism

The higher base threshold should:
- Increase competition between features early in training
- Lead to better feature separation
- Potentially improve unlearning performance
- Help reduce absorption score further

Let's run this with:

```bash
python experiment.py --out_dir=run_3
```

The key metrics to watch will be:
- Absorption score (hoping to see further reduction)
- Unlearning score (aiming for improvement)
- Sparse probing accuracy (monitoring for any impact), type: <class 'str'>
Step 0: 31236.6171875
Step 100: 4581.349609375
Step 200: 4064.3232421875
Step 300: 3816.846923828125
Step 400: 3594.546875
Step 500: 3450.83203125
Step 600: 3419.283447265625
Step 700: 3240.6611328125
Step 800: 3210.846923828125
Step 900: 3110.325927734375
Step 1000: 3243.682373046875
Step 1100: 3127.03076171875
Step 1200: 3023.27392578125
Step 1300: 3062.0703125
Step 1400: 3067.908203125
Step 1500: 3075.30810546875
Step 1600: 3008.4091796875
Step 1700: 3038.2197265625
Step 1800: 3002.5732421875
Step 1900: 3000.58251953125
Step 2000: 3003.7998046875
Step 2100: 3024.24560546875
Step 2200: 2915.2890625
Step 2300: 2905.2099609375
Step 2400: 2876.8251953125

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_3/final_info.json

[2025-02-06 16:57:08]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 559.05s

[2025-02-06 17:06:27]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_3/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 223.74s

[2025-02-06 17:10:10]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 0: 0.956000030040741
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 1: 0.9600000381469727
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 2: 0.9570000171661377
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 6: 0.9930000305175781
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 9: 0.9830000400543213
Num non-zero elements: 1
Test accuracy for 0: 0.603
Num non-zero elements: 1
Test accuracy for 1: 0.551
Num non-zero elements: 1
Test accuracy for 2: 0.648
Num non-zero elements: 1
Test accuracy for 6: 0.754
Num non-zero elements: 1
Test accuracy for 9: 0.945
Num non-zero elements: 2
Test accuracy for 0: 0.678
Num non-zero elements: 2
Test accuracy for 1: 0.643
Num non-zero elements: 2
Test accuracy for 2: 0.779
Num non-zero elements: 2
Test accuracy for 6: 0.967
Num non-zero elements: 2
Test accuracy for 9: 0.945
Num non-zero elements: 5
Test accuracy for 0: 0.743
Num non-zero elements: 5
Test accuracy for 1: 0.796
Num non-zero elements: 5
Test accuracy for 2: 0.867
Num non-zero elements: 5
Test accuracy for 6: 0.971
Num non-zero elements: 5
Test accuracy for 9: 0.943
Num non-zero elements: 10
Test accuracy for 0: 0.822
Num non-zero elements: 10
Test accuracy for 1: 0.806
Num non-zero elements: 10
Test accuracy for 2: 0.88
Num non-zero elements: 10
Test accuracy for 6: 0.981
Num non-zero elements: 10
Test accuracy for 9: 0.952
Num non-zero elements: 20
Test accuracy for 0: 0.873
Num non-zero elements: 20
Test accuracy for 1: 0.901
Num non-zero elements: 20
Test accuracy for 2: 0.908
Num non-zero elements: 20
Test accuracy for 6: 0.98
Num non-zero elements: 20
Test accuracy for 9: 0.95
Num non-zero elements: 50
Test accuracy for 0: 0.907
Num non-zero elements: 50
Test accuracy for 1: 0.937
Num non-zero elements: 50
Test accuracy for 2: 0.935
Num non-zero elements: 50
Test accuracy for 6: 0.989
Num non-zero elements: 50
Test accuracy for 9: 0.964
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.957
Num non-zero elements: 2304
Test accuracy for 13: 0.939
Num non-zero elements: 2304
Test accuracy for 14: 0.948
Num non-zero elements: 2304
Test accuracy for 18: 0.91
Num non-zero elements: 2304
Test accuracy for 19: 0.962
Num non-zero elements: 1
Test accuracy for 11: 0.566
Num non-zero elements: 1
Test accuracy for 13: 0.673
Num non-zero elements: 1
Test accuracy for 14: 0.634
Num non-zero elements: 1
Test accuracy for 18: 0.703
Num non-zero elements: 1
Test accuracy for 19: 0.798
Num non-zero elements: 2
Test accuracy for 11: 0.686
Num non-zero elements: 2
Test accuracy for 13: 0.715
Num non-zero elements: 2
Test accuracy for 14: 0.693
Num non-zero elements: 2
Test accuracy for 18: 0.718
Num non-zero elements: 2
Test accuracy for 19: 0.771
Num non-zero elements: 5
Test accuracy for 11: 0.79
Num non-zero elements: 5
Test accuracy for 13: 0.75
Num non-zero elements: 5
Test accuracy for 14: 0.732
Num non-zero elements: 5
Test accuracy for 18: 0.733
Num non-zero elements: 5
Test accuracy for 19: 0.832
Num non-zero elements: 10
Test accuracy for 11: 0.845
Num non-zero elements: 10
Test accuracy for 13: 0.792
Num non-zero elements: 10
Test accuracy for 14: 0.775
Num non-zero elements: 10
Test accuracy for 18: 0.742
Num non-zero elements: 10
Test accuracy for 19: 0.853
Num non-zero elements: 20
Test accuracy for 11: 0.908
Num non-zero elements: 20
Test accuracy for 13: 0.866
Num non-zero elements: 20
Test accuracy for 14: 0.883
Num non-zero elements: 20
Test accuracy for 18: 0.772
Num non-zero elements: 20
Test accuracy for 19: 0.911
Num non-zero elements: 50
Test accuracy for 11: 0.94
Num non-zero elements: 50
Test accuracy for 13: 0.919
Num non-zero elements: 50
Test accuracy for 14: 0.901
Num non-zero elements: 50
Test accuracy for 18: 0.829
Num non-zero elements: 50
Test accuracy for 19: 0.926
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 11: 0.9550000429153442
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 13: 0.956000030040741
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 14: 0.9650000333786011
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 18: 0.9340000152587891
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 19: 0.9700000286102295
Num non-zero elements: 1
Test accuracy for 11: 0.848
Num non-zero elements: 1
Test accuracy for 13: 0.802
Num non-zero elements: 1
Test accuracy for 14: 0.795
Num non-zero elements: 1
Test accuracy for 18: 0.736
Num non-zero elements: 1
Test accuracy for 19: 0.851
Num non-zero elements: 2
Test accuracy for 11: 0.853
Num non-zero elements: 2
Test accuracy for 13: 0.807
Num non-zero elements: 2
Test accuracy for 14: 0.826
Num non-zero elements: 2
Test accuracy for 18: 0.753
Num non-zero elements: 2
Test accuracy for 19: 0.851
Num non-zero elements: 5
Test accuracy for 11: 0.85
Num non-zero elements: 5
Test accuracy for 13: 0.863
Num non-zero elements: 5
Test accuracy for 14: 0.848
Num non-zero elements: 5
Test accuracy for 18: 0.752
Num non-zero elements: 5
Test accuracy for 19: 0.891
Num non-zero elements: 10
Test accuracy for 11: 0.938
Num non-zero elements: 10
Test accuracy for 13: 0.907
Num non-zero elements: 10
Test accuracy for 14: 0.86
Num non-zero elements: 10
Test accuracy for 18: 0.878
Num non-zero elements: 10
Test accuracy for 19: 0.909
Num non-zero elements: 20
Test accuracy for 11: 0.947
Num non-zero elements: 20
Test accuracy for 13: 0.92
Num non-zero elements: 20
Test accuracy for 14: 0.917
Num non-zero elements: 20
Test accuracy for 18: 0.868
Num non-zero elements: 20
Test accuracy for 19: 0.923
Num non-zero elements: 50
Test accuracy for 11: 0.949
Num non-zero elements: 50
Test accuracy for 13: 0.921
Num non-zero elements: 50
Test accuracy for 14: 0.921
Num non-zero elements: 50
Test accuracy for 18: 0.871
Num non-zero elements: 50
Test accuracy for 19: 0.936
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.955
Num non-zero elements: 2304
Test accuracy for 21: 0.904
Num non-zero elements: 2304
Test accuracy for 22: 0.909
Num non-zero elements: 2304
Test accuracy for 25: 0.965
Num non-zero elements: 2304
Test accuracy for 26: 0.873
Num non-zero elements: 1
Test accuracy for 20: 0.718
Num non-zero elements: 1
Test accuracy for 21: 0.759
Num non-zero elements: 1
Test accuracy for 22: 0.651
Num non-zero elements: 1
Test accuracy for 25: 0.707
Num non-zero elements: 1
Test accuracy for 26: 0.64
Num non-zero elements: 2
Test accuracy for 20: 0.8
Num non-zero elements: 2
Test accuracy for 21: 0.756
Num non-zero elements: 2
Test accuracy for 22: 0.67
Num non-zero elements: 2
Test accuracy for 25: 0.771
Num non-zero elements: 2
Test accuracy for 26: 0.665
Num non-zero elements: 5
Test accuracy for 20: 0.855
Num non-zero elements: 5
Test accuracy for 21: 0.775
Num non-zero elements: 5
Test accuracy for 22: 0.719
Num non-zero elements: 5
Test accuracy for 25: 0.791
Num non-zero elements: 5
Test accuracy for 26: 0.688
Num non-zero elements: 10
Test accuracy for 20: 0.867
Num non-zero elements: 10
Test accuracy for 21: 0.82
Num non-zero elements: 10
Test accuracy for 22: 0.732
Num non-zero elements: 10
Test accuracy for 25: 0.853
Num non-zero elements: 10
Test accuracy for 26: 0.714
Num non-zero elements: 20
Test accuracy for 20: 0.906
Num non-zero elements: 20
Test accuracy for 21: 0.844
Num non-zero elements: 20
Test accuracy for 22: 0.801
Num non-zero elements: 20
Test accuracy for 25: 0.903
Num non-zero elements: 20
Test accuracy for 26: 0.825
Num non-zero elements: 50
Test accuracy for 20: 0.933
Num non-zero elements: 50
Test accuracy for 21: 0.889
Num non-zero elements: 50
Test accuracy for 22: 0.866
Num non-zero elements: 50
Test accuracy for 25: 0.932
Num non-zero elements: 50
Test accuracy for 26: 0.852
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 20: 0.9580000638961792
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 21: 0.9240000247955322
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 22: 0.9050000309944153
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 25: 0.9660000205039978
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 26: 0.8910000324249268
Num non-zero elements: 1
Test accuracy for 20: 0.864
Num non-zero elements: 1
Test accuracy for 21: 0.779
Num non-zero elements: 1
Test accuracy for 22: 0.751
Num non-zero elements: 1
Test accuracy for 25: 0.856
Num non-zero elements: 1
Test accuracy for 26: 0.677
Num non-zero elements: 2
Test accuracy for 20: 0.902
Num non-zero elements: 2
Test accuracy for 21: 0.834
Num non-zero elements: 2
Test accuracy for 22: 0.741
Num non-zero elements: 2
Test accuracy for 25: 0.852
Num non-zero elements: 2
Test accuracy for 26: 0.783
Num non-zero elements: 5
Test accuracy for 20: 0.914
Num non-zero elements: 5
Test accuracy for 21: 0.842
Num non-zero elements: 5
Test accuracy for 22: 0.834
Num non-zero elements: 5
Test accuracy for 25: 0.923
Num non-zero elements: 5
Test accuracy for 26: 0.801
Num non-zero elements: 10
Test accuracy for 20: 0.911
Num non-zero elements: 10
Test accuracy for 21: 0.849
Num non-zero elements: 10
Test accuracy for 22: 0.842
Num non-zero elements: 10
Test accuracy for 25: 0.928
Num non-zero elements: 10
Test accuracy for 26: 0.801
Num non-zero elements: 20
Test accuracy for 20: 0.92
Num non-zero elements: 20
Test accuracy for 21: 0.885
Num non-zero elements: 20
Test accuracy for 22: 0.87
Num non-zero elements: 20
Test accuracy for 25: 0.924
Num non-zero elements: 20
Test accuracy for 26: 0.839
Num non-zero elements: 50
Test accuracy for 20: 0.931
Num non-zero elements: 50
Test accuracy for 21: 0.886
Num non-zero elements: 50
Test accuracy for 22: 0.886
Num non-zero elements: 50
Test accuracy for 25: 0.943
Num non-zero elements: 50
Test accuracy for 26: 0.855
Removing 0 from train set
Removing 31 from train set
Removing 29 from train set
Removing 28 from train set
Removing 7 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.955
Num non-zero elements: 2304
Test accuracy for 2: 0.908
Num non-zero elements: 2304
Test accuracy for 3: 0.907
Num non-zero elements: 2304
Test accuracy for 5: 0.911
Num non-zero elements: 2304
Test accuracy for 6: 0.83
Num non-zero elements: 1
Test accuracy for 1: 0.659
Num non-zero elements: 1
Test accuracy for 2: 0.59
Num non-zero elements: 1
Test accuracy for 3: 0.597
Num non-zero elements: 1
Test accuracy for 5: 0.581
Num non-zero elements: 1
Test accuracy for 6: 0.592
Num non-zero elements: 2
Test accuracy for 1: 0.723
Num non-zero elements: 2
Test accuracy for 2: 0.651
Num non-zero elements: 2
Test accuracy for 3: 0.619
Num non-zero elements: 2
Test accuracy for 5: 0.582
Num non-zero elements: 2
Test accuracy for 6: 0.615
Num non-zero elements: 5
Test accuracy for 1: 0.774
Num non-zero elements: 5
Test accuracy for 2: 0.639
Num non-zero elements: 5
Test accuracy for 3: 0.643
Num non-zero elements: 5
Test accuracy for 5: 0.646
Num non-zero elements: 5
Test accuracy for 6: 0.672
Num non-zero elements: 10
Test accuracy for 1: 0.808
Num non-zero elements: 10
Test accuracy for 2: 0.761
Num non-zero elements: 10
Test accuracy for 3: 0.755
Num non-zero elements: 10
Test accuracy for 5: 0.691
Num non-zero elements: 10
Test accuracy for 6: 0.727
Num non-zero elements: 20
Test accuracy for 1: 0.879
Num non-zero elements: 20
Test accuracy for 2: 0.836
Num non-zero elements: 20
Test accuracy for 3: 0.784
Num non-zero elements: 20
Test accuracy for 5: 0.771
Num non-zero elements: 20
Test accuracy for 6: 0.75
Num non-zero elements: 50
Test accuracy for 1: 0.917
Num non-zero elements: 50
Test accuracy for 2: 0.889
Num non-zero elements: 50
Test accuracy for 3: 0.852
Num non-zero elements: 50
Test accuracy for 5: 0.842
Num non-zero elements: 50
Test accuracy for 6: 0.787
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 1: 0.9490000605583191
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 2: 0.9290000200271606
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 3: 0.921000063419342
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 39 epochs
Test accuracy for 5: 0.9230000376701355
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 6: 0.8650000691413879
Num non-zero elements: 1
Test accuracy for 1: 0.834
Num non-zero elements: 1
Test accuracy for 2: 0.641
Num non-zero elements: 1
Test accuracy for 3: 0.63
Num non-zero elements: 1
Test accuracy for 5: 0.616
Num non-zero elements: 1
Test accuracy for 6: 0.727
Num non-zero elements: 2
Test accuracy for 1: 0.887
Num non-zero elements: 2
Test accuracy for 2: 0.681
Num non-zero elements: 2
Test accuracy for 3: 0.663
Num non-zero elements: 2
Test accuracy for 5: 0.834
Num non-zero elements: 2
Test accuracy for 6: 0.748
Num non-zero elements: 5
Test accuracy for 1: 0.92
Num non-zero elements: 5
Test accuracy for 2: 0.708
Num non-zero elements: 5
Test accuracy for 3: 0.747
Num non-zero elements: 5
Test accuracy for 5: 0.843
Num non-zero elements: 5
Test accuracy for 6: 0.784
Num non-zero elements: 10
Test accuracy for 1: 0.922
Num non-zero elements: 10
Test accuracy for 2: 0.864
Num non-zero elements: 10
Test accuracy for 3: 0.78
Num non-zero elements: 10
Test accuracy for 5: 0.886
Num non-zero elements: 10
Test accuracy for 6: 0.795
Num non-zero elements: 20
Test accuracy for 1: 0.922
Num non-zero elements: 20
Test accuracy for 2: 0.864
Num non-zero elements: 20
Test accuracy for 3: 0.829
Num non-zero elements: 20
Test accuracy for 5: 0.885
Num non-zero elements: 20
Test accuracy for 6: 0.771
Num non-zero elements: 50
Test accuracy for 1: 0.938
Num non-zero elements: 50
Test accuracy for 2: 0.887
Num non-zero elements: 50
Test accuracy for 3: 0.89
Num non-zero elements: 50
Test accuracy for 5: 0.902
Num non-zero elements: 50
Test accuracy for 6: 0.791
Num non-zero elements: 2304
Test accuracy for 1.0: 0.982
Num non-zero elements: 2304
Test accuracy for 5.0: 0.982
Num non-zero elements: 1
Test accuracy for 1.0: 0.671
Num non-zero elements: 1
Test accuracy for 5.0: 0.671
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.847
Num non-zero elements: 20
Test accuracy for 5.0: 0.847
Num non-zero elements: 50
Test accuracy for 1.0: 0.932
Num non-zero elements: 50
Test accuracy for 5.0: 0.933
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 1.0: 0.9800000190734863
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 5.0: 0.9800000190734863
Num non-zero elements: 1
Test accuracy for 1.0: 0.893
Num non-zero elements: 1
Test accuracy for 5.0: 0.893
Num non-zero elements: 2
Test accuracy for 1.0: 0.914
Num non-zero elements: 2
Test accuracy for 5.0: 0.914
Num non-zero elements: 5
Test accuracy for 1.0: 0.921
Num non-zero elements: 5
Test accuracy for 5.0: 0.921
Num non-zero elements: 10
Test accuracy for 1.0: 0.934
Num non-zero elements: 10
Test accuracy for 5.0: 0.934
Num non-zero elements: 20
Test accuracy for 1.0: 0.946
Num non-zero elements: 20
Test accuracy for 5.0: 0.946
Num non-zero elements: 50
Test accuracy for 1.0: 0.955
Num non-zero elements: 50
Test accuracy for 5.0: 0.955
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.953
Num non-zero elements: 2304
Test accuracy for Python: 0.986
Num non-zero elements: 2304
Test accuracy for HTML: 0.985
Num non-zero elements: 2304
Test accuracy for Java: 0.949
Num non-zero elements: 2304
Test accuracy for PHP: 0.956
Num non-zero elements: 1
Test accuracy for C: 0.675
Num non-zero elements: 1
Test accuracy for Python: 0.628
Num non-zero elements: 1
Test accuracy for HTML: 0.786
Num non-zero elements: 1
Test accuracy for Java: 0.628
Num non-zero elements: 1
Test accuracy for PHP: 0.586
Num non-zero elements: 2
Test accuracy for C: 0.671
Num non-zero elements: 2
Test accuracy for Python: 0.677
Num non-zero elements: 2
Test accuracy for HTML: 0.832
Num non-zero elements: 2
Test accuracy for Java: 0.668
Num non-zero elements: 2
Test accuracy for PHP: 0.646
Num non-zero elements: 5
Test accuracy for C: 0.753
Num non-zero elements: 5
Test accuracy for Python: 0.725
Num non-zero elements: 5
Test accuracy for HTML: 0.898
Num non-zero elements: 5
Test accuracy for Java: 0.726
Num non-zero elements: 5
Test accuracy for PHP: 0.71
Num non-zero elements: 10
Test accuracy for C: 0.795
Num non-zero elements: 10
Test accuracy for Python: 0.749
Num non-zero elements: 10
Test accuracy for HTML: 0.945
Num non-zero elements: 10
Test accuracy for Java: 0.775
Num non-zero elements: 10
Test accuracy for PHP: 0.724
Num non-zero elements: 20
Test accuracy for C: 0.859
Num non-zero elements: 20
Test accuracy for Python: 0.857
Num non-zero elements: 20
Test accuracy for HTML: 0.962
Num non-zero elements: 20
Test accuracy for Java: 0.829
Num non-zero elements: 20
Test accuracy for PHP: 0.835
Num non-zero elements: 50
Test accuracy for C: 0.906
Num non-zero elements: 50
Test accuracy for Python: 0.949
Num non-zero elements: 50
Test accuracy for HTML: 0.966
Num non-zero elements: 50
Test accuracy for Java: 0.913
Num non-zero elements: 50
Test accuracy for PHP: 0.897
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for C: 0.956000030040741
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 41 epochs
Test accuracy for Python: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for HTML: 0.9890000224113464
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for Java: 0.9610000252723694
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for PHP: 0.9530000686645508
Num non-zero elements: 1
Test accuracy for C: 0.596
Num non-zero elements: 1
Test accuracy for Python: 0.545
Num non-zero elements: 1
Test accuracy for HTML: 0.844
Num non-zero elements: 1
Test accuracy for Java: 0.632
Num non-zero elements: 1
Test accuracy for PHP: 0.609
Num non-zero elements: 2
Test accuracy for C: 0.695
Num non-zero elements: 2
Test accuracy for Python: 0.547
Num non-zero elements: 2
Test accuracy for HTML: 0.926
Num non-zero elements: 2
Test accuracy for Java: 0.655
Num non-zero elements: 2
Test accuracy for PHP: 0.607
Num non-zero elements: 5
Test accuracy for C: 0.703
Num non-zero elements: 5
Test accuracy for Python: 0.697
Num non-zero elements: 5
Test accuracy for HTML: 0.922
Num non-zero elements: 5
Test accuracy for Java: 0.658
Num non-zero elements: 5
Test accuracy for PHP: 0.698
Num non-zero elements: 10
Test accuracy for C: 0.833
Num non-zero elements: 10
Test accuracy for Python: 0.74
Num non-zero elements: 10
Test accuracy for HTML: 0.953
Num non-zero elements: 10
Test accuracy for Java: 0.749
Num non-zero elements: 10
Test accuracy for PHP: 0.741
Num non-zero elements: 20
Test accuracy for C: 0.829
Num non-zero elements: 20
Test accuracy for Python: 0.771
Num non-zero elements: 20
Test accuracy for HTML: 0.959
Num non-zero elements: 20
Test accuracy for Java: 0.832
Num non-zero elements: 20
Test accuracy for PHP: 0.91
Num non-zero elements: 50
Test accuracy for C: 0.887
Num non-zero elements: 50
Test accuracy for Python: 0.846
Num non-zero elements: 50
Test accuracy for HTML: 0.973
Num non-zero elements: 50
Test accuracy for Java: 0.889
Num non-zero elements: 50
Test accuracy for PHP: 0.937
Num non-zero elements: 2304
Test accuracy for 0: 0.923
Num non-zero elements: 2304
Test accuracy for 1: 0.989
Num non-zero elements: 2304
Test accuracy for 2: 0.923
Num non-zero elements: 2304
Test accuracy for 3: 0.933
Num non-zero elements: 1
Test accuracy for 0: 0.568
Num non-zero elements: 1
Test accuracy for 1: 0.666
Num non-zero elements: 1
Test accuracy for 2: 0.674
Num non-zero elements: 1
Test accuracy for 3: 0.631
Num non-zero elements: 2
Test accuracy for 0: 0.803
Num non-zero elements: 2
Test accuracy for 1: 0.808
Num non-zero elements: 2
Test accuracy for 2: 0.693
Num non-zero elements: 2
Test accuracy for 3: 0.756
Num non-zero elements: 5
Test accuracy for 0: 0.818
Num non-zero elements: 5
Test accuracy for 1: 0.876
Num non-zero elements: 5
Test accuracy for 2: 0.752
Num non-zero elements: 5
Test accuracy for 3: 0.84
Num non-zero elements: 10
Test accuracy for 0: 0.871
Num non-zero elements: 10
Test accuracy for 1: 0.934
Num non-zero elements: 10
Test accuracy for 2: 0.806
Num non-zero elements: 10
Test accuracy for 3: 0.886
Num non-zero elements: 20
Test accuracy for 0: 0.893
Num non-zero elements: 20
Test accuracy for 1: 0.954
Num non-zero elements: 20
Test accuracy for 2: 0.838
Num non-zero elements: 20
Test accuracy for 3: 0.894
Num non-zero elements: 50
Test accuracy for 0: 0.902
Num non-zero elements: 50
Test accuracy for 1: 0.974
Num non-zero elements: 50
Test accuracy for 2: 0.893
Num non-zero elements: 50
Test accuracy for 3: 0.922
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 0: 0.9430000185966492
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 1: 0.9920000433921814
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 2: 0.9300000667572021
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 3: 0.9580000638961792
Num non-zero elements: 1
Test accuracy for 0: 0.797
Num non-zero elements: 1
Test accuracy for 1: 0.976
Num non-zero elements: 1
Test accuracy for 2: 0.814
Num non-zero elements: 1
Test accuracy for 3: 0.667
Num non-zero elements: 2
Test accuracy for 0: 0.877
Num non-zero elements: 2
Test accuracy for 1: 0.979
Num non-zero elements: 2
Test accuracy for 2: 0.829
Num non-zero elements: 2
Test accuracy for 3: 0.727
Num non-zero elements: 5
Test accuracy for 0: 0.902
Num non-zero elements: 5
Test accuracy for 1: 0.978
Num non-zero elements: 5
Test accuracy for 2: 0.843
Num non-zero elements: 5
Test accuracy for 3: 0.838
Num non-zero elements: 10
Test accuracy for 0: 0.902
Num non-zero elements: 10
Test accuracy for 1: 0.977
Num non-zero elements: 10
Test accuracy for 2: 0.864
Num non-zero elements: 10
Test accuracy for 3: 0.869
Num non-zero elements: 20
Test accuracy for 0: 0.914
Num non-zero elements: 20
Test accuracy for 1: 0.98
Num non-zero elements: 20
Test accuracy for 2: 0.878
Num non-zero elements: 20
Test accuracy for 3: 0.898
Num non-zero elements: 50
Test accuracy for 0: 0.921
Num non-zero elements: 50
Test accuracy for 1: 0.979
Num non-zero elements: 50
Test accuracy for 2: 0.883
Num non-zero elements: 50
Test accuracy for 3: 0.944
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.999
Num non-zero elements: 2304
Test accuracy for fr: 1.0
Num non-zero elements: 2304
Test accuracy for de: 1.0
Num non-zero elements: 2304
Test accuracy for es: 0.999
Num non-zero elements: 2304
Test accuracy for nl: 0.999
Num non-zero elements: 1
Test accuracy for en: 0.732
Num non-zero elements: 1
Test accuracy for fr: 0.594
Num non-zero elements: 1
Test accuracy for de: 0.75
Num non-zero elements: 1
Test accuracy for es: 0.495
Num non-zero elements: 1
Test accuracy for nl: 0.664
Num non-zero elements: 2
Test accuracy for en: 0.828
Num non-zero elements: 2
Test accuracy for fr: 0.597
Num non-zero elements: 2
Test accuracy for de: 0.825
Num non-zero elements: 2
Test accuracy for es: 0.963
Num non-zero elements: 2
Test accuracy for nl: 0.759
Num non-zero elements: 5
Test accuracy for en: 0.887
Num non-zero elements: 5
Test accuracy for fr: 0.903
Num non-zero elements: 5
Test accuracy for de: 0.912
Num non-zero elements: 5
Test accuracy for es: 0.981
Num non-zero elements: 5
Test accuracy for nl: 0.866
Num non-zero elements: 10
Test accuracy for en: 0.976
Num non-zero elements: 10
Test accuracy for fr: 0.971
Num non-zero elements: 10
Test accuracy for de: 0.963
Num non-zero elements: 10
Test accuracy for es: 0.989
Num non-zero elements: 10
Test accuracy for nl: 0.921
Num non-zero elements: 20
Test accuracy for en: 0.992
Num non-zero elements: 20
Test accuracy for fr: 0.987
Num non-zero elements: 20
Test accuracy for de: 0.982
Num non-zero elements: 20
Test accuracy for es: 0.996
Num non-zero elements: 20
Test accuracy for nl: 0.979
Num non-zero elements: 50
Test accuracy for en: 0.999
Num non-zero elements: 50
Test accuracy for fr: 1.0
Num non-zero elements: 50
Test accuracy for de: 0.997
Num non-zero elements: 50
Test accuracy for es: 0.999
Num non-zero elements: 50
Test accuracy for nl: 0.995
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for en: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for fr: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for de: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for es: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.996
Num non-zero elements: 1
Test accuracy for fr: 0.81
Num non-zero elements: 1
Test accuracy for de: 0.84
Num non-zero elements: 1
Test accuracy for es: 0.863
Num non-zero elements: 1
Test accuracy for nl: 0.764
Num non-zero elements: 2
Test accuracy for en: 1.0
Num non-zero elements: 2
Test accuracy for fr: 0.84
Num non-zero elements: 2
Test accuracy for de: 0.876
Num non-zero elements: 2
Test accuracy for es: 0.869
Num non-zero elements: 2
Test accuracy for nl: 0.851
Num non-zero elements: 5
Test accuracy for en: 0.997
Num non-zero elements: 5
Test accuracy for fr: 0.858
Num non-zero elements: 5
Test accuracy for de: 0.892
Num non-zero elements: 5
Test accuracy for es: 0.882
Num non-zero elements: 5
Test accuracy for nl: 0.92
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.883
Num non-zero elements: 10
Test accuracy for de: 0.966
Num non-zero elements: 10
Test accuracy for es: 0.928
Num non-zero elements: 10
Test accuracy for nl: 0.927
Num non-zero elements: 20
Test accuracy for en: 1.0
Num non-zero elements: 20
Test accuracy for fr: 0.922
Num non-zero elements: 20
Test accuracy for de: 0.963
Num non-zero elements: 20
Test accuracy for es: 0.968
Num non-zero elements: 20
Test accuracy for nl: 0.974
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 0.967
Num non-zero elements: 50
Test accuracy for de: 0.98
Num non-zero elements: 50
Test accuracy for es: 0.986
Num non-zero elements: 50
Test accuracy for nl: 0.971
Time spent on sparse_probing: 924.17s

[2025-02-06 17:25:35]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.001841 0.004826 0.027409 ... 0.017248 0.033188 0.002655]
(16384,)
Retain sparsity: [0.002255 0.00668  0.02557  ... 0.018389 0.045116 0.00262 ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15494 12897  6853  4299  7792   735  7492 14419  6532 12036 15408 16303
  3314  5592  9963  5447 13558  5235 14021  4321  5102  5787 16312  9376
  4335  3109 14359  3158 11204 12264 11447  4582   788   976   888  4625
  1833  9704  1140  5829  9234 10138 11844  6346  4614  5742 12057  8940
  1647 11787  8948  2895 12208  5917 12650  8911   889 13492 14471 13944
  8097  4013  1398  6074 14189  3667 10259 15811  4566  4350 13088 10688
 15916  2945 10581 12598  9102  5585 10332  7225  1259  5403  2916  7987
  9364  2270  5145  9564  9060  7753 11197   666   223 12816  9594 13211
 13048 15790   195  1557 12827 15053 10532  4410   106   364 12108 14395
 14496 15306 15211  9511  9129 15923  9430 13450  2659 14885  7528   846
  3341  3036  7769  2705  1188 15688 10961  9863 16076 11982   932  2042
  9209  4821 16197 14796 11918 14192 15504  8671  6073  3233 11669  7385
  4789 15820  4991  7328 10449  9637  6300  1579 10744  6823  3028  3465
    84  2874  3772 13991 12260  1207 14532   638  4849  6399  7700 10024
 15816 15374 14707   711 15846 16304  8749  9689  5484  3873  4458  8631
  6066   456 14204  1462 12861  9000  6940 10277  1871 15322  2655 10807
  1119  1634 13476 12173  7141 12933 11432 13265  8680  8545 10083  6060
  3809  4488 16174 11625 14949  1500 11996  2709  4361 12205 13930 15509
 11529 10901  3707  5119  7907 13058 10289  8039  1360  2528  7609  6039
  5417   689  3380 13094  5620  2789  4874 11578  4125   168  8549 13463
 15436  2755  7988  3622   399 12747   263  7968    79  5993 11034 10302
  8327  8959  6662  1455  4595   987  8817  9475  1201  1975  4223  6079
  9230  1896  2132 11902  5079  1444 10455  4662  7309  5761  6159  5706
  4143  9175   639 12244  4178 11929  4855  1182 15735   883 12418 15577
 15928  7756   499  5749  5921 10895 15113  7556 11221 15955 11788  6264
 10993 12600  9537 11163 14505 11828  4445  6178  3760  9504 10681 10387
  7144  7249  6784  9237 15222 15814  1535  3907  5897  6061 11514  2046
  7455  6115  8821  5571 12610   588  3941 15741 11451  7918 13403  8571
  9635  6737 13685  2847 16176 11454  9201  1571 15808 13371 13513 10313
 10866  9212 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036]), array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036, 15408, 16303,  3314,  5592,  9963,  5447, 13558,  5235,
       14021,  4321])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [12681 11085 10405 ... 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343]), array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343, 14751,   276, 13183, 10253,  4801, 14016,    14,  8289,
        1329, 13470])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   1.000000                1.000000  ...          25              1.000000
1   0.831461                0.972727  ...         200              0.937908
2   0.973783                1.000000  ...         100              0.986928
3   0.722846                0.645455  ...         200              0.660131
4   0.782772                0.927273  ...         200              0.875817
5   0.994382                1.000000  ...          50              1.000000
6   0.996255                0.981818  ...         100              0.980392
7   0.964419                0.990909  ...         100              0.970588
8   1.000000                1.000000  ...          50              0.996732
9   0.998127                1.000000  ...          50              1.000000
10  1.000000                1.000000  ...          25              1.000000
11  0.953184                0.990909  ...         100              0.970588
12  1.000000                1.000000  ...          25              1.000000
13  1.000000                1.000000  ...          25              1.000000
14  0.970037                0.981818  ...         200              0.960784
15  0.998127                0.990909  ...          50              0.993464

[16 rows x 10 columns]
Currently on iteration 1 of run 4
Let me first add the Run 3 results to notes.txt, then proceed with Run 4 implementation.                                                                 

notes.txt                                                                                                                                                

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 ## Run 2: Dynamic Threshold Implementation                                                                                                              
 Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation  
 thresholds.                                                                                                                                             
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Reduced orthogonality weight from 0.1 to 0.05                                                                                                         
 - Added dynamic threshold computation: threshold = base_threshold * (1-progress)                                                                        
 - Implemented smooth competition coefficients                                                                                                           
 - Base threshold = 0.1                                                                                                                                  
 - Added threshold tracking                                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03676 (target <0.01)                                                                                                             
    - Improved from Run 1 (0.04128)                                                                                                                      
    - Still higher than baseline (0.02959)                                                                                                               
    - Dynamic thresholding shows promise                                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7748 (target >0.74)                                                                                                                
    - Similar to Run 1 (0.7756)                                                                                                                          
    - Maintains good feature separation                                                                                                                  
    - Consistent performance across datasets                                                                                                             
                                                                                                                                                         
 3. Unlearning Score: 0.0300 (target >0.1)                                                                                                               
    - Slight improvement from Run 1 (0.0262)                                                                                                             
    - Still below baseline (0.0637)                                                                                                                      
    - May need stronger competition                                                                                                                      
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375 (unchanged)                                                                                                               
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Dynamic thresholding improved absorption without sacrificing probing                                                                                 
 2. Lower orthogonality weight helped balance constraints                                                                                                
 3. Competition coefficients provide smoother transitions                                                                                                
 4. Need stronger feature competition for unlearning                                                                                                     
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Increase base threshold to 0.2 for stronger competition                                                                                              
 2. Keep reduced orthogonality weight                                                                                                                    
 3. Maintain dynamic threshold computation                                                                                                               
 =======                                                                                                                                                 
 ## Run 2: Dynamic Threshold Implementation                                                                                                              
 Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation  
 thresholds.                                                                                                                                             
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Reduced orthogonality weight from 0.1 to 0.05                                                                                                         
 - Added dynamic threshold computation: threshold = base_threshold * (1-progress)                                                                        
 - Implemented smooth competition coefficients                                                                                                           
 - Base threshold = 0.1                                                                                                                                  
 - Added threshold tracking                                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03676 (target <0.01)                                                                                                             
    - Improved from Run 1 (0.04128)                                                                                                                      
    - Still higher than baseline (0.02959)                                                                                                               
    - Dynamic thresholding shows promise                                                                                                                 
                                                                                                                                                         
 2. Sparse Probing: 0.7748 (target >0.74)                                                                                                                
    - Similar to Run 1 (0.7756)                                                                                                                          
    - Maintains good feature separation                                                                                                                  
    - Consistent performance across datasets                                                                                                             
                                                                                                                                                         
 3. Unlearning Score: 0.0300 (target >0.1)                                                                                                               
    - Slight improvement from Run 1 (0.0262)                                                                                                             
    - Still below baseline (0.0637)                                                                                                                      
    - May need stronger competition                                                                                                                      
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.59375 (unchanged)                                                                                                               
 - KL Divergence Score: 0.9878                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Dynamic thresholding improved absorption without sacrificing probing                                                                                 
 2. Lower orthogonality weight helped balance constraints                                                                                                
 3. Competition coefficients provide smoother transitions                                                                                                
 4. Need stronger feature competition for unlearning                                                                                                     
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Increase base threshold to 0.2 for stronger competition                                                                                              
 2. Keep reduced orthogonality weight                                                                                                                    
 3. Maintain dynamic threshold computation                                                                                                               
                                                                                                                                                         
 ## Run 3: Increased Competition Threshold                                                                                                               
 Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.                  
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Increased base threshold from 0.1 to 0.2                                                                                                              
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                      
 - Kept dynamic threshold computation                                                                                                                    
 - Retained smooth competition coefficients                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03592 (target <0.01)                                                                                                             
    - Slight improvement from Run 2 (0.03676)                                                                                                            
    - Still higher than baseline (0.02959)                                                                                                               
    - Higher threshold shows diminishing returns                                                                                                         
                                                                                                                                                         
 2. Sparse Probing: 0.7735 (target >0.74)                                                                                                                
    - Similar to previous runs (0.7748)                                                                                                                  
    - Maintains strong feature separation                                                                                                                
    - Robust across different datasets                                                                                                                   
                                                                                                                                                         
 3. Unlearning Score: 0.0056 (target >0.1)                                                                                                               
    - Significant decrease from Run 2 (0.0300)                                                                                                           
    - Far below baseline (0.0637)                                                                                                                        
    - Suggests competition may be too aggressive                                                                                                         
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.6015625 (slightly worse)                                                                                                        
 - KL Divergence Score: 0.9877                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Higher threshold improved absorption marginally                                                                                                      
 2. Unlearning performance degraded significantly                                                                                                        
 3. Feature separation remains stable                                                                                                                    
 4. Reconstruction quality slightly decreased                                                                                                            
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Lower base threshold back to 0.1                                                                                                                     
 2. Implement exponential decay for threshold                                                                                                            
 3. Add progressive sparsity scaling                                                                                                                     
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

For Run 4, let's implement exponential decay for the threshold and add progressive sparsity scaling. Here are the changes:                               

experiment.py                                                                                                                                            

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                   
         base_threshold=0.2,  # increased threshold for stronger competition                                                                             
 =======                                                                                                                                                 
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                   
         base_threshold=0.1,  # return to original threshold                                                                                             
         decay_rate=2.0,     # exponential decay rate                                                                                                    
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold"]                                              
 =======                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale"]                            
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         self.threshold = 0.0                                                                                                                            
         self.ortho_lambda = ortho_lambda                                                                                                                
         self.base_threshold = base_threshold                                                                                                            
 =======                                                                                                                                                 
         self.threshold = 0.0                                                                                                                            
         self.sparsity_scale = 1.0                                                                                                                       
         self.ortho_lambda = ortho_lambda                                                                                                                
         self.base_threshold = base_threshold                                                                                                            
         self.decay_rate = decay_rate                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def compute_threshold(self, progress):                                                                                                              
         """Compute dynamic threshold based on training progress"""                                                                                      
         return self.base_threshold * (1 - progress)                                                                                                     
 =======                                                                                                                                                 
     def compute_threshold(self, progress):                                                                                                              
         """Compute dynamic threshold with exponential decay"""                                                                                          
         return self.base_threshold * np.exp(-self.decay_rate * progress)                                                                                
                                                                                                                                                         
     def compute_sparsity_scale(self, progress):                                                                                                         
         """Compute progressive sparsity scaling"""                                                                                                      
         return 1.0 + progress  # Linear increase from 1.0 to 2.0                                                                                        
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         # Get current threshold                                                                                                                         
         threshold = self.compute_threshold(self.progress)                                                                                               
         self.threshold = threshold                                                                                                                      
                                                                                                                                                         
         # Compute competition coefficients                                                                                                              
         coefficients = self.compute_competition_coefficient(correlations, threshold)                                                                    
 =======                                                                                                                                                 
         # Get current threshold and sparsity scale                                                                                                      
         threshold = self.compute_threshold(self.progress)                                                                                               
         self.threshold = threshold                                                                                                                      
                                                                                                                                                         
         sparsity_scale = self.compute_sparsity_scale(self.progress)                                                                                     
         self.sparsity_scale = sparsity_scale                                                                                                            
                                                                                                                                                         
         # Compute competition coefficients with sparsity scaling                                                                                        
         coefficients = self.compute_competition_coefficient(correlations * sparsity_scale, threshold)                                                   
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

These changes:                                                                                                                                           

 1 Return base threshold to 0.1 but add exponential decay                                                                                                
 2 Add progressive sparsity scaling that increases with training                                                                                         
 3 Apply sparsity scaling to correlations before thresholding                                                                                            
 4 Track sparsity scale in logging parameters                                                                                                            

The key differences from Run 3:                                                                                                                          

 • More gradual threshold decay using exponential function                                                                                               
 • Progressive increase in sparsity pressure                                                                                                             
 • Combined effect should provide better balance between competition and stability                                                                       

Let's run this with:                                                                                                                                     

                                                                                                                                                         
 python experiment.py --out_dir=run_4                                                                                                                    
                                                                                                                                                         

This should provide better control over feature relationships while maintaining strong separation.                                                       

Tokens: 72k sent, 2.1k received. Cost: $0.25 message, $0.80 session.
Applied edit to notes.txt
Applied edit to experiment.py

python experiment.py --out_dir=run_4
coder_out: Let me first add the Run 3 results to notes.txt, then proceed with Run 4 implementation.

notes.txt
```python
<<<<<<< SEARCH
## Run 2: Dynamic Threshold Implementation
Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation thresholds.

Key Implementation Details:
- Reduced orthogonality weight from 0.1 to 0.05
- Added dynamic threshold computation: threshold = base_threshold * (1-progress)
- Implemented smooth competition coefficients
- Base threshold = 0.1
- Added threshold tracking

Results Analysis:
1. Absorption Score: 0.03676 (target <0.01)
   - Improved from Run 1 (0.04128)
   - Still higher than baseline (0.02959)
   - Dynamic thresholding shows promise

2. Sparse Probing: 0.7748 (target >0.74)
   - Similar to Run 1 (0.7756)
   - Maintains good feature separation
   - Consistent performance across datasets

3. Unlearning Score: 0.0300 (target >0.1)
   - Slight improvement from Run 1 (0.0262)
   - Still below baseline (0.0637)
   - May need stronger competition

Core Metrics:
- Reconstruction MSE: 1.59375 (unchanged)
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. Dynamic thresholding improved absorption without sacrificing probing
2. Lower orthogonality weight helped balance constraints
3. Competition coefficients provide smoother transitions
4. Need stronger feature competition for unlearning

Next Steps:
1. Increase base threshold to 0.2 for stronger competition
2. Keep reduced orthogonality weight
3. Maintain dynamic threshold computation
=======
## Run 2: Dynamic Threshold Implementation
Description: Implemented dynamic threshold computation and reduced orthogonality weight to λ=0.05. Added competition coefficients based on correlation thresholds.

Key Implementation Details:
- Reduced orthogonality weight from 0.1 to 0.05
- Added dynamic threshold computation: threshold = base_threshold * (1-progress)
- Implemented smooth competition coefficients
- Base threshold = 0.1
- Added threshold tracking

Results Analysis:
1. Absorption Score: 0.03676 (target <0.01)
   - Improved from Run 1 (0.04128)
   - Still higher than baseline (0.02959)
   - Dynamic thresholding shows promise

2. Sparse Probing: 0.7748 (target >0.74)
   - Similar to Run 1 (0.7756)
   - Maintains good feature separation
   - Consistent performance across datasets

3. Unlearning Score: 0.0300 (target >0.1)
   - Slight improvement from Run 1 (0.0262)
   - Still below baseline (0.0637)
   - May need stronger competition

Core Metrics:
- Reconstruction MSE: 1.59375 (unchanged)
- KL Divergence Score: 0.9878
- L0 Sparsity: 320.0

Observations:
1. Dynamic thresholding improved absorption without sacrificing probing
2. Lower orthogonality weight helped balance constraints
3. Competition coefficients provide smoother transitions
4. Need stronger feature competition for unlearning

Next Steps:
1. Increase base threshold to 0.2 for stronger competition
2. Keep reduced orthogonality weight
3. Maintain dynamic threshold computation

## Run 3: Increased Competition Threshold
Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.

Key Implementation Details:
- Increased base threshold from 0.1 to 0.2
- Maintained reduced orthogonality weight (λ=0.05)
- Kept dynamic threshold computation
- Retained smooth competition coefficients

Results Analysis:
1. Absorption Score: 0.03592 (target <0.01)
   - Slight improvement from Run 2 (0.03676)
   - Still higher than baseline (0.02959)
   - Higher threshold shows diminishing returns

2. Sparse Probing: 0.7735 (target >0.74)
   - Similar to previous runs (0.7748)
   - Maintains strong feature separation
   - Robust across different datasets

3. Unlearning Score: 0.0056 (target >0.1)
   - Significant decrease from Run 2 (0.0300)
   - Far below baseline (0.0637)
   - Suggests competition may be too aggressive

Core Metrics:
- Reconstruction MSE: 1.6015625 (slightly worse)
- KL Divergence Score: 0.9877
- L0 Sparsity: 320.0

Observations:
1. Higher threshold improved absorption marginally
2. Unlearning performance degraded significantly
3. Feature separation remains stable
4. Reconstruction quality slightly decreased

Next Steps:
1. Lower base threshold back to 0.1
2. Implement exponential decay for threshold
3. Add progressive sparsity scaling
>>>>>>> REPLACE
```

For Run 4, let's implement exponential decay for the threshold and add progressive sparsity scaling. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.2,  # increased threshold for stronger competition
=======
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.1,  # return to original threshold
        decay_rate=2.0,     # exponential decay rate
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold"]
=======
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale"]
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.threshold = 0.0
        self.ortho_lambda = ortho_lambda
        self.base_threshold = base_threshold
=======
        self.threshold = 0.0
        self.sparsity_scale = 1.0
        self.ortho_lambda = ortho_lambda
        self.base_threshold = base_threshold
        self.decay_rate = decay_rate
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def compute_threshold(self, progress):
        """Compute dynamic threshold based on training progress"""
        return self.base_threshold * (1 - progress)
=======
    def compute_threshold(self, progress):
        """Compute dynamic threshold with exponential decay"""
        return self.base_threshold * np.exp(-self.decay_rate * progress)

    def compute_sparsity_scale(self, progress):
        """Compute progressive sparsity scaling"""
        return 1.0 + progress  # Linear increase from 1.0 to 2.0
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        # Get current threshold
        threshold = self.compute_threshold(self.progress)
        self.threshold = threshold
        
        # Compute competition coefficients
        coefficients = self.compute_competition_coefficient(correlations, threshold)
=======
        # Get current threshold and sparsity scale
        threshold = self.compute_threshold(self.progress)
        self.threshold = threshold
        
        sparsity_scale = self.compute_sparsity_scale(self.progress)
        self.sparsity_scale = sparsity_scale
        
        # Compute competition coefficients with sparsity scaling
        coefficients = self.compute_competition_coefficient(correlations * sparsity_scale, threshold)
>>>>>>> REPLACE
```

These changes:
1. Return base threshold to 0.1 but add exponential decay
2. Add progressive sparsity scaling that increases with training
3. Apply sparsity scaling to correlations before thresholding
4. Track sparsity scale in logging parameters

The key differences from Run 3:
- More gradual threshold decay using exponential function
- Progressive increase in sparsity pressure
- Combined effect should provide better balance between competition and stability

Let's run this with:

```bash
python experiment.py --out_dir=run_4
```

This should provide better control over feature relationships while maintaining strong separation., type: <class 'str'>
Step 0: 31236.6171875
Step 100: 4581.783203125
Step 200: 4064.11572265625
Step 300: 3816.431640625
Step 400: 3593.2529296875
Step 500: 3451.551025390625
Step 600: 3421.739990234375
Step 700: 3238.438720703125
Step 800: 3212.693115234375
Step 900: 3192.978271484375
Step 1000: 3257.638916015625
Step 1100: 3153.897216796875
Step 1200: 3062.421630859375
Step 1300: 3096.732177734375
Step 1400: 3097.35107421875
Step 1500: 3105.277587890625
Step 1600: 3038.31298828125
Step 1700: 3056.642822265625
Step 1800: 3022.04345703125
Step 1900: 3002.35107421875
Step 2000: 3021.23828125
Step 2100: 3040.5615234375
Step 2200: 2933.1513671875
Step 2300: 2916.0185546875
Step 2400: 2890.176513671875

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_4/final_info.json

[2025-02-06 17:40:33]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 412.51s

[2025-02-06 17:47:25]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_4/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 350.65s

[2025-02-06 17:53:16]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 0: 0.9550000429153442
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 1: 0.9610000252723694
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 2: 0.9620000720024109
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 6: 0.9900000691413879
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 9: 0.9720000624656677
Num non-zero elements: 1
Test accuracy for 0: 0.652
Num non-zero elements: 1
Test accuracy for 1: 0.539
Num non-zero elements: 1
Test accuracy for 2: 0.656
Num non-zero elements: 1
Test accuracy for 6: 0.752
Num non-zero elements: 1
Test accuracy for 9: 0.885
Num non-zero elements: 2
Test accuracy for 0: 0.68
Num non-zero elements: 2
Test accuracy for 1: 0.658
Num non-zero elements: 2
Test accuracy for 2: 0.782
Num non-zero elements: 2
Test accuracy for 6: 0.959
Num non-zero elements: 2
Test accuracy for 9: 0.935
Num non-zero elements: 5
Test accuracy for 0: 0.757
Num non-zero elements: 5
Test accuracy for 1: 0.834
Num non-zero elements: 5
Test accuracy for 2: 0.868
Num non-zero elements: 5
Test accuracy for 6: 0.98
Num non-zero elements: 5
Test accuracy for 9: 0.942
Num non-zero elements: 10
Test accuracy for 0: 0.796
Num non-zero elements: 10
Test accuracy for 1: 0.899
Num non-zero elements: 10
Test accuracy for 2: 0.896
Num non-zero elements: 10
Test accuracy for 6: 0.983
Num non-zero elements: 10
Test accuracy for 9: 0.945
Num non-zero elements: 20
Test accuracy for 0: 0.88
Num non-zero elements: 20
Test accuracy for 1: 0.912
Num non-zero elements: 20
Test accuracy for 2: 0.916
Num non-zero elements: 20
Test accuracy for 6: 0.976
Num non-zero elements: 20
Test accuracy for 9: 0.954
Num non-zero elements: 50
Test accuracy for 0: 0.912
Num non-zero elements: 50
Test accuracy for 1: 0.947
Num non-zero elements: 50
Test accuracy for 2: 0.936
Num non-zero elements: 50
Test accuracy for 6: 0.992
Num non-zero elements: 50
Test accuracy for 9: 0.955
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.961
Num non-zero elements: 2304
Test accuracy for 13: 0.943
Num non-zero elements: 2304
Test accuracy for 14: 0.939
Num non-zero elements: 2304
Test accuracy for 18: 0.936
Num non-zero elements: 2304
Test accuracy for 19: 0.963
Num non-zero elements: 1
Test accuracy for 11: 0.561
Num non-zero elements: 1
Test accuracy for 13: 0.665
Num non-zero elements: 1
Test accuracy for 14: 0.664
Num non-zero elements: 1
Test accuracy for 18: 0.697
Num non-zero elements: 1
Test accuracy for 19: 0.798
Num non-zero elements: 2
Test accuracy for 11: 0.694
Num non-zero elements: 2
Test accuracy for 13: 0.726
Num non-zero elements: 2
Test accuracy for 14: 0.672
Num non-zero elements: 2
Test accuracy for 18: 0.728
Num non-zero elements: 2
Test accuracy for 19: 0.77
Num non-zero elements: 5
Test accuracy for 11: 0.772
Num non-zero elements: 5
Test accuracy for 13: 0.756
Num non-zero elements: 5
Test accuracy for 14: 0.721
Num non-zero elements: 5
Test accuracy for 18: 0.732
Num non-zero elements: 5
Test accuracy for 19: 0.84
Num non-zero elements: 10
Test accuracy for 11: 0.845
Num non-zero elements: 10
Test accuracy for 13: 0.772
Num non-zero elements: 10
Test accuracy for 14: 0.778
Num non-zero elements: 10
Test accuracy for 18: 0.753
Num non-zero elements: 10
Test accuracy for 19: 0.859
Num non-zero elements: 20
Test accuracy for 11: 0.908
Num non-zero elements: 20
Test accuracy for 13: 0.873
Num non-zero elements: 20
Test accuracy for 14: 0.888
Num non-zero elements: 20
Test accuracy for 18: 0.749
Num non-zero elements: 20
Test accuracy for 19: 0.904
Num non-zero elements: 50
Test accuracy for 11: 0.935
Num non-zero elements: 50
Test accuracy for 13: 0.914
Num non-zero elements: 50
Test accuracy for 14: 0.915
Num non-zero elements: 50
Test accuracy for 18: 0.836
Num non-zero elements: 50
Test accuracy for 19: 0.923
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 11: 0.9670000672340393
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 13: 0.9500000476837158
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 14: 0.9580000638961792
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 18: 0.937000036239624
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 19: 0.9640000462532043
Num non-zero elements: 1
Test accuracy for 11: 0.851
Num non-zero elements: 1
Test accuracy for 13: 0.807
Num non-zero elements: 1
Test accuracy for 14: 0.787
Num non-zero elements: 1
Test accuracy for 18: 0.749
Num non-zero elements: 1
Test accuracy for 19: 0.845
Num non-zero elements: 2
Test accuracy for 11: 0.851
Num non-zero elements: 2
Test accuracy for 13: 0.811
Num non-zero elements: 2
Test accuracy for 14: 0.832
Num non-zero elements: 2
Test accuracy for 18: 0.741
Num non-zero elements: 2
Test accuracy for 19: 0.853
Num non-zero elements: 5
Test accuracy for 11: 0.872
Num non-zero elements: 5
Test accuracy for 13: 0.869
Num non-zero elements: 5
Test accuracy for 14: 0.854
Num non-zero elements: 5
Test accuracy for 18: 0.862
Num non-zero elements: 5
Test accuracy for 19: 0.9
Num non-zero elements: 10
Test accuracy for 11: 0.885
Num non-zero elements: 10
Test accuracy for 13: 0.907
Num non-zero elements: 10
Test accuracy for 14: 0.868
Num non-zero elements: 10
Test accuracy for 18: 0.86
Num non-zero elements: 10
Test accuracy for 19: 0.909
Num non-zero elements: 20
Test accuracy for 11: 0.953
Num non-zero elements: 20
Test accuracy for 13: 0.921
Num non-zero elements: 20
Test accuracy for 14: 0.914
Num non-zero elements: 20
Test accuracy for 18: 0.871
Num non-zero elements: 20
Test accuracy for 19: 0.917
Num non-zero elements: 50
Test accuracy for 11: 0.954
Num non-zero elements: 50
Test accuracy for 13: 0.928
Num non-zero elements: 50
Test accuracy for 14: 0.919
Num non-zero elements: 50
Test accuracy for 18: 0.889
Num non-zero elements: 50
Test accuracy for 19: 0.937
Removing 24 from test set
Removing 12 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.951
Num non-zero elements: 2304
Test accuracy for 21: 0.909
Num non-zero elements: 2304
Test accuracy for 22: 0.895
Num non-zero elements: 2304
Test accuracy for 25: 0.964
Num non-zero elements: 2304
Test accuracy for 26: 0.864
Num non-zero elements: 1
Test accuracy for 20: 0.708
Num non-zero elements: 1
Test accuracy for 21: 0.773
Num non-zero elements: 1
Test accuracy for 22: 0.644
Num non-zero elements: 1
Test accuracy for 25: 0.686
Num non-zero elements: 1
Test accuracy for 26: 0.626
Num non-zero elements: 2
Test accuracy for 20: 0.813
Num non-zero elements: 2
Test accuracy for 21: 0.77
Num non-zero elements: 2
Test accuracy for 22: 0.682
Num non-zero elements: 2
Test accuracy for 25: 0.75
Num non-zero elements: 2
Test accuracy for 26: 0.681
Num non-zero elements: 5
Test accuracy for 20: 0.822
Num non-zero elements: 5
Test accuracy for 21: 0.769
Num non-zero elements: 5
Test accuracy for 22: 0.695
Num non-zero elements: 5
Test accuracy for 25: 0.792
Num non-zero elements: 5
Test accuracy for 26: 0.681
Num non-zero elements: 10
Test accuracy for 20: 0.895
Num non-zero elements: 10
Test accuracy for 21: 0.814
Num non-zero elements: 10
Test accuracy for 22: 0.74
Num non-zero elements: 10
Test accuracy for 25: 0.848
Num non-zero elements: 10
Test accuracy for 26: 0.707
Num non-zero elements: 20
Test accuracy for 20: 0.909
Num non-zero elements: 20
Test accuracy for 21: 0.835
Num non-zero elements: 20
Test accuracy for 22: 0.79
Num non-zero elements: 20
Test accuracy for 25: 0.909
Num non-zero elements: 20
Test accuracy for 26: 0.809
Num non-zero elements: 50
Test accuracy for 20: 0.916
Num non-zero elements: 50
Test accuracy for 21: 0.884
Num non-zero elements: 50
Test accuracy for 22: 0.882
Num non-zero elements: 50
Test accuracy for 25: 0.919
Num non-zero elements: 50
Test accuracy for 26: 0.838
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 20: 0.9550000429153442
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 21: 0.9260000586509705
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 22: 0.9180000424385071
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 25: 0.9690000414848328
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 26: 0.8880000710487366
Num non-zero elements: 1
Test accuracy for 20: 0.882
Num non-zero elements: 1
Test accuracy for 21: 0.78
Num non-zero elements: 1
Test accuracy for 22: 0.749
Num non-zero elements: 1
Test accuracy for 25: 0.841
Num non-zero elements: 1
Test accuracy for 26: 0.679
Num non-zero elements: 2
Test accuracy for 20: 0.911
Num non-zero elements: 2
Test accuracy for 21: 0.784
Num non-zero elements: 2
Test accuracy for 22: 0.77
Num non-zero elements: 2
Test accuracy for 25: 0.905
Num non-zero elements: 2
Test accuracy for 26: 0.788
Num non-zero elements: 5
Test accuracy for 20: 0.911
Num non-zero elements: 5
Test accuracy for 21: 0.833
Num non-zero elements: 5
Test accuracy for 22: 0.842
Num non-zero elements: 5
Test accuracy for 25: 0.892
Num non-zero elements: 5
Test accuracy for 26: 0.802
Num non-zero elements: 10
Test accuracy for 20: 0.925
Num non-zero elements: 10
Test accuracy for 21: 0.875
Num non-zero elements: 10
Test accuracy for 22: 0.86
Num non-zero elements: 10
Test accuracy for 25: 0.918
Num non-zero elements: 10
Test accuracy for 26: 0.809
Num non-zero elements: 20
Test accuracy for 20: 0.92
Num non-zero elements: 20
Test accuracy for 21: 0.874
Num non-zero elements: 20
Test accuracy for 22: 0.868
Num non-zero elements: 20
Test accuracy for 25: 0.924
Num non-zero elements: 20
Test accuracy for 26: 0.841
Num non-zero elements: 50
Test accuracy for 20: 0.937
Num non-zero elements: 50
Test accuracy for 21: 0.871
Num non-zero elements: 50
Test accuracy for 22: 0.878
Num non-zero elements: 50
Test accuracy for 25: 0.945
Num non-zero elements: 50
Test accuracy for 26: 0.853
Removing 28 from train set
Removing 29 from train set
Removing 31 from train set
Removing 0 from train set
Removing 7 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.941
Num non-zero elements: 2304
Test accuracy for 2: 0.924
Num non-zero elements: 2304
Test accuracy for 3: 0.899
Num non-zero elements: 2304
Test accuracy for 5: 0.916
Num non-zero elements: 2304
Test accuracy for 6: 0.839
Num non-zero elements: 1
Test accuracy for 1: 0.666
Num non-zero elements: 1
Test accuracy for 2: 0.598
Num non-zero elements: 1
Test accuracy for 3: 0.599
Num non-zero elements: 1
Test accuracy for 5: 0.562
Num non-zero elements: 1
Test accuracy for 6: 0.592
Num non-zero elements: 2
Test accuracy for 1: 0.763
Num non-zero elements: 2
Test accuracy for 2: 0.641
Num non-zero elements: 2
Test accuracy for 3: 0.622
Num non-zero elements: 2
Test accuracy for 5: 0.62
Num non-zero elements: 2
Test accuracy for 6: 0.613
Num non-zero elements: 5
Test accuracy for 1: 0.774
Num non-zero elements: 5
Test accuracy for 2: 0.738
Num non-zero elements: 5
Test accuracy for 3: 0.614
Num non-zero elements: 5
Test accuracy for 5: 0.646
Num non-zero elements: 5
Test accuracy for 6: 0.657
Num non-zero elements: 10
Test accuracy for 1: 0.805
Num non-zero elements: 10
Test accuracy for 2: 0.78
Num non-zero elements: 10
Test accuracy for 3: 0.733
Num non-zero elements: 10
Test accuracy for 5: 0.682
Num non-zero elements: 10
Test accuracy for 6: 0.69
Num non-zero elements: 20
Test accuracy for 1: 0.873
Num non-zero elements: 20
Test accuracy for 2: 0.835
Num non-zero elements: 20
Test accuracy for 3: 0.798
Num non-zero elements: 20
Test accuracy for 5: 0.792
Num non-zero elements: 20
Test accuracy for 6: 0.759
Num non-zero elements: 50
Test accuracy for 1: 0.928
Num non-zero elements: 50
Test accuracy for 2: 0.885
Num non-zero elements: 50
Test accuracy for 3: 0.852
Num non-zero elements: 50
Test accuracy for 5: 0.865
Num non-zero elements: 50
Test accuracy for 6: 0.802
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 1: 0.9580000638961792
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for 2: 0.9420000314712524
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 3: 0.9260000586509705
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 5: 0.9130000472068787
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 6: 0.8810000419616699
Num non-zero elements: 1
Test accuracy for 1: 0.84
Num non-zero elements: 1
Test accuracy for 2: 0.671
Num non-zero elements: 1
Test accuracy for 3: 0.621
Num non-zero elements: 1
Test accuracy for 5: 0.599
Num non-zero elements: 1
Test accuracy for 6: 0.738
Num non-zero elements: 2
Test accuracy for 1: 0.852
Num non-zero elements: 2
Test accuracy for 2: 0.694
Num non-zero elements: 2
Test accuracy for 3: 0.615
Num non-zero elements: 2
Test accuracy for 5: 0.819
Num non-zero elements: 2
Test accuracy for 6: 0.746
Num non-zero elements: 5
Test accuracy for 1: 0.916
Num non-zero elements: 5
Test accuracy for 2: 0.724
Num non-zero elements: 5
Test accuracy for 3: 0.744
Num non-zero elements: 5
Test accuracy for 5: 0.862
Num non-zero elements: 5
Test accuracy for 6: 0.772
Num non-zero elements: 10
Test accuracy for 1: 0.914
Num non-zero elements: 10
Test accuracy for 2: 0.785
Num non-zero elements: 10
Test accuracy for 3: 0.767
Num non-zero elements: 10
Test accuracy for 5: 0.866
Num non-zero elements: 10
Test accuracy for 6: 0.772
Num non-zero elements: 20
Test accuracy for 1: 0.919
Num non-zero elements: 20
Test accuracy for 2: 0.846
Num non-zero elements: 20
Test accuracy for 3: 0.789
Num non-zero elements: 20
Test accuracy for 5: 0.889
Num non-zero elements: 20
Test accuracy for 6: 0.797
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.876
Num non-zero elements: 50
Test accuracy for 3: 0.879
Num non-zero elements: 50
Test accuracy for 5: 0.916
Num non-zero elements: 50
Test accuracy for 6: 0.788
Num non-zero elements: 2304
Test accuracy for 1.0: 0.982
Num non-zero elements: 2304
Test accuracy for 5.0: 0.981
Num non-zero elements: 1
Test accuracy for 1.0: 0.671
Num non-zero elements: 1
Test accuracy for 5.0: 0.671
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.848
Num non-zero elements: 20
Test accuracy for 5.0: 0.848
Num non-zero elements: 50
Test accuracy for 1.0: 0.932
Num non-zero elements: 50
Test accuracy for 5.0: 0.933
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 1.0: 0.9810000658035278
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 5.0: 0.9800000190734863
Num non-zero elements: 1
Test accuracy for 1.0: 0.885
Num non-zero elements: 1
Test accuracy for 5.0: 0.885
Num non-zero elements: 2
Test accuracy for 1.0: 0.909
Num non-zero elements: 2
Test accuracy for 5.0: 0.909
Num non-zero elements: 5
Test accuracy for 1.0: 0.918
Num non-zero elements: 5
Test accuracy for 5.0: 0.918
Num non-zero elements: 10
Test accuracy for 1.0: 0.929
Num non-zero elements: 10
Test accuracy for 5.0: 0.929
Num non-zero elements: 20
Test accuracy for 1.0: 0.944
Num non-zero elements: 20
Test accuracy for 5.0: 0.944
Num non-zero elements: 50
Test accuracy for 1.0: 0.958
Num non-zero elements: 50
Test accuracy for 5.0: 0.958
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.949
Num non-zero elements: 2304
Test accuracy for Python: 0.984
Num non-zero elements: 2304
Test accuracy for HTML: 0.984
Num non-zero elements: 2304
Test accuracy for Java: 0.96
Num non-zero elements: 2304
Test accuracy for PHP: 0.952
Num non-zero elements: 1
Test accuracy for C: 0.661
Num non-zero elements: 1
Test accuracy for Python: 0.646
Num non-zero elements: 1
Test accuracy for HTML: 0.728
Num non-zero elements: 1
Test accuracy for Java: 0.641
Num non-zero elements: 1
Test accuracy for PHP: 0.607
Num non-zero elements: 2
Test accuracy for C: 0.674
Num non-zero elements: 2
Test accuracy for Python: 0.676
Num non-zero elements: 2
Test accuracy for HTML: 0.793
Num non-zero elements: 2
Test accuracy for Java: 0.678
Num non-zero elements: 2
Test accuracy for PHP: 0.658
Num non-zero elements: 5
Test accuracy for C: 0.757
Num non-zero elements: 5
Test accuracy for Python: 0.709
Num non-zero elements: 5
Test accuracy for HTML: 0.901
Num non-zero elements: 5
Test accuracy for Java: 0.737
Num non-zero elements: 5
Test accuracy for PHP: 0.689
Num non-zero elements: 10
Test accuracy for C: 0.785
Num non-zero elements: 10
Test accuracy for Python: 0.739
Num non-zero elements: 10
Test accuracy for HTML: 0.95
Num non-zero elements: 10
Test accuracy for Java: 0.783
Num non-zero elements: 10
Test accuracy for PHP: 0.731
Num non-zero elements: 20
Test accuracy for C: 0.872
Num non-zero elements: 20
Test accuracy for Python: 0.869
Num non-zero elements: 20
Test accuracy for HTML: 0.96
Num non-zero elements: 20
Test accuracy for Java: 0.845
Num non-zero elements: 20
Test accuracy for PHP: 0.839
Num non-zero elements: 50
Test accuracy for C: 0.918
Num non-zero elements: 50
Test accuracy for Python: 0.949
Num non-zero elements: 50
Test accuracy for HTML: 0.972
Num non-zero elements: 50
Test accuracy for Java: 0.9
Num non-zero elements: 50
Test accuracy for PHP: 0.913
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for C: 0.9470000267028809
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for Python: 0.9850000739097595
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for HTML: 0.9860000610351562
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for Java: 0.9660000205039978
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for PHP: 0.9550000429153442
Num non-zero elements: 1
Test accuracy for C: 0.56
Num non-zero elements: 1
Test accuracy for Python: 0.548
Num non-zero elements: 1
Test accuracy for HTML: 0.865
Num non-zero elements: 1
Test accuracy for Java: 0.64
Num non-zero elements: 1
Test accuracy for PHP: 0.6
Num non-zero elements: 2
Test accuracy for C: 0.717
Num non-zero elements: 2
Test accuracy for Python: 0.546
Num non-zero elements: 2
Test accuracy for HTML: 0.922
Num non-zero elements: 2
Test accuracy for Java: 0.644
Num non-zero elements: 2
Test accuracy for PHP: 0.61
Num non-zero elements: 5
Test accuracy for C: 0.709
Num non-zero elements: 5
Test accuracy for Python: 0.687
Num non-zero elements: 5
Test accuracy for HTML: 0.918
Num non-zero elements: 5
Test accuracy for Java: 0.65
Num non-zero elements: 5
Test accuracy for PHP: 0.716
Num non-zero elements: 10
Test accuracy for C: 0.827
Num non-zero elements: 10
Test accuracy for Python: 0.723
Num non-zero elements: 10
Test accuracy for HTML: 0.952
Num non-zero elements: 10
Test accuracy for Java: 0.71
Num non-zero elements: 10
Test accuracy for PHP: 0.741
Num non-zero elements: 20
Test accuracy for C: 0.831
Num non-zero elements: 20
Test accuracy for Python: 0.883
Num non-zero elements: 20
Test accuracy for HTML: 0.957
Num non-zero elements: 20
Test accuracy for Java: 0.811
Num non-zero elements: 20
Test accuracy for PHP: 0.907
Num non-zero elements: 50
Test accuracy for C: 0.912
Num non-zero elements: 50
Test accuracy for Python: 0.939
Num non-zero elements: 50
Test accuracy for HTML: 0.972
Num non-zero elements: 50
Test accuracy for Java: 0.897
Num non-zero elements: 50
Test accuracy for PHP: 0.923
Num non-zero elements: 2304
Test accuracy for 0: 0.931
Num non-zero elements: 2304
Test accuracy for 1: 0.99
Num non-zero elements: 2304
Test accuracy for 2: 0.914
Num non-zero elements: 2304
Test accuracy for 3: 0.929
Num non-zero elements: 1
Test accuracy for 0: 0.572
Num non-zero elements: 1
Test accuracy for 1: 0.655
Num non-zero elements: 1
Test accuracy for 2: 0.666
Num non-zero elements: 1
Test accuracy for 3: 0.628
Num non-zero elements: 2
Test accuracy for 0: 0.81
Num non-zero elements: 2
Test accuracy for 1: 0.794
Num non-zero elements: 2
Test accuracy for 2: 0.681
Num non-zero elements: 2
Test accuracy for 3: 0.792
Num non-zero elements: 5
Test accuracy for 0: 0.813
Num non-zero elements: 5
Test accuracy for 1: 0.879
Num non-zero elements: 5
Test accuracy for 2: 0.753
Num non-zero elements: 5
Test accuracy for 3: 0.842
Num non-zero elements: 10
Test accuracy for 0: 0.871
Num non-zero elements: 10
Test accuracy for 1: 0.944
Num non-zero elements: 10
Test accuracy for 2: 0.789
Num non-zero elements: 10
Test accuracy for 3: 0.876
Num non-zero elements: 20
Test accuracy for 0: 0.877
Num non-zero elements: 20
Test accuracy for 1: 0.954
Num non-zero elements: 20
Test accuracy for 2: 0.852
Num non-zero elements: 20
Test accuracy for 3: 0.89
Num non-zero elements: 50
Test accuracy for 0: 0.903
Num non-zero elements: 50
Test accuracy for 1: 0.969
Num non-zero elements: 50
Test accuracy for 2: 0.904
Num non-zero elements: 50
Test accuracy for 3: 0.92
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.940000057220459
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 1: 0.9850000739097595
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 2: 0.9280000329017639
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 3: 0.9520000219345093
Num non-zero elements: 1
Test accuracy for 0: 0.797
Num non-zero elements: 1
Test accuracy for 1: 0.972
Num non-zero elements: 1
Test accuracy for 2: 0.804
Num non-zero elements: 1
Test accuracy for 3: 0.705
Num non-zero elements: 2
Test accuracy for 0: 0.891
Num non-zero elements: 2
Test accuracy for 1: 0.976
Num non-zero elements: 2
Test accuracy for 2: 0.798
Num non-zero elements: 2
Test accuracy for 3: 0.725
Num non-zero elements: 5
Test accuracy for 0: 0.893
Num non-zero elements: 5
Test accuracy for 1: 0.974
Num non-zero elements: 5
Test accuracy for 2: 0.828
Num non-zero elements: 5
Test accuracy for 3: 0.846
Num non-zero elements: 10
Test accuracy for 0: 0.9
Num non-zero elements: 10
Test accuracy for 1: 0.981
Num non-zero elements: 10
Test accuracy for 2: 0.868
Num non-zero elements: 10
Test accuracy for 3: 0.88
Num non-zero elements: 20
Test accuracy for 0: 0.909
Num non-zero elements: 20
Test accuracy for 1: 0.977
Num non-zero elements: 20
Test accuracy for 2: 0.875
Num non-zero elements: 20
Test accuracy for 3: 0.908
Num non-zero elements: 50
Test accuracy for 0: 0.921
Num non-zero elements: 50
Test accuracy for 1: 0.98
Num non-zero elements: 50
Test accuracy for 2: 0.886
Num non-zero elements: 50
Test accuracy for 3: 0.938
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.999
Num non-zero elements: 2304
Test accuracy for fr: 0.999
Num non-zero elements: 2304
Test accuracy for de: 1.0
Num non-zero elements: 2304
Test accuracy for es: 1.0
Num non-zero elements: 2304
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.746
Num non-zero elements: 1
Test accuracy for fr: 0.589
Num non-zero elements: 1
Test accuracy for de: 0.75
Num non-zero elements: 1
Test accuracy for es: 0.482
Num non-zero elements: 1
Test accuracy for nl: 0.662
Num non-zero elements: 2
Test accuracy for en: 0.828
Num non-zero elements: 2
Test accuracy for fr: 0.816
Num non-zero elements: 2
Test accuracy for de: 0.823
Num non-zero elements: 2
Test accuracy for es: 0.911
Num non-zero elements: 2
Test accuracy for nl: 0.74
Num non-zero elements: 5
Test accuracy for en: 0.893
Num non-zero elements: 5
Test accuracy for fr: 0.919
Num non-zero elements: 5
Test accuracy for de: 0.917
Num non-zero elements: 5
Test accuracy for es: 0.983
Num non-zero elements: 5
Test accuracy for nl: 0.862
Num non-zero elements: 10
Test accuracy for en: 0.975
Num non-zero elements: 10
Test accuracy for fr: 0.974
Num non-zero elements: 10
Test accuracy for de: 0.96
Num non-zero elements: 10
Test accuracy for es: 0.988
Num non-zero elements: 10
Test accuracy for nl: 0.919
Num non-zero elements: 20
Test accuracy for en: 0.996
Num non-zero elements: 20
Test accuracy for fr: 0.99
Num non-zero elements: 20
Test accuracy for de: 0.979
Num non-zero elements: 20
Test accuracy for es: 0.996
Num non-zero elements: 20
Test accuracy for nl: 0.983
Num non-zero elements: 50
Test accuracy for en: 0.997
Num non-zero elements: 50
Test accuracy for fr: 0.999
Num non-zero elements: 50
Test accuracy for de: 0.997
Num non-zero elements: 50
Test accuracy for es: 1.0
Num non-zero elements: 50
Test accuracy for nl: 0.996
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for en: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for fr: 1.0
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for de: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for es: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.997
Num non-zero elements: 1
Test accuracy for fr: 0.813
Num non-zero elements: 1
Test accuracy for de: 0.756
Num non-zero elements: 1
Test accuracy for es: 0.862
Num non-zero elements: 1
Test accuracy for nl: 0.762
Num non-zero elements: 2
Test accuracy for en: 0.999
Num non-zero elements: 2
Test accuracy for fr: 0.842
Num non-zero elements: 2
Test accuracy for de: 0.876
Num non-zero elements: 2
Test accuracy for es: 0.875
Num non-zero elements: 2
Test accuracy for nl: 0.852
Num non-zero elements: 5
Test accuracy for en: 1.0
Num non-zero elements: 5
Test accuracy for fr: 0.859
Num non-zero elements: 5
Test accuracy for de: 0.902
Num non-zero elements: 5
Test accuracy for es: 0.885
Num non-zero elements: 5
Test accuracy for nl: 0.856
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.872
Num non-zero elements: 10
Test accuracy for de: 0.965
Num non-zero elements: 10
Test accuracy for es: 0.914
Num non-zero elements: 10
Test accuracy for nl: 0.944
Num non-zero elements: 20
Test accuracy for en: 0.998
Num non-zero elements: 20
Test accuracy for fr: 0.93
Num non-zero elements: 20
Test accuracy for de: 0.978
Num non-zero elements: 20
Test accuracy for es: 0.954
Num non-zero elements: 20
Test accuracy for nl: 0.966
Num non-zero elements: 50
Test accuracy for en: 0.998
Num non-zero elements: 50
Test accuracy for fr: 0.966
Num non-zero elements: 50
Test accuracy for de: 0.989
Num non-zero elements: 50
Test accuracy for es: 0.992
Num non-zero elements: 50
Test accuracy for nl: 0.984
Time spent on sparse_probing: 979.42s

[2025-02-06 18:09:35]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.001841 0.004826 0.027409 ... 0.017248 0.033188 0.002655]
(16384,)
Retain sparsity: [0.002255 0.00668  0.02557  ... 0.018389 0.045116 0.00262 ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15494 12897  6853  4299  7792   735  7492 14419  6532 12036 15408 16303
  3314  5592  9963  5447 13558  5235 14021  4321  5102  5787 16312  9376
  4335  3109 14359  3158 11204 12264 11447  4582   788   976   888  4625
  1833  9704  1140  5829  9234 10138 11844  6346  4614  5742 12057  8940
  1647 11787  8948  2895 12208  5917 12650  8911   889 13492 14471 13944
  8097  4013  1398  6074 14189  3667 10259 15811  4566  4350 13088 10688
 15916  2945 10581 12598  9102  5585 10332  7225  1259  5403  2916  7987
  9364  2270  5145  9564  9060  7753 11197   666   223 12816  9594 13211
 13048 15790   195  1557 12827 15053 10532  4410   106   364 12108 14395
 14496 15306 15211  9511  9129 15923  9430 13450  2659 14885  7528   846
  3341  3036  7769  2705  1188 15688 10961  9863 16076 11982   932  2042
  9209  4821 16197 14796 11918 14192 15504  8671  6073  3233 11669  7385
  4789 15820  4991  7328 10449  9637  6300  1579 10744  6823  3028  3465
    84  2874  3772 13991 12260  1207 14532   638  4849  6399  7700 10024
 15816 15374 14707   711 15846 16304  8749  9689  5484  3873  4458  8631
  6066   456 14204  1462 12861  9000  6940 10277  1871 15322  2655 10807
  1119  1634 13476 12173  7141 12933 11432 13265  8680  8545 10083  6060
  3809  4488 16174 11625 14949  1500 11996  2709  4361 12205 13930 15509
 11529 10901  3707  5119  7907 13058 10289  8039  1360  2528  7609  6039
  5417   689  3380 13094  5620  2789  4874 11578  4125   168  8549 13463
 15436  2755  7988  3622   399 12747   263  7968    79  5993 11034 10302
  8327  8959  6662  1455  4595   987  8817  9475  1201  1975  4223  6079
  9230  1896  2132 11902  5079  1444 10455  4662  7309  5761  6159  5706
  4143  9175   639 12244  4178 11929  4855  1182 15735   883 12418 15577
 15928  7756   499  5749  5921 10895 15113  7556 11221 15955 11788  6264
 10993 12600  9537 11163 14505 11828  4445  6178  3760  9504 10681 10387
  7144  7249  6784  9237 15222 15814  1535  3907  5897  6061 11514  2046
  7455  6115  8821  5571 12610   588  3941 15741 11451  7918 13403  8571
  9635  6737 13685  2847 16176 11454  9201  1571 15808 13371 13513 10313
 10866  9212 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036]), array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036, 15408, 16303,  3314,  5592,  9963,  5447, 13558,  5235,
       14021,  4321])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [12681 11085 10405 ... 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343]), array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343, 14751,   276, 13183, 10253,  4801, 14016,    14,  8289,
        1329, 13470])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   1.000000                1.000000  ...          25              1.000000
1   0.853933                0.963636  ...         200              0.944444
2   0.973783                1.000000  ...         100              1.000000
3   0.790262                0.790909  ...         200              0.761438
4   0.829588                0.909091  ...         200              0.905229
5   0.988764                1.000000  ...          50              1.000000
6   0.996255                0.990909  ...         100              0.993464
7   0.968165                0.990909  ...         100              0.986928
8   1.000000                1.000000  ...          50              1.000000
9   0.998127                1.000000  ...          50              1.000000
10  1.000000                1.000000  ...          25              1.000000
11  0.956929                0.990909  ...         100              0.986928
12  1.000000                1.000000  ...          25              1.000000
13  1.000000                1.000000  ...          25              1.000000
14  0.983146                0.972727  ...         200              0.973856
15  0.996255                1.000000  ...          50              1.000000

[16 rows x 10 columns]
Currently on iteration 1 of run 5
Let me first add the Run 4 results to notes.txt, then proceed with Run 5 implementation.                                                                 

notes.txt                                                                                                                                                

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
 ## Run 3: Increased Competition Threshold                                                                                                               
 Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.                  
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Increased base threshold from 0.1 to 0.2                                                                                                              
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                      
 - Kept dynamic threshold computation                                                                                                                    
 - Retained smooth competition coefficients                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03592 (target <0.01)                                                                                                             
    - Slight improvement from Run 2 (0.03676)                                                                                                            
    - Still higher than baseline (0.02959)                                                                                                               
    - Higher threshold shows diminishing returns                                                                                                         
                                                                                                                                                         
 2. Sparse Probing: 0.7735 (target >0.74)                                                                                                                
    - Similar to previous runs (0.7748)                                                                                                                  
    - Maintains strong feature separation                                                                                                                
    - Robust across different datasets                                                                                                                   
                                                                                                                                                         
 3. Unlearning Score: 0.0056 (target >0.1)                                                                                                               
    - Significant decrease from Run 2 (0.0300)                                                                                                           
    - Far below baseline (0.0637)                                                                                                                        
    - Suggests competition may be too aggressive                                                                                                         
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.6015625 (slightly worse)                                                                                                        
 - KL Divergence Score: 0.9877                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Higher threshold improved absorption marginally                                                                                                      
 2. Unlearning performance degraded significantly                                                                                                        
 3. Feature separation remains stable                                                                                                                    
 4. Reconstruction quality slightly decreased                                                                                                            
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Lower base threshold back to 0.1                                                                                                                     
 2. Implement exponential decay for threshold                                                                                                            
 3. Add progressive sparsity scaling                                                                                                                     
 =======                                                                                                                                                 
 ## Run 3: Increased Competition Threshold                                                                                                               
 Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.                  
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Increased base threshold from 0.1 to 0.2                                                                                                              
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                      
 - Kept dynamic threshold computation                                                                                                                    
 - Retained smooth competition coefficients                                                                                                              
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03592 (target <0.01)                                                                                                             
    - Slight improvement from Run 2 (0.03676)                                                                                                            
    - Still higher than baseline (0.02959)                                                                                                               
    - Higher threshold shows diminishing returns                                                                                                         
                                                                                                                                                         
 2. Sparse Probing: 0.7735 (target >0.74)                                                                                                                
    - Similar to previous runs (0.7748)                                                                                                                  
    - Maintains strong feature separation                                                                                                                
    - Robust across different datasets                                                                                                                   
                                                                                                                                                         
 3. Unlearning Score: 0.0056 (target >0.1)                                                                                                               
    - Significant decrease from Run 2 (0.0300)                                                                                                           
    - Far below baseline (0.0637)                                                                                                                        
    - Suggests competition may be too aggressive                                                                                                         
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.6015625 (slightly worse)                                                                                                        
 - KL Divergence Score: 0.9877                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Higher threshold improved absorption marginally                                                                                                      
 2. Unlearning performance degraded significantly                                                                                                        
 3. Feature separation remains stable                                                                                                                    
 4. Reconstruction quality slightly decreased                                                                                                            
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Lower base threshold back to 0.1                                                                                                                     
 2. Implement exponential decay for threshold                                                                                                            
 3. Add progressive sparsity scaling                                                                                                                     
                                                                                                                                                         
 ## Run 4: Exponential Decay and Progressive Sparsity                                                                                                    
 Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.                          
                                                                                                                                                         
 Key Implementation Details:                                                                                                                             
 - Reverted base threshold to 0.1                                                                                                                        
 - Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)                                                                            
 - Implemented progressive sparsity scaling: scale = 1.0 + progress                                                                                      
 - Applied sparsity scale to correlations before thresholding                                                                                            
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                      
                                                                                                                                                         
 Results Analysis:                                                                                                                                       
 1. Absorption Score: 0.03768 (target <0.01)                                                                                                             
    - Slight regression from Run 3 (0.03592)                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                     
    - Suggests exponential decay may be too aggressive                                                                                                   
                                                                                                                                                         
 2. Sparse Probing: 0.7712 (target >0.74)                                                                                                                
    - Slight regression from Run 3 (0.7735)                                                                                                              
    - Still well above target threshold                                                                                                                  
    - Maintains good feature separation                                                                                                                  
                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                               
    - Improvement from Run 3 (0.0056)                                                                                                                    
    - Still below baseline (0.0637)                                                                                                                      
    - Progressive sparsity helps unlearning                                                                                                              
                                                                                                                                                         
 Core Metrics:                                                                                                                                           
 - Reconstruction MSE: 1.609375 (stable)                                                                                                                 
 - KL Divergence Score: 0.9876                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                    
                                                                                                                                                         
 Observations:                                                                                                                                           
 1. Exponential decay may be too aggressive early in training                                                                                            
 2. Progressive sparsity helps with unlearning                                                                                                           
 3. Feature separation remains above target                                                                                                              
 4. Need better balance between early and late training phases                                                                                           
                                                                                                                                                         
 Next Steps:                                                                                                                                             
 1. Reduce decay rate to 1.0 for gentler decay                                                                                                           
 2. Add quadratic sparsity scaling                                                                                                                       
 3. Increase base threshold to 0.15                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

For Run 5, let's implement these changes to better balance the training phases. Here are the changes:                                                    

experiment.py                                                                                                                                            

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                   
         base_threshold=0.1,  # return to original threshold                                                                                             
         decay_rate=2.0,     # exponential decay rate                                                                                                    
 =======                                                                                                                                                 
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                   
         base_threshold=0.15,  # moderate threshold                                                                                                      
         decay_rate=1.0,     # gentler decay rate                                                                                                        
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                          
     def compute_sparsity_scale(self, progress):                                                                                                         
         """Compute progressive sparsity scaling"""                                                                                                      
         return 1.0 + progress  # Linear increase from 1.0 to 2.0                                                                                        
 =======                                                                                                                                                 
     def compute_sparsity_scale(self, progress):                                                                                                         
         """Compute quadratic sparsity scaling"""                                                                                                        
         return 1.0 + progress * progress  # Quadratic increase from 1.0 to 2.0                                                                          
 >>>>>>> REPLACE                                                                                                                                         
                                                                                                                                                         

These changes:                                                                                                                                           

 1 Increase base threshold to 0.15 (middle ground between previous values)                                                                               
 2 Reduce decay rate to 1.0 for more gradual decay                                                                                                       
 3 Change sparsity scaling from linear to quadratic                                                                                                      
 4 Maintain other mechanisms from Run 4                                                                                                                  

The key differences:                                                                                                                                     

 • More gradual exponential decay                                                                                                                        
 • Slower sparsity increase early in training                                                                                                            
 • Moderate initial threshold                                                                                                                            
 • Better balance between early and late phases                                                                                                          

Let's run this with:                                                                                                                                     

                                                                                                                                                         
 python experiment.py --out_dir=run_5                                                                                                                    
                                                                                                                                                         

This should provide better balance between feature separation and competition throughout training.                                                       

Tokens: 83k sent, 1.7k received. Cost: $0.27 message, $1.07 session.
Applied edit to notes.txt
Applied edit to experiment.py

python experiment.py --out_dir=run_5
coder_out: Let me first add the Run 4 results to notes.txt, then proceed with Run 5 implementation.

notes.txt
```python
<<<<<<< SEARCH
## Run 3: Increased Competition Threshold
Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.

Key Implementation Details:
- Increased base threshold from 0.1 to 0.2
- Maintained reduced orthogonality weight (λ=0.05)
- Kept dynamic threshold computation
- Retained smooth competition coefficients

Results Analysis:
1. Absorption Score: 0.03592 (target <0.01)
   - Slight improvement from Run 2 (0.03676)
   - Still higher than baseline (0.02959)
   - Higher threshold shows diminishing returns

2. Sparse Probing: 0.7735 (target >0.74)
   - Similar to previous runs (0.7748)
   - Maintains strong feature separation
   - Robust across different datasets

3. Unlearning Score: 0.0056 (target >0.1)
   - Significant decrease from Run 2 (0.0300)
   - Far below baseline (0.0637)
   - Suggests competition may be too aggressive

Core Metrics:
- Reconstruction MSE: 1.6015625 (slightly worse)
- KL Divergence Score: 0.9877
- L0 Sparsity: 320.0

Observations:
1. Higher threshold improved absorption marginally
2. Unlearning performance degraded significantly
3. Feature separation remains stable
4. Reconstruction quality slightly decreased

Next Steps:
1. Lower base threshold back to 0.1
2. Implement exponential decay for threshold
3. Add progressive sparsity scaling
=======
## Run 3: Increased Competition Threshold
Description: Increased base threshold to 0.2 to encourage stronger feature competition while maintaining reduced orthogonality weight.

Key Implementation Details:
- Increased base threshold from 0.1 to 0.2
- Maintained reduced orthogonality weight (λ=0.05)
- Kept dynamic threshold computation
- Retained smooth competition coefficients

Results Analysis:
1. Absorption Score: 0.03592 (target <0.01)
   - Slight improvement from Run 2 (0.03676)
   - Still higher than baseline (0.02959)
   - Higher threshold shows diminishing returns

2. Sparse Probing: 0.7735 (target >0.74)
   - Similar to previous runs (0.7748)
   - Maintains strong feature separation
   - Robust across different datasets

3. Unlearning Score: 0.0056 (target >0.1)
   - Significant decrease from Run 2 (0.0300)
   - Far below baseline (0.0637)
   - Suggests competition may be too aggressive

Core Metrics:
- Reconstruction MSE: 1.6015625 (slightly worse)
- KL Divergence Score: 0.9877
- L0 Sparsity: 320.0

Observations:
1. Higher threshold improved absorption marginally
2. Unlearning performance degraded significantly
3. Feature separation remains stable
4. Reconstruction quality slightly decreased

Next Steps:
1. Lower base threshold back to 0.1
2. Implement exponential decay for threshold
3. Add progressive sparsity scaling

## Run 4: Exponential Decay and Progressive Sparsity
Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.

Key Implementation Details:
- Reverted base threshold to 0.1
- Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)
- Implemented progressive sparsity scaling: scale = 1.0 + progress
- Applied sparsity scale to correlations before thresholding
- Maintained reduced orthogonality weight (λ=0.05)

Results Analysis:
1. Absorption Score: 0.03768 (target <0.01)
   - Slight regression from Run 3 (0.03592)
   - Higher than baseline (0.02959)
   - Suggests exponential decay may be too aggressive

2. Sparse Probing: 0.7712 (target >0.74)
   - Slight regression from Run 3 (0.7735)
   - Still well above target threshold
   - Maintains good feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Improvement from Run 3 (0.0056)
   - Still below baseline (0.0637)
   - Progressive sparsity helps unlearning

Core Metrics:
- Reconstruction MSE: 1.609375 (stable)
- KL Divergence Score: 0.9876
- L0 Sparsity: 320.0

Observations:
1. Exponential decay may be too aggressive early in training
2. Progressive sparsity helps with unlearning
3. Feature separation remains above target
4. Need better balance between early and late training phases

Next Steps:
1. Reduce decay rate to 1.0 for gentler decay
2. Add quadratic sparsity scaling
3. Increase base threshold to 0.15
>>>>>>> REPLACE
```

For Run 5, let's implement these changes to better balance the training phases. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.1,  # return to original threshold
        decay_rate=2.0,     # exponential decay rate
=======
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.15,  # moderate threshold
        decay_rate=1.0,     # gentler decay rate
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def compute_sparsity_scale(self, progress):
        """Compute progressive sparsity scaling"""
        return 1.0 + progress  # Linear increase from 1.0 to 2.0
=======
    def compute_sparsity_scale(self, progress):
        """Compute quadratic sparsity scaling"""
        return 1.0 + progress * progress  # Quadratic increase from 1.0 to 2.0
>>>>>>> REPLACE
```

These changes:
1. Increase base threshold to 0.15 (middle ground between previous values)
2. Reduce decay rate to 1.0 for more gradual decay
3. Change sparsity scaling from linear to quadratic
4. Maintain other mechanisms from Run 4

The key differences:
- More gradual exponential decay
- Slower sparsity increase early in training
- Moderate initial threshold
- Better balance between early and late phases

Let's run this with:

```bash
python experiment.py --out_dir=run_5
```

This should provide better balance between feature separation and competition throughout training., type: <class 'str'>
Step 0: 31236.6171875
Step 100: 4581.24658203125
Step 200: 4064.673095703125
Step 300: 3814.70751953125
Step 400: 3595.725341796875
Step 500: 3452.337646484375
Step 600: 3420.9248046875
Step 700: 3241.21533203125
Step 800: 3210.8525390625
Step 900: 3188.6015625
Step 1000: 3255.43603515625
Step 1100: 3150.840576171875
Step 1200: 3059.12890625
Step 1300: 3096.74755859375
Step 1400: 3095.1240234375
Step 1500: 3106.738525390625
Step 1600: 3035.9541015625
Step 1700: 3057.525634765625
Step 1800: 3022.451171875
Step 1900: 3018.89453125
Step 2000: 3021.214111328125
Step 2100: 3040.8935546875
Step 2200: 2924.739990234375
Step 2300: 2905.10791015625
Step 2400: 2882.23486328125

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_5/final_info.json

[2025-02-06 18:22:20]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
Building metadata
Time spent on absorption: 938.34s

[2025-02-06 18:37:58]  

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_5/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json
Time spent on core: 568.80s

[2025-02-06 18:47:27]  

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 0: 0.935
Num non-zero elements: 2304
Test accuracy for 1: 0.952
Num non-zero elements: 2304
Test accuracy for 2: 0.942
Num non-zero elements: 2304
Test accuracy for 6: 0.988
Num non-zero elements: 2304
Test accuracy for 9: 0.981
Num non-zero elements: 1
Test accuracy for 0: 0.57
Num non-zero elements: 1
Test accuracy for 1: 0.629
Num non-zero elements: 1
Test accuracy for 2: 0.686
Num non-zero elements: 1
Test accuracy for 6: 0.782
Num non-zero elements: 1
Test accuracy for 9: 0.549
Num non-zero elements: 2
Test accuracy for 0: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.661
Num non-zero elements: 2
Test accuracy for 2: 0.713
Num non-zero elements: 2
Test accuracy for 6: 0.801
Num non-zero elements: 2
Test accuracy for 9: 0.701
Num non-zero elements: 5
Test accuracy for 0: 0.721
Num non-zero elements: 5
Test accuracy for 1: 0.7
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 6: 0.902
Num non-zero elements: 5
Test accuracy for 9: 0.87
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.799
Num non-zero elements: 10
Test accuracy for 2: 0.801
Num non-zero elements: 10
Test accuracy for 6: 0.886
Num non-zero elements: 10
Test accuracy for 9: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.817
Num non-zero elements: 20
Test accuracy for 1: 0.88
Num non-zero elements: 20
Test accuracy for 2: 0.879
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.938
Num non-zero elements: 50
Test accuracy for 0: 0.91
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.921
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.954
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 0: 0.9530000686645508
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 1: 0.9630000591278076
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 2: 0.9530000686645508
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 6: 0.9900000691413879
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 9: 0.9830000400543213
Num non-zero elements: 1
Test accuracy for 0: 0.653
Num non-zero elements: 1
Test accuracy for 1: 0.557
Num non-zero elements: 1
Test accuracy for 2: 0.66
Num non-zero elements: 1
Test accuracy for 6: 0.756
Num non-zero elements: 1
Test accuracy for 9: 0.878
Num non-zero elements: 2
Test accuracy for 0: 0.685
Num non-zero elements: 2
Test accuracy for 1: 0.644
Num non-zero elements: 2
Test accuracy for 2: 0.786
Num non-zero elements: 2
Test accuracy for 6: 0.967
Num non-zero elements: 2
Test accuracy for 9: 0.939
Num non-zero elements: 5
Test accuracy for 0: 0.799
Num non-zero elements: 5
Test accuracy for 1: 0.781
Num non-zero elements: 5
Test accuracy for 2: 0.852
Num non-zero elements: 5
Test accuracy for 6: 0.97
Num non-zero elements: 5
Test accuracy for 9: 0.946
Num non-zero elements: 10
Test accuracy for 0: 0.801
Num non-zero elements: 10
Test accuracy for 1: 0.897
Num non-zero elements: 10
Test accuracy for 2: 0.884
Num non-zero elements: 10
Test accuracy for 6: 0.98
Num non-zero elements: 10
Test accuracy for 9: 0.954
Num non-zero elements: 20
Test accuracy for 0: 0.87
Num non-zero elements: 20
Test accuracy for 1: 0.911
Num non-zero elements: 20
Test accuracy for 2: 0.91
Num non-zero elements: 20
Test accuracy for 6: 0.981
Num non-zero elements: 20
Test accuracy for 9: 0.957
Num non-zero elements: 50
Test accuracy for 0: 0.911
Num non-zero elements: 50
Test accuracy for 1: 0.945
Num non-zero elements: 50
Test accuracy for 2: 0.931
Num non-zero elements: 50
Test accuracy for 6: 0.991
Num non-zero elements: 50
Test accuracy for 9: 0.964
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 11: 0.958
Num non-zero elements: 2304
Test accuracy for 13: 0.952
Num non-zero elements: 2304
Test accuracy for 14: 0.953
Num non-zero elements: 2304
Test accuracy for 18: 0.928
Num non-zero elements: 2304
Test accuracy for 19: 0.959
Num non-zero elements: 1
Test accuracy for 11: 0.566
Num non-zero elements: 1
Test accuracy for 13: 0.676
Num non-zero elements: 1
Test accuracy for 14: 0.653
Num non-zero elements: 1
Test accuracy for 18: 0.695
Num non-zero elements: 1
Test accuracy for 19: 0.8
Num non-zero elements: 2
Test accuracy for 11: 0.689
Num non-zero elements: 2
Test accuracy for 13: 0.718
Num non-zero elements: 2
Test accuracy for 14: 0.675
Num non-zero elements: 2
Test accuracy for 18: 0.724
Num non-zero elements: 2
Test accuracy for 19: 0.765
Num non-zero elements: 5
Test accuracy for 11: 0.791
Num non-zero elements: 5
Test accuracy for 13: 0.744
Num non-zero elements: 5
Test accuracy for 14: 0.754
Num non-zero elements: 5
Test accuracy for 18: 0.721
Num non-zero elements: 5
Test accuracy for 19: 0.835
Num non-zero elements: 10
Test accuracy for 11: 0.849
Num non-zero elements: 10
Test accuracy for 13: 0.8
Num non-zero elements: 10
Test accuracy for 14: 0.779
Num non-zero elements: 10
Test accuracy for 18: 0.746
Num non-zero elements: 10
Test accuracy for 19: 0.875
Num non-zero elements: 20
Test accuracy for 11: 0.896
Num non-zero elements: 20
Test accuracy for 13: 0.882
Num non-zero elements: 20
Test accuracy for 14: 0.876
Num non-zero elements: 20
Test accuracy for 18: 0.772
Num non-zero elements: 20
Test accuracy for 19: 0.903
Num non-zero elements: 50
Test accuracy for 11: 0.939
Num non-zero elements: 50
Test accuracy for 13: 0.908
Num non-zero elements: 50
Test accuracy for 14: 0.907
Num non-zero elements: 50
Test accuracy for 18: 0.84
Num non-zero elements: 50
Test accuracy for 19: 0.927
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 11: 0.9690000414848328
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 13: 0.9510000348091125
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 14: 0.9570000171661377
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 18: 0.937000036239624
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 19: 0.9650000333786011
Num non-zero elements: 1
Test accuracy for 11: 0.836
Num non-zero elements: 1
Test accuracy for 13: 0.805
Num non-zero elements: 1
Test accuracy for 14: 0.789
Num non-zero elements: 1
Test accuracy for 18: 0.728
Num non-zero elements: 1
Test accuracy for 19: 0.83
Num non-zero elements: 2
Test accuracy for 11: 0.846
Num non-zero elements: 2
Test accuracy for 13: 0.814
Num non-zero elements: 2
Test accuracy for 14: 0.824
Num non-zero elements: 2
Test accuracy for 18: 0.749
Num non-zero elements: 2
Test accuracy for 19: 0.855
Num non-zero elements: 5
Test accuracy for 11: 0.843
Num non-zero elements: 5
Test accuracy for 13: 0.864
Num non-zero elements: 5
Test accuracy for 14: 0.851
Num non-zero elements: 5
Test accuracy for 18: 0.748
Num non-zero elements: 5
Test accuracy for 19: 0.899
Num non-zero elements: 10
Test accuracy for 11: 0.943
Num non-zero elements: 10
Test accuracy for 13: 0.907
Num non-zero elements: 10
Test accuracy for 14: 0.85
Num non-zero elements: 10
Test accuracy for 18: 0.748
Num non-zero elements: 10
Test accuracy for 19: 0.906
Num non-zero elements: 20
Test accuracy for 11: 0.952
Num non-zero elements: 20
Test accuracy for 13: 0.917
Num non-zero elements: 20
Test accuracy for 14: 0.905
Num non-zero elements: 20
Test accuracy for 18: 0.856
Num non-zero elements: 20
Test accuracy for 19: 0.925
Num non-zero elements: 50
Test accuracy for 11: 0.959
Num non-zero elements: 50
Test accuracy for 13: 0.929
Num non-zero elements: 50
Test accuracy for 14: 0.931
Num non-zero elements: 50
Test accuracy for 18: 0.884
Num non-zero elements: 50
Test accuracy for 19: 0.939
Removing 12 from test set
Removing 24 from test set
Num non-zero elements: 2304
Test accuracy for 20: 0.95
Num non-zero elements: 2304
Test accuracy for 21: 0.91
Num non-zero elements: 2304
Test accuracy for 22: 0.896
Num non-zero elements: 2304
Test accuracy for 25: 0.953
Num non-zero elements: 2304
Test accuracy for 26: 0.862
Num non-zero elements: 1
Test accuracy for 20: 0.711
Num non-zero elements: 1
Test accuracy for 21: 0.757
Num non-zero elements: 1
Test accuracy for 22: 0.634
Num non-zero elements: 1
Test accuracy for 25: 0.686
Num non-zero elements: 1
Test accuracy for 26: 0.624
Num non-zero elements: 2
Test accuracy for 20: 0.802
Num non-zero elements: 2
Test accuracy for 21: 0.765
Num non-zero elements: 2
Test accuracy for 22: 0.68
Num non-zero elements: 2
Test accuracy for 25: 0.772
Num non-zero elements: 2
Test accuracy for 26: 0.679
Num non-zero elements: 5
Test accuracy for 20: 0.857
Num non-zero elements: 5
Test accuracy for 21: 0.783
Num non-zero elements: 5
Test accuracy for 22: 0.706
Num non-zero elements: 5
Test accuracy for 25: 0.79
Num non-zero elements: 5
Test accuracy for 26: 0.687
Num non-zero elements: 10
Test accuracy for 20: 0.874
Num non-zero elements: 10
Test accuracy for 21: 0.818
Num non-zero elements: 10
Test accuracy for 22: 0.752
Num non-zero elements: 10
Test accuracy for 25: 0.855
Num non-zero elements: 10
Test accuracy for 26: 0.699
Num non-zero elements: 20
Test accuracy for 20: 0.906
Num non-zero elements: 20
Test accuracy for 21: 0.836
Num non-zero elements: 20
Test accuracy for 22: 0.805
Num non-zero elements: 20
Test accuracy for 25: 0.904
Num non-zero elements: 20
Test accuracy for 26: 0.824
Num non-zero elements: 50
Test accuracy for 20: 0.935
Num non-zero elements: 50
Test accuracy for 21: 0.872
Num non-zero elements: 50
Test accuracy for 22: 0.876
Num non-zero elements: 50
Test accuracy for 25: 0.938
Num non-zero elements: 50
Test accuracy for 26: 0.848
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 20: 0.9600000381469727
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 21: 0.9340000152587891
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 22: 0.9190000295639038
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 25: 0.9630000591278076
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 26: 0.8960000276565552
Num non-zero elements: 1
Test accuracy for 20: 0.877
Num non-zero elements: 1
Test accuracy for 21: 0.792
Num non-zero elements: 1
Test accuracy for 22: 0.734
Num non-zero elements: 1
Test accuracy for 25: 0.82
Num non-zero elements: 1
Test accuracy for 26: 0.675
Num non-zero elements: 2
Test accuracy for 20: 0.894
Num non-zero elements: 2
Test accuracy for 21: 0.788
Num non-zero elements: 2
Test accuracy for 22: 0.754
Num non-zero elements: 2
Test accuracy for 25: 0.841
Num non-zero elements: 2
Test accuracy for 26: 0.775
Num non-zero elements: 5
Test accuracy for 20: 0.913
Num non-zero elements: 5
Test accuracy for 21: 0.837
Num non-zero elements: 5
Test accuracy for 22: 0.799
Num non-zero elements: 5
Test accuracy for 25: 0.906
Num non-zero elements: 5
Test accuracy for 26: 0.795
Num non-zero elements: 10
Test accuracy for 20: 0.918
Num non-zero elements: 10
Test accuracy for 21: 0.863
Num non-zero elements: 10
Test accuracy for 22: 0.844
Num non-zero elements: 10
Test accuracy for 25: 0.915
Num non-zero elements: 10
Test accuracy for 26: 0.806
Num non-zero elements: 20
Test accuracy for 20: 0.924
Num non-zero elements: 20
Test accuracy for 21: 0.87
Num non-zero elements: 20
Test accuracy for 22: 0.856
Num non-zero elements: 20
Test accuracy for 25: 0.934
Num non-zero elements: 20
Test accuracy for 26: 0.846
Num non-zero elements: 50
Test accuracy for 20: 0.935
Num non-zero elements: 50
Test accuracy for 21: 0.887
Num non-zero elements: 50
Test accuracy for 22: 0.878
Num non-zero elements: 50
Test accuracy for 25: 0.949
Num non-zero elements: 50
Test accuracy for 26: 0.85
Removing 7 from train set
Removing 29 from train set
Removing 31 from train set
Removing 0 from train set
Removing 28 from train set
Num non-zero elements: 2304
Test accuracy for 1: 0.935
Num non-zero elements: 2304
Test accuracy for 2: 0.894
Num non-zero elements: 2304
Test accuracy for 3: 0.913
Num non-zero elements: 2304
Test accuracy for 5: 0.897
Num non-zero elements: 2304
Test accuracy for 6: 0.823
Num non-zero elements: 1
Test accuracy for 1: 0.678
Num non-zero elements: 1
Test accuracy for 2: 0.601
Num non-zero elements: 1
Test accuracy for 3: 0.582
Num non-zero elements: 1
Test accuracy for 5: 0.576
Num non-zero elements: 1
Test accuracy for 6: 0.59
Num non-zero elements: 2
Test accuracy for 1: 0.734
Num non-zero elements: 2
Test accuracy for 2: 0.644
Num non-zero elements: 2
Test accuracy for 3: 0.616
Num non-zero elements: 2
Test accuracy for 5: 0.583
Num non-zero elements: 2
Test accuracy for 6: 0.616
Num non-zero elements: 5
Test accuracy for 1: 0.759
Num non-zero elements: 5
Test accuracy for 2: 0.648
Num non-zero elements: 5
Test accuracy for 3: 0.626
Num non-zero elements: 5
Test accuracy for 5: 0.654
Num non-zero elements: 5
Test accuracy for 6: 0.657
Num non-zero elements: 10
Test accuracy for 1: 0.814
Num non-zero elements: 10
Test accuracy for 2: 0.784
Num non-zero elements: 10
Test accuracy for 3: 0.758
Num non-zero elements: 10
Test accuracy for 5: 0.745
Num non-zero elements: 10
Test accuracy for 6: 0.725
Num non-zero elements: 20
Test accuracy for 1: 0.866
Num non-zero elements: 20
Test accuracy for 2: 0.837
Num non-zero elements: 20
Test accuracy for 3: 0.788
Num non-zero elements: 20
Test accuracy for 5: 0.791
Num non-zero elements: 20
Test accuracy for 6: 0.763
Num non-zero elements: 50
Test accuracy for 1: 0.923
Num non-zero elements: 50
Test accuracy for 2: 0.875
Num non-zero elements: 50
Test accuracy for 3: 0.847
Num non-zero elements: 50
Test accuracy for 5: 0.85
Num non-zero elements: 50
Test accuracy for 6: 0.793
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 1: 0.9490000605583191
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 2: 0.937000036239624
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 3: 0.9070000648498535
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 5: 0.9260000586509705
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 36 epochs
Test accuracy for 6: 0.8740000128746033
Num non-zero elements: 1
Test accuracy for 1: 0.83
Num non-zero elements: 1
Test accuracy for 2: 0.676
Num non-zero elements: 1
Test accuracy for 3: 0.609
Num non-zero elements: 1
Test accuracy for 5: 0.604
Num non-zero elements: 1
Test accuracy for 6: 0.734
Num non-zero elements: 2
Test accuracy for 1: 0.854
Num non-zero elements: 2
Test accuracy for 2: 0.682
Num non-zero elements: 2
Test accuracy for 3: 0.629
Num non-zero elements: 2
Test accuracy for 5: 0.641
Num non-zero elements: 2
Test accuracy for 6: 0.747
Num non-zero elements: 5
Test accuracy for 1: 0.916
Num non-zero elements: 5
Test accuracy for 2: 0.706
Num non-zero elements: 5
Test accuracy for 3: 0.727
Num non-zero elements: 5
Test accuracy for 5: 0.869
Num non-zero elements: 5
Test accuracy for 6: 0.772
Num non-zero elements: 10
Test accuracy for 1: 0.922
Num non-zero elements: 10
Test accuracy for 2: 0.78
Num non-zero elements: 10
Test accuracy for 3: 0.774
Num non-zero elements: 10
Test accuracy for 5: 0.867
Num non-zero elements: 10
Test accuracy for 6: 0.77
Num non-zero elements: 20
Test accuracy for 1: 0.92
Num non-zero elements: 20
Test accuracy for 2: 0.861
Num non-zero elements: 20
Test accuracy for 3: 0.777
Num non-zero elements: 20
Test accuracy for 5: 0.888
Num non-zero elements: 20
Test accuracy for 6: 0.787
Num non-zero elements: 50
Test accuracy for 1: 0.928
Num non-zero elements: 50
Test accuracy for 2: 0.876
Num non-zero elements: 50
Test accuracy for 3: 0.872
Num non-zero elements: 50
Test accuracy for 5: 0.893
Num non-zero elements: 50
Test accuracy for 6: 0.802
Num non-zero elements: 2304
Test accuracy for 1.0: 0.982
Num non-zero elements: 2304
Test accuracy for 5.0: 0.981
Num non-zero elements: 1
Test accuracy for 1.0: 0.671
Num non-zero elements: 1
Test accuracy for 5.0: 0.671
Num non-zero elements: 2
Test accuracy for 1.0: 0.724
Num non-zero elements: 2
Test accuracy for 5.0: 0.724
Num non-zero elements: 5
Test accuracy for 1.0: 0.766
Num non-zero elements: 5
Test accuracy for 5.0: 0.766
Num non-zero elements: 10
Test accuracy for 1.0: 0.826
Num non-zero elements: 10
Test accuracy for 5.0: 0.826
Num non-zero elements: 20
Test accuracy for 1.0: 0.847
Num non-zero elements: 20
Test accuracy for 5.0: 0.847
Num non-zero elements: 50
Test accuracy for 1.0: 0.933
Num non-zero elements: 50
Test accuracy for 5.0: 0.933
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 1.0: 0.9820000529289246
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 5.0: 0.9810000658035278
Num non-zero elements: 1
Test accuracy for 1.0: 0.881
Num non-zero elements: 1
Test accuracy for 5.0: 0.881
Num non-zero elements: 2
Test accuracy for 1.0: 0.903
Num non-zero elements: 2
Test accuracy for 5.0: 0.903
Num non-zero elements: 5
Test accuracy for 1.0: 0.916
Num non-zero elements: 5
Test accuracy for 5.0: 0.916
Num non-zero elements: 10
Test accuracy for 1.0: 0.925
Num non-zero elements: 10
Test accuracy for 5.0: 0.925
Num non-zero elements: 20
Test accuracy for 1.0: 0.948
Num non-zero elements: 20
Test accuracy for 5.0: 0.948
Num non-zero elements: 50
Test accuracy for 1.0: 0.953
Num non-zero elements: 50
Test accuracy for 5.0: 0.953
Loading dataset codeparrot/github-code, this usually takes ~30 seconds
Num non-zero elements: 2304
Test accuracy for C: 0.946
Num non-zero elements: 2304
Test accuracy for Python: 0.985
Num non-zero elements: 2304
Test accuracy for HTML: 0.983
Num non-zero elements: 2304
Test accuracy for Java: 0.959
Num non-zero elements: 2304
Test accuracy for PHP: 0.952
Num non-zero elements: 1
Test accuracy for C: 0.656
Num non-zero elements: 1
Test accuracy for Python: 0.639
Num non-zero elements: 1
Test accuracy for HTML: 0.788
Num non-zero elements: 1
Test accuracy for Java: 0.62
Num non-zero elements: 1
Test accuracy for PHP: 0.581
Num non-zero elements: 2
Test accuracy for C: 0.748
Num non-zero elements: 2
Test accuracy for Python: 0.685
Num non-zero elements: 2
Test accuracy for HTML: 0.795
Num non-zero elements: 2
Test accuracy for Java: 0.674
Num non-zero elements: 2
Test accuracy for PHP: 0.636
Num non-zero elements: 5
Test accuracy for C: 0.769
Num non-zero elements: 5
Test accuracy for Python: 0.728
Num non-zero elements: 5
Test accuracy for HTML: 0.928
Num non-zero elements: 5
Test accuracy for Java: 0.733
Num non-zero elements: 5
Test accuracy for PHP: 0.68
Num non-zero elements: 10
Test accuracy for C: 0.81
Num non-zero elements: 10
Test accuracy for Python: 0.742
Num non-zero elements: 10
Test accuracy for HTML: 0.95
Num non-zero elements: 10
Test accuracy for Java: 0.784
Num non-zero elements: 10
Test accuracy for PHP: 0.729
Num non-zero elements: 20
Test accuracy for C: 0.859
Num non-zero elements: 20
Test accuracy for Python: 0.856
Num non-zero elements: 20
Test accuracy for HTML: 0.959
Num non-zero elements: 20
Test accuracy for Java: 0.841
Num non-zero elements: 20
Test accuracy for PHP: 0.837
Num non-zero elements: 50
Test accuracy for C: 0.912
Num non-zero elements: 50
Test accuracy for Python: 0.954
Num non-zero elements: 50
Test accuracy for HTML: 0.965
Num non-zero elements: 50
Test accuracy for Java: 0.903
Num non-zero elements: 50
Test accuracy for PHP: 0.903
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for C: 0.9610000252723694
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for Python: 0.984000027179718
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for HTML: 0.984000027179718
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 43 epochs
Test accuracy for Java: 0.9700000286102295
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for PHP: 0.9620000720024109
Num non-zero elements: 1
Test accuracy for C: 0.573
Num non-zero elements: 1
Test accuracy for Python: 0.551
Num non-zero elements: 1
Test accuracy for HTML: 0.863
Num non-zero elements: 1
Test accuracy for Java: 0.625
Num non-zero elements: 1
Test accuracy for PHP: 0.602
Num non-zero elements: 2
Test accuracy for C: 0.636
Num non-zero elements: 2
Test accuracy for Python: 0.665
Num non-zero elements: 2
Test accuracy for HTML: 0.878
Num non-zero elements: 2
Test accuracy for Java: 0.648
Num non-zero elements: 2
Test accuracy for PHP: 0.614
Num non-zero elements: 5
Test accuracy for C: 0.714
Num non-zero elements: 5
Test accuracy for Python: 0.692
Num non-zero elements: 5
Test accuracy for HTML: 0.924
Num non-zero elements: 5
Test accuracy for Java: 0.657
Num non-zero elements: 5
Test accuracy for PHP: 0.688
Num non-zero elements: 10
Test accuracy for C: 0.822
Num non-zero elements: 10
Test accuracy for Python: 0.726
Num non-zero elements: 10
Test accuracy for HTML: 0.949
Num non-zero elements: 10
Test accuracy for Java: 0.755
Num non-zero elements: 10
Test accuracy for PHP: 0.754
Num non-zero elements: 20
Test accuracy for C: 0.848
Num non-zero elements: 20
Test accuracy for Python: 0.769
Num non-zero elements: 20
Test accuracy for HTML: 0.962
Num non-zero elements: 20
Test accuracy for Java: 0.838
Num non-zero elements: 20
Test accuracy for PHP: 0.913
Num non-zero elements: 50
Test accuracy for C: 0.906
Num non-zero elements: 50
Test accuracy for Python: 0.859
Num non-zero elements: 50
Test accuracy for HTML: 0.962
Num non-zero elements: 50
Test accuracy for Java: 0.905
Num non-zero elements: 50
Test accuracy for PHP: 0.918
Num non-zero elements: 2304
Test accuracy for 0: 0.931
Num non-zero elements: 2304
Test accuracy for 1: 0.989
Num non-zero elements: 2304
Test accuracy for 2: 0.912
Num non-zero elements: 2304
Test accuracy for 3: 0.937
Num non-zero elements: 1
Test accuracy for 0: 0.578
Num non-zero elements: 1
Test accuracy for 1: 0.677
Num non-zero elements: 1
Test accuracy for 2: 0.678
Num non-zero elements: 1
Test accuracy for 3: 0.643
Num non-zero elements: 2
Test accuracy for 0: 0.795
Num non-zero elements: 2
Test accuracy for 1: 0.804
Num non-zero elements: 2
Test accuracy for 2: 0.696
Num non-zero elements: 2
Test accuracy for 3: 0.82
Num non-zero elements: 5
Test accuracy for 0: 0.829
Num non-zero elements: 5
Test accuracy for 1: 0.884
Num non-zero elements: 5
Test accuracy for 2: 0.763
Num non-zero elements: 5
Test accuracy for 3: 0.831
Num non-zero elements: 10
Test accuracy for 0: 0.872
Num non-zero elements: 10
Test accuracy for 1: 0.931
Num non-zero elements: 10
Test accuracy for 2: 0.796
Num non-zero elements: 10
Test accuracy for 3: 0.879
Num non-zero elements: 20
Test accuracy for 0: 0.886
Num non-zero elements: 20
Test accuracy for 1: 0.952
Num non-zero elements: 20
Test accuracy for 2: 0.85
Num non-zero elements: 20
Test accuracy for 3: 0.902
Num non-zero elements: 50
Test accuracy for 0: 0.909
Num non-zero elements: 50
Test accuracy for 1: 0.966
Num non-zero elements: 50
Test accuracy for 2: 0.891
Num non-zero elements: 50
Test accuracy for 3: 0.932
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 0: 0.9450000524520874
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 1: 0.9880000352859497
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 2: 0.9330000281333923
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 3: 0.9510000348091125
Num non-zero elements: 1
Test accuracy for 0: 0.798
Num non-zero elements: 1
Test accuracy for 1: 0.976
Num non-zero elements: 1
Test accuracy for 2: 0.804
Num non-zero elements: 1
Test accuracy for 3: 0.673
Num non-zero elements: 2
Test accuracy for 0: 0.881
Num non-zero elements: 2
Test accuracy for 1: 0.976
Num non-zero elements: 2
Test accuracy for 2: 0.809
Num non-zero elements: 2
Test accuracy for 3: 0.734
Num non-zero elements: 5
Test accuracy for 0: 0.894
Num non-zero elements: 5
Test accuracy for 1: 0.981
Num non-zero elements: 5
Test accuracy for 2: 0.871
Num non-zero elements: 5
Test accuracy for 3: 0.839
Num non-zero elements: 10
Test accuracy for 0: 0.895
Num non-zero elements: 10
Test accuracy for 1: 0.977
Num non-zero elements: 10
Test accuracy for 2: 0.881
Num non-zero elements: 10
Test accuracy for 3: 0.876
Num non-zero elements: 20
Test accuracy for 0: 0.91
Num non-zero elements: 20
Test accuracy for 1: 0.975
Num non-zero elements: 20
Test accuracy for 2: 0.886
Num non-zero elements: 20
Test accuracy for 3: 0.897
Num non-zero elements: 50
Test accuracy for 0: 0.913
Num non-zero elements: 50
Test accuracy for 1: 0.987
Num non-zero elements: 50
Test accuracy for 2: 0.886
Num non-zero elements: 50
Test accuracy for 3: 0.941
Loading dataset Helsinki-NLP/europarl, this usually takes ~10 seconds
Num non-zero elements: 2304
Test accuracy for en: 0.998
Num non-zero elements: 2304
Test accuracy for fr: 1.0
Num non-zero elements: 2304
Test accuracy for de: 1.0
Num non-zero elements: 2304
Test accuracy for es: 0.999
Num non-zero elements: 2304
Test accuracy for nl: 1.0
Num non-zero elements: 1
Test accuracy for en: 0.739
Num non-zero elements: 1
Test accuracy for fr: 0.595
Num non-zero elements: 1
Test accuracy for de: 0.758
Num non-zero elements: 1
Test accuracy for es: 0.482
Num non-zero elements: 1
Test accuracy for nl: 0.648
Num non-zero elements: 2
Test accuracy for en: 0.818
Num non-zero elements: 2
Test accuracy for fr: 0.802
Num non-zero elements: 2
Test accuracy for de: 0.836
Num non-zero elements: 2
Test accuracy for es: 0.907
Num non-zero elements: 2
Test accuracy for nl: 0.74
Num non-zero elements: 5
Test accuracy for en: 0.886
Num non-zero elements: 5
Test accuracy for fr: 0.897
Num non-zero elements: 5
Test accuracy for de: 0.85
Num non-zero elements: 5
Test accuracy for es: 0.986
Num non-zero elements: 5
Test accuracy for nl: 0.864
Num non-zero elements: 10
Test accuracy for en: 0.978
Num non-zero elements: 10
Test accuracy for fr: 0.97
Num non-zero elements: 10
Test accuracy for de: 0.968
Num non-zero elements: 10
Test accuracy for es: 0.987
Num non-zero elements: 10
Test accuracy for nl: 0.934
Num non-zero elements: 20
Test accuracy for en: 0.994
Num non-zero elements: 20
Test accuracy for fr: 0.992
Num non-zero elements: 20
Test accuracy for de: 0.985
Num non-zero elements: 20
Test accuracy for es: 0.997
Num non-zero elements: 20
Test accuracy for nl: 0.983
Num non-zero elements: 50
Test accuracy for en: 0.999
Num non-zero elements: 50
Test accuracy for fr: 0.998
Num non-zero elements: 50
Test accuracy for de: 0.997
Num non-zero elements: 50
Test accuracy for es: 0.998
Num non-zero elements: 50
Test accuracy for nl: 0.997
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for en: 0.9980000257492065
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for fr: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for de: 0.999000072479248
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for es: 0.9980000257492065
Num non-zero elements: 16384
Training probe with dim: 16384, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for nl: 0.999000072479248
Num non-zero elements: 1
Test accuracy for en: 0.999
Num non-zero elements: 1
Test accuracy for fr: 0.818
Num non-zero elements: 1
Test accuracy for de: 0.834
Num non-zero elements: 1
Test accuracy for es: 0.863
Num non-zero elements: 1
Test accuracy for nl: 0.753
Num non-zero elements: 2
Test accuracy for en: 1.0
Num non-zero elements: 2
Test accuracy for fr: 0.843
Num non-zero elements: 2
Test accuracy for de: 0.879
Num non-zero elements: 2
Test accuracy for es: 0.865
Num non-zero elements: 2
Test accuracy for nl: 0.848
Num non-zero elements: 5
Test accuracy for en: 0.999
Num non-zero elements: 5
Test accuracy for fr: 0.855
Num non-zero elements: 5
Test accuracy for de: 0.908
Num non-zero elements: 5
Test accuracy for es: 0.883
Num non-zero elements: 5
Test accuracy for nl: 0.854
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.869
Num non-zero elements: 10
Test accuracy for de: 0.963
Num non-zero elements: 10
Test accuracy for es: 0.889
Num non-zero elements: 10
Test accuracy for nl: 0.931
Num non-zero elements: 20
Test accuracy for en: 0.999
Num non-zero elements: 20
Test accuracy for fr: 0.911
Num non-zero elements: 20
Test accuracy for de: 0.967
Num non-zero elements: 20
Test accuracy for es: 0.949
Num non-zero elements: 20
Test accuracy for nl: 0.957
Num non-zero elements: 50
Test accuracy for en: 0.999
Num non-zero elements: 50
Test accuracy for fr: 0.965
Num non-zero elements: 50
Test accuracy for de: 0.986
Num non-zero elements: 50
Test accuracy for es: 0.989
Num non-zero elements: 50
Test accuracy for nl: 0.979
Time spent on sparse_probing: 1613.17s

[2025-02-06 19:14:20]  

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.001841 0.004826 0.027409 ... 0.017248 0.033188 0.002655]
(16384,)
Retain sparsity: [0.002255 0.00668  0.02557  ... 0.018389 0.045116 0.00262 ]
(16384,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [15494 12897  6853  4299  7792   735  7492 14419  6532 12036 15408 16303
  3314  5592  9963  5447 13558  5235 14021  4321  5102  5787 16312  9376
  4335  3109 14359  3158 11204 12264 11447  4582   788   976   888  4625
  1833  9704  1140  5829  9234 10138 11844  6346  4614  5742 12057  8940
  1647 11787  8948  2895 12208  5917 12650  8911   889 13492 14471 13944
  8097  4013  1398  6074 14189  3667 10259 15811  4566  4350 13088 10688
 15916  2945 10581 12598  9102  5585 10332  7225  1259  5403  2916  7987
  9364  2270  5145  9564  9060  7753 11197   666   223 12816  9594 13211
 13048 15790   195  1557 12827 15053 10532  4410   106   364 12108 14395
 14496 15306 15211  9511  9129 15923  9430 13450  2659 14885  7528   846
  3341  3036  7769  2705  1188 15688 10961  9863 16076 11982   932  2042
  9209  4821 16197 14796 11918 14192 15504  8671  6073  3233 11669  7385
  4789 15820  4991  7328 10449  9637  6300  1579 10744  6823  3028  3465
    84  2874  3772 13991 12260  1207 14532   638  4849  6399  7700 10024
 15816 15374 14707   711 15846 16304  8749  9689  5484  3873  4458  8631
  6066   456 14204  1462 12861  9000  6940 10277  1871 15322  2655 10807
  1119  1634 13476 12173  7141 12933 11432 13265  8680  8545 10083  6060
  3809  4488 16174 11625 14949  1500 11996  2709  4361 12205 13930 15509
 11529 10901  3707  5119  7907 13058 10289  8039  1360  2528  7609  6039
  5417   689  3380 13094  5620  2789  4874 11578  4125   168  8549 13463
 15436  2755  7988  3622   399 12747   263  7968    79  5993 11034 10302
  8327  8959  6662  1455  4595   987  8817  9475  1201  1975  4223  6079
  9230  1896  2132 11902  5079  1444 10455  4662  7309  5761  6159  5706
  4143  9175   639 12244  4178 11929  4855  1182 15735   883 12418 15577
 15928  7756   499  5749  5921 10895 15113  7556 11221 15955 11788  6264
 10993 12600  9537 11163 14505 11828  4445  6178  3760  9504 10681 10387
  7144  7249  6784  9237 15222 15814  1535  3907  5897  6061 11514  2046
  7455  6115  8821  5571 12610   588  3941 15741 11451  7918 13403  8571
  9635  6737 13685  2847 16176 11454  9201  1571 15808 13371 13513 10313
 10866  9212 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036]), array([15494, 12897,  6853,  4299,  7792,   735,  7492, 14419,  6532,
       12036, 15408, 16303,  3314,  5592,  9963,  5447, 13558,  5235,
       14021,  4321])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [12681 11085 10405 ... 10000  8633 15454]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343]), array([12681, 11085, 10405,  1790,  1675, 13468,  2499,  3141, 10203,
        2343, 14751,   276, 13183, 10253,  4801, 14016,    14,  8289,
        1329, 13470])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Attempt 1 failed: 504 Server Error: Gateway Time-out for url: https://huggingface.co/api/datasets/cais/mmlu/paths-info/c30699e8356da336a370243923dbaf21066bb9fe. Retrying in 20 seconds...
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0   1.000000                1.000000  ...          25              1.000000
1   0.833333                0.963636  ...         200              0.928105
2   0.973783                1.000000  ...         100              0.996732
3   0.790262                0.836364  ...         200              0.807190
4   0.795880                0.909091  ...         200              0.875817
5   0.990637                1.000000  ...          50              1.000000
6   0.992509                1.000000  ...         100              0.996732
7   0.966292                0.990909  ...         100              0.983660
8   0.998127                1.000000  ...          50              0.996732
9   0.996255                1.000000  ...          50              1.000000
10  1.000000                1.000000  ...          25              1.000000
11  0.958802                0.990909  ...         100              0.986928
12  1.000000                1.000000  ...          25              1.000000
13  1.000000                1.000000  ...          25              1.000000
14  0.971910                0.990909  ...         200              0.973856
15  0.996255                1.000000  ...          50              1.000000

[16 rows x 10 columns]
Currently on iteration 1 of run 6
Let me first add the Run 5 results to notes.txt, then proceed with Run 6 implementation.                                                                                 

notes.txt                                                                                                                                                                

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
 ## Run 4: Exponential Decay and Progressive Sparsity                                                                                                                    
 Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.                                          
                                                                                                                                                                         
 Key Implementation Details:                                                                                                                                             
 - Reverted base threshold to 0.1                                                                                                                                        
 - Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)                                                                                            
 - Implemented progressive sparsity scaling: scale = 1.0 + progress                                                                                                      
 - Applied sparsity scale to correlations before thresholding                                                                                                            
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                                      
                                                                                                                                                                         
 Results Analysis:                                                                                                                                                       
 1. Absorption Score: 0.03768 (target <0.01)                                                                                                                             
    - Slight regression from Run 3 (0.03592)                                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                                     
    - Suggests exponential decay may be too aggressive                                                                                                                   
                                                                                                                                                                         
 2. Sparse Probing: 0.7712 (target >0.74)                                                                                                                                
    - Slight regression from Run 3 (0.7735)                                                                                                                              
    - Still well above target threshold                                                                                                                                  
    - Maintains good feature separation                                                                                                                                  
                                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                                               
    - Improvement from Run 3 (0.0056)                                                                                                                                    
    - Still below baseline (0.0637)                                                                                                                                      
    - Progressive sparsity helps unlearning                                                                                                                              
                                                                                                                                                                         
 Core Metrics:                                                                                                                                                           
 - Reconstruction MSE: 1.609375 (stable)                                                                                                                                 
 - KL Divergence Score: 0.9876                                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                                    
                                                                                                                                                                         
 Observations:                                                                                                                                                           
 1. Exponential decay may be too aggressive early in training                                                                                                            
 2. Progressive sparsity helps with unlearning                                                                                                                           
 3. Feature separation remains above target                                                                                                                              
 4. Need better balance between early and late training phases                                                                                                           
                                                                                                                                                                         
 Next Steps:                                                                                                                                                             
 1. Reduce decay rate to 1.0 for gentler decay                                                                                                                           
 2. Add quadratic sparsity scaling                                                                                                                                       
 3. Increase base threshold to 0.15                                                                                                                                      
 =======                                                                                                                                                                 
 ## Run 4: Exponential Decay and Progressive Sparsity                                                                                                                    
 Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.                                          
                                                                                                                                                                         
 Key Implementation Details:                                                                                                                                             
 - Reverted base threshold to 0.1                                                                                                                                        
 - Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)                                                                                            
 - Implemented progressive sparsity scaling: scale = 1.0 + progress                                                                                                      
 - Applied sparsity scale to correlations before thresholding                                                                                                            
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                                      
                                                                                                                                                                         
 Results Analysis:                                                                                                                                                       
 1. Absorption Score: 0.03768 (target <0.01)                                                                                                                             
    - Slight regression from Run 3 (0.03592)                                                                                                                             
    - Higher than baseline (0.02959)                                                                                                                                     
    - Suggests exponential decay may be too aggressive                                                                                                                   
                                                                                                                                                                         
 2. Sparse Probing: 0.7712 (target >0.74)                                                                                                                                
    - Slight regression from Run 3 (0.7735)                                                                                                                              
    - Still well above target threshold                                                                                                                                  
    - Maintains good feature separation                                                                                                                                  
                                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                                               
    - Improvement from Run 3 (0.0056)                                                                                                                                    
    - Still below baseline (0.0637)                                                                                                                                      
    - Progressive sparsity helps unlearning                                                                                                                              
                                                                                                                                                                         
 Core Metrics:                                                                                                                                                           
 - Reconstruction MSE: 1.609375 (stable)                                                                                                                                 
 - KL Divergence Score: 0.9876                                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                                    
                                                                                                                                                                         
 Observations:                                                                                                                                                           
 1. Exponential decay may be too aggressive early in training                                                                                                            
 2. Progressive sparsity helps with unlearning                                                                                                                           
 3. Feature separation remains above target                                                                                                                              
 4. Need better balance between early and late training phases                                                                                                           
                                                                                                                                                                         
 Next Steps:                                                                                                                                                             
 1. Reduce decay rate to 1.0 for gentler decay                                                                                                                           
 2. Add quadratic sparsity scaling                                                                                                                                       
 3. Increase base threshold to 0.15                                                                                                                                      
                                                                                                                                                                         
 ## Run 5: Balanced Progressive Training                                                                                                                                 
 Description: Implemented gentler decay rate and quadratic sparsity scaling with moderate base threshold.                                                                
                                                                                                                                                                         
 Key Implementation Details:                                                                                                                                             
 - Increased base threshold to 0.15 (middle ground)                                                                                                                      
 - Reduced decay rate from 2.0 to 1.0 for gentler decay                                                                                                                  
 - Changed sparsity scaling to quadratic: scale = 1.0 + progress^2                                                                                                       
 - Maintained reduced orthogonality weight (λ=0.05)                                                                                                                      
 - Kept exponential threshold decay mechanism                                                                                                                            
                                                                                                                                                                         
 Results Analysis:                                                                                                                                                       
 1. Absorption Score: 0.03744 (target <0.01)                                                                                                                             
    - Similar to Run 4 (0.03768)                                                                                                                                         
    - Still higher than baseline (0.02959)                                                                                                                               
    - Changes show minimal impact on absorption                                                                                                                          
                                                                                                                                                                         
 2. Sparse Probing: 0.7698 (target >0.74)                                                                                                                                
    - Slight regression from Run 4 (0.7712)                                                                                                                              
    - Maintains strong feature separation                                                                                                                                
    - Consistent across different datasets                                                                                                                               
                                                                                                                                                                         
 3. Unlearning Score: 0.0262 (target >0.1)                                                                                                                               
    - Same as Run 4 (0.0262)                                                                                                                                             
    - Still below baseline (0.0637)                                                                                                                                      
    - No improvement in unlearning capability                                                                                                                            
                                                                                                                                                                         
 Core Metrics:                                                                                                                                                           
 - Reconstruction MSE: 1.609375 (unchanged)                                                                                                                              
 - KL Divergence Score: 0.9875                                                                                                                                           
 - L0 Sparsity: 320.0                                                                                                                                                    
                                                                                                                                                                         
 Observations:                                                                                                                                                           
 1. Gentler decay rate didn't improve performance                                                                                                                        
 2. Quadratic sparsity scaling shows minimal impact                                                                                                                      
 3. Feature separation remains robust                                                                                                                                    
 4. Need more fundamental changes to architecture                                                                                                                        
                                                                                                                                                                         
 Next Steps:                                                                                                                                                             
 1. Add adaptive competition threshold based on feature statistics                                                                                                       
 2. Implement feature-wise scaling                                                                                                                                       
 3. Consider multi-stage training approach                                                                                                                               
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

For Run 6, let's implement adaptive competition thresholds based on feature statistics. Here are the changes:                                                            

experiment.py                                                                                                                                                            

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                                   
         base_threshold=0.15,  # moderate threshold                                                                                                                      
         decay_rate=1.0,     # gentler decay rate                                                                                                                        
 =======                                                                                                                                                                 
         ortho_lambda=0.05,  # maintain reduced weight                                                                                                                   
         base_threshold=0.15,  # keep moderate threshold                                                                                                                 
         decay_rate=1.0,     # keep gentle decay                                                                                                                         
         feature_adapt_rate=0.1,  # feature adaptation rate                                                                                                              
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale"]                                            
 =======                                                                                                                                                                 
         self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale", "feature_scales"]                          
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
         self.threshold = 0.0                                                                                                                                            
         self.sparsity_scale = 1.0                                                                                                                                       
         self.ortho_lambda = ortho_lambda                                                                                                                                
         self.base_threshold = base_threshold                                                                                                                            
         self.decay_rate = decay_rate                                                                                                                                    
 =======                                                                                                                                                                 
         self.threshold = 0.0                                                                                                                                            
         self.sparsity_scale = 1.0                                                                                                                                       
         self.feature_scales = torch.ones(dict_size, device=device)                                                                                                      
         self.ortho_lambda = ortho_lambda                                                                                                                                
         self.base_threshold = base_threshold                                                                                                                            
         self.decay_rate = decay_rate                                                                                                                                    
         self.feature_adapt_rate = feature_adapt_rate                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

                                                                                                                                                                         
 <<<<<<< SEARCH                                                                                                                                                          
     def compute_orthogonality_loss(self, f):                                                                                                                            
         # Normalize features                                                                                                                                            
         f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)                                                                                                        
                                                                                                                                                                         
         # Compute correlations                                                                                                                                          
         correlations = torch.abs(torch.mm(f_norm.T, f_norm))                                                                                                            
                                                                                                                                                                         
         # Zero out diagonal                                                                                                                                             
         mask = 1 - torch.eye(correlations.shape[0], device=f.device)                                                                                                    
         correlations = correlations * mask                                                                                                                              
                                                                                                                                                                         
         # Get current threshold and sparsity scale                                                                                                                      
         threshold = self.compute_threshold(self.progress)                                                                                                               
         self.threshold = threshold                                                                                                                                      
                                                                                                                                                                         
         sparsity_scale = self.compute_sparsity_scale(self.progress)                                                                                                     
         self.sparsity_scale = sparsity_scale                                                                                                                            
                                                                                                                                                                         
         # Compute competition coefficients with sparsity scaling                                                                                                        
         coefficients = self.compute_competition_coefficient(correlations * sparsity_scale, threshold)                                                                   
 =======                                                                                                                                                                 
     def update_feature_scales(self, f):                                                                                                                                 
         """Update feature-wise scaling factors based on activation statistics"""                                                                                        
         with torch.no_grad():                                                                                                                                           
             # Compute feature activation statistics                                                                                                                     
             feature_norms = torch.norm(f, dim=0)                                                                                                                        
             feature_sparsity = (f != 0).float().mean(0)                                                                                                                 
                                                                                                                                                                         
             # Compute adaptive scales based on both norm and sparsity                                                                                                   
             target_scales = 1.0 / (feature_norms * feature_sparsity + 1e-8)                                                                                             
             target_scales = target_scales / target_scales.mean()                                                                                                        
                                                                                                                                                                         
             # Smooth update                                                                                                                                             
             self.feature_scales = (1 - self.feature_adapt_rate) * self.feature_scales + \                                                                               
                                 self.feature_adapt_rate * target_scales                                                                                                 
                                                                                                                                                                         
     def compute_orthogonality_loss(self, f):                                                                                                                            
         # Update feature-wise scaling factors                                                                                                                           
         self.update_feature_scales(f)                                                                                                                                   
                                                                                                                                                                         
         # Apply feature-wise scaling                                                                                                                                    
         f_scaled = f * self.feature_scales.unsqueeze(0)                                                                                                                 
                                                                                                                                                                         
         # Normalize features                                                                                                                                            
         f_norm = f_scaled / (torch.norm(f_scaled, dim=0, keepdim=True) + 1e-8)                                                                                          
                                                                                                                                                                         
         # Compute correlations                                                                                                                                          
         correlations = torch.abs(torch.mm(f_norm.T, f_norm))                                                                                                            
                                                                                                                                                                         
         # Zero out diagonal                                                                                                                                             
         mask = 1 - torch.eye(correlations.shape[0], device=f.device)                                                                                                    
         correlations = correlations * mask                                                                                                                              
                                                                                                                                                                         
         # Get current threshold and sparsity scale                                                                                                                      
         threshold = self.compute_threshold(self.progress)                                                                                                               
         self.threshold = threshold                                                                                                                                      
                                                                                                                                                                         
         sparsity_scale = self.compute_sparsity_scale(self.progress)                                                                                                     
         self.sparsity_scale = sparsity_scale                                                                                                                            
                                                                                                                                                                         
         # Compute competition coefficients with adaptive scaling                                                                                                        
         coefficients = self.compute_competition_coefficient(                                                                                                            
             correlations * sparsity_scale * self.feature_scales.unsqueeze(0),                                                                                           
             threshold                                                                                                                                                   
         )                                                                                                                                                               
 >>>>>>> REPLACE                                                                                                                                                         
                                                                                                                                                                         

These changes:                                                                                                                                                           

 1 Add feature-wise adaptive scaling based on activation statistics                                                                                                      
 2 Consider both feature norms and sparsity patterns                                                                                                                     
 3 Implement smooth updates for feature scales                                                                                                                           
 4 Apply adaptive scaling in orthogonality computation                                                                                                                   
 5 Track feature scales in logging parameters                                                                                                                            

The key differences:                                                                                                                                                     

 • Dynamic per-feature scaling factors                                                                                                                                   
 • Adaptation based on both magnitude and sparsity                                                                                                                       
 • Smooth updates to maintain stability                                                                                                                                  
 • Integration with existing progressive mechanisms                                                                                                                      

Let's run this with:                                                                                                                                                     

                                                                                                                                                                         
 python experiment.py --out_dir=run_6                                                                                                                                    
                                                                                                                                                                         

This should provide better control over feature competition based on actual usage patterns.                                                                              

Tokens: 93k sent, 2.7k received. Cost: $0.32 message, $1.39 session.
Applied edit to notes.txt
Applied edit to experiment.py

python experiment.py --out_dir=run_6
coder_out: Let me first add the Run 5 results to notes.txt, then proceed with Run 6 implementation.

notes.txt
```python
<<<<<<< SEARCH
## Run 4: Exponential Decay and Progressive Sparsity
Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.

Key Implementation Details:
- Reverted base threshold to 0.1
- Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)
- Implemented progressive sparsity scaling: scale = 1.0 + progress
- Applied sparsity scale to correlations before thresholding
- Maintained reduced orthogonality weight (λ=0.05)

Results Analysis:
1. Absorption Score: 0.03768 (target <0.01)
   - Slight regression from Run 3 (0.03592)
   - Higher than baseline (0.02959)
   - Suggests exponential decay may be too aggressive

2. Sparse Probing: 0.7712 (target >0.74)
   - Slight regression from Run 3 (0.7735)
   - Still well above target threshold
   - Maintains good feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Improvement from Run 3 (0.0056)
   - Still below baseline (0.0637)
   - Progressive sparsity helps unlearning

Core Metrics:
- Reconstruction MSE: 1.609375 (stable)
- KL Divergence Score: 0.9876
- L0 Sparsity: 320.0

Observations:
1. Exponential decay may be too aggressive early in training
2. Progressive sparsity helps with unlearning
3. Feature separation remains above target
4. Need better balance between early and late training phases

Next Steps:
1. Reduce decay rate to 1.0 for gentler decay
2. Add quadratic sparsity scaling
3. Increase base threshold to 0.15
=======
## Run 4: Exponential Decay and Progressive Sparsity
Description: Implemented exponential threshold decay and progressive sparsity scaling while reverting to lower base threshold.

Key Implementation Details:
- Reverted base threshold to 0.1
- Added exponential decay: threshold = base_threshold * exp(-2.0 * progress)
- Implemented progressive sparsity scaling: scale = 1.0 + progress
- Applied sparsity scale to correlations before thresholding
- Maintained reduced orthogonality weight (λ=0.05)

Results Analysis:
1. Absorption Score: 0.03768 (target <0.01)
   - Slight regression from Run 3 (0.03592)
   - Higher than baseline (0.02959)
   - Suggests exponential decay may be too aggressive

2. Sparse Probing: 0.7712 (target >0.74)
   - Slight regression from Run 3 (0.7735)
   - Still well above target threshold
   - Maintains good feature separation

3. Unlearning Score: 0.0262 (target >0.1)
   - Improvement from Run 3 (0.0056)
   - Still below baseline (0.0637)
   - Progressive sparsity helps unlearning

Core Metrics:
- Reconstruction MSE: 1.609375 (stable)
- KL Divergence Score: 0.9876
- L0 Sparsity: 320.0

Observations:
1. Exponential decay may be too aggressive early in training
2. Progressive sparsity helps with unlearning
3. Feature separation remains above target
4. Need better balance between early and late training phases

Next Steps:
1. Reduce decay rate to 1.0 for gentler decay
2. Add quadratic sparsity scaling
3. Increase base threshold to 0.15

## Run 5: Balanced Progressive Training
Description: Implemented gentler decay rate and quadratic sparsity scaling with moderate base threshold.

Key Implementation Details:
- Increased base threshold to 0.15 (middle ground)
- Reduced decay rate from 2.0 to 1.0 for gentler decay
- Changed sparsity scaling to quadratic: scale = 1.0 + progress^2
- Maintained reduced orthogonality weight (λ=0.05)
- Kept exponential threshold decay mechanism

Results Analysis:
1. Absorption Score: 0.03744 (target <0.01)
   - Similar to Run 4 (0.03768)
   - Still higher than baseline (0.02959)
   - Changes show minimal impact on absorption

2. Sparse Probing: 0.7698 (target >0.74)
   - Slight regression from Run 4 (0.7712)
   - Maintains strong feature separation
   - Consistent across different datasets

3. Unlearning Score: 0.0262 (target >0.1)
   - Same as Run 4 (0.0262)
   - Still below baseline (0.0637)
   - No improvement in unlearning capability

Core Metrics:
- Reconstruction MSE: 1.609375 (unchanged)
- KL Divergence Score: 0.9875
- L0 Sparsity: 320.0

Observations:
1. Gentler decay rate didn't improve performance
2. Quadratic sparsity scaling shows minimal impact
3. Feature separation remains robust
4. Need more fundamental changes to architecture

Next Steps:
1. Add adaptive competition threshold based on feature statistics
2. Implement feature-wise scaling
3. Consider multi-stage training approach
>>>>>>> REPLACE
```

For Run 6, let's implement adaptive competition thresholds based on feature statistics. Here are the changes:

experiment.py
```python
<<<<<<< SEARCH
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.15,  # moderate threshold
        decay_rate=1.0,     # gentler decay rate
=======
        ortho_lambda=0.05,  # maintain reduced weight
        base_threshold=0.15,  # keep moderate threshold
        decay_rate=1.0,     # keep gentle decay
        feature_adapt_rate=0.1,  # feature adaptation rate
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale"]
=======
        self.logging_parameters = ["effective_l0", "dead_features", "progress", "ortho_loss", "threshold", "sparsity_scale", "feature_scales"]
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
        self.threshold = 0.0
        self.sparsity_scale = 1.0
        self.ortho_lambda = ortho_lambda
        self.base_threshold = base_threshold
        self.decay_rate = decay_rate
=======
        self.threshold = 0.0
        self.sparsity_scale = 1.0
        self.feature_scales = torch.ones(dict_size, device=device)
        self.ortho_lambda = ortho_lambda
        self.base_threshold = base_threshold
        self.decay_rate = decay_rate
        self.feature_adapt_rate = feature_adapt_rate
>>>>>>> REPLACE
```

```python
<<<<<<< SEARCH
    def compute_orthogonality_loss(self, f):
        # Normalize features
        f_norm = f / (torch.norm(f, dim=0, keepdim=True) + 1e-8)
        
        # Compute correlations
        correlations = torch.abs(torch.mm(f_norm.T, f_norm))
        
        # Zero out diagonal
        mask = 1 - torch.eye(correlations.shape[0], device=f.device)
        correlations = correlations * mask
        
        # Get current threshold and sparsity scale
        threshold = self.compute_threshold(self.progress)
        self.threshold = threshold
        
        sparsity_scale = self.compute_sparsity_scale(self.progress)
        self.sparsity_scale = sparsity_scale
        
        # Compute competition coefficients with sparsity scaling
        coefficients = self.compute_competition_coefficient(correlations * sparsity_scale, threshold)
=======
    def update_feature_scales(self, f):
        """Update feature-wise scaling factors based on activation statistics"""
        with torch.no_grad():
            # Compute feature activation statistics
            feature_norms = torch.norm(f, dim=0)
            feature_sparsity = (f != 0).float().mean(0)
            
            # Compute adaptive scales based on both norm and sparsity
            target_scales = 1.0 / (feature_norms * feature_sparsity + 1e-8)
            target_scales = target_scales / target_scales.mean()
            
            # Smooth update
            self.feature_scales = (1 - self.feature_adapt_rate) * self.feature_scales + \
                                self.feature_adapt_rate * target_scales

    def compute_orthogonality_loss(self, f):
        # Update feature-wise scaling factors
        self.update_feature_scales(f)
        
        # Apply feature-wise scaling
        f_scaled = f * self.feature_scales.unsqueeze(0)
        
        # Normalize features
        f_norm = f_scaled / (torch.norm(f_scaled, dim=0, keepdim=True) + 1e-8)
        
        # Compute correlations
        correlations = torch.abs(torch.mm(f_norm.T, f_norm))
        
        # Zero out diagonal
        mask = 1 - torch.eye(correlations.shape[0], device=f.device)
        correlations = correlations * mask
        
        # Get current threshold and sparsity scale
        threshold = self.compute_threshold(self.progress)
        self.threshold = threshold
        
        sparsity_scale = self.compute_sparsity_scale(self.progress)
        self.sparsity_scale = sparsity_scale
        
        # Compute competition coefficients with adaptive scaling
        coefficients = self.compute_competition_coefficient(
            correlations * sparsity_scale * self.feature_scales.unsqueeze(0),
            threshold
        )
>>>>>>> REPLACE
```

These changes:
1. Add feature-wise adaptive scaling based on activation statistics
2. Consider both feature norms and sparsity patterns
3. Implement smooth updates for feature scales
4. Apply adaptive scaling in orthogonality computation
5. Track feature scales in logging parameters

The key differences:
- Dynamic per-feature scaling factors
- Adaptation based on both magnitude and sparsity
- Smooth updates to maintain stability
- Integration with existing progressive mechanisms

Let's run this with:

```bash
python experiment.py --out_dir=run_6
```

This should provide better control over feature competition based on actual usage patterns., type: <class 'str'>
Step 0: 31236.6171875
Step 100: 4580.24267578125
Step 200: 4064.1806640625
Step 300: 3816.154296875
Step 400: 3594.33251953125
Step 500: 3452.787841796875
Step 600: 3420.01318359375
Step 700: 3241.3388671875
Step 800: 3206.17822265625
Step 900: 3107.1416015625
Step 1000: 3190.019287109375
Step 1100: 3110.19677734375
Step 1200: 3011.27001953125
Step 1300: 3051.8779296875
Step 1400: 3053.55517578125
Step 1500: 3063.359130859375
Step 1600: 2998.14404296875
Step 1700: 3027.41748046875
Step 1800: 2994.435546875
Step 1900: 2991.947998046875
Step 2000: 2997.721435546875
Step 2100: 3013.06787109375
Step 2200: 2906.814453125
Step 2300: 2895.33984375
Step 2400: 2874.520263671875

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/orthogonal/20250206_151232_adaptive_sparse_orthogonal_sae/run_6/final_info.json

[2025-02-06 19:35:09]  

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading probe and training data
Training k-sparse probes
Loading validation data
Evaluating raw k-sparse probing scores
