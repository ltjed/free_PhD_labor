Step 0: 34179.484375
Step 100: 5619.9189453125
Step 200: 4209.8505859375
Step 300: 3805.7236328125
Step 400: 3532.6767578125
Step 500: 3362.6005859375
Step 600: 3329.419921875
Step 700: 3140.32666015625
Step 800: 3098.66748046875
Step 900: 2999.3408203125
Step 1000: 3016.58984375
Step 1100: 2920.307373046875
Step 1200: 2832.3388671875
Step 1300: 2825.6494140625
Step 1400: 2801.23291015625
Step 1500: 2803.621337890625
Step 1600: 2731.13427734375
Step 1700: 2729.80224609375
Step 1800: 2658.783935546875
Step 1900: 2634.8359375
Step 2000: 2624.325927734375
Step 2100: 2656.170166015625
Step 2200: 2558.855712890625
Step 2300: 2514.558837890625
Step 2400: 2494.544189453125
Step 2500: 2471.1025390625
Step 2600: 2549.85400390625
Step 2700: 2544.625
Step 2800: 2525.760986328125
Step 2900: 2483.77783203125
Step 3000: 2405.17236328125
Step 3100: 2447.744140625
Step 3200: 2442.62744140625
Step 3300: 2465.121337890625
Step 3400: 2389.19580078125
Step 3500: 2379.29541015625
Step 3600: 2405.6650390625
Step 3700: 2435.85888671875
Step 3800: 2401.037841796875
Step 3900: 2358.02587890625
Step 4000: 2384.60205078125
Step 4100: 2368.970947265625
Step 4200: 2357.166015625
Step 4300: 2372.5458984375
Step 4400: 2318.302490234375
Step 4500: 2315.694091796875
Step 4600: 2314.056640625
Step 4700: 2304.47509765625
Step 4800: 2323.94873046875
Step 4900: 2309.01220703125
Step 5000: 2268.7978515625
Step 5100: 2316.0869140625
Step 5200: 2322.062255859375
Step 5300: 2312.13916015625
Step 5400: 2285.52880859375
Step 5500: 2290.33154296875
Step 5600: 2238.473876953125
Step 5700: 2288.243896484375
Step 5800: 2252.950927734375
Step 5900: 2279.1474609375
Step 6000: 2291.39599609375
Step 6100: 2278.93408203125
Step 6200: 2257.869384765625
Step 6300: 2251.421875
Step 6400: 2226.724365234375
Step 6500: 2187.5771484375
Step 6600: 2244.947509765625
Step 6700: 2217.171142578125
Step 6800: 2163.3349609375
Step 6900: 2191.00830078125
Step 7000: 2186.972412109375
Step 7100: 2225.977294921875
Step 7200: 2193.47216796875
Step 7300: 2198.420166015625
Step 7400: 2235.6181640625
Step 7500: 2233.31298828125
Step 7600: 2172.966796875
Step 7700: 2163.3125
Step 7800: 2215.368408203125
Step 7900: 2165.919921875
Step 8000: 2173.90087890625
Step 8100: 2114.8046875
Step 8200: 2182.755859375
Step 8300: 2180.517333984375
Step 8400: 2135.9736328125
Step 8500: 2117.759521484375
Step 8600: 2127.5205078125
Step 8700: 2158.78955078125
Step 8800: 2110.90673828125
Step 8900: 2150.58935546875
Step 9000: 2127.0732421875
Step 9100: 2106.10595703125
Step 9200: 2131.12255859375
Step 9300: 2123.24658203125
Step 9400: 2098.824951171875
Step 9500: 2075.89013671875
Step 9600: 2089.07958984375
Step 9700: 2131.02880859375
Step 9800: 2069.69189453125
Step 9900: 2062.411376953125
Step 10000: 2093.6669921875
Step 10100: 2031.370849609375
Step 10200: 2050.41259765625
Step 10300: 2060.757080078125
Step 10400: 2104.92138671875
Step 10500: 2059.67626953125
Step 10600: 2032.57763671875
Step 10700: 2066.51611328125
Step 10800: 1992.707763671875
Step 10900: 2008.51220703125
Step 11000: 2041.60498046875
Step 11100: 1998.2784423828125
Step 11200: 2032.624755859375
Step 11300: 1991.258056640625
Step 11400: 2027.71044921875
Step 11500: 2022.081787109375
Step 11600: 2038.494873046875
Step 11700: 2009.5196533203125
Step 11800: 1961.53125
Step 11900: 2036.0001220703125
Step 12000: 1987.718994140625
Step 12100: 1951.697509765625
Step 12200: 1990.5169677734375
Step 12300: 1956.749755859375
Step 12400: 1965.9063720703125
Step 12500: 1960.1533203125
Step 12600: 1957.97705078125
Step 12700: 1927.7877197265625
Step 12800: 1958.58154296875
Step 12900: 1919.116455078125
Step 13000: 1974.102294921875
Step 13100: 1930.8701171875
Step 13200: 1950.965087890625
Step 13300: 1884.23876953125
Step 13400: 1917.632080078125
Step 13500: 1920.732666015625
Step 13600: 1890.9228515625
Step 13700: 1950.09521484375
Step 13800: 1888.416015625
Step 13900: 1887.229736328125
Step 14000: 1928.6229248046875
Step 14100: 1865.918212890625
Step 14200: 1917.1513671875
Step 14300: 1883.3616943359375
Step 14400: 1905.5506591796875
Step 14500: 1876.230712890625
Step 14600: 1844.2490234375
Step 14700: 1917.031982421875
Step 14800: 1848.018798828125
Step 14900: 1848.525634765625
Step 15000: 1876.814453125
Step 15100: 1874.94677734375
Step 15200: 1807.29541015625
Step 15300: 1834.4423828125
Step 15400: 1855.01708984375
Step 15500: 1846.749267578125
Step 15600: 1868.347900390625
Step 15700: 1866.495361328125
Step 15800: 1858.92236328125
Step 15900: 1836.628662109375
Step 16000: 1866.039306640625
Step 16100: 1859.05517578125
Step 16200: 1825.74462890625
Step 16300: 1838.145751953125
Step 16400: 1924.98828125
Step 16500: 1818.2216796875
Step 16600: 1877.052734375
Step 16700: 1808.5611572265625
Step 16800: 1832.7052001953125
Step 16900: 1832.617431640625
Step 17000: 1856.1865234375
Step 17100: 1834.2757568359375
Step 17200: 1816.6785888671875
Step 17300: 1826.7021484375
Step 17400: 1781.1009521484375
Step 17500: 1833.57177734375
Step 17600: 1829.291748046875
Step 17700: 1805.99609375
Step 17800: 1812.3544921875
Step 17900: 1806.5006103515625
Step 18000: 1788.1649169921875
Step 18100: 1768.8798828125
Step 18200: 1771.251708984375
Step 18300: 1785.855224609375
Step 18400: 1816.020751953125
Step 18500: 1751.1444091796875
Step 18600: 1839.238037109375
Step 18700: 1808.750244140625
Step 18800: 1772.22802734375
Step 18900: 1744.39208984375
Step 19000: 1772.92431640625
Step 19100: 1777.1475830078125
Step 19200: 1814.1627197265625
Step 19300: 1710.7894287109375
Step 19400: 1768.47607421875
Step 19500: 1819.7816162109375
Step 19600: 1761.1920166015625
Step 19700: 1776.423095703125
Step 19800: 1771.14111328125
Step 19900: 1738.95361328125
Step 20000: 1739.4576416015625
Step 20100: 1740.120361328125
Step 20200: 1798.876708984375
Step 20300: 1721.1685791015625
Step 20400: 1778.89111328125
Step 20500: 1756.890380859375
Step 20600: 1776.4586181640625
Step 20700: 1744.721435546875
Step 20800: 1735.1448974609375
Step 20900: 1713.3748779296875
Step 21000: 1773.5489501953125
Step 21100: 1764.574951171875
Step 21200: 1753.2757568359375
Step 21300: 1745.4312744140625
Step 21400: 1729.3599853515625
Step 21500: 1752.6611328125
Step 21600: 1738.296875
Step 21700: 1728.2490234375
Step 21800: 1692.62841796875
Step 21900: 1696.638671875
Step 22000: 1750.698486328125
Step 22100: 1728.492919921875
Step 22200: 1713.093017578125
Step 22300: 1726.710693359375
Step 22400: 1724.49072265625
Step 22500: 1712.372314453125
Step 22600: 1718.780517578125
Step 22700: 1691.0679931640625
Step 22800: 1705.543701171875
Step 22900: 1737.4638671875
Step 23000: 1682.27392578125
Step 23100: 1685.5439453125
Step 23200: 1735.728271484375
Step 23300: 1689.239013671875
Step 23400: 1722.593994140625
Step 23500: 1720.6392822265625
Step 23600: 1735.94189453125
Step 23700: 1738.96875
Step 23800: 1704.09375
Step 23900: 1700.628662109375
Step 24000: 1713.178466796875
Step 24100: 1695.8125
Step 24200: 1729.0308837890625
Step 24300: 1725.445556640625
Step 24400: 1701.6485595703125
Step 24500: 1701.178466796875
Step 24600: 1711.80712890625
Step 24700: 1704.87841796875
Step 24800: 1680.9998779296875
Step 24900: 1706.6510009765625
Step 25000: 1677.849853515625
Step 25100: 1693.77978515625
Step 25200: 1692.3961181640625
Step 25300: 1710.916259765625
Step 25400: 1693.631103515625
Step 25500: 1705.752685546875
Step 25600: 1642.31201171875
Step 25700: 1678.2080078125
Step 25800: 1677.8812255859375
Step 25900: 1685.732177734375
Step 26000: 1701.274169921875
Step 26100: 1666.56103515625
Step 26200: 1700.0897216796875
Step 26300: 1645.313232421875
Step 26400: 1726.865966796875
Step 26500: 1701.159423828125
Step 26600: 1693.122802734375
Step 26700: 1679.504638671875
Step 26800: 1683.01171875
Step 26900: 1675.98291015625
Step 27000: 1638.220703125
Step 27100: 1679.177734375
Step 27200: 1723.19482421875
Step 27300: 1658.1015625
Step 27400: 1647.864013671875
Step 27500: 1706.125
Step 27600: 1641.7135009765625
Step 27700: 1664.914794921875
Step 27800: 1690.110595703125
Step 27900: 1656.521728515625
Step 28000: 1672.639892578125
Step 28100: 1663.63916015625
Step 28200: 1677.02685546875
Step 28300: 1661.3123779296875
Step 28400: 1643.20654296875
Step 28500: 1633.4088134765625
Step 28600: 1631.5848388671875
Step 28700: 1654.5537109375
Step 28800: 1669.32763671875
Step 28900: 1634.709228515625
Step 29000: 1652.970947265625
Step 29100: 1656.9490966796875
Step 29200: 1672.354248046875

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/templates/sae_variants/run_0/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_0/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.940000057220459, scr_score: 0.45312510186348476
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9450000524520874, scr_score: 0.5312500873115584
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9440000653266907, scr_score: 0.5156252764866015
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9450000524520874, scr_score: 0.5312500873115584
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9460000395774841, scr_score: 0.5468748981365152
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9420000314712524, scr_score: 0.4843747235133985
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.921000063419342, scr_score: 0.1562499708961472
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5910000205039978, scr_score: 0.019656056358159712
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5950000286102295, scr_score: 0.02948408453723957
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.597000002861023, scr_score: 0.03439802540241183
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6040000319480896, scr_score: 0.05159711132798542
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6150000095367432, scr_score: 0.07862407898390353
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6370000243186951, scr_score: 0.13267816074447508
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5870000123977661, scr_score: 0.009828028179079856
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8880000710487366, scr_score: 0.3495147428927238
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8420000672340393, scr_score: -0.09708729998443334
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8390000462532043, scr_score: -0.12621372145414522
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.843000054359436, scr_score: -0.08737868572318094
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8490000367164612, scr_score: -0.029126421469711866
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8400000333786011, scr_score: -0.11650510719289281
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8420000672340393, scr_score: -0.09708729998443334
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6670000553131104, scr_score: 0.07736393061404061
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6980000138282776, scr_score: 0.16618903247138295
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7230000495910645, scr_score: 0.23782237746377266
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7420000433921814, scr_score: 0.29226362401729183
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7800000309944153, scr_score: 0.4011461171243302
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7970000505447388, scr_score: 0.44985677805619845
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.7980000376701355, scr_score: 0.45272207086702393
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9430000185966492, scr_score: 0.5238092084411499
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9440000653266907, scr_score: 0.5396829601737049
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9430000185966492, scr_score: 0.5238092084411499
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9390000700950623, scr_score: 0.4603179859314168
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9310000538825989, scr_score: 0.33333364870170723
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9360000491142273, scr_score: 0.4126986229439953
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9040000438690186, scr_score: -0.09523777986972133
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.5990000367164612, scr_score: 0.02784819256855401
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6060000061988831, scr_score: 0.04556963247779803
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6210000514984131, scr_score: 0.08354442680784264
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6220000386238098, scr_score: 0.08607603952376044
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6510000228881836, scr_score: 0.1594937136722931
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.5879999995231628, scr_score: 0.0
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.5840000510215759, scr_score: -0.010126450863671215
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.89000004529953, scr_score: 0.3828126309672156
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8660000562667847, scr_score: 0.19531274738251833
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8520000576972961, scr_score: 0.08593777648634401
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8540000319480896, scr_score: 0.10156257275956422
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8100000619888306, scr_score: -0.2421871362021789
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8580000400543213, scr_score: 0.1328126309672156
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.9050000309944153, scr_score: 0.5
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7000000476837158, scr_score: 0.12759656040532572
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.737000048160553, scr_score: 0.23738884264763468
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7540000081062317, scr_score: 0.2878338257938472
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7350000143051147, scr_score: 0.23145402430383724
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7330000400543213, scr_score: 0.22551938282841538
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7480000257492065, scr_score: 0.27002972449920604
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6320000290870667, scr_score: -0.07418390278465108
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8100000619888306, scr_score: 0.016393554752069144
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.812000036239624, scr_score: 0.027322374114521025
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8160000443458557, scr_score: 0.04918033854781611
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8140000104904175, scr_score: 0.03825119347697291
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8060000538825989, scr_score: -0.00546440968122594
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8040000200271606, scr_score: -0.016393554752069144
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7970000505447388, scr_score: -0.05464474822904205
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7800000309944153, scr_score: 0.35686274876460633
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8050000667572021, scr_score: 0.45490208361431267
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.815000057220459, scr_score: 0.4941177240567333
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8330000638961792, scr_score: 0.5647059703505524
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8550000190734863, scr_score: 0.6509802858264159
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8750000596046448, scr_score: 0.7294118004549119
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.8920000195503235, scr_score: 0.7960782957095651
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.7970000505447388, scr_score: 0.07692300638503319
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8080000281333923, scr_score: 0.1333331499344196
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8160000443458557, scr_score: 0.17435887560571312
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8290000557899475, scr_score: 0.24102560340535104
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8420000672340393, scr_score: 0.30769233120498896
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8480000495910645, scr_score: 0.338461472626031
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8710000514984131, scr_score: 0.45641020468235766
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7220000624656677, scr_score: 0.024193715075407077
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7350000143051147, scr_score: 0.07661287027579163
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7400000095367432, scr_score: 0.09677413927777444
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7510000467300415, scr_score: 0.14112912335516434
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7690000534057617, scr_score: 0.2137097878988163
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7790000438690186, scr_score: 0.25403232590278196
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.8640000224113464, scr_score: 0.5967741392777745
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7750000357627869, scr_score: 0.08968601715063591
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7910000085830688, scr_score: 0.16143477741406173
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8040000200271606, scr_score: 0.2197307954761409
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8180000185966492, scr_score: 0.28251106093866896
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8440000414848328, scr_score: 0.3991030970628273
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8560000658035278, scr_score: 0.4529148677244576
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8050000667572021, scr_score: 0.22421531016200436
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7680000066757202, scr_score: 0.12946419424511926
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7900000214576721, scr_score: 0.22767852985167758
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8270000219345093, scr_score: 0.3928570763341127
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8470000624656677, scr_score: 0.48214295692740233
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8660000562667847, scr_score: 0.5669643439219371
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8770000338554382, scr_score: 0.616071378679156
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.8950000405311584, scr_score: 0.6964285381670564
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7390000224113464, scr_score: 0.042918406627421406
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7440000176429749, scr_score: 0.0643776099411321
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7540000081062317, scr_score: 0.10729601656855352
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7610000371932983, scr_score: 0.13733905469607777
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7640000581741333, scr_score: 0.15021467900985702
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7580000162124634, scr_score: 0.1244634303822985
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7930000424385071, scr_score: 0.27467810939215553
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.781000018119812, scr_score: 0.09523798711177515
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7860000133514404, scr_score: 0.11904748388971893
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7900000214576721, scr_score: 0.13809513807839202
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8020000457763672, scr_score: 0.19523810064441124
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8070000410079956, scr_score: 0.21904759742235502
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8060000538825989, scr_score: 0.21428575483308432
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.8490000367164612, scr_score: 0.419047540656037

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 0: 0.9500000476837158
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 1: 0.9670000672340393
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 2: 0.9470000267028809
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 6: 0.987000048160553
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 9: 0.9790000319480896
Num non-zero elements: 1
Test accuracy for 0: 0.614
Num non-zero elements: 1
Test accuracy for 1: 0.803
Num non-zero elements: 1
Test accuracy for 2: 0.885
Num non-zero elements: 1
Test accuracy for 6: 0.979
Num non-zero elements: 1
Test accuracy for 9: 0.94
Num non-zero elements: 2
Test accuracy for 0: 0.618
Num non-zero elements: 2
Test accuracy for 1: 0.801
Num non-zero elements: 2
Test accuracy for 2: 0.884
Num non-zero elements: 2
Test accuracy for 6: 0.981
Num non-zero elements: 2
Test accuracy for 9: 0.944
Num non-zero elements: 5
Test accuracy for 0: 0.814
Num non-zero elements: 5
Test accuracy for 1: 0.84
Num non-zero elements: 5
Test accuracy for 2: 0.887
Num non-zero elements: 5
Test accuracy for 6: 0.98
Num non-zero elements: 5
Test accuracy for 9: 0.956
Num non-zero elements: 10
Test accuracy for 0: 0.86
Num non-zero elements: 10
Test accuracy for 1: 0.862
Num non-zero elements: 10
Test accuracy for 2: 0.9
Num non-zero elements: 10
Test accuracy for 6: 0.982
Num non-zero elements: 10
Test accuracy for 9: 0.961
Num non-zero elements: 20
Test accuracy for 0: 0.912
Num non-zero elements: 20
Test accuracy for 1: 0.958
Num non-zero elements: 20
Test accuracy for 2: 0.919
Num non-zero elements: 20
Test accuracy for 6: 0.992
Num non-zero elements: 20
Test accuracy for 9: 0.97
Num non-zero elements: 50
Test accuracy for 0: 0.934
Num non-zero elements: 50
Test accuracy for 1: 0.959
Num non-zero elements: 50
Test accuracy for 2: 0.924
Num non-zero elements: 50
Test accuracy for 6: 0.992
Num non-zero elements: 50
Test accuracy for 9: 0.975
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for 11: 0.9640000462532043
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 13: 0.9570000171661377
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 14: 0.9530000686645508
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 18: 0.9350000619888306
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 19: 0.9660000205039978
Num non-zero elements: 1
Test accuracy for 11: 0.732
Num non-zero elements: 1
Test accuracy for 13: 0.678
Num non-zero elements: 1
Test accuracy for 14: 0.861
Num non-zero elements: 1
Test accuracy for 18: 0.654
Num non-zero elements: 1
Test accuracy for 19: 0.847
Num non-zero elements: 2
Test accuracy for 11: 0.856
Num non-zero elements: 2
Test accuracy for 13: 0.674
Num non-zero elements: 2
Test accuracy for 14: 0.878
Num non-zero elements: 2
Test accuracy for 18: 0.675
Num non-zero elements: 2
Test accuracy for 19: 0.84
Num non-zero elements: 5
Test accuracy for 11: 0.864
Num non-zero elements: 5
Test accuracy for 13: 0.708
Num non-zero elements: 5
Test accuracy for 14: 0.879
Num non-zero elements: 5
Test accuracy for 18: 0.862
Num non-zero elements: 5
Test accuracy for 19: 0.848
Num non-zero elements: 10
Test accuracy for 11: 0.934
Num non-zero elements: 10
Test accuracy for 13: 0.875
Num non-zero elements: 10
Test accuracy for 14: 0.897
Num non-zero elements: 10
Test accuracy for 18: 0.873
Num non-zero elements: 10
Test accuracy for 19: 0.875
Num non-zero elements: 20
Test accuracy for 11: 0.928
Num non-zero elements: 20
Test accuracy for 13: 0.887
Num non-zero elements: 20
Test accuracy for 14: 0.906
Num non-zero elements: 20
Test accuracy for 18: 0.909
Num non-zero elements: 20
Test accuracy for 19: 0.927
Num non-zero elements: 50
Test accuracy for 11: 0.961
Num non-zero elements: 50
Test accuracy for 13: 0.925
Num non-zero elements: 50
Test accuracy for 14: 0.941
Num non-zero elements: 50
Test accuracy for 18: 0.919
Num non-zero elements: 50
Test accuracy for 19: 0.938
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 20: 0.9600000381469727
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 21: 0.9270000457763672
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 22: 0.9130000472068787
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 25: 0.9650000333786011
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 26: 0.893000066280365
Num non-zero elements: 1
Test accuracy for 20: 0.919
Num non-zero elements: 1
Test accuracy for 21: 0.608
Num non-zero elements: 1
Test accuracy for 22: 0.859
Num non-zero elements: 1
Test accuracy for 25: 0.885
Num non-zero elements: 1
Test accuracy for 26: 0.629
Num non-zero elements: 2
Test accuracy for 20: 0.924
Num non-zero elements: 2
Test accuracy for 21: 0.824
Num non-zero elements: 2
Test accuracy for 22: 0.875
Num non-zero elements: 2
Test accuracy for 25: 0.883
Num non-zero elements: 2
Test accuracy for 26: 0.611
Num non-zero elements: 5
Test accuracy for 20: 0.938
Num non-zero elements: 5
Test accuracy for 21: 0.841
Num non-zero elements: 5
Test accuracy for 22: 0.881
Num non-zero elements: 5
Test accuracy for 25: 0.875
Num non-zero elements: 5
Test accuracy for 26: 0.721
Num non-zero elements: 10
Test accuracy for 20: 0.938
Num non-zero elements: 10
Test accuracy for 21: 0.874
Num non-zero elements: 10
Test accuracy for 22: 0.888
Num non-zero elements: 10
Test accuracy for 25: 0.922
Num non-zero elements: 10
Test accuracy for 26: 0.762
Num non-zero elements: 20
Test accuracy for 20: 0.945
Num non-zero elements: 20
Test accuracy for 21: 0.866
Num non-zero elements: 20
Test accuracy for 22: 0.893
Num non-zero elements: 20
Test accuracy for 25: 0.932
Num non-zero elements: 20
Test accuracy for 26: 0.88
Num non-zero elements: 50
Test accuracy for 20: 0.947
Num non-zero elements: 50
Test accuracy for 21: 0.893
Num non-zero elements: 50
Test accuracy for 22: 0.909
Num non-zero elements: 50
Test accuracy for 25: 0.948
Num non-zero elements: 50
Test accuracy for 26: 0.87
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 1: 0.9440000653266907
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 2: 0.9420000314712524
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 3: 0.9280000329017639
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 5: 0.9220000505447388
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 6: 0.8610000610351562
Num non-zero elements: 1
Test accuracy for 1: 0.846
Num non-zero elements: 1
Test accuracy for 2: 0.865
Num non-zero elements: 1
Test accuracy for 3: 0.579
Num non-zero elements: 1
Test accuracy for 5: 0.826
Num non-zero elements: 1
Test accuracy for 6: 0.58
Num non-zero elements: 2
Test accuracy for 1: 0.913
Num non-zero elements: 2
Test accuracy for 2: 0.875
Num non-zero elements: 2
Test accuracy for 3: 0.607
Num non-zero elements: 2
Test accuracy for 5: 0.845
Num non-zero elements: 2
Test accuracy for 6: 0.74
Num non-zero elements: 5
Test accuracy for 1: 0.918
Num non-zero elements: 5
Test accuracy for 2: 0.88
Num non-zero elements: 5
Test accuracy for 3: 0.78
Num non-zero elements: 5
Test accuracy for 5: 0.872
Num non-zero elements: 5
Test accuracy for 6: 0.75
Num non-zero elements: 10
Test accuracy for 1: 0.924
Num non-zero elements: 10
Test accuracy for 2: 0.864
Num non-zero elements: 10
Test accuracy for 3: 0.834
Num non-zero elements: 10
Test accuracy for 5: 0.877
Num non-zero elements: 10
Test accuracy for 6: 0.763
Num non-zero elements: 20
Test accuracy for 1: 0.93
Num non-zero elements: 20
Test accuracy for 2: 0.891
Num non-zero elements: 20
Test accuracy for 3: 0.859
Num non-zero elements: 20
Test accuracy for 5: 0.876
Num non-zero elements: 20
Test accuracy for 6: 0.785
Num non-zero elements: 50
Test accuracy for 1: 0.932
Num non-zero elements: 50
Test accuracy for 2: 0.92
Num non-zero elements: 50
Test accuracy for 3: 0.889
Num non-zero elements: 50
Test accuracy for 5: 0.896
Num non-zero elements: 50
Test accuracy for 6: 0.837
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 1.0: 0.9780000448226929
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 5.0: 0.9760000705718994
Num non-zero elements: 1
Test accuracy for 1.0: 0.584
Num non-zero elements: 1
Test accuracy for 5.0: 0.584
Num non-zero elements: 2
Test accuracy for 1.0: 0.95
Num non-zero elements: 2
Test accuracy for 5.0: 0.95
Num non-zero elements: 5
Test accuracy for 1.0: 0.963
Num non-zero elements: 5
Test accuracy for 5.0: 0.963
Num non-zero elements: 10
Test accuracy for 1.0: 0.971
Num non-zero elements: 10
Test accuracy for 5.0: 0.971
Num non-zero elements: 20
Test accuracy for 1.0: 0.968
Num non-zero elements: 20
Test accuracy for 5.0: 0.968
Num non-zero elements: 50
Test accuracy for 1.0: 0.974
Num non-zero elements: 50
Test accuracy for 5.0: 0.974
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for C: 0.9480000734329224
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for Python: 0.9880000352859497
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for HTML: 0.9850000739097595
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for Java: 0.9640000462532043
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for PHP: 0.956000030040741
Num non-zero elements: 1
Test accuracy for C: 0.645
Num non-zero elements: 1
Test accuracy for Python: 0.561
Num non-zero elements: 1
Test accuracy for HTML: 0.688
Num non-zero elements: 1
Test accuracy for Java: 0.636
Num non-zero elements: 1
Test accuracy for PHP: 0.629
Num non-zero elements: 2
Test accuracy for C: 0.638
Num non-zero elements: 2
Test accuracy for Python: 0.599
Num non-zero elements: 2
Test accuracy for HTML: 0.765
Num non-zero elements: 2
Test accuracy for Java: 0.651
Num non-zero elements: 2
Test accuracy for PHP: 0.913
Num non-zero elements: 5
Test accuracy for C: 0.854
Num non-zero elements: 5
Test accuracy for Python: 0.94
Num non-zero elements: 5
Test accuracy for HTML: 0.927
Num non-zero elements: 5
Test accuracy for Java: 0.631
Num non-zero elements: 5
Test accuracy for PHP: 0.919
Num non-zero elements: 10
Test accuracy for C: 0.866
Num non-zero elements: 10
Test accuracy for Python: 0.95
Num non-zero elements: 10
Test accuracy for HTML: 0.947
Num non-zero elements: 10
Test accuracy for Java: 0.749
Num non-zero elements: 10
Test accuracy for PHP: 0.926
Num non-zero elements: 20
Test accuracy for C: 0.897
Num non-zero elements: 20
Test accuracy for Python: 0.962
Num non-zero elements: 20
Test accuracy for HTML: 0.96
Num non-zero elements: 20
Test accuracy for Java: 0.897
Num non-zero elements: 20
Test accuracy for PHP: 0.933
Num non-zero elements: 50
Test accuracy for C: 0.934
Num non-zero elements: 50
Test accuracy for Python: 0.974
Num non-zero elements: 50
Test accuracy for HTML: 0.966
Num non-zero elements: 50
Test accuracy for Java: 0.938
Num non-zero elements: 50
Test accuracy for PHP: 0.943
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 0: 0.9430000185966492
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 1: 0.987000048160553
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 2: 0.9320000410079956
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 3: 0.9540000557899475
Num non-zero elements: 1
Test accuracy for 0: 0.737
Num non-zero elements: 1
Test accuracy for 1: 0.977
Num non-zero elements: 1
Test accuracy for 2: 0.776
Num non-zero elements: 1
Test accuracy for 3: 0.614
Num non-zero elements: 2
Test accuracy for 0: 0.854
Num non-zero elements: 2
Test accuracy for 1: 0.98
Num non-zero elements: 2
Test accuracy for 2: 0.778
Num non-zero elements: 2
Test accuracy for 3: 0.691
Num non-zero elements: 5
Test accuracy for 0: 0.853
Num non-zero elements: 5
Test accuracy for 1: 0.973
Num non-zero elements: 5
Test accuracy for 2: 0.837
Num non-zero elements: 5
Test accuracy for 3: 0.862
Num non-zero elements: 10
Test accuracy for 0: 0.871
Num non-zero elements: 10
Test accuracy for 1: 0.975
Num non-zero elements: 10
Test accuracy for 2: 0.848
Num non-zero elements: 10
Test accuracy for 3: 0.867
Num non-zero elements: 20
Test accuracy for 0: 0.914
Num non-zero elements: 20
Test accuracy for 1: 0.974
Num non-zero elements: 20
Test accuracy for 2: 0.868
Num non-zero elements: 20
Test accuracy for 3: 0.925
Num non-zero elements: 50
Test accuracy for 0: 0.924
Num non-zero elements: 50
Test accuracy for 1: 0.985
Num non-zero elements: 50
Test accuracy for 2: 0.885
Num non-zero elements: 50
Test accuracy for 3: 0.926
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for en: 0.9980000257492065
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for fr: 0.999000072479248
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for de: 1.0
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 13 epochs
Test accuracy for es: 1.0
Num non-zero elements: 65536
Training probe with dim: 65536, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for nl: 0.999000072479248
Num non-zero elements: 1
Test accuracy for en: 0.998
Num non-zero elements: 1
Test accuracy for fr: 0.994
Num non-zero elements: 1
Test accuracy for de: 0.917
Num non-zero elements: 1
Test accuracy for es: 0.929
Num non-zero elements: 1
Test accuracy for nl: 0.999
Num non-zero elements: 2
Test accuracy for en: 1.0
Num non-zero elements: 2
Test accuracy for fr: 0.992
Num non-zero elements: 2
Test accuracy for de: 0.918
Num non-zero elements: 2
Test accuracy for es: 0.953
Num non-zero elements: 2
Test accuracy for nl: 0.998
Num non-zero elements: 5
Test accuracy for en: 0.998
Num non-zero elements: 5
Test accuracy for fr: 0.997
Num non-zero elements: 5
Test accuracy for de: 0.966
Num non-zero elements: 5
Test accuracy for es: 0.998
Num non-zero elements: 5
Test accuracy for nl: 0.998
Num non-zero elements: 10
Test accuracy for en: 0.999
Num non-zero elements: 10
Test accuracy for fr: 0.997
Num non-zero elements: 10
Test accuracy for de: 0.997
Num non-zero elements: 10
Test accuracy for es: 0.998
Num non-zero elements: 10
Test accuracy for nl: 0.998
Num non-zero elements: 20
Test accuracy for en: 0.999
Num non-zero elements: 20
Test accuracy for fr: 0.996
Num non-zero elements: 20
Test accuracy for de: 0.999
Num non-zero elements: 20
Test accuracy for es: 0.999
Num non-zero elements: 20
Test accuracy for nl: 1.0
Num non-zero elements: 50
Test accuracy for en: 0.995
Num non-zero elements: 50
Test accuracy for fr: 1.0
Num non-zero elements: 50
Test accuracy for de: 1.0
Num non-zero elements: 50
Test accuracy for es: 0.999
Num non-zero elements: 50
Test accuracy for nl: 0.999

Running unlearning evaluation...
Loaded pretrained model google/gemma-2-2b-it into HookedTransformer
Running evaluation for SAE: google/gemma-2-2b_layer_12_sae_custom_sae
Sparsity calculation for google/gemma-2-2b_layer_12_sae_custom_sae is already done
Forget sparsity: [0.055334 0.060503 0.026297 ... 0.110983 0.10498  0.903909]
(65536,)
Retain sparsity: [0.077796 0.049172 0.035404 ... 0.125102 0.112777 0.923225]
(65536,)
Starting metrics calculation...
running on datasets: ['wmdp-bio', 'high_school_us_history', 'college_computer_science', 'high_school_geography', 'human_aging']
Calculating metrics for retain threshold: 0.001
Top features for ablation: [10506 25842 45314  2077 56357 23027 39119 61347  2779 21104 20107 36120
 44205 21364 34103 17564 40779  9286 19471 31912 48339 36359 34559 65092
 22709 17153 28389  8491 42849 17326 50353  7897 53329 47303 11280 23881
 18455 54730 19873 32604 25749 31533 22212 62161 15985 16021 37693 38727
 64081 19944 27614 26232 54135 62951 26348 20822  1147 63697 58458 29914
 41995 52815 37793 19592  9135 17785 36803 61383 12346  2086 55085 14808
 27546 12828 57693 14060 27484 31026 25288  6378 62809 11105 55176 52073
 37081   190 42057 58038 34781 26769  1647 25831 28120  4879 15161 38806
  2924 62297 52236  5062 65524 59740 38292 43825 26854  6470 61641   230
 20669 39117 55369 19350 24176 45545 25200 60009 31765  4073 18990 63512
 43530   706 60541  9344 59973 48315  9336 16748 16835 34065  7706 41502
 58280  1096 19856 41726 22679 42491 62964 37951  7286 38142 35941  3511
  5229 37863 30113  9746 43667 21987 56116  8685 17274 48219 42589 43262
 52831 45285 49092 44118 55453 36319 63961 14767 61353 59834 47157 62300
 61637  7579 44794 34961 29889  1361 12581 42694 46048 17960 61054 64070
 39113 31454  4036 16436 32365 28740 34350 27024 15704 56320 14154  4034
 53433  7719 59048 44493 17617 45592 19731  8031 11518  9566 19830 40956
 64581 54639  7639  3239 55189 26063 52898 53800 23010 15894 64990 41124
 24731  3234 11273 55705 63924 22399  5300  7722 25968 34665 55757 61991
 59378 32074 46580 52984  9664 47173 10561 11803  7957 22593 52156 63398
 10656 51744 38395 22005 49485 29408 46217  1747 14907 14643  8511 42447
   740 60643 48739 64472 47878 35043 26078 51965  8010 30902  1837  4592
  1689 17568 55642 60098  9398 42494  2878 38463  3733 34405 49866 60759
 62948 41590 40827  2859 63109 44631  5662 22776 45075 54818 23783 16902
 61668 61516  1735 50236 29268 22175 32192 19186 33598 25504 49321 22802
 51150 17172 17134  5634 18834 10485 20371 25058 50721  1441 43925 31556
 26491 36918 15765 62863 57872 53277 51824 42701 30872  8247 56959 32030
 46027 14935 34083 28931 29847 36177 27808 33248 41597 39376 25091 31798
 36705 46804 55151 12400 33041 46696 46871  7531 27366  5375 35575 54390
 21451 30426  4855 53328  5190 19500  1683 42836 17464 50856 34971 23317
 59937 58894 48457 16425 46397 16379 37054 56669 41526 61844  4849 19225
 20948 18541 45435 33513 60649 53646  1464 27010 34449 39383 13799 38274
 59571  7274 55794 40614 14013 53247 35950  8497 16946   422 18209 53259
 33609 18662 62254 48889 55331 25781 62158 51010   594 15822 54211 45911
 37872 51514 63205 38235 33699 33167 38868 21402 14274 27136 63107 61623
 48551 63495  2888 44172 27805  6881 39395 49755 62004 51537 34056 49969
 31523 24251 38466 14621   515 15761 34015 11217  3150 14766 30440  7820
 48013 54140 37355 34965 21912 63510 31599 47162 21304 23967 16753  2403
 42613  6567  3295 23108 44270 60426 35324 61432 21509 12551 61649 42333
 34645 14104 45293 22913 57985  2025 57709  5735 57664 51285 58462 24362
  2467 58358 31068 38375 47016 43450 51146 61208 18727 18938 43391 24615
 29569 26762 20131 18794 52099 58863 13755 28813  3051 35113 27137 47129
 35317 28631 47629 41209 54863 57379 55758 14153 62251 32121 30381 42916
 58248 41010  7302  1617 19749 25255 12261 13728 22524  5335 50037 54488
  2654  4231  9570  3567 26484 24050 37779 51288 32492 60238  6128 34842
 49897 35414 14799 30143 36912 42187 18777 39909 49499 25674 19489 33780
 63440 44980 13770 45157 17635 33622 22347  5174 56714 50625 32370 61232
 28422 60712 34610 42840 48083  4655  2455 65342   893  5200  5881 39170
 52903 33159  2479 40699 43941 38827 49854 30568  2784 63251 37077 17668
 21614 24726 12404 61521 27056 22236 13986  4547 38472 28575 40412  6704
  7082 37095 16135 37680 56754 34588 43753  7005 33871 17372 35651 50546
 47283 44224 27043 44967 64181 21230 23860   701  7996 26403 14475  2486
  4731 48100 43977 22137  9583 53833 64146 40976 33487 63537 45858 12056
 56059 62347 12524 18108 42110 42224 44086 53934 12451  6597 34007  5374
 15920 57656 19880 62502 16004 36981 31484  1462 41057 26673 36304 44176
 26906  1291 33669 62645  9818 30473  3641 17202   782 12701  9196  1078
 51533  7671 46698 18545 12820 58639 63646 25189  8591 13686 44767 28679
 55150 49804 24065 49549 51061  8195 12223 16731 19219 24501 27916 22070
 65401 48976 41300  8484 20533  1640 26567  9980 50959 21471 39872  3723
 29947 17188 42636 38897 52176 47108 55298 30391 61962 46163 16651 12906
 21518 38630  8749 56464 41418  5406 63598 48110 45811 39754 29077 22222
 29776  5074 15679 58065 53938 37253 62343 62163 40719  9498 46058 57519
 41234 45885 24098 18375 19744 42009 40538 16898 53368 17467 50624 59008
  5211 62675 58629 46556 37600 35825 15722  4697  1094  3789  1832 24826
 20122 63520 59713 57460 56755 34314 54672 51393 64813 28448 56548 21182
  2649 25990 57867 15553 23720]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([10506, 25842, 45314,  2077, 56357, 23027, 39119, 61347,  2779,
       21104]), array([10506, 25842, 45314,  2077, 56357, 23027, 39119, 61347,  2779,
       21104, 20107, 36120, 44205, 21364, 34103, 17564, 40779,  9286,
       19471, 31912])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Calculating metrics for retain threshold: 0.01
Top features for ablation: [53269 56968 12105 ... 57867 15553 23720]
Calculating metrics list with parameters: {'intervention_method': 'clamp_feature_activation'} {'features_to_ablate': [array([53269, 56968, 12105, 51412, 36614, 10812, 28826,  7585, 31297,
       31809]), array([53269, 56968, 12105, 51412, 36614, 10812, 28826,  7585, 31297,
       31809, 40159, 13356, 49187, 26211, 45538,  6749, 31925, 43322,
       56757, 32266])], 'multiplier': [25, 50, 100, 200]}
All target question ids for gemma-2-2b-it on wmdp-bio already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_us_history already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on college_computer_science already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on high_school_geography already exist. No need to generate target ids.
All target question ids for gemma-2-2b-it on human_aging already exist. No need to generate target ids.
Unlearning scores DataFrame:     wmdp-bio  high_school_us_history  ...  multiplier  all_side_effects_mcq
0        1.0                     1.0  ...          25                   1.0
1        1.0                     1.0  ...         200                   1.0
2        1.0                     1.0  ...         100                   1.0
3        1.0                     1.0  ...         200                   1.0
4        1.0                     1.0  ...         200                   1.0
5        1.0                     1.0  ...          50                   1.0
6        1.0                     1.0  ...         100                   1.0
7        1.0                     1.0  ...         100                   1.0
8        1.0                     1.0  ...          50                   1.0
9        1.0                     1.0  ...          50                   1.0
10       1.0                     1.0  ...          25                   1.0
11       1.0                     1.0  ...         100                   1.0
12       1.0                     1.0  ...          25                   1.0
13       1.0                     1.0  ...          25                   1.0
14       1.0                     1.0  ...         200                   1.0
15       1.0                     1.0  ...          50                   1.0

[16 rows x 10 columns]
