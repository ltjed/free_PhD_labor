Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.
ideas
Loaded existing ideas:
[{'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True, 'NOTE': "IGNORE the benchmark 'UNLEARNING'"}]
Processing idea: adaptive_orthogonal_sae
2025-01-26 02:39:13
*Starting idea: 20250126_023610_adaptive_orthogonal_sae*
2025-01-26 02:39:13
*Starting Experiments*
Starting experiment with prompt for coder: Your goal is to implement the following idea: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact.
The implementation plan is as follows: 1. Add batch_topk_pair_selector using torch.topk
2. Modify loss to use instantaneous top pairs
3. Implement L2 weight normalization via hook
4. Add τ as fixed config parameter
5. Integrate pair stability analysis
6. Add top-k percentage parameter
7. Update config with normalization type.

You can also refer to other information in the idea: {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True, 'NOTE': "IGNORE the benchmark 'UNLEARNING'"}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

{'training results': {'training_log': {'step 0': {'loss': 280.3485107421875, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}, 'step 1': {'loss': 243.578369140625, 'l1_loss': 2632.935546875, 'l2_loss': 138.26095581054688}, 'step 2': {'loss': 249.86807250976562, 'l1_loss': 2702.075927734375, 'l2_loss': 141.7850341796875}, 'step 3': {'loss': 249.8067626953125, 'l1_loss': 2702.37939453125, 'l2_loss': 141.71157836914062}, 'step 4': {'loss': 252.651123046875, 'l1_loss': 2733.4326171875, 'l2_loss': 143.31381225585938}, 'step 5': {'loss': 244.89901733398438, 'l1_loss': 2648.08056640625, 'l2_loss': 138.97579956054688}, 'step 6': {'loss': 247.49098205566406, 'l1_loss': 2678.156005859375, 'l2_loss': 140.36474609375}, 'step 7': {'loss': 250.32537841796875, 'l1_loss': 2710.021484375, 'l2_loss': 141.92453002929688}, 'step 8': {'loss': 247.69447326660156, 'l1_loss': 2682.05615234375, 'l2_loss': 140.4122314453125}, 'step 9': {'loss': 246.83868408203125, 'l1_loss': 2673.4755859375, 'l2_loss': 139.89967346191406}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 5, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_5'}, 'final_info': {'training_steps': 48, 'final_loss': 226.3124237060547, 'layer': 5, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_5_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_5_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.2546583850931677, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 7.5}, 'model_performance_preservation': {'ce_loss_score': 0.23684210526315788, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 10.1875, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -1.390625, 'mse': 6.53125, 'cossim': 0.0242919921875}, 'shrinkage': {'l2_norm_in': 90.0, 'l2_norm_out': 72.0, 'l2_ratio': 0.80078125, 'relative_reconstruction_bias': 34.25}, 'sparsity': {'l0': 1100.94287109375, 'l1': 2272.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'a1ef0f0f-8bd4-42f5-bebb-87b6e6c7c843', 'datetime_epoch_millis': 1737870711988, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.02863244406650325, 'scr_metric_threshold_2': -0.0016716282676866603, 'scr_dir2_threshold_2': -0.0016716282676866603, 'scr_dir1_threshold_5': -0.014163769825244135, 'scr_metric_threshold_5': -0.0014869121509806411, 'scr_dir2_threshold_5': -0.0014869121509806411, 'scr_dir1_threshold_10': -0.06220171617182817, 'scr_metric_threshold_10': -0.004677122392150762, 'scr_dir2_threshold_10': -0.004677122392150762, 'scr_dir1_threshold_20': -0.004443416068216732, 'scr_metric_threshold_20': 0.0068963592576969015, 'scr_dir2_threshold_20': 0.0068963592576969015, 'scr_dir1_threshold_50': -0.11900962496206421, 'scr_metric_threshold_50': 0.028485643456313173, 'scr_dir2_threshold_50': 0.028485643456313173, 'scr_dir1_threshold_100': -0.07172709132039413, 'scr_metric_threshold_100': 0.02666520527062645, 'scr_dir2_threshold_100': 0.02666520527062645, 'scr_dir1_threshold_500': -0.17745045796083983, 'scr_metric_threshold_500': 0.027854774058978304, 'scr_dir2_threshold_500': 0.027854774058978304}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': -0.034482333379235786, 'scr_metric_threshold_2': 0.002347387623557211, 'scr_dir2_threshold_2': 0.002347387623557211, 'scr_dir1_threshold_5': -0.41379416655191054, 'scr_metric_threshold_5': 0.00469491516412125, 'scr_dir2_threshold_5': 0.00469491516412125, 'scr_dir1_threshold_10': -0.7586216110116558, 'scr_metric_threshold_10': 0.009389690411235671, 'scr_dir2_threshold_10': 0.009389690411235671, 'scr_dir1_threshold_20': -0.37930977783898107, 'scr_metric_threshold_20': 0.002347387623557211, 'scr_dir2_threshold_20': 0.002347387623557211, 'scr_dir1_threshold_50': -0.3448274444597453, 'scr_metric_threshold_50': 0.0, 'scr_dir2_threshold_50': 0.0, 'scr_dir1_threshold_100': -0.7931039443908916, 'scr_metric_threshold_100': 0.009389690411235671, 'scr_dir2_threshold_100': 0.009389690411235671, 'scr_dir1_threshold_500': -0.4482764999311463, 'scr_metric_threshold_500': 0.01643199319891413, 'scr_dir2_threshold_500': 0.01643199319891413}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 0.05970163859464643, 'scr_metric_threshold_2': 0.012953356274431536, 'scr_dir2_threshold_2': 0.012953356274431536, 'scr_dir1_threshold_5': 0.10447720032417823, 'scr_metric_threshold_5': 0.0077719211149466375, 'scr_dir2_threshold_5': 0.0077719211149466375, 'scr_dir1_threshold_10': 0.0746268258378237, 'scr_metric_threshold_10': 0.0, 'scr_dir2_threshold_10': 0.0, 'scr_dir1_threshold_20': 0.0447755617295318, 'scr_metric_threshold_20': 0.0077719211149466375, 'scr_dir2_threshold_20': 0.0077719211149466375, 'scr_dir1_threshold_50': 0.029850374486354535, 'scr_metric_threshold_50': 0.0077719211149466375, 'scr_dir2_threshold_50': 0.0077719211149466375, 'scr_dir1_threshold_100': 0.10447720032417823, 'scr_metric_threshold_100': 0.002590640371648879, 'scr_dir2_threshold_100': 0.002590640371648879, 'scr_dir1_threshold_500': -0.34328375470276395, 'scr_metric_threshold_500': 0.015543996646080416, 'scr_dir2_threshold_500': 0.015543996646080416}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': 0.023256877745449607, 'scr_metric_threshold_2': -0.0076336410429387, 'scr_dir2_threshold_2': -0.0076336410429387, 'scr_dir1_threshold_5': 0.023256877745449607, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0, 'scr_dir1_threshold_10': 0.023256877745449607, 'scr_metric_threshold_10': 0.0, 'scr_dir2_threshold_10': 0.0, 'scr_dir1_threshold_20': 0.04651236933773761, 'scr_metric_threshold_20': -0.002544648124819707, 'scr_dir2_threshold_20': -0.002544648124819707, 'scr_dir1_threshold_50': -0.1162788441146016, 'scr_metric_threshold_50': 0.0, 'scr_dir2_threshold_50': 0.0, 'scr_dir1_threshold_100': 0.023256877745449607, 'scr_metric_threshold_100': -0.010178137501998197, 'scr_dir2_threshold_100': -0.010178137501998197, 'scr_dir1_threshold_500': -0.9999986138468384, 'scr_metric_threshold_500': 0.061068673346228966, 'scr_dir2_threshold_500': 0.061068673346228966}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': 0.16049390892266485, 'scr_metric_threshold_2': -0.016128983156081637, 'scr_dir2_threshold_2': -0.016128983156081637, 'scr_dir1_threshold_5': 0.18518493989867213, 'scr_metric_threshold_5': -0.005376274309519518, 'scr_dir2_threshold_5': -0.005376274309519518, 'scr_dir1_threshold_10': 0.08642008013510381, 'scr_metric_threshold_10': 0.00806457169180236, 'scr_dir2_threshold_10': 0.00806457169180236, 'scr_dir1_threshold_20': 0.14814839343466119, 'scr_metric_threshold_20': 0.021505417693124237, 'scr_dir2_threshold_20': 0.021505417693124237, 'scr_dir1_threshold_50': -0.49382724225599817, 'scr_metric_threshold_50': 0.12096777423941997, 'scr_dir2_threshold_50': 0.12096777423941997, 'scr_dir1_threshold_100': -0.04938279781155373, 'scr_metric_threshold_100': 0.15053760339682348, 'scr_dir2_threshold_100': 0.15053760339682348, 'scr_dir1_threshold_500': 0.30864230235732604, 'scr_metric_threshold_500': 0.03763440084920587, 'scr_dir2_threshold_500': 0.03763440084920587}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.0057145523538213215, 'scr_metric_threshold_2': -0.009132575437985308, 'scr_dir2_threshold_2': -0.009132575437985308, 'scr_dir1_threshold_5': 0.01714297586557008, 'scr_metric_threshold_5': -0.03652975741724071, 'scr_dir2_threshold_5': -0.03652975741724071, 'scr_dir1_threshold_10': 0.09142875048577782, 'scr_metric_threshold_10': -0.027397181979255406, 'scr_dir2_threshold_10': -0.027397181979255406, 'scr_dir1_threshold_20': 0.0971429622416522, 'scr_metric_threshold_20': 0.0, 'scr_dir2_threshold_20': 0.0, 'scr_dir1_threshold_50': 0.0114287641096957, 'scr_metric_threshold_50': 0.03652975741724071, 'scr_dir2_threshold_50': 0.03652975741724071, 'scr_dir1_threshold_100': 0.028571399377318833, 'scr_metric_threshold_100': 0.02283103034393788, 'scr_dir2_threshold_100': 0.02283103034393788, 'scr_dir1_threshold_500': -0.028571399377318833, 'scr_metric_threshold_500': 0.13242003042830977, 'scr_dir2_threshold_500': 0.13242003042830977}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.007692208939046454, 'scr_metric_threshold_2': -0.012145834962205343, 'scr_dir2_threshold_2': -0.012145834962205343, 'scr_dir1_threshold_5': 0.007692208939046454, 'scr_metric_threshold_5': -0.008097303746260835, 'scr_dir2_threshold_5': -0.008097303746260835, 'scr_dir1_threshold_10': -0.030769294253470136, 'scr_metric_threshold_10': -0.0323887323562997, 'scr_dir2_threshold_10': -0.0323887323562997, 'scr_dir1_threshold_20': 0.0, 'scr_metric_threshold_20': -0.008097303746260835, 'scr_dir2_threshold_20': -0.008097303746260835, 'scr_dir1_threshold_50': 0.023076626817139363, 'scr_metric_threshold_50': 0.012145593647833528, 'scr_dir2_threshold_50': 0.012145593647833528, 'scr_dir1_threshold_100': 0.10000009169945687, 'scr_metric_threshold_100': 0.004048531215944509, 'scr_dir2_threshold_100': 0.004048531215944509, 'scr_dir1_threshold_500': -0.023077085314423682, 'scr_metric_threshold_500': 0.03238849104192789, 'scr_dir2_threshold_500': 0.03238849104192789}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.017045568074408247, 'scr_metric_threshold_2': 0.021367498507973648, 'scr_dir2_threshold_2': 0.021367498507973648, 'scr_dir1_threshold_5': -0.011363486274511132, 'scr_metric_threshold_5': 0.025641201986108045, 'scr_dir2_threshold_5': 0.025641201986108045, 'scr_dir1_threshold_10': 0.005682081799897114, 'scr_metric_threshold_10': 0.029914650743567858, 'scr_dir2_threshold_10': 0.029914650743567858, 'scr_dir1_threshold_20': 0.02272731121166381, 'scr_metric_threshold_20': 0.03418809950102767, 'scr_dir2_threshold_20': 0.03418809950102767, 'scr_dir1_threshold_50': -0.051136365560583194, 'scr_metric_threshold_50': 0.08547024875256917, 'scr_dir2_threshold_50': 0.08547024875256917, 'scr_dir1_threshold_100': 0.02272731121166381, 'scr_metric_threshold_100': 0.06410249552392094, 'scr_dir2_threshold_100': 0.06410249552392094, 'scr_dir1_threshold_500': 0.18750023283056608, 'scr_metric_threshold_500': -0.07264939303884058, 'scr_dir2_threshold_500': -0.07264939303884058}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': -0.010362868718775122, 'scr_metric_threshold_2': -0.005000233948244687, 'scr_dir2_threshold_2': -0.005000233948244687, 'scr_dir1_threshold_5': -0.025906708548447916, 'scr_metric_threshold_5': 0.0, 'scr_dir2_threshold_5': 0.0, 'scr_dir1_threshold_10': 0.01036255988644853, 'scr_metric_threshold_10': -0.02499997764825688, 'scr_dir2_threshold_10': -0.02499997764825688, 'scr_dir1_threshold_20': -0.015544148661999387, 'scr_metric_threshold_20': 0.0, 'scr_dir2_threshold_20': 0.0, 'scr_dir1_threshold_50': -0.010362868718775122, 'scr_metric_threshold_50': -0.03500014752150461, 'scr_dir2_threshold_50': -0.03500014752150461, 'scr_dir1_threshold_100': -0.010362868718775122, 'scr_metric_threshold_100': -0.030000211596501566, 'scr_dir2_threshold_100': -0.030000211596501566, 'scr_dir1_threshold_500': -0.07253884570211949, 'scr_metric_threshold_500': 0.0, 'scr_dir2_threshold_500': 0.0}], 'sae_bench_commit_hash': 'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_5_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 5, 'hook_name': 'blocks.5.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': '3ed934fc-775e-4d6c-8b9e-07722a992dbc', 'datetime_epoch_millis': 1737870918914, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.006710192260088017, 'mean_num_split_features': 1.4230769230769231}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.0007671653241273494, 'num_absorption': 2, 'num_probe_true_positives': 2607, 'num_split_features': 3}, {'first_letter': 'b', 'absorption_rate': 0.01322418136020151, 'num_absorption': 21, 'num_probe_true_positives': 1588, 'num_split_features': 2}, {'first_letter': 'c', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 2851, 'num_split_features': 3}, {'first_letter': 'd', 'absorption_rate': 0.012582384661473937, 'num_absorption': 21, 'num_probe_true_positives': 1669, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.0018656716417910447, 'num_absorption': 3, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 1255, 'num_split_features': 2}, {'first_letter': 'g', 'absorption_rate': 0.004642525533890436, 'num_absorption': 5, 'num_probe_true_positives': 1077, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.006349206349206349, 'num_absorption': 6, 'num_probe_true_positives': 945, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.000612369871402327, 'num_absorption': 1, 'num_probe_true_positives': 1633, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.009174311926605505, 'num_absorption': 4, 'num_probe_true_positives': 436, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.005934718100890208, 'num_absorption': 4, 'num_probe_true_positives': 674, 'num_split_features': 1}, {'first_letter': 'l', 'absorption_rate': 0.002437043054427295, 'num_absorption': 3, 'num_probe_true_positives': 1231, 'num_split_features': 2}, {'first_letter': 'm', 'absorption_rate': 0.009836065573770493, 'num_absorption': 18, 'num_probe_true_positives': 1830, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.0035335689045936395, 'num_absorption': 3, 'num_probe_true_positives': 849, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.029906542056074768, 'num_absorption': 32, 'num_probe_true_positives': 1070, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.000423908435777872, 'num_absorption': 1, 'num_probe_true_positives': 2359, 'num_split_features': 3}, {'first_letter': 'q', 'absorption_rate': 0.01092896174863388, 'num_absorption': 2, 'num_probe_true_positives': 183, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.008860011813349085, 'num_absorption': 15, 'num_probe_true_positives': 1693, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.0010504201680672268, 'num_absorption': 3, 'num_probe_true_positives': 2856, 'num_split_features': 2}, {'first_letter': 't', 'absorption_rate': 0.007597895967270602, 'num_absorption': 13, 'num_probe_true_positives': 1711, 'num_split_features': 1}, {'first_letter': 'u', 'absorption_rate': 0.014492753623188406, 'num_absorption': 11, 'num_probe_true_positives': 759, 'num_split_features': 1}, {'first_letter': 'v', 'absorption_rate': 0.005820721769499418, 'num_absorption': 5, 'num_probe_true_positives': 859, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004531722054380665, 'num_absorption': 3, 'num_probe_true_positives': 662, 'num_split_features': 1}, {'first_letter': 'x', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 99, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.012345679012345678, 'num_absorption': 2, 'num_probe_true_positives': 162, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.007547169811320755, 'num_absorption': 2, 'num_probe_true_positives': 265, 'num_split_features': 1}], 'sae_bench_commit_hash': 'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_5_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 5, 'hook_name': 'blocks.5.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help plan and implement the experiments for the Instantaneous Top-k Orthogonality Constraints idea. Here's the experiment plan:                                                                                                  

Run 1: Initial implementation with fixed parameters                                                                                                                                                                                   

 • Add top-k pair selection (0.1%)                                                                                                                                                                                                    
 • Implement L2 normalization                                                                                                                                                                                                         
 • Add orthogonality loss                                                                                                                                                                                                             
 • Fixed τ=0.2                                                                                                                                                                                                                        

Run 2: Test lower top-k percentage (0.05%) Run 3: Test higher top-k percentage (0.2%) Run 4: Test adaptive τ Run 5: Ablation without L2 normalization                                                                                 

Let's start with Run 1. We need to modify the CustomSAE and CustomTrainer classes to implement the core functionality.                                                                                                                

Here are the changes for Run 1:                                                                                                                                                                                                       

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
 class CustomSAE(nn.Module):                                                                                                                                                                                                          
     """Implementation of a Custom Sparse Autoencoder."""                                                                                                                                                                             
 =======                                                                                                                                                                                                                              
 class AdaptiveOrthogonalSAE(nn.Module):                                                                                                                                                                                              
     """Implementation of SAE with instantaneous top-k orthogonality constraints."""                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
     def __init__(                                                                                                                                                                                                                    
         self,                                                                                                                                                                                                                        
         d_in: int,                                                                                                                                                                                                                   
         d_sae: int,                                                                                                                                                                                                                  
         hook_layer: int,                                                                                                                                                                                                             
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                                                                                           
         hook_name: Optional[str] = None,                                                                                                                                                                                             
     ):                                                                                                                                                                                                                               
         super().__init__()                                                                                                                                                                                                           
 =======                                                                                                                                                                                                                              
     def __init__(                                                                                                                                                                                                                    
         self,                                                                                                                                                                                                                        
         d_in: int,                                                                                                                                                                                                                   
         d_sae: int,                                                                                                                                                                                                                  
         hook_layer: int,                                                                                                                                                                                                             
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                                                                                           
         hook_name: Optional[str] = None,                                                                                                                                                                                             
         topk_fraction: float = 0.001,  # 0.1%                                                                                                                                                                                        
         tau: float = 0.2,                                                                                                                                                                                                            
     ):                                                                                                                                                                                                                               
         super().__init__()                                                                                                                                                                                                           
         self.topk_fraction = topk_fraction                                                                                                                                                                                           
         self.tau = tau                                                                                                                                                                                                               
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
 class CustomTrainer(SAETrainer):                                                                                                                                                                                                     
     """Trainer for Custom Sparse Autoencoder using L1 regularization."""                                                                                                                                                             
 =======                                                                                                                                                                                                                              
 class AdaptiveOrthogonalTrainer(SAETrainer):                                                                                                                                                                                         
     """Trainer for SAE with instantaneous top-k orthogonality constraints."""                                                                                                                                                        
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
         self.ae = CustomSAE(d_in=activation_dim, d_sae=dict_size, hook_layer=layer, model_name=lm_name)                                                                                                                              
 =======                                                                                                                                                                                                                              
         self.ae = AdaptiveOrthogonalSAE(                                                                                                                                                                                             
             d_in=activation_dim,                                                                                                                                                                                                     
             d_sae=dict_size,                                                                                                                                                                                                         
             hook_layer=layer,                                                                                                                                                                                                        
             model_name=lm_name,                                                                                                                                                                                                      
             topk_fraction=0.001,  # 0.1%                                                                                                                                                                                             
             tau=0.2                                                                                                                                                                                                                  
         )                                                                                                                                                                                                                            
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
     def loss(self, x, logging=False, **kwargs):                                                                                                                                                                                      
         x_hat, f = self.ae(x, output_features=True)                                                                                                                                                                                  
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                                                                                        
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                                                                                         
                                                                                                                                                                                                                                      
         if self.steps_since_active is not None:                                                                                                                                                                                      
             # Update steps_since_active                                                                                                                                                                                              
             deads = (f == 0).all(dim=0)                                                                                                                                                                                              
             self.steps_since_active[deads] += 1                                                                                                                                                                                      
             self.steps_since_active[~deads] = 0                                                                                                                                                                                      
                                                                                                                                                                                                                                      
         loss = l2_loss + self.l1_penalty * l1_loss                                                                                                                                                                                   
 =======                                                                                                                                                                                                                              
     def loss(self, x, logging=False, **kwargs):                                                                                                                                                                                      
         x_hat, f = self.ae(x, output_features=True)                                                                                                                                                                                  
                                                                                                                                                                                                                                      
         # Reconstruction loss                                                                                                                                                                                                        
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                                                                                        
                                                                                                                                                                                                                                      
         # L1 sparsity loss                                                                                                                                                                                                           
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                                                                                         
                                                                                                                                                                                                                                      
         # Compute pairwise feature correlations                                                                                                                                                                                      
         f_normed = f / (f.norm(dim=0, keepdim=True) + 1e-8)                                                                                                                                                                          
         correlations = torch.abs(f_normed.T @ f_normed)                                                                                                                                                                              
                                                                                                                                                                                                                                      
         # Get top k% most correlated pairs (excluding self-correlations)                                                                                                                                                             
         mask = torch.ones_like(correlations, dtype=torch.bool)                                                                                                                                                                       
         mask.fill_diagonal_(0)                                                                                                                                                                                                       
         correlations = correlations * mask                                                                                                                                                                                           
                                                                                                                                                                                                                                      
         k = int(self.ae.topk_fraction * (correlations.numel() - correlations.size(0)))                                                                                                                                               
         top_correlations = torch.topk(correlations.view(-1), k=k).values                                                                                                                                                             
                                                                                                                                                                                                                                      
         # Orthogonality loss on top pairs                                                                                                                                                                                            
         ortho_loss = top_correlations.mean()                                                                                                                                                                                         
                                                                                                                                                                                                                                      
         if self.steps_since_active is not None:                                                                                                                                                                                      
             deads = (f == 0).all(dim=0)                                                                                                                                                                                              
             self.steps_since_active[deads] += 1                                                                                                                                                                                      
             self.steps_since_active[~deads] = 0                                                                                                                                                                                      
                                                                                                                                                                                                                                      
         # Combined loss with orthogonality penalty                                                                                                                                                                                   
         loss = l2_loss + self.l1_penalty * l1_loss + self.ae.tau * ortho_loss                                                                                                                                                        
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
         # Initialize optimizer with constrained decoder weights                                                                                                                                                                      
         self.optimizer = ConstrainedAdam(                                                                                                                                                                                            
             self.ae.parameters(),                                                                                                                                                                                                    
             [self.ae.W_dec],  # Constrain decoder weights                                                                                                                                                                            
             lr=lr                                                                                                                                                                                                                    
         )                                                                                                                                                                                                                            
 =======                                                                                                                                                                                                                              
         # Initialize optimizer with L2 normalized decoder weights                                                                                                                                                                    
         self.optimizer = ConstrainedAdam(                                                                                                                                                                                            
             self.ae.parameters(),                                                                                                                                                                                                    
             [self.ae.W_dec],  # Apply L2 normalization to decoder weights                                                                                                                                                            
             lr=lr                                                                                                                                                                                                                    
         )                                                                                                                                                                                                                            
                                                                                                                                                                                                                                      
         # Register L2 normalization hook for decoder weights                                                                                                                                                                         
         def l2_norm_hook(grad):                                                                                                                                                                                                      
             if grad is None:                                                                                                                                                                                                         
                 return None                                                                                                                                                                                                          
             return grad / (grad.norm(dim=0, keepdim=True) + 1e-8)                                                                                                                                                                    
                                                                                                                                                                                                                                      
         self.ae.W_dec.register_hook(l2_norm_hook)                                                                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
             'trainer_class': 'CustomTrainer',                                                                                                                                                                                        
 =======                                                                                                                                                                                                                              
             'trainer_class': 'AdaptiveOrthogonalTrainer',                                                                                                                                                                            
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

Let's run this first implementation and evaluate the results. The command to run will be:                                                                                                                                             

                                                                                                                                                                                                                                      
 python experiment.py --out_dir=run_1                                                                                                                                                                                                 
                                                                                                                                                                                                                                      

This implements:                                                                                                                                                                                                                      

 1 Top 0.1% pair selection using torch.topk                                                                                                                                                                                           
 2 Orthogonality loss on selected pairs                                                                                                                                                                                               
 3 L2 weight normalization via hook                                                                                                                                                                                                   
 4 Fixed τ=0.2                                                                                                                                                                                                                        

After we see the results, we can proceed with the subsequent runs to test different parameters and ablations.                                                                                                                         

Tokens: 29k sent, 1.8k received. Cost: $0.11 message, $0.11 session.
Applied edit to experiment.py

# Fix any errors below, if possible.

## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated experiment.py

experiment.py:441:15: F821 undefined name 'CustomTrainer'
    trainer = CustomTrainer(
              ^

## See relevant line below marked with █.

experiment.py:
...⋮...
381│def run_sae_training(
382│    layer: int,
383│    dict_size: int,
384│    num_tokens: int,
385│    out_dir: str,  # Changed from save_dir to out_dir for consistency
386│    device: str,
387│    model_name: str = "google/gemma-2-2b",
388│    context_length: int = 128,
389│    buffer_size: int = 2048,
390│    llm_batch_size: int = 24,
...⋮...
401│    out_dir = os.path.abspath(out_dir)
...⋮...
438│    )
439│
440│    # Initialize trainer  
441█    trainer = CustomTrainer(
442│        activation_dim=activation_dim,
443│        dict_size=dict_size,
444│        lr=learning_rate,
445│        l1_penalty=sparsity_penalty,
446│        warmup_steps=warmup_steps,
447│        seed=seed,
448│        device=device,
449│        layer=layer,
450│        lm_name=model_name,
...⋮...


Ah yes, I need to fix the trainer class name in the run_sae_training function to match the new AdaptiveOrthogonalTrainer class name. Here's the fix:                                                                                  

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
     # Initialize trainer                                                                                                                                                                                                             
     trainer = CustomTrainer(                                                                                                                                                                                                         
 =======                                                                                                                                                                                                                              
     # Initialize trainer                                                                                                                                                                                                             
     trainer = AdaptiveOrthogonalTrainer(                                                                                                                                                                                             
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

This change ensures the trainer class name matches the new implementation we created earlier. Now the code should run without the undefined name error.                                                                               

You can run the experiment with:                                                                                                                                                                                                      

                                                                                                                                                                                                                                      
 python experiment.py --out_dir=run_1                                                                                                                                                                                                 
                                                                                                                                                                                                                                      

Tokens: 22k sent, 157 received. Cost: $0.07 message, $0.18 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
Step 0: {'loss': 280.53546142578125, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}
Step 10: {'loss': 245.77554321289062, 'l1_loss': 2660.20263671875, 'l2_loss': 139.1837615966797}
Step 20: {'loss': 243.61239624023438, 'l1_loss': 2651.396484375, 'l2_loss': 137.37295532226562}
Step 30: {'loss': 245.04888916015625, 'l1_loss': 2691.1494140625, 'l2_loss': 137.214599609375}
Step 40: {'loss': 232.91836547851562, 'l1_loss': 2579.314697265625, 'l2_loss': 129.55838012695312}
Step 50: {'loss': 220.88876342773438, 'l1_loss': 2454.52978515625, 'l2_loss': 122.5245361328125}
Step 60: {'loss': 216.31524658203125, 'l1_loss': 2409.60400390625, 'l2_loss': 119.74504089355469}
Step 70: {'loss': 207.1753387451172, 'l1_loss': 2295.95947265625, 'l2_loss': 115.1529541015625}
Step 80: {'loss': 197.4080352783203, 'l1_loss': 2160.083984375, 'l2_loss': 110.82364654541016}
Step 90: {'loss': 195.02960205078125, 'l1_loss': 2112.6689453125, 'l2_loss': 110.33668518066406}
Step 100: {'loss': 186.62132263183594, 'l1_loss': 1985.2532958984375, 'l2_loss': 107.02483367919922}
Step 110: {'loss': 191.6060028076172, 'l1_loss': 2021.717041015625, 'l2_loss': 110.54493713378906}
Step 120: {'loss': 179.97604370117188, 'l1_loss': 1839.861328125, 'l2_loss': 106.19068145751953}
Step 130: {'loss': 172.33900451660156, 'l1_loss': 1718.4560546875, 'l2_loss': 103.40901947021484}
Step 140: {'loss': 165.0399627685547, 'l1_loss': 1588.2647705078125, 'l2_loss': 101.31781768798828}
Step 150: {'loss': 158.50765991210938, 'l1_loss': 1467.154541015625, 'l2_loss': 99.62956237792969}
Step 160: {'loss': 154.9335479736328, 'l1_loss': 1398.00927734375, 'l2_loss': 98.81890106201172}
Step 170: {'loss': 144.07162475585938, 'l1_loss': 1228.3319091796875, 'l2_loss': 94.7455825805664}
Step 180: {'loss': 139.49649047851562, 'l1_loss': 1137.354248046875, 'l2_loss': 93.80888366699219}
Step 190: {'loss': 133.22402954101562, 'l1_loss': 1018.2975463867188, 'l2_loss': 92.29889678955078}
Step 200: {'loss': 132.48052978515625, 'l1_loss': 985.11865234375, 'l2_loss': 92.88027954101562}
Step 210: {'loss': 132.5753936767578, 'l1_loss': 969.4880981445312, 'l2_loss': 93.59889221191406}
Step 220: {'loss': 121.12246704101562, 'l1_loss': 789.5736083984375, 'l2_loss': 89.3441162109375}
Step 230: {'loss': 119.65937805175781, 'l1_loss': 759.8196411132812, 'l2_loss': 89.06977844238281}
Step 240: {'loss': 111.91761779785156, 'l1_loss': 625.9907836914062, 'l2_loss': 86.6832275390625}
Step 250: {'loss': 114.77115631103516, 'l1_loss': 654.0809326171875, 'l2_loss': 88.41033172607422}
Step 260: {'loss': 113.03602600097656, 'l1_loss': 610.3032836914062, 'l2_loss': 88.42597198486328}
Step 270: {'loss': 109.08537292480469, 'l1_loss': 536.251708984375, 'l2_loss': 87.43767547607422}
Step 280: {'loss': 107.71290588378906, 'l1_loss': 510.48101806640625, 'l2_loss': 87.0955810546875}
Step 290: {'loss': 108.06727600097656, 'l1_loss': 521.09765625, 'l2_loss': 87.02464294433594}
Step 300: {'loss': 105.70146179199219, 'l1_loss': 492.53955078125, 'l2_loss': 85.80111694335938}
Step 310: {'loss': 102.40155029296875, 'l1_loss': 433.07537841796875, 'l2_loss': 84.87985229492188}
Step 320: {'loss': 107.92668151855469, 'l1_loss': 532.673583984375, 'l2_loss': 86.42034912109375}
Step 330: {'loss': 99.7461929321289, 'l1_loss': 389.96563720703125, 'l2_loss': 83.94871520996094}
Step 340: {'loss': 97.21258544921875, 'l1_loss': 363.42279052734375, 'l2_loss': 82.4769515991211}
Step 350: {'loss': 102.23346710205078, 'l1_loss': 473.2292175292969, 'l2_loss': 83.10480499267578}
Step 360: {'loss': 100.48406219482422, 'l1_loss': 436.9228515625, 'l2_loss': 82.80772399902344}
Step 370: {'loss': 99.32516479492188, 'l1_loss': 432.22021484375, 'l2_loss': 81.83686828613281}
Step 380: {'loss': 99.71222686767578, 'l1_loss': 466.8548583984375, 'l2_loss': 80.83844757080078}
Step 390: {'loss': 97.71058654785156, 'l1_loss': 432.7586364746094, 'l2_loss': 80.20065307617188}
Step 400: {'loss': 94.66997528076172, 'l1_loss': 395.33221435546875, 'l2_loss': 78.65719604492188}
Step 410: {'loss': 94.16678619384766, 'l1_loss': 391.7842712402344, 'l2_loss': 78.29591369628906}
Step 420: {'loss': 92.96597290039062, 'l1_loss': 391.3336181640625, 'l2_loss': 77.11309814453125}
Step 430: {'loss': 94.4561996459961, 'l1_loss': 436.59796142578125, 'l2_loss': 76.79261779785156}
Step 440: {'loss': 90.9578628540039, 'l1_loss': 389.0408020019531, 'l2_loss': 75.19667053222656}
Step 450: {'loss': 91.0156021118164, 'l1_loss': 402.2669677734375, 'l2_loss': 74.72528839111328}
Step 460: {'loss': 91.79811096191406, 'l1_loss': 428.3752746582031, 'l2_loss': 74.46341705322266}
Step 470: {'loss': 90.9346694946289, 'l1_loss': 434.4246520996094, 'l2_loss': 73.35796356201172}
Step 480: {'loss': 90.15755462646484, 'l1_loss': 426.11572265625, 'l2_loss': 72.91321563720703}
Step 490: {'loss': 90.57203674316406, 'l1_loss': 459.4145202636719, 'l2_loss': 71.99569702148438}
Step 500: {'loss': 87.28498077392578, 'l1_loss': 388.3587951660156, 'l2_loss': 71.55097961425781}
Step 510: {'loss': 87.44689178466797, 'l1_loss': 405.14990234375, 'l2_loss': 71.04120635986328}
Step 520: {'loss': 85.26651000976562, 'l1_loss': 363.11688232421875, 'l2_loss': 70.5422592163086}
Step 530: {'loss': 86.07603454589844, 'l1_loss': 401.6518859863281, 'l2_loss': 69.81025695800781}
Step 540: {'loss': 85.00354766845703, 'l1_loss': 393.4061279296875, 'l2_loss': 69.06759643554688}
Step 550: {'loss': 83.06718444824219, 'l1_loss': 359.68359375, 'l2_loss': 68.48023986816406}
Step 560: {'loss': 84.79486083984375, 'l1_loss': 400.83941650390625, 'l2_loss': 68.56157684326172}
Step 570: {'loss': 81.8821792602539, 'l1_loss': 351.5361328125, 'l2_loss': 67.62112426757812}
Step 580: {'loss': 82.38034057617188, 'l1_loss': 366.05450439453125, 'l2_loss': 67.53851318359375}
Step 590: {'loss': 82.03129577636719, 'l1_loss': 389.4790344238281, 'l2_loss': 66.2524185180664}
Step 600: {'loss': 81.91231536865234, 'l1_loss': 374.7048034667969, 'l2_loss': 66.72442626953125}
Step 610: {'loss': 81.27520751953125, 'l1_loss': 379.46649169921875, 'l2_loss': 65.89682006835938}
Step 620: {'loss': 80.21475219726562, 'l1_loss': 356.1630859375, 'l2_loss': 65.76858520507812}
Step 630: {'loss': 80.27263641357422, 'l1_loss': 345.9415588378906, 'l2_loss': 66.2353744506836}
Step 640: {'loss': 80.63961791992188, 'l1_loss': 380.65374755859375, 'l2_loss': 65.21376037597656}
Step 650: {'loss': 80.66192626953125, 'l1_loss': 380.5043029785156, 'l2_loss': 65.24208068847656}
Step 660: {'loss': 80.89990997314453, 'l1_loss': 388.3609619140625, 'l2_loss': 65.16572570800781}
Step 670: {'loss': 80.06178283691406, 'l1_loss': 379.67120361328125, 'l2_loss': 64.67526245117188}
Step 680: {'loss': 78.18343353271484, 'l1_loss': 336.25286865234375, 'l2_loss': 64.53376770019531}
Step 690: {'loss': 78.72765350341797, 'l1_loss': 353.5157470703125, 'l2_loss': 64.38739776611328}
Step 700: {'loss': 77.78712463378906, 'l1_loss': 342.2115478515625, 'l2_loss': 63.899078369140625}
Step 710: {'loss': 77.36241912841797, 'l1_loss': 344.75390625, 'l2_loss': 63.37272644042969}
Step 720: {'loss': 76.56895446777344, 'l1_loss': 312.2169189453125, 'l2_loss': 63.88096237182617}
Step 730: {'loss': 76.14006042480469, 'l1_loss': 326.0904541015625, 'l2_loss': 62.8969612121582}
Step 740: {'loss': 76.12105560302734, 'l1_loss': 311.27508544921875, 'l2_loss': 63.47071075439453}
Step 750: {'loss': 75.59193420410156, 'l1_loss': 327.23333740234375, 'l2_loss': 62.30317687988281}
Step 760: {'loss': 74.84375762939453, 'l1_loss': 292.957763671875, 'l2_loss': 62.926307678222656}
Step 770: {'loss': 76.24011993408203, 'l1_loss': 342.32354736328125, 'l2_loss': 62.34764862060547}
Step 780: {'loss': 75.58480072021484, 'l1_loss': 329.8899841308594, 'l2_loss': 62.189842224121094}
Step 790: {'loss': 74.29119110107422, 'l1_loss': 274.5855407714844, 'l2_loss': 63.10911560058594}
Step 800: {'loss': 74.53396606445312, 'l1_loss': 315.20513916015625, 'l2_loss': 61.72646713256836}
Step 810: {'loss': 75.57906341552734, 'l1_loss': 309.6197814941406, 'l2_loss': 62.995140075683594}
Step 820: {'loss': 73.67020416259766, 'l1_loss': 282.4344482421875, 'l2_loss': 62.17411422729492}
Step 830: {'loss': 73.48743438720703, 'l1_loss': 288.95159912109375, 'l2_loss': 61.73059844970703}
Step 840: {'loss': 74.32258605957031, 'l1_loss': 303.0504150390625, 'l2_loss': 62.0017204284668}
Step 850: {'loss': 74.20846557617188, 'l1_loss': 324.18682861328125, 'l2_loss': 61.04180145263672}
Step 860: {'loss': 73.28892517089844, 'l1_loss': 281.5656433105469, 'l2_loss': 61.827964782714844}
Step 870: {'loss': 72.03409576416016, 'l1_loss': 276.2594299316406, 'l2_loss': 60.78577423095703}
Step 880: {'loss': 73.2977066040039, 'l1_loss': 304.55535888671875, 'l2_loss': 60.91701889038086}
Step 890: {'loss': 73.37908935546875, 'l1_loss': 280.50531005859375, 'l2_loss': 61.96112823486328}
Step 900: {'loss': 71.67841339111328, 'l1_loss': 263.07958984375, 'l2_loss': 60.95846176147461}
Step 910: {'loss': 72.67760467529297, 'l1_loss': 272.68023681640625, 'l2_loss': 61.5737190246582}
Step 920: {'loss': 73.25751495361328, 'l1_loss': 293.63385009765625, 'l2_loss': 61.31526565551758}
Step 930: {'loss': 71.22480773925781, 'l1_loss': 260.66534423828125, 'l2_loss': 60.60285568237305}
Step 940: {'loss': 70.67566680908203, 'l1_loss': 256.7689514160156, 'l2_loss': 60.21116638183594}
Step 950: {'loss': 70.70825958251953, 'l1_loss': 262.9980773925781, 'l2_loss': 59.99497985839844}
Step 960: {'loss': 70.34709930419922, 'l1_loss': 246.41744995117188, 'l2_loss': 60.2990608215332}
Step 970: {'loss': 70.30645751953125, 'l1_loss': 247.0946502685547, 'l2_loss': 60.23219299316406}
Step 980: {'loss': 70.82991790771484, 'l1_loss': 256.0318908691406, 'l2_loss': 60.3980827331543}
Step 990: {'loss': 70.32435607910156, 'l1_loss': 245.19815063476562, 'l2_loss': 60.32624816894531}
Step 1000: {'loss': 71.67159271240234, 'l1_loss': 265.9918518066406, 'l2_loss': 60.84095001220703}
Step 1010: {'loss': 69.43038940429688, 'l1_loss': 231.71841430664062, 'l2_loss': 59.97467041015625}
Step 1020: {'loss': 69.6204605102539, 'l1_loss': 255.71072387695312, 'l2_loss': 59.2037353515625}
Step 1030: {'loss': 69.92841339111328, 'l1_loss': 255.86904907226562, 'l2_loss': 59.50596618652344}
Step 1040: {'loss': 69.3418960571289, 'l1_loss': 226.04611206054688, 'l2_loss': 60.11762619018555}
Step 1050: {'loss': 69.13582611083984, 'l1_loss': 224.37564086914062, 'l2_loss': 59.98274230957031}
Step 1060: {'loss': 69.2343521118164, 'l1_loss': 231.035400390625, 'l2_loss': 59.813446044921875}
Step 1070: {'loss': 68.44081115722656, 'l1_loss': 233.65472412109375, 'l2_loss': 58.914608001708984}
Step 1080: {'loss': 68.70594024658203, 'l1_loss': 224.34811401367188, 'l2_loss': 59.55481719970703}
Step 1090: {'loss': 68.97200012207031, 'l1_loss': 233.24928283691406, 'l2_loss': 59.4670524597168}
Step 1100: {'loss': 68.83930969238281, 'l1_loss': 231.20388793945312, 'l2_loss': 59.4022331237793}
Step 1110: {'loss': 68.3204116821289, 'l1_loss': 225.65481567382812, 'l2_loss': 59.1224365234375}
Step 1120: {'loss': 68.71742248535156, 'l1_loss': 237.85958862304688, 'l2_loss': 59.02958679199219}
Step 1130: {'loss': 68.41975402832031, 'l1_loss': 228.0133056640625, 'l2_loss': 59.13084411621094}
Step 1140: {'loss': 67.8241195678711, 'l1_loss': 224.78314208984375, 'l2_loss': 58.666900634765625}
Step 1150: {'loss': 69.12091064453125, 'l1_loss': 236.91986083984375, 'l2_loss': 59.47552490234375}
Step 1160: {'loss': 67.93585205078125, 'l1_loss': 218.68209838867188, 'l2_loss': 59.02595520019531}
Step 1170: {'loss': 67.09481811523438, 'l1_loss': 211.85643005371094, 'l2_loss': 58.46068572998047}
Step 1180: {'loss': 67.25928497314453, 'l1_loss': 227.33148193359375, 'l2_loss': 58.00187301635742}
Step 1190: {'loss': 67.24166870117188, 'l1_loss': 214.86978149414062, 'l2_loss': 58.49299621582031}
Step 1200: {'loss': 67.19731903076172, 'l1_loss': 204.62619018554688, 'l2_loss': 58.858970642089844}
Step 1210: {'loss': 67.6855239868164, 'l1_loss': 222.30206298828125, 'l2_loss': 58.637874603271484}
Step 1220: {'loss': 67.15966033935547, 'l1_loss': 219.48089599609375, 'l2_loss': 58.22420883178711}
Step 1230: {'loss': 67.20417785644531, 'l1_loss': 225.3411407470703, 'l2_loss': 58.032413482666016}
Step 1240: {'loss': 67.11927795410156, 'l1_loss': 216.92950439453125, 'l2_loss': 58.29156494140625}
Step 1250: {'loss': 67.58104705810547, 'l1_loss': 222.90463256835938, 'l2_loss': 58.510501861572266}
Step 1260: {'loss': 67.71671295166016, 'l1_loss': 226.47171020507812, 'l2_loss': 58.509071350097656}
Step 1270: {'loss': 66.46773529052734, 'l1_loss': 216.0902099609375, 'l2_loss': 57.67174530029297}
Step 1280: {'loss': 66.37032318115234, 'l1_loss': 212.54066467285156, 'l2_loss': 57.720890045166016}
Step 1290: {'loss': 67.05248260498047, 'l1_loss': 222.46145629882812, 'l2_loss': 58.00812530517578}
Step 1300: {'loss': 66.73915100097656, 'l1_loss': 215.86276245117188, 'l2_loss': 57.954795837402344}
Step 1310: {'loss': 66.69149017333984, 'l1_loss': 203.55056762695312, 'l2_loss': 58.40436553955078}
Step 1320: {'loss': 66.22639465332031, 'l1_loss': 227.402587890625, 'l2_loss': 56.984169006347656}
Step 1330: {'loss': 66.4395751953125, 'l1_loss': 211.12142944335938, 'l2_loss': 57.85139083862305}
Step 1340: {'loss': 66.22203063964844, 'l1_loss': 218.6275634765625, 'l2_loss': 57.331947326660156}
Step 1350: {'loss': 66.26388549804688, 'l1_loss': 211.90573120117188, 'l2_loss': 57.64002227783203}
Step 1360: {'loss': 65.51251983642578, 'l1_loss': 212.33177185058594, 'l2_loss': 56.87895965576172}
Step 1370: {'loss': 65.98999786376953, 'l1_loss': 207.54962158203125, 'l2_loss': 57.54659652709961}
Step 1380: {'loss': 66.3077621459961, 'l1_loss': 216.6517333984375, 'l2_loss': 57.496429443359375}
Step 1390: {'loss': 65.5788803100586, 'l1_loss': 204.7777557373047, 'l2_loss': 57.247013092041016}
Step 1400: {'loss': 65.91079711914062, 'l1_loss': 216.38528442382812, 'l2_loss': 57.117706298828125}
Step 1410: {'loss': 66.01710510253906, 'l1_loss': 207.45095825195312, 'l2_loss': 57.560691833496094}
Step 1420: {'loss': 65.09190368652344, 'l1_loss': 213.94656372070312, 'l2_loss': 56.39643096923828}
Step 1430: {'loss': 65.59307861328125, 'l1_loss': 204.604736328125, 'l2_loss': 57.273536682128906}
Step 1440: {'loss': 65.69306945800781, 'l1_loss': 205.34658813476562, 'l2_loss': 57.339454650878906}
Step 1450: {'loss': 65.49201202392578, 'l1_loss': 207.1400604248047, 'l2_loss': 57.0635986328125}
Step 1460: {'loss': 64.80493927001953, 'l1_loss': 211.75018310546875, 'l2_loss': 56.19461441040039}
Step 1470: {'loss': 65.49897003173828, 'l1_loss': 209.21273803710938, 'l2_loss': 56.985260009765625}
Step 1480: {'loss': 65.5239486694336, 'l1_loss': 223.41653442382812, 'l2_loss': 56.413761138916016}
Step 1490: {'loss': 65.34506225585938, 'l1_loss': 215.16563415527344, 'l2_loss': 56.60063552856445}
Step 1500: {'loss': 65.65901184082031, 'l1_loss': 207.7958526611328, 'l2_loss': 57.21335983276367}
Step 1510: {'loss': 64.21858978271484, 'l1_loss': 204.91128540039062, 'l2_loss': 55.881011962890625}
Step 1520: {'loss': 65.35972595214844, 'l1_loss': 224.88839721679688, 'l2_loss': 56.226295471191406}
Step 1530: {'loss': 64.5631332397461, 'l1_loss': 197.49343872070312, 'l2_loss': 56.52281188964844}
Step 1540: {'loss': 64.84000396728516, 'l1_loss': 211.49392700195312, 'l2_loss': 56.2396240234375}
Step 1550: {'loss': 64.3582534790039, 'l1_loss': 203.86148071289062, 'l2_loss': 56.06403350830078}
Step 1560: {'loss': 64.6168441772461, 'l1_loss': 199.59832763671875, 'l2_loss': 56.49571228027344}
Step 1570: {'loss': 65.22203826904297, 'l1_loss': 210.53997802734375, 'l2_loss': 56.66071319580078}
Step 1580: {'loss': 64.7853012084961, 'l1_loss': 212.38400268554688, 'l2_loss': 56.15178680419922}
Step 1590: {'loss': 64.30648040771484, 'l1_loss': 202.45803833007812, 'l2_loss': 56.070274353027344}
Step 1600: {'loss': 64.89588928222656, 'l1_loss': 208.6661376953125, 'l2_loss': 56.40613555908203}
Step 1610: {'loss': 63.87861633300781, 'l1_loss': 207.6990966796875, 'l2_loss': 55.4315185546875}
Step 1620: {'loss': 65.58858489990234, 'l1_loss': 216.91903686523438, 'l2_loss': 56.775638580322266}
Step 1630: {'loss': 65.2552261352539, 'l1_loss': 221.10899353027344, 'l2_loss': 56.275840759277344}
Step 1640: {'loss': 63.61117172241211, 'l1_loss': 205.3541259765625, 'l2_loss': 55.26137924194336}
Step 1650: {'loss': 64.491943359375, 'l1_loss': 200.50563049316406, 'l2_loss': 56.338584899902344}
Step 1660: {'loss': 63.83415985107422, 'l1_loss': 207.12191772460938, 'l2_loss': 55.413761138916016}
Step 1670: {'loss': 64.37619018554688, 'l1_loss': 206.85336303710938, 'l2_loss': 55.95527648925781}
Step 1680: {'loss': 64.29703521728516, 'l1_loss': 213.32516479492188, 'l2_loss': 55.62591552734375}
Step 1690: {'loss': 62.754669189453125, 'l1_loss': 197.98165893554688, 'l2_loss': 54.7015495300293}
Step 1700: {'loss': 63.80342483520508, 'l1_loss': 205.38194274902344, 'l2_loss': 55.4509162902832}
Step 1710: {'loss': 64.55790710449219, 'l1_loss': 218.4120635986328, 'l2_loss': 55.689186096191406}
Step 1720: {'loss': 63.952606201171875, 'l1_loss': 214.98101806640625, 'l2_loss': 55.21882629394531}
Step 1730: {'loss': 63.91402053833008, 'l1_loss': 204.3565673828125, 'l2_loss': 55.60713195800781}
Step 1740: {'loss': 64.23973083496094, 'l1_loss': 221.83920288085938, 'l2_loss': 55.23455047607422}
Step 1750: {'loss': 63.705875396728516, 'l1_loss': 197.96649169921875, 'l2_loss': 55.65098571777344}
Step 1760: {'loss': 64.97576141357422, 'l1_loss': 218.26461791992188, 'l2_loss': 56.11436080932617}
Step 1770: {'loss': 64.12871551513672, 'l1_loss': 208.46151733398438, 'l2_loss': 55.656368255615234}
Step 1780: {'loss': 63.78364944458008, 'l1_loss': 208.48582458496094, 'l2_loss': 55.31373596191406}
Step 1790: {'loss': 63.638031005859375, 'l1_loss': 210.42242431640625, 'l2_loss': 55.090972900390625}
Step 1800: {'loss': 63.96046447753906, 'l1_loss': 212.99856567382812, 'l2_loss': 55.31243896484375}
Step 1810: {'loss': 63.64399337768555, 'l1_loss': 205.62107849121094, 'l2_loss': 55.28520965576172}
Step 1820: {'loss': 64.16390228271484, 'l1_loss': 212.57666015625, 'l2_loss': 55.521217346191406}
Step 1830: {'loss': 63.38886642456055, 'l1_loss': 201.41464233398438, 'l2_loss': 55.20092010498047}
Step 1840: {'loss': 63.34014892578125, 'l1_loss': 204.85536193847656, 'l2_loss': 55.01701354980469}
Step 1850: {'loss': 63.638084411621094, 'l1_loss': 208.62918090820312, 'l2_loss': 55.163856506347656}
Step 1860: {'loss': 63.39806365966797, 'l1_loss': 206.99990844726562, 'l2_loss': 54.977867126464844}
Step 1870: {'loss': 64.18278503417969, 'l1_loss': 213.8408660888672, 'l2_loss': 55.503746032714844}
Step 1880: {'loss': 62.159889221191406, 'l1_loss': 213.2886962890625, 'l2_loss': 53.50141906738281}
Step 1890: {'loss': 63.07252502441406, 'l1_loss': 204.70582580566406, 'l2_loss': 54.75362014770508}
Step 1900: {'loss': 63.388946533203125, 'l1_loss': 210.47122192382812, 'l2_loss': 54.83884811401367}
Step 1910: {'loss': 63.432838439941406, 'l1_loss': 211.15951538085938, 'l2_loss': 54.85845947265625}
Step 1920: {'loss': 64.0212631225586, 'l1_loss': 207.79519653320312, 'l2_loss': 55.57862091064453}
Step 1930: {'loss': 63.721595764160156, 'l1_loss': 204.14373779296875, 'l2_loss': 55.426063537597656}
Step 1940: {'loss': 63.0863037109375, 'l1_loss': 211.39956665039062, 'l2_loss': 54.50216293334961}
Step 1950: {'loss': 63.73795700073242, 'l1_loss': 206.96078491210938, 'l2_loss': 55.33736801147461}
Step 1960: {'loss': 63.652400970458984, 'l1_loss': 216.12725830078125, 'l2_loss': 54.876129150390625}
Step 1970: {'loss': 62.569427490234375, 'l1_loss': 211.1971435546875, 'l2_loss': 53.99247360229492}
Step 1980: {'loss': 63.03730392456055, 'l1_loss': 205.21951293945312, 'l2_loss': 54.70253372192383}
Step 1990: {'loss': 63.39080810546875, 'l1_loss': 204.40731811523438, 'l2_loss': 55.08686828613281}
Step 2000: {'loss': 63.37541961669922, 'l1_loss': 212.02902221679688, 'l2_loss': 54.76924133300781}
Step 2010: {'loss': 62.96628189086914, 'l1_loss': 219.2382354736328, 'l2_loss': 54.065547943115234}
Step 2020: {'loss': 62.74567794799805, 'l1_loss': 204.974365234375, 'l2_loss': 54.424522399902344}
Step 2030: {'loss': 63.61088562011719, 'l1_loss': 211.1604461669922, 'l2_loss': 55.04001235961914}
Step 2040: {'loss': 63.41742706298828, 'l1_loss': 212.60617065429688, 'l2_loss': 54.78764343261719}
Step 2050: {'loss': 62.024871826171875, 'l1_loss': 202.99053955078125, 'l2_loss': 53.78268814086914}
Step 2060: {'loss': 62.670772552490234, 'l1_loss': 204.91824340820312, 'l2_loss': 54.34784698486328}
Step 2070: {'loss': 62.59236145019531, 'l1_loss': 207.979736328125, 'l2_loss': 54.152503967285156}
Step 2080: {'loss': 62.99086380004883, 'l1_loss': 211.66867065429688, 'l2_loss': 54.399147033691406}
Step 2090: {'loss': 61.84820556640625, 'l1_loss': 210.37576293945312, 'l2_loss': 53.31193542480469}
Step 2100: {'loss': 62.95623016357422, 'l1_loss': 209.48666381835938, 'l2_loss': 54.45698165893555}
Step 2110: {'loss': 62.323280334472656, 'l1_loss': 207.2908477783203, 'l2_loss': 53.91120147705078}
Step 2120: {'loss': 63.16386032104492, 'l1_loss': 212.45962524414062, 'l2_loss': 54.54030227661133}
Step 2130: {'loss': 62.31937026977539, 'l1_loss': 216.84011840820312, 'l2_loss': 53.522216796875}
Step 2140: {'loss': 62.295623779296875, 'l1_loss': 215.2205352783203, 'l2_loss': 53.5657844543457}
Step 2150: {'loss': 61.813655853271484, 'l1_loss': 210.80706787109375, 'l2_loss': 53.25988006591797}
Step 2160: {'loss': 62.22859573364258, 'l1_loss': 213.6026611328125, 'l2_loss': 53.56499099731445}
Step 2170: {'loss': 61.986236572265625, 'l1_loss': 210.29421997070312, 'l2_loss': 53.45087432861328}
Step 2180: {'loss': 62.96766662597656, 'l1_loss': 212.82931518554688, 'l2_loss': 54.331932067871094}
Step 2190: {'loss': 62.686309814453125, 'l1_loss': 223.94912719726562, 'l2_loss': 53.60261535644531}
Step 2200: {'loss': 62.156646728515625, 'l1_loss': 203.04367065429688, 'l2_loss': 53.91136169433594}
Step 2210: {'loss': 62.47418212890625, 'l1_loss': 217.30038452148438, 'l2_loss': 53.662620544433594}
Step 2220: {'loss': 63.02334213256836, 'l1_loss': 219.31959533691406, 'l2_loss': 54.131168365478516}
Step 2230: {'loss': 62.63554000854492, 'l1_loss': 218.48114013671875, 'l2_loss': 53.7788200378418}
Step 2240: {'loss': 62.501747131347656, 'l1_loss': 207.33795166015625, 'l2_loss': 54.08512878417969}
Step 2250: {'loss': 61.77992630004883, 'l1_loss': 208.05865478515625, 'l2_loss': 53.34046173095703}
Step 2260: {'loss': 61.49113464355469, 'l1_loss': 211.51922607421875, 'l2_loss': 52.912689208984375}
Step 2270: {'loss': 61.77246856689453, 'l1_loss': 214.05238342285156, 'l2_loss': 53.08948516845703}
Step 2280: {'loss': 61.342750549316406, 'l1_loss': 212.35772705078125, 'l2_loss': 52.72721862792969}
Step 2290: {'loss': 62.67946243286133, 'l1_loss': 212.3917694091797, 'l2_loss': 54.066619873046875}
Step 2300: {'loss': 61.3054313659668, 'l1_loss': 215.24673461914062, 'l2_loss': 52.57811737060547}
Step 2310: {'loss': 61.929664611816406, 'l1_loss': 211.16079711914062, 'l2_loss': 53.36799621582031}
Step 2320: {'loss': 61.780609130859375, 'l1_loss': 207.71087646484375, 'l2_loss': 53.35594940185547}
Step 2330: {'loss': 61.93442916870117, 'l1_loss': 215.8070526123047, 'l2_loss': 53.17912292480469}
Step 2340: {'loss': 60.99917984008789, 'l1_loss': 209.36959838867188, 'l2_loss': 52.50035095214844}
Step 2350: {'loss': 61.825138092041016, 'l1_loss': 209.80514526367188, 'l2_loss': 53.315834045410156}
Step 2360: {'loss': 62.484859466552734, 'l1_loss': 215.3665771484375, 'l2_loss': 53.752445220947266}
Step 2370: {'loss': 62.51559066772461, 'l1_loss': 220.51119995117188, 'l2_loss': 53.57617950439453}
Step 2380: {'loss': 60.852230072021484, 'l1_loss': 215.93585205078125, 'l2_loss': 52.09708023071289}
Step 2390: {'loss': 61.43061447143555, 'l1_loss': 210.4690704345703, 'l2_loss': 52.895172119140625}
Step 2400: {'loss': 61.65476608276367, 'l1_loss': 214.4602813720703, 'l2_loss': 52.962181091308594}
Step 2410: {'loss': 61.70557403564453, 'l1_loss': 208.2456512451172, 'l2_loss': 53.260101318359375}
Step 2420: {'loss': 61.34031295776367, 'l1_loss': 216.98812866210938, 'l2_loss': 52.54541778564453}
Step 2430: {'loss': 61.894683837890625, 'l1_loss': 220.46095275878906, 'l2_loss': 52.960147857666016}
Step 2440: {'loss': 62.14889144897461, 'l1_loss': 214.0100555419922, 'l2_loss': 53.472171783447266}
Step 2450: {'loss': 61.62594223022461, 'l1_loss': 214.17733764648438, 'l2_loss': 52.94270706176758}
Step 2460: {'loss': 61.70455551147461, 'l1_loss': 210.64535522460938, 'l2_loss': 53.16552734375}
Step 2470: {'loss': 62.17851257324219, 'l1_loss': 217.62059020996094, 'l2_loss': 53.35650634765625}
Step 2480: {'loss': 61.9212532043457, 'l1_loss': 215.732666015625, 'l2_loss': 53.17344665527344}
Step 2490: {'loss': 61.650657653808594, 'l1_loss': 209.71339416503906, 'l2_loss': 53.15170669555664}
Step 2500: {'loss': 61.19190216064453, 'l1_loss': 217.49603271484375, 'l2_loss': 52.37917709350586}
Step 2510: {'loss': 62.01823043823242, 'l1_loss': 223.8741455078125, 'l2_loss': 52.95135498046875}
Step 2520: {'loss': 61.761051177978516, 'l1_loss': 210.5, 'l2_loss': 53.227752685546875}
Step 2530: {'loss': 60.70975875854492, 'l1_loss': 209.32460021972656, 'l2_loss': 52.221900939941406}
Step 2540: {'loss': 61.628639221191406, 'l1_loss': 232.68875122070312, 'l2_loss': 52.20662307739258}
Step 2550: {'loss': 61.1657829284668, 'l1_loss': 209.88287353515625, 'l2_loss': 52.66236114501953}
Step 2560: {'loss': 61.22361755371094, 'l1_loss': 219.11749267578125, 'l2_loss': 52.34536361694336}
Step 2570: {'loss': 61.735694885253906, 'l1_loss': 224.10696411132812, 'l2_loss': 52.65776062011719}
Step 2580: {'loss': 61.00856399536133, 'l1_loss': 225.5338592529297, 'l2_loss': 51.87823486328125}
Step 2590: {'loss': 60.821739196777344, 'l1_loss': 214.24464416503906, 'l2_loss': 52.140262603759766}
Step 2600: {'loss': 61.7321891784668, 'l1_loss': 224.911376953125, 'l2_loss': 52.625831604003906}
Step 2610: {'loss': 61.08353042602539, 'l1_loss': 218.9293975830078, 'l2_loss': 52.2181282043457}
Step 2620: {'loss': 61.62458419799805, 'l1_loss': 214.9608154296875, 'l2_loss': 52.9157600402832}
Step 2630: {'loss': 61.131160736083984, 'l1_loss': 218.65478515625, 'l2_loss': 52.27241897583008}
Step 2640: {'loss': 61.008148193359375, 'l1_loss': 215.9212646484375, 'l2_loss': 52.26048278808594}
Step 2650: {'loss': 61.928531646728516, 'l1_loss': 219.2061767578125, 'l2_loss': 53.0538215637207}
Step 2660: {'loss': 61.21242904663086, 'l1_loss': 212.33242797851562, 'l2_loss': 52.61182403564453}
Step 2670: {'loss': 61.270912170410156, 'l1_loss': 223.77650451660156, 'l2_loss': 52.208457946777344}
Step 2680: {'loss': 60.8325080871582, 'l1_loss': 215.84222412109375, 'l2_loss': 52.0892448425293}
Step 2690: {'loss': 61.5377082824707, 'l1_loss': 214.2325897216797, 'l2_loss': 52.86011505126953}
Step 2700: {'loss': 61.34671401977539, 'l1_loss': 218.85487365722656, 'l2_loss': 52.48619079589844}
Step 2710: {'loss': 60.83270263671875, 'l1_loss': 211.97645568847656, 'l2_loss': 52.24748992919922}
Step 2720: {'loss': 60.75027847290039, 'l1_loss': 218.73385620117188, 'l2_loss': 51.89600372314453}
Step 2730: {'loss': 61.334041595458984, 'l1_loss': 213.64804077148438, 'l2_loss': 52.68280029296875}
Step 2740: {'loss': 60.17192077636719, 'l1_loss': 212.98870849609375, 'l2_loss': 51.54503631591797}
Step 2750: {'loss': 60.87092208862305, 'l1_loss': 215.81838989257812, 'l2_loss': 52.12956237792969}
Step 2760: {'loss': 60.84353256225586, 'l1_loss': 221.6981201171875, 'l2_loss': 51.86820602416992}
Step 2770: {'loss': 61.175594329833984, 'l1_loss': 217.6648406982422, 'l2_loss': 52.36058807373047}
Step 2780: {'loss': 60.30876159667969, 'l1_loss': 211.5657196044922, 'l2_loss': 51.742156982421875}
Step 2790: {'loss': 60.686283111572266, 'l1_loss': 223.07940673828125, 'l2_loss': 51.65867233276367}
Step 2800: {'loss': 61.59156036376953, 'l1_loss': 217.73568725585938, 'l2_loss': 52.773372650146484}
Step 2810: {'loss': 61.51420211791992, 'l1_loss': 226.14794921875, 'l2_loss': 52.360137939453125}
Step 2820: {'loss': 60.26144027709961, 'l1_loss': 217.53201293945312, 'l2_loss': 51.45020294189453}
Step 2830: {'loss': 61.1464958190918, 'l1_loss': 229.86361694335938, 'l2_loss': 51.84161376953125}
Step 2840: {'loss': 60.920753479003906, 'l1_loss': 218.48837280273438, 'l2_loss': 52.074989318847656}
Step 2850: {'loss': 61.5888786315918, 'l1_loss': 224.02456665039062, 'l2_loss': 52.52174377441406}
Step 2860: {'loss': 60.48878479003906, 'l1_loss': 218.25173950195312, 'l2_loss': 51.64957046508789}
Step 2870: {'loss': 60.57172775268555, 'l1_loss': 217.69903564453125, 'l2_loss': 51.75811767578125}
Step 2880: {'loss': 60.82008361816406, 'l1_loss': 216.73098754882812, 'l2_loss': 52.045555114746094}
Step 2890: {'loss': 60.41499710083008, 'l1_loss': 217.81483459472656, 'l2_loss': 51.598838806152344}
Step 2900: {'loss': 60.588932037353516, 'l1_loss': 214.632568359375, 'l2_loss': 51.899085998535156}
Step 2910: {'loss': 61.08580780029297, 'l1_loss': 216.98597717285156, 'l2_loss': 52.30094909667969}
Step 2920: {'loss': 60.13868713378906, 'l1_loss': 217.730224609375, 'l2_loss': 51.32395935058594}
Step 2930: {'loss': 60.587100982666016, 'l1_loss': 224.17552185058594, 'l2_loss': 51.51689147949219}
Step 2940: {'loss': 59.8867073059082, 'l1_loss': 218.45657348632812, 'l2_loss': 51.04478454589844}
Step 2950: {'loss': 60.76165771484375, 'l1_loss': 215.50320434570312, 'l2_loss': 52.041160583496094}
Step 2960: {'loss': 59.726287841796875, 'l1_loss': 213.52377319335938, 'l2_loss': 51.080989837646484}
Step 2970: {'loss': 60.31797409057617, 'l1_loss': 223.17115783691406, 'l2_loss': 51.28966522216797}
Step 2980: {'loss': 61.37961959838867, 'l1_loss': 223.75054931640625, 'l2_loss': 52.326393127441406}
Step 2990: {'loss': 61.474205017089844, 'l1_loss': 225.80113220214844, 'l2_loss': 52.3388557434082}
Step 3000: {'loss': 60.43565368652344, 'l1_loss': 225.51693725585938, 'l2_loss': 51.3143196105957}
Step 3010: {'loss': 60.258384704589844, 'l1_loss': 221.4336395263672, 'l2_loss': 51.29662322998047}
Step 3020: {'loss': 61.48470687866211, 'l1_loss': 226.15203857421875, 'l2_loss': 52.33505630493164}
Step 3030: {'loss': 59.99361801147461, 'l1_loss': 225.34286499023438, 'l2_loss': 50.87653350830078}
Step 3040: {'loss': 60.343238830566406, 'l1_loss': 223.315673828125, 'l2_loss': 51.308372497558594}
Step 3050: {'loss': 60.66294860839844, 'l1_loss': 224.11605834960938, 'l2_loss': 51.59627151489258}
Step 3060: {'loss': 59.729286193847656, 'l1_loss': 222.07510375976562, 'l2_loss': 50.7440185546875}
Step 3070: {'loss': 59.73191452026367, 'l1_loss': 215.571533203125, 'l2_loss': 51.006160736083984}
Step 3080: {'loss': 60.648189544677734, 'l1_loss': 225.1715850830078, 'l2_loss': 51.53515625}
Step 3090: {'loss': 60.81667709350586, 'l1_loss': 220.71493530273438, 'l2_loss': 51.88642883300781}
Step 3100: {'loss': 60.337623596191406, 'l1_loss': 224.21829223632812, 'l2_loss': 51.26726531982422}
Step 3110: {'loss': 60.612491607666016, 'l1_loss': 221.730712890625, 'l2_loss': 51.64118194580078}
Step 3120: {'loss': 60.356178283691406, 'l1_loss': 224.83197021484375, 'l2_loss': 51.26356887817383}
Step 3130: {'loss': 60.18883514404297, 'l1_loss': 222.04986572265625, 'l2_loss': 51.20573043823242}
Step 3140: {'loss': 59.582542419433594, 'l1_loss': 220.66908264160156, 'l2_loss': 50.65696716308594}
Step 3150: {'loss': 59.86265182495117, 'l1_loss': 227.755859375, 'l2_loss': 50.64960479736328}
Step 3160: {'loss': 59.96321105957031, 'l1_loss': 221.79229736328125, 'l2_loss': 50.99372100830078}
Step 3170: {'loss': 60.60095977783203, 'l1_loss': 225.3807830810547, 'l2_loss': 51.48756408691406}
Step 3180: {'loss': 59.79247283935547, 'l1_loss': 217.55300903320312, 'l2_loss': 50.992279052734375}
Step 3190: {'loss': 60.1330451965332, 'l1_loss': 222.13560485839844, 'l2_loss': 51.14850997924805}
Step 3200: {'loss': 60.0892219543457, 'l1_loss': 222.8390350341797, 'l2_loss': 51.076988220214844}
Step 3210: {'loss': 60.75810623168945, 'l1_loss': 229.27227783203125, 'l2_loss': 51.48928451538086}
Step 3220: {'loss': 60.18720626831055, 'l1_loss': 218.94903564453125, 'l2_loss': 51.33241653442383}
Step 3230: {'loss': 60.69868469238281, 'l1_loss': 219.93026733398438, 'l2_loss': 51.802330017089844}
Step 3240: {'loss': 59.5684700012207, 'l1_loss': 223.01409912109375, 'l2_loss': 50.550445556640625}
Step 3250: {'loss': 60.07242202758789, 'l1_loss': 222.42759704589844, 'l2_loss': 51.07611083984375}
Step 3260: {'loss': 59.733333587646484, 'l1_loss': 223.56787109375, 'l2_loss': 50.696144104003906}
Step 3270: {'loss': 60.1987190246582, 'l1_loss': 219.7977752685547, 'l2_loss': 51.309085845947266}
Step 3280: {'loss': 60.177616119384766, 'l1_loss': 223.2135772705078, 'l2_loss': 51.15210723876953}
Step 3290: {'loss': 59.90346908569336, 'l1_loss': 220.75303649902344, 'l2_loss': 50.974082946777344}
Step 3300: {'loss': 60.42041015625, 'l1_loss': 221.1431427001953, 'l2_loss': 51.47877502441406}
Step 3310: {'loss': 59.5762825012207, 'l1_loss': 217.16720581054688, 'l2_loss': 50.79339599609375}
Step 3320: {'loss': 59.89609909057617, 'l1_loss': 219.95550537109375, 'l2_loss': 51.000953674316406}
Step 3330: {'loss': 59.560794830322266, 'l1_loss': 218.58140563964844, 'l2_loss': 50.72197723388672}
Step 3340: {'loss': 60.68039321899414, 'l1_loss': 227.5892333984375, 'l2_loss': 51.473419189453125}
Step 3350: {'loss': 60.04731750488281, 'l1_loss': 222.33572387695312, 'l2_loss': 51.05517578125}
Step 3360: {'loss': 59.677947998046875, 'l1_loss': 223.5271453857422, 'l2_loss': 50.642608642578125}
Step 3370: {'loss': 59.570743560791016, 'l1_loss': 219.41851806640625, 'l2_loss': 50.69783020019531}
Step 3380: {'loss': 60.548831939697266, 'l1_loss': 217.08258056640625, 'l2_loss': 51.76917266845703}
Step 3390: {'loss': 60.166500091552734, 'l1_loss': 221.20840454101562, 'l2_loss': 51.22166442871094}
Step 3400: {'loss': 59.561824798583984, 'l1_loss': 229.44061279296875, 'l2_loss': 50.289207458496094}
Step 3410: {'loss': 60.34424591064453, 'l1_loss': 225.27920532226562, 'l2_loss': 51.23647689819336}
Step 3420: {'loss': 59.940242767333984, 'l1_loss': 226.495849609375, 'l2_loss': 50.78310012817383}
Step 3430: {'loss': 60.071800231933594, 'l1_loss': 224.24267578125, 'l2_loss': 51.007545471191406}
Step 3440: {'loss': 60.10230255126953, 'l1_loss': 222.97976684570312, 'l2_loss': 51.087745666503906}
Step 3450: {'loss': 59.06503677368164, 'l1_loss': 226.28302001953125, 'l2_loss': 49.91632843017578}
Step 3460: {'loss': 59.89065170288086, 'l1_loss': 226.8739013671875, 'l2_loss': 50.718326568603516}
Step 3470: {'loss': 59.10639953613281, 'l1_loss': 225.06265258789062, 'l2_loss': 50.00740051269531}
Step 3480: {'loss': 59.57106018066406, 'l1_loss': 222.10220336914062, 'l2_loss': 50.591636657714844}
Step 3490: {'loss': 59.951629638671875, 'l1_loss': 230.16029357910156, 'l2_loss': 50.64883804321289}
Step 3500: {'loss': 58.796783447265625, 'l1_loss': 221.08848571777344, 'l2_loss': 49.85658264160156}
Step 3510: {'loss': 59.57569885253906, 'l1_loss': 225.95620727539062, 'l2_loss': 50.44059753417969}
Step 3520: {'loss': 59.70620346069336, 'l1_loss': 226.86441040039062, 'l2_loss': 50.53791427612305}
Step 3530: {'loss': 59.77472686767578, 'l1_loss': 224.8267822265625, 'l2_loss': 50.68694305419922}
Step 3540: {'loss': 59.72078323364258, 'l1_loss': 223.06396484375, 'l2_loss': 50.70615768432617}
Step 3550: {'loss': 59.81907653808594, 'l1_loss': 222.99725341796875, 'l2_loss': 50.80433654785156}
Step 3560: {'loss': 59.630184173583984, 'l1_loss': 226.522705078125, 'l2_loss': 50.474700927734375}
Step 3570: {'loss': 59.995506286621094, 'l1_loss': 224.51416015625, 'l2_loss': 50.91935729980469}
Step 3580: {'loss': 59.99306106567383, 'l1_loss': 222.72567749023438, 'l2_loss': 50.98829650878906}
Step 3590: {'loss': 59.71453857421875, 'l1_loss': 229.751220703125, 'l2_loss': 50.43244171142578}
Step 3600: {'loss': 59.94892883300781, 'l1_loss': 229.57412719726562, 'l2_loss': 50.67197799682617}
Step 3610: {'loss': 59.03701400756836, 'l1_loss': 226.5582733154297, 'l2_loss': 49.881771087646484}
Step 3620: {'loss': 58.9951286315918, 'l1_loss': 227.04083251953125, 'l2_loss': 49.81882858276367}
Step 3630: {'loss': 59.275516510009766, 'l1_loss': 225.83578491210938, 'l2_loss': 50.15000534057617}
Step 3640: {'loss': 59.27517318725586, 'l1_loss': 229.62205505371094, 'l2_loss': 49.998291015625}
Step 3650: {'loss': 59.60000991821289, 'l1_loss': 229.79995727539062, 'l2_loss': 50.31559753417969}
Step 3660: {'loss': 59.79255676269531, 'l1_loss': 227.93411254882812, 'l2_loss': 50.577632904052734}
Step 3670: {'loss': 59.10883331298828, 'l1_loss': 226.8095245361328, 'l2_loss': 49.94398880004883}
Step 3680: {'loss': 59.25504684448242, 'l1_loss': 229.46823120117188, 'l2_loss': 49.985565185546875}
Step 3690: {'loss': 59.506656646728516, 'l1_loss': 228.75491333007812, 'l2_loss': 50.26439666748047}
Step 3700: {'loss': 59.810020446777344, 'l1_loss': 226.35572814941406, 'l2_loss': 50.663352966308594}
Step 3710: {'loss': 58.84048080444336, 'l1_loss': 225.8568115234375, 'l2_loss': 49.71438217163086}
Step 3720: {'loss': 59.64204025268555, 'l1_loss': 232.18710327148438, 'l2_loss': 50.261207580566406}
Step 3730: {'loss': 60.24693298339844, 'l1_loss': 230.1400909423828, 'l2_loss': 50.949607849121094}
Step 3740: {'loss': 58.844146728515625, 'l1_loss': 222.8952178955078, 'l2_loss': 49.8360481262207}
Step 3750: {'loss': 58.720706939697266, 'l1_loss': 223.67762756347656, 'l2_loss': 49.68351364135742}
Step 3760: {'loss': 59.15730285644531, 'l1_loss': 228.0355224609375, 'l2_loss': 49.94228744506836}
Step 3770: {'loss': 60.27824020385742, 'l1_loss': 228.63772583007812, 'l2_loss': 51.041812896728516}
Step 3780: {'loss': 59.66984939575195, 'l1_loss': 224.78829956054688, 'l2_loss': 50.586891174316406}
Step 3790: {'loss': 59.70514678955078, 'l1_loss': 235.78457641601562, 'l2_loss': 50.18016052246094}
Step 3800: {'loss': 59.14340591430664, 'l1_loss': 230.18893432617188, 'l2_loss': 49.8424186706543}
Step 3810: {'loss': 59.977088928222656, 'l1_loss': 231.07159423828125, 'l2_loss': 50.64122009277344}
Step 3820: {'loss': 59.93838119506836, 'l1_loss': 233.65597534179688, 'l2_loss': 50.50151824951172}
Step 3830: {'loss': 59.505985260009766, 'l1_loss': 225.09051513671875, 'l2_loss': 50.410858154296875}
Step 3840: {'loss': 59.08087921142578, 'l1_loss': 230.40394592285156, 'l2_loss': 49.77423095703125}
Step 3850: {'loss': 58.81081008911133, 'l1_loss': 225.08123779296875, 'l2_loss': 49.71556091308594}
Step 3860: {'loss': 59.614933013916016, 'l1_loss': 232.3051300048828, 'l2_loss': 50.233612060546875}
Step 3870: {'loss': 59.17726516723633, 'l1_loss': 232.6851806640625, 'l2_loss': 49.778160095214844}
Step 3880: {'loss': 58.58750534057617, 'l1_loss': 231.99844360351562, 'l2_loss': 49.219459533691406}
Step 3890: {'loss': 59.67353057861328, 'l1_loss': 226.3916778564453, 'l2_loss': 50.52677917480469}
Step 3900: {'loss': 58.597129821777344, 'l1_loss': 228.63577270507812, 'l2_loss': 49.36393737792969}
Step 3910: {'loss': 59.468753814697266, 'l1_loss': 228.89126586914062, 'l2_loss': 50.223453521728516}
Step 3920: {'loss': 59.107643127441406, 'l1_loss': 235.99777221679688, 'l2_loss': 49.57847595214844}
Step 3930: {'loss': 59.45444107055664, 'l1_loss': 227.8863983154297, 'l2_loss': 50.24637222290039}
Step 3940: {'loss': 59.3514289855957, 'l1_loss': 228.84963989257812, 'l2_loss': 50.108299255371094}
Step 3950: {'loss': 58.941627502441406, 'l1_loss': 226.7804718017578, 'l2_loss': 49.78078079223633}
Step 3960: {'loss': 59.34341049194336, 'l1_loss': 233.26986694335938, 'l2_loss': 49.925533294677734}
Step 3970: {'loss': 59.97040557861328, 'l1_loss': 235.71356201171875, 'l2_loss': 50.45244216918945}
Step 3980: {'loss': 59.365821838378906, 'l1_loss': 231.5996551513672, 'l2_loss': 50.01283264160156}
Step 3990: {'loss': 59.119049072265625, 'l1_loss': 232.7845458984375, 'l2_loss': 49.71760559082031}
Step 4000: {'loss': 59.319984436035156, 'l1_loss': 225.7364959716797, 'l2_loss': 50.19979476928711}
Step 4010: {'loss': 58.91956329345703, 'l1_loss': 232.27125549316406, 'l2_loss': 49.539459228515625}
Step 4020: {'loss': 58.48780822753906, 'l1_loss': 229.32107543945312, 'l2_loss': 49.22687530517578}
Step 4030: {'loss': 58.536521911621094, 'l1_loss': 228.8484649658203, 'l2_loss': 49.292152404785156}
Step 4040: {'loss': 59.16115951538086, 'l1_loss': 231.25909423828125, 'l2_loss': 49.82072830200195}
Step 4050: {'loss': 59.198585510253906, 'l1_loss': 225.89801025390625, 'l2_loss': 50.073646545410156}
Step 4060: {'loss': 59.51765060424805, 'l1_loss': 228.42141723632812, 'l2_loss': 50.29197692871094}
Step 4070: {'loss': 59.93558883666992, 'l1_loss': 234.8480682373047, 'l2_loss': 50.452674865722656}
Step 4080: {'loss': 58.58513259887695, 'l1_loss': 227.83480834960938, 'l2_loss': 49.38346862792969}
Step 4090: {'loss': 59.78807067871094, 'l1_loss': 229.12258911132812, 'l2_loss': 50.53571319580078}
Step 4100: {'loss': 58.210731506347656, 'l1_loss': 226.0348358154297, 'l2_loss': 49.080116271972656}
Step 4110: {'loss': 58.9737663269043, 'l1_loss': 233.61227416992188, 'l2_loss': 49.53907012939453}
Step 4120: {'loss': 60.3016242980957, 'l1_loss': 234.65750122070312, 'l2_loss': 50.825721740722656}
Step 4130: {'loss': 59.29401397705078, 'l1_loss': 232.2579345703125, 'l2_loss': 49.91682434082031}
Step 4140: {'loss': 59.387725830078125, 'l1_loss': 233.10391235351562, 'l2_loss': 49.973968505859375}
Step 4150: {'loss': 59.06755828857422, 'l1_loss': 227.54051208496094, 'l2_loss': 49.87773895263672}
Step 4160: {'loss': 58.621212005615234, 'l1_loss': 234.91067504882812, 'l2_loss': 49.13570785522461}
Step 4170: {'loss': 58.34341049194336, 'l1_loss': 233.84466552734375, 'l2_loss': 48.90148162841797}
Step 4180: {'loss': 58.19660568237305, 'l1_loss': 234.50204467773438, 'l2_loss': 48.7286376953125}
Step 4190: {'loss': 58.14619064331055, 'l1_loss': 235.91448974609375, 'l2_loss': 48.618690490722656}
Step 4200: {'loss': 58.8524055480957, 'l1_loss': 232.85531616210938, 'l2_loss': 49.45012664794922}
Step 4210: {'loss': 59.185157775878906, 'l1_loss': 231.46641540527344, 'l2_loss': 49.83934020996094}
Step 4220: {'loss': 58.747215270996094, 'l1_loss': 234.07958984375, 'l2_loss': 49.29764938354492}
Step 4230: {'loss': 59.39805603027344, 'l1_loss': 232.22640991210938, 'l2_loss': 50.019508361816406}
Step 4240: {'loss': 58.3319206237793, 'l1_loss': 232.93263244628906, 'l2_loss': 48.92646408081055}
Step 4250: {'loss': 58.089298248291016, 'l1_loss': 234.37030029296875, 'l2_loss': 48.627525329589844}
Step 4260: {'loss': 57.71055221557617, 'l1_loss': 234.0242156982422, 'l2_loss': 48.26245880126953}
Step 4270: {'loss': 59.22309494018555, 'l1_loss': 230.80567932128906, 'l2_loss': 49.89975357055664}
Step 4280: {'loss': 58.54302978515625, 'l1_loss': 231.3477783203125, 'l2_loss': 49.200416564941406}
Step 4290: {'loss': 58.316837310791016, 'l1_loss': 231.17080688476562, 'l2_loss': 48.98112106323242}
Step 4300: {'loss': 58.84016418457031, 'l1_loss': 233.5897979736328, 'l2_loss': 49.40728759765625}
Step 4310: {'loss': 58.85259246826172, 'l1_loss': 239.73126220703125, 'l2_loss': 49.175540924072266}
Step 4320: {'loss': 58.65668869018555, 'l1_loss': 229.6962890625, 'l2_loss': 49.380645751953125}
Step 4330: {'loss': 58.6237678527832, 'l1_loss': 236.62744140625, 'l2_loss': 49.071044921875}
Step 4340: {'loss': 57.91904067993164, 'l1_loss': 233.38900756835938, 'l2_loss': 48.49506759643555}
Step 4350: {'loss': 58.579898834228516, 'l1_loss': 234.71737670898438, 'l2_loss': 49.10240173339844}
Step 4360: {'loss': 58.32712173461914, 'l1_loss': 238.33047485351562, 'l2_loss': 48.705360412597656}
Step 4370: {'loss': 58.22554397583008, 'l1_loss': 236.36093139648438, 'l2_loss': 48.6826057434082}
Step 4380: {'loss': 58.31560134887695, 'l1_loss': 233.13148498535156, 'l2_loss': 48.90503692626953}
Step 4390: {'loss': 58.864845275878906, 'l1_loss': 233.31045532226562, 'l2_loss': 49.44416809082031}
Step 4400: {'loss': 58.003665924072266, 'l1_loss': 231.20753479003906, 'l2_loss': 48.666229248046875}
Step 4410: {'loss': 59.217140197753906, 'l1_loss': 233.05526733398438, 'l2_loss': 49.80730438232422}
Step 4420: {'loss': 58.29669189453125, 'l1_loss': 236.83480834960938, 'l2_loss': 48.737159729003906}
Step 4430: {'loss': 58.24351119995117, 'l1_loss': 233.266357421875, 'l2_loss': 48.826019287109375}
Step 4440: {'loss': 58.73734664916992, 'l1_loss': 230.44790649414062, 'l2_loss': 49.43130111694336}
Step 4450: {'loss': 59.036598205566406, 'l1_loss': 235.3636474609375, 'l2_loss': 49.537620544433594}
Step 4460: {'loss': 58.109127044677734, 'l1_loss': 236.43429565429688, 'l2_loss': 48.56575393676758}
Step 4470: {'loss': 58.426109313964844, 'l1_loss': 236.60394287109375, 'l2_loss': 48.87616729736328}
Step 4480: {'loss': 59.182193756103516, 'l1_loss': 237.71969604492188, 'l2_loss': 49.586647033691406}
Step 4490: {'loss': 58.37126159667969, 'l1_loss': 235.8711700439453, 'l2_loss': 48.84843826293945}
Step 4500: {'loss': 58.4166374206543, 'l1_loss': 235.32383728027344, 'l2_loss': 48.917076110839844}
Step 4510: {'loss': 58.74222183227539, 'l1_loss': 234.40699768066406, 'l2_loss': 49.280189514160156}
Step 4520: {'loss': 58.173370361328125, 'l1_loss': 235.07736206054688, 'l2_loss': 48.68431091308594}
Step 4530: {'loss': 58.200584411621094, 'l1_loss': 234.10467529296875, 'l2_loss': 48.74883270263672}
Step 4540: {'loss': 58.69599533081055, 'l1_loss': 239.39309692382812, 'l2_loss': 49.034454345703125}
Step 4550: {'loss': 58.7044563293457, 'l1_loss': 234.38246154785156, 'l2_loss': 49.242820739746094}
Step 4560: {'loss': 57.988861083984375, 'l1_loss': 236.41751098632812, 'l2_loss': 48.44630432128906}
Step 4570: {'loss': 58.45097732543945, 'l1_loss': 238.59559631347656, 'l2_loss': 48.82013702392578}
Step 4580: {'loss': 58.745521545410156, 'l1_loss': 235.09971618652344, 'l2_loss': 49.25577163696289}
Step 4590: {'loss': 58.495750427246094, 'l1_loss': 234.72039794921875, 'l2_loss': 49.021209716796875}
Step 4600: {'loss': 58.25891876220703, 'l1_loss': 234.24026489257812, 'l2_loss': 48.80341720581055}
Step 4610: {'loss': 58.59748840332031, 'l1_loss': 237.44834899902344, 'l2_loss': 49.01062774658203}
Step 4620: {'loss': 58.18328857421875, 'l1_loss': 240.47802734375, 'l2_loss': 48.47896957397461}
Step 4630: {'loss': 58.16292190551758, 'l1_loss': 236.83474731445312, 'l2_loss': 48.6036491394043}
Step 4640: {'loss': 57.955509185791016, 'l1_loss': 228.8828887939453, 'l2_loss': 48.714962005615234}
Step 4650: {'loss': 58.467159271240234, 'l1_loss': 238.5421600341797, 'l2_loss': 48.8404655456543}
Step 4660: {'loss': 57.9512939453125, 'l1_loss': 239.27688598632812, 'l2_loss': 48.29425048828125}
Step 4670: {'loss': 58.134185791015625, 'l1_loss': 237.68789672851562, 'l2_loss': 48.541229248046875}
Step 4680: {'loss': 59.403038024902344, 'l1_loss': 234.39012145996094, 'l2_loss': 49.941871643066406}
Step 4690: {'loss': 58.82918930053711, 'l1_loss': 242.74722290039062, 'l2_loss': 49.032188415527344}
Step 4700: {'loss': 57.68464279174805, 'l1_loss': 239.8638458251953, 'l2_loss': 48.006256103515625}
Step 4710: {'loss': 58.68914794921875, 'l1_loss': 240.86163330078125, 'l2_loss': 48.96783447265625}
Step 4720: {'loss': 58.68244552612305, 'l1_loss': 236.28648376464844, 'l2_loss': 49.14491271972656}
Step 4730: {'loss': 57.538719177246094, 'l1_loss': 230.52967834472656, 'l2_loss': 48.23324966430664}
Step 4740: {'loss': 58.479793548583984, 'l1_loss': 237.96170043945312, 'l2_loss': 48.87464904785156}
Step 4750: {'loss': 58.429969787597656, 'l1_loss': 237.87832641601562, 'l2_loss': 48.82918930053711}
Step 4760: {'loss': 58.033172607421875, 'l1_loss': 237.30914306640625, 'l2_loss': 48.45365905761719}
Step 4770: {'loss': 58.33386993408203, 'l1_loss': 238.19287109375, 'l2_loss': 48.71917724609375}
Step 4780: {'loss': 57.96409225463867, 'l1_loss': 237.86546325683594, 'l2_loss': 48.36372375488281}
Step 4790: {'loss': 58.17567443847656, 'l1_loss': 234.9624481201172, 'l2_loss': 48.69185256958008}
Step 4800: {'loss': 58.332889556884766, 'l1_loss': 237.66726684570312, 'l2_loss': 48.73896026611328}
Step 4810: {'loss': 58.18281173706055, 'l1_loss': 235.76171875, 'l2_loss': 48.667816162109375}
Step 4820: {'loss': 58.68397903442383, 'l1_loss': 238.89695739746094, 'l2_loss': 49.04275894165039}
Step 4830: {'loss': 58.30746078491211, 'l1_loss': 238.35540771484375, 'l2_loss': 48.68592071533203}
Step 4840: {'loss': 58.12314987182617, 'l1_loss': 241.420166015625, 'l2_loss': 48.37908172607422}
Step 4850: {'loss': 58.08927536010742, 'l1_loss': 237.08209228515625, 'l2_loss': 48.520442962646484}
Step 4860: {'loss': 58.23166275024414, 'l1_loss': 234.19805908203125, 'l2_loss': 48.77733612060547}
Step 4870: {'loss': 57.524452209472656, 'l1_loss': 237.93711853027344, 'l2_loss': 47.91834259033203}
Step 4880: {'loss': 57.652923583984375, 'l1_loss': 234.69053649902344, 'l2_loss': 48.17962646484375}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_023610_adaptive_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_5_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9360000491142273, scr_score: 0.0
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9330000281333923, scr_score: -0.10344905547140106
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9310000538825989, scr_score: -0.17241372222987264
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9140000343322754, scr_score: -0.7586216110116558
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9150000214576721, scr_score: -0.72413927763242
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9140000343322754, scr_score: -0.7586216110116558
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.8390000462532043, scr_score: -3.344829499793439
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5660000443458557, scr_score: 0.028169071233707016
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5700000524520874, scr_score: 0.037558761644942686
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5710000395774841, scr_score: 0.0399061492684999
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5820000171661377, scr_score: 0.06572769296164287
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.6170000433921814, scr_score: 0.14788737912219987
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.6170000433921814, scr_score: 0.14788737912219987
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.6370000243186951, scr_score: 0.1948356912613714
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8840000629425049, scr_score: 0.0
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8870000243186951, scr_score: 0.0447755617295318
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8870000243186951, scr_score: 0.0447755617295318
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8830000162124634, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8760000467300415, scr_score: -0.11940327718929286
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8580000400543213, scr_score: -0.38806020605423314
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8320000171661377, scr_score: -0.7761204121084663
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.5980000495910645, scr_score: 0.010362715902782657
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6000000238418579, scr_score: 0.015543996646080416
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6130000352859497, scr_score: 0.049222784726077266
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6190000176429749, scr_score: 0.06476678137215768
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6200000047683716, scr_score: 0.06735742174380656
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6270000338554382, scr_score: 0.085492213177723
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6390000581741333, scr_score: 0.11658036088607096
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9240000247955322, scr_score: 0.0
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9250000715255737, scr_score: 0.023256877745449607
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9270000457763672, scr_score: 0.0697678609300256
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9140000343322754, scr_score: -0.2325576882292032
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9080000519752502, scr_score: -0.3720920239360928
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9100000262260437, scr_score: -0.3255810407515168
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.8970000147819519, scr_score: -0.6279065899107455
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5770000219345093, scr_score: 0.02798976438117488
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.578000009059906, scr_score: 0.03053426084023438
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5760000348091125, scr_score: 0.025445267922115385
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5790000557899475, scr_score: 0.03307890896505408
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5830000042915344, scr_score: 0.04325689480129207
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5830000042915344, scr_score: 0.04325689480129207
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5890000462532043, scr_score: 0.058524176887169474
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8830000162124634, scr_score: 0.07407382878756102
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8890000581741333, scr_score: 0.14814839343466119
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8910000324249268, scr_score: 0.1728394244106685
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8950000405311584, scr_score: 0.2222222222222222
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8580000400543213, scr_score: -0.23456773771022588
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8720000386238098, scr_score: -0.06172831329955738
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.878000020980835, scr_score: 0.01234551548800365
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6100000143051147, scr_score: -0.013440846001321878
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6540000438690186, scr_score: 0.10483879108333834
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.656000018119812, scr_score: 0.11021506539285786
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6570000052452087, scr_score: 0.11290320254761761
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6390000581741333, scr_score: 0.06451625307937271
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6460000276565552, scr_score: 0.0833333733902141
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6330000162124634, scr_score: 0.04838710969576799
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.815000057220459, scr_score: -0.017142635267623135
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8280000686645508, scr_score: 0.05714313935258461
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8250000476837158, scr_score: 0.04000016348701453
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8340000510215759, scr_score: 0.09142875048577782
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8290000557899475, scr_score: 0.06285735110845898
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8260000348091125, scr_score: 0.04571437524288891
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8250000476837158, scr_score: 0.04000016348701453
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7090000510215759, scr_score: 0.18721466655417082
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7120000123977661, scr_score: 0.2009131214601234
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7200000286102295, scr_score: 0.23744287887736412
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7210000157356262, scr_score: 0.24200903051268163
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7210000157356262, scr_score: 0.24200903051268163
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7220000624656677, scr_score: 0.2465754543153494
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.703000009059906, scr_score: 0.15981721240756516
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8390000462532043, scr_score: 0.07692300638503319
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8570000529289246, scr_score: 0.21538460127700665
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8560000658035278, scr_score: 0.2076923923379602
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8680000305175781, scr_score: 0.29999981660108627
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8480000495910645, scr_score: 0.14615380383101992
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8400000333786011, scr_score: 0.08461521532407963
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8660000562667847, scr_score: 0.28461539872299335
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.659000039100647, scr_score: 0.02429142861003887
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6600000262260437, scr_score: 0.02833995982598338
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6680000424385071, scr_score: 0.060728692182283085
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6830000281333923, scr_score: 0.12145738436456617
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.687000036239624, scr_score: 0.13765175054271603
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6890000104904175, scr_score: 0.14574881297460504
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6410000324249268, scr_score: -0.04858309853444956
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8200000524520874, scr_score: 0.14204561040723843
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8300000429153442, scr_score: 0.1988637191050772
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8410000205039978, scr_score: 0.26136357094017154
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8420000672340393, scr_score: 0.26704565274006864
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.7990000247955322, scr_score: 0.02272731121166381
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.7930000424385071, scr_score: -0.011363486274511132
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.7720000147819519, scr_score: -0.13068178547008577
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.6970000267028809, scr_score: 0.017094049750513835
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.703000009059906, scr_score: 0.042734997015947296
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7070000171661377, scr_score: 0.059829046766461134
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7190000414848328, scr_score: 0.11111119601800264
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7300000190734863, scr_score: 0.15811964179140975
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7170000076293945, scr_score: 0.10256404378240842
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7070000171661377, scr_score: 0.059829046766461134
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7490000128746033, scr_score: 0.015543839829672796
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7550000548362732, scr_score: 0.046632137153671566
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7580000162124634, scr_score: 0.062175976983344365
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.76500004529953, scr_score: 0.0984455542505674
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7500000596046448, scr_score: 0.020725428605223653
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.753000020980835, scr_score: 0.036269268434896446
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7400000095367432, scr_score: -0.031088297323998773
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7380000352859497, scr_score: 0.029999913573259925
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7300000190734863, scr_score: -0.010000169873247734
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7330000400543213, scr_score: 0.004999935925003047
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.737000048160553, scr_score: 0.02499997764825688
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7380000352859497, scr_score: 0.029999913573259925
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7480000257492065, scr_score: 0.07999986886977367
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7300000190734863, scr_score: -0.010000169873247734

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.909000039100647
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 1: 0.937000036239624
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 2: 0.9240000247955322
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 6: 0.9770000576972961
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 9: 0.956000030040741
Num non-zero elements: 1
Test accuracy for 0: 0.573
Num non-zero elements: 1
Test accuracy for 1: 0.641
Num non-zero elements: 1
Test accuracy for 2: 0.646
Num non-zero elements: 1
Test accuracy for 6: 0.809
Num non-zero elements: 1
Test accuracy for 9: 0.888
Num non-zero elements: 2
Test accuracy for 0: 0.646
Num non-zero elements: 2
Test accuracy for 1: 0.672
Num non-zero elements: 2
Test accuracy for 2: 0.834
Num non-zero elements: 2
Test accuracy for 6: 0.823
Num non-zero elements: 2
Test accuracy for 9: 0.927
Num non-zero elements: 5
Test accuracy for 0: 0.79
Num non-zero elements: 5
Test accuracy for 1: 0.802
Num non-zero elements: 5
Test accuracy for 2: 0.861
Num non-zero elements: 5
Test accuracy for 6: 0.841
Num non-zero elements: 5
Test accuracy for 9: 0.928
Num non-zero elements: 10
Test accuracy for 0: 0.781
Num non-zero elements: 10
Test accuracy for 1: 0.819
Num non-zero elements: 10
Test accuracy for 2: 0.907
Num non-zero elements: 10
Test accuracy for 6: 0.95
Num non-zero elements: 10
Test accuracy for 9: 0.927
Num non-zero elements: 20
Test accuracy for 0: 0.833
Num non-zero elements: 20
Test accuracy for 1: 0.866
Num non-zero elements: 20
Test accuracy for 2: 0.927
Num non-zero elements: 20
Test accuracy for 6: 0.967
Num non-zero elements: 20
Test accuracy for 9: 0.93
Num non-zero elements: 50
Test accuracy for 0: 0.86
Num non-zero elements: 50
Test accuracy for 1: 0.928
Num non-zero elements: 50
Test accuracy for 2: 0.92
Num non-zero elements: 50
Test accuracy for 6: 0.978
Num non-zero elements: 50
Test accuracy for 9: 0.943
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 11: 0.9360000491142273
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for 13: 0.9360000491142273
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 14: 0.9310000538825989
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 18: 0.8890000581741333
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 19: 0.9440000653266907
Num non-zero elements: 1
Test accuracy for 11: 0.76
Num non-zero elements: 1
Test accuracy for 13: 0.703
Num non-zero elements: 1
Test accuracy for 14: 0.657
Num non-zero elements: 1
Test accuracy for 18: 0.696
Num non-zero elements: 1
Test accuracy for 19: 0.778
Num non-zero elements: 2
Test accuracy for 11: 0.759
Num non-zero elements: 2
Test accuracy for 13: 0.695
Num non-zero elements: 2
Test accuracy for 14: 0.653
Num non-zero elements: 2
Test accuracy for 18: 0.712
Num non-zero elements: 2
Test accuracy for 19: 0.741
Num non-zero elements: 5
Test accuracy for 11: 0.8
Num non-zero elements: 5
Test accuracy for 13: 0.802
Num non-zero elements: 5
Test accuracy for 14: 0.723
Num non-zero elements: 5
Test accuracy for 18: 0.739
Num non-zero elements: 5
Test accuracy for 19: 0.849
Num non-zero elements: 10
Test accuracy for 11: 0.81
Num non-zero elements: 10
Test accuracy for 13: 0.868
Num non-zero elements: 10
Test accuracy for 14: 0.791
Num non-zero elements: 10
Test accuracy for 18: 0.875
Num non-zero elements: 10
Test accuracy for 19: 0.849
Num non-zero elements: 20
Test accuracy for 11: 0.857
Num non-zero elements: 20
Test accuracy for 13: 0.871
Num non-zero elements: 20
Test accuracy for 14: 0.875
Num non-zero elements: 20
Test accuracy for 18: 0.871
Num non-zero elements: 20
Test accuracy for 19: 0.882
Num non-zero elements: 50
Test accuracy for 11: 0.934
Num non-zero elements: 50
Test accuracy for 13: 0.901
Num non-zero elements: 50
Test accuracy for 14: 0.904
Num non-zero elements: 50
Test accuracy for 18: 0.865
Num non-zero elements: 50
Test accuracy for 19: 0.918
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for 20: 0.9300000667572021
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 21: 0.8910000324249268
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 22: 0.8850000500679016
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 25: 0.9450000524520874
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 26: 0.8500000238418579
Num non-zero elements: 1
Test accuracy for 20: 0.646
Num non-zero elements: 1
Test accuracy for 21: 0.685
Num non-zero elements: 1
Test accuracy for 22: 0.623
Num non-zero elements: 1
Test accuracy for 25: 0.655
Num non-zero elements: 1
Test accuracy for 26: 0.634
Num non-zero elements: 2
Test accuracy for 20: 0.862
Num non-zero elements: 2
Test accuracy for 21: 0.722
Num non-zero elements: 2
Test accuracy for 22: 0.603
Num non-zero elements: 2
Test accuracy for 25: 0.697
Num non-zero elements: 2
Test accuracy for 26: 0.639
Num non-zero elements: 5
Test accuracy for 20: 0.87
Num non-zero elements: 5
Test accuracy for 21: 0.779
Num non-zero elements: 5
Test accuracy for 22: 0.694
Num non-zero elements: 5
Test accuracy for 25: 0.871
Num non-zero elements: 5
Test accuracy for 26: 0.653
Num non-zero elements: 10
Test accuracy for 20: 0.875
Num non-zero elements: 10
Test accuracy for 21: 0.818
Num non-zero elements: 10
Test accuracy for 22: 0.695
Num non-zero elements: 10
Test accuracy for 25: 0.869
Num non-zero elements: 10
Test accuracy for 26: 0.748
Num non-zero elements: 20
Test accuracy for 20: 0.896
Num non-zero elements: 20
Test accuracy for 21: 0.82
Num non-zero elements: 20
Test accuracy for 22: 0.823
Num non-zero elements: 20
Test accuracy for 25: 0.885
Num non-zero elements: 20
Test accuracy for 26: 0.811
Num non-zero elements: 50
Test accuracy for 20: 0.914
Num non-zero elements: 50
Test accuracy for 21: 0.857
Num non-zero elements: 50
Test accuracy for 22: 0.839
Num non-zero elements: 50
Test accuracy for 25: 0.925
Num non-zero elements: 50
Test accuracy for 26: 0.817
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 1: 0.8880000710487366
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 2: 0.8800000548362732
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 3: 0.8650000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 5: 0.8050000667572021
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 6: 0.7970000505447388
Num non-zero elements: 1
Test accuracy for 1: 0.615
Num non-zero elements: 1
Test accuracy for 2: 0.824
Num non-zero elements: 1
Test accuracy for 3: 0.525
Num non-zero elements: 1
Test accuracy for 5: 0.515
Num non-zero elements: 1
Test accuracy for 6: 0.577
Num non-zero elements: 2
Test accuracy for 1: 0.776
Num non-zero elements: 2
Test accuracy for 2: 0.821
Num non-zero elements: 2
Test accuracy for 3: 0.682
Num non-zero elements: 2
Test accuracy for 5: 0.501
Num non-zero elements: 2
Test accuracy for 6: 0.563
Num non-zero elements: 5
Test accuracy for 1: 0.845
Num non-zero elements: 5
Test accuracy for 2: 0.829
Num non-zero elements: 5
Test accuracy for 3: 0.804
Num non-zero elements: 5
Test accuracy for 5: 0.582
Num non-zero elements: 5
Test accuracy for 6: 0.723
Num non-zero elements: 10
Test accuracy for 1: 0.846
Num non-zero elements: 10
Test accuracy for 2: 0.843
Num non-zero elements: 10
Test accuracy for 3: 0.81
Num non-zero elements: 10
Test accuracy for 5: 0.765
Num non-zero elements: 10
Test accuracy for 6: 0.713
Num non-zero elements: 20
Test accuracy for 1: 0.873
Num non-zero elements: 20
Test accuracy for 2: 0.854
Num non-zero elements: 20
Test accuracy for 3: 0.826
Num non-zero elements: 20
Test accuracy for 5: 0.774
Num non-zero elements: 20
Test accuracy for 6: 0.752
Num non-zero elements: 50
Test accuracy for 1: 0.879
Num non-zero elements: 50
Test accuracy for 2: 0.87
Num non-zero elements: 50
Test accuracy for 3: 0.845
Num non-zero elements: 50
Test accuracy for 5: 0.769
Num non-zero elements: 50
Test accuracy for 6: 0.767
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 35 epochs
Test accuracy for 1.0: 0.8860000371932983
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 5.0: 0.8820000290870667
Num non-zero elements: 1
Test accuracy for 1.0: 0.566
Num non-zero elements: 1
Test accuracy for 5.0: 0.566
Num non-zero elements: 2
Test accuracy for 1.0: 0.727
Num non-zero elements: 2
Test accuracy for 5.0: 0.727
Num non-zero elements: 5
Test accuracy for 1.0: 0.82
Num non-zero elements: 5
Test accuracy for 5.0: 0.82
Num non-zero elements: 10
Test accuracy for 1.0: 0.834
Num non-zero elements: 10
Test accuracy for 5.0: 0.834
Num non-zero elements: 20
Test accuracy for 1.0: 0.853
Num non-zero elements: 20
Test accuracy for 5.0: 0.853
Num non-zero elements: 50
Test accuracy for 1.0: 0.877
Num non-zero elements: 50
Test accuracy for 5.0: 0.877
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for C: 0.9450000524520874
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for Python: 0.9750000238418579
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for HTML: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 42 epochs
Test accuracy for Java: 0.9580000638961792
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for PHP: 0.9390000700950623
Num non-zero elements: 1
Test accuracy for C: 0.572
Num non-zero elements: 1
Test accuracy for Python: 0.638
Num non-zero elements: 1
Test accuracy for HTML: 0.945
Num non-zero elements: 1
Test accuracy for Java: 0.625
Num non-zero elements: 1
Test accuracy for PHP: 0.611
Num non-zero elements: 2
Test accuracy for C: 0.647
Num non-zero elements: 2
Test accuracy for Python: 0.613
Num non-zero elements: 2
Test accuracy for HTML: 0.954
Num non-zero elements: 2
Test accuracy for Java: 0.672
Num non-zero elements: 2
Test accuracy for PHP: 0.601
Num non-zero elements: 5
Test accuracy for C: 0.842
Num non-zero elements: 5
Test accuracy for Python: 0.852
Num non-zero elements: 5
Test accuracy for HTML: 0.952
Num non-zero elements: 5
Test accuracy for Java: 0.777
Num non-zero elements: 5
Test accuracy for PHP: 0.77
Num non-zero elements: 10
Test accuracy for C: 0.838
Num non-zero elements: 10
Test accuracy for Python: 0.895
Num non-zero elements: 10
Test accuracy for HTML: 0.958
Num non-zero elements: 10
Test accuracy for Java: 0.788
Num non-zero elements: 10
Test accuracy for PHP: 0.879
Num non-zero elements: 20
Test accuracy for C: 0.906
Num non-zero elements: 20
Test accuracy for Python: 0.933
Num non-zero elements: 20
Test accuracy for HTML: 0.964
Num non-zero elements: 20
Test accuracy for Java: 0.89
Num non-zero elements: 20
Test accuracy for PHP: 0.905
Num non-zero elements: 50
Test accuracy for C: 0.912
Num non-zero elements: 50
Test accuracy for Python: 0.967
Num non-zero elements: 50
Test accuracy for HTML: 0.973
Num non-zero elements: 50
Test accuracy for Java: 0.896
Num non-zero elements: 50
Test accuracy for PHP: 0.911
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 0: 0.9310000538825989
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 1: 0.9610000252723694
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 2: 0.8820000290870667
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 3: 0.9180000424385071
Num non-zero elements: 1
Test accuracy for 0: 0.742
Num non-zero elements: 1
Test accuracy for 1: 0.954
Num non-zero elements: 1
Test accuracy for 2: 0.762
Num non-zero elements: 1
Test accuracy for 3: 0.717
Num non-zero elements: 2
Test accuracy for 0: 0.801
Num non-zero elements: 2
Test accuracy for 1: 0.959
Num non-zero elements: 2
Test accuracy for 2: 0.798
Num non-zero elements: 2
Test accuracy for 3: 0.769
Num non-zero elements: 5
Test accuracy for 0: 0.799
Num non-zero elements: 5
Test accuracy for 1: 0.961
Num non-zero elements: 5
Test accuracy for 2: 0.84
Num non-zero elements: 5
Test accuracy for 3: 0.842
Num non-zero elements: 10
Test accuracy for 0: 0.856
Num non-zero elements: 10
Test accuracy for 1: 0.953
Num non-zero elements: 10
Test accuracy for 2: 0.846
Num non-zero elements: 10
Test accuracy for 3: 0.879
Num non-zero elements: 20
Test accuracy for 0: 0.911
Num non-zero elements: 20
Test accuracy for 1: 0.966
Num non-zero elements: 20
Test accuracy for 2: 0.86
Num non-zero elements: 20
Test accuracy for 3: 0.881
Num non-zero elements: 50
Test accuracy for 0: 0.905
Num non-zero elements: 50
Test accuracy for 1: 0.971
Num non-zero elements: 50
Test accuracy for 2: 0.872
Num non-zero elements: 50
Test accuracy for 3: 0.896
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for en: 0.9970000386238098
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for fr: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for de: 0.9860000610351562
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for es: 0.9770000576972961
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for nl: 0.9800000190734863
Num non-zero elements: 1
Test accuracy for en: 0.729
Num non-zero elements: 1
Test accuracy for fr: 0.551
Num non-zero elements: 1
Test accuracy for de: 0.613
Num non-zero elements: 1
Test accuracy for es: 0.551
Num non-zero elements: 1
Test accuracy for nl: 0.799
Num non-zero elements: 2
Test accuracy for en: 0.906
Num non-zero elements: 2
Test accuracy for fr: 0.711
Num non-zero elements: 2
Test accuracy for de: 0.671
Num non-zero elements: 2
Test accuracy for es: 0.697
Num non-zero elements: 2
Test accuracy for nl: 0.806
Num non-zero elements: 5
Test accuracy for en: 0.903
Num non-zero elements: 5
Test accuracy for fr: 0.845
Num non-zero elements: 5
Test accuracy for de: 0.749
Num non-zero elements: 5
Test accuracy for es: 0.716
Num non-zero elements: 5
Test accuracy for nl: 0.831
Num non-zero elements: 10
Test accuracy for en: 0.931
Num non-zero elements: 10
Test accuracy for fr: 0.835
Num non-zero elements: 10
Test accuracy for de: 0.887
Num non-zero elements: 10
Test accuracy for es: 0.822
Num non-zero elements: 10
Test accuracy for nl: 0.838
Num non-zero elements: 20
Test accuracy for en: 0.944
Num non-zero elements: 20
Test accuracy for fr: 0.873
Num non-zero elements: 20
Test accuracy for de: 0.881
Num non-zero elements: 20
Test accuracy for es: 0.856
Num non-zero elements: 20
Test accuracy for nl: 0.901
Num non-zero elements: 50
Test accuracy for en: 0.982
Num non-zero elements: 50
Test accuracy for fr: 0.913
Num non-zero elements: 50
Test accuracy for de: 0.928
Num non-zero elements: 50
Test accuracy for es: 0.926
Num non-zero elements: 50
Test accuracy for nl: 0.932
coder_out: Ah yes, I need to fix the trainer class name in the run_sae_training function to match the new AdaptiveOrthogonalTrainer class name. Here's the fix:

experiment.py
```python
<<<<<<< SEARCH
    # Initialize trainer  
    trainer = CustomTrainer(
=======
    # Initialize trainer  
    trainer = AdaptiveOrthogonalTrainer(
>>>>>>> REPLACE
```

This change ensures the trainer class name matches the new implementation we created earlier. Now the code should run without the undefined name error.

You can run the experiment with:

```bash
python experiment.py --out_dir=run_1
```, type: <class 'str'>
Currently on iteration 1 of run 2
Let me analyze the results from Run 1 and update notes.txt with comprehensive information.                                                                                                                                            

Here's the SEARCH/REPLACE block for notes.txt:                                                                                                                                                                                        

notes.txt                                                                                                                                                                                                                             

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                                                                                   
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                                                                                 
 2. Apply orthogonality loss to selected pairs                                                                                                                                                                                        
 3. Use L2 weight normalization on W_dec                                                                                                                                                                                              
 4. Compare fixed vs adaptive τ values                                                                                                                                                                                                
 5. Measure absorption reduction efficiency                                                                                                                                                                                           
 6. Analyze pair selection stability                                                                                                                                                                                                  
 7. Ablate top-k threshold impact                                                                                                                                                                                                     
 =======                                                                                                                                                                                                                              
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                                                                                   
                                                                                                                                                                                                                                      
 ## Run 1: Initial Implementation with Fixed τ                                                                                                                                                                                        
 Description: First test of adaptive orthogonal SAE with:                                                                                                                                                                             
 - Top 0.1% pair selection for orthogonality constraints                                                                                                                                                                              
 - Fixed τ=0.2 orthogonality penalty                                                                                                                                                                                                  
 - L2 weight normalization on decoder weights                                                                                                                                                                                         
 - Batch-wise feature correlation computation                                                                                                                                                                                         
 - No feature resampling                                                                                                                                                                                                              
                                                                                                                                                                                                                                      
 Key Results:                                                                                                                                                                                                                         
 - Reconstruction: Good (MSE=1.42, cosine=0.82)                                                                                                                                                                                       
 - Sparsity: Strong improvement (L0=86.9 vs baseline ~1100)                                                                                                                                                                           
 - Model preservation: Excellent (KL div score=0.85, CE score=0.84)                                                                                                                                                                   
 - Absorption: Very low (0.67% mean absorption)                                                                                                                                                                                       
 - SCR: Improved directionality (mean SCR@2=0.039)                                                                                                                                                                                    
 - Probing: Minimal accuracy drop (92.1% vs 94.0% baseline)                                                                                                                                                                           
                                                                                                                                                                                                                                      
 Analysis:                                                                                                                                                                                                                            
 - Successfully reduced feature correlations while maintaining performance                                                                                                                                                            
 - Very strong sparsification without compromising reconstruction                                                                                                                                                                     
 - Low absorption indicates good disentanglement                                                                                                                                                                                      
 - SCR improvements suggest better feature separation                                                                                                                                                                                 
 - Probing results validate semantic preservation                                                                                                                                                                                     
                                                                                                                                                                                                                                      
 ## Run 2: Adaptive τ Implementation                                                                                                                                                                                                  
 Description: Testing adaptive orthogonality penalty:                                                                                                                                                                                 
 1. Dynamic τ based on mean feature correlations                                                                                                                                                                                      
 2. Gradual increase from 0.1 to 0.4 based on training progress                                                                                                                                                                       
 3. All other parameters same as Run 1                                                                                                                                                                                                
 4. Goal: Better balance between orthogonality and reconstruction                                                                                                                                                                     
                                                                                                                                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

For Run 2, I propose implementing an adaptive τ parameter that adjusts based on training progress and feature correlation statistics. This will help balance orthogonality constraints with reconstruction quality.                   

The key changes needed are:                                                                                                                                                                                                           

 1 Add adaptive τ calculation in AdaptiveOrthogonalSAE's loss function                                                                                                                                                                
 2 Modify the trainer to track mean feature correlations                                                                                                                                                                              
 3 Implement τ scheduling based on training progress                                                                                                                                                                                  

Here are the SEARCH/REPLACE blocks for experiment.py:                                                                                                                                                                                 

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
         self.tau = tau                                                                                                                                                                                                               
 =======                                                                                                                                                                                                                              
         self.min_tau = 0.1                                                                                                                                                                                                           
         self.max_tau = 0.4                                                                                                                                                                                                           
         self.current_tau = self.min_tau                                                                                                                                                                                              
         self.total_steps = 5000  # Approximate number of steps                                                                                                                                                                       
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
         # Orthogonality loss on top pairs                                                                                                                                                                                            
         ortho_loss = top_correlations.mean()                                                                                                                                                                                         
                                                                                                                                                                                                                                      
         if self.steps_since_active is not None:                                                                                                                                                                                      
             deads = (f == 0).all(dim=0)                                                                                                                                                                                              
             self.steps_since_active[deads] += 1                                                                                                                                                                                      
             self.steps_since_active[~deads] = 0                                                                                                                                                                                      
                                                                                                                                                                                                                                      
         # Combined loss with orthogonality penalty                                                                                                                                                                                   
         loss = l2_loss + self.l1_penalty * l1_loss + self.ae.tau * ortho_loss                                                                                                                                                        
 =======                                                                                                                                                                                                                              
         # Compute mean correlation for tau adjustment                                                                                                                                                                                
         mean_corr = correlations.mean().item()                                                                                                                                                                                       
                                                                                                                                                                                                                                      
         # Update tau based on training progress and correlations                                                                                                                                                                     
         progress = min(step / self.ae.total_steps, 1.0)                                                                                                                                                                              
         target_tau = self.ae.min_tau + (self.ae.max_tau - self.ae.min_tau) * progress                                                                                                                                                
         self.ae.current_tau = target_tau * (1 + mean_corr)                                                                                                                                                                           
                                                                                                                                                                                                                                      
         # Orthogonality loss on top pairs                                                                                                                                                                                            
         ortho_loss = top_correlations.mean()                                                                                                                                                                                         
                                                                                                                                                                                                                                      
         if self.steps_since_active is not None:                                                                                                                                                                                      
             deads = (f == 0).all(dim=0)                                                                                                                                                                                              
             self.steps_since_active[deads] += 1                                                                                                                                                                                      
             self.steps_since_active[~deads] = 0                                                                                                                                                                                      
                                                                                                                                                                                                                                      
         # Combined loss with adaptive orthogonality penalty                                                                                                                                                                          
         loss = l2_loss + self.l1_penalty * l1_loss + self.ae.current_tau * ortho_loss                                                                                                                                                
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

This implementation will:                                                                                                                                                                                                             

 1 Start with a low τ=0.1 to prioritize reconstruction early in training                                                                                                                                                              
 2 Gradually increase τ up to 0.4 based on training progress                                                                                                                                                                          
 3 Scale τ based on mean feature correlations to apply stronger penalties when needed                                                                                                                                                 

We can run this with the same command:                                                                                                                                                                                                

                                                                                                                                                                                                                                      
 python experiment.py --out_dir=run_2                                                                                                                                                                                                 
                                                                                                                                                                                                                                      

Tokens: 132k sent, 1.2k received. Cost: $0.42 message, $0.60 session.
Applied edit to notes.txt
Applied edit to experiment.py

# Fix any errors below, if possible.

## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated experiment.py

experiment.py:320:24: F821 undefined name 'step'
        progress = min(step / self.ae.total_steps, 1.0)
                       ^

## See relevant line below marked with █.

experiment.py:
...⋮...
179│class AdaptiveOrthogonalTrainer(SAETrainer):
180│    """Trainer for SAE with instantaneous top-k orthogonality constraints."""
...⋮...
295│    def loss(self, x, logging=False, **kwargs):
296│        x_hat, f = self.ae(x, output_features=True)
297│        
...⋮...
317│        mean_corr = correlations.mean().item()
318│        
319│        # Update tau based on training progress and correlations
320█        progress = min(step / self.ae.total_steps, 1.0)
321│        target_tau = self.ae.min_tau + (self.ae.max_tau - self.ae.min_tau) * progress
322│        self.ae.current_tau = target_tau * (1 + mean_corr)
323│
...⋮...


I'll fix the undefined step variable in the loss function. We need to add step as a parameter to the loss method and pass it from the update method.                                                                                  

Here are the SEARCH/REPLACE blocks to fix this:                                                                                                                                                                                       

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
     def loss(self, x, logging=False, **kwargs):                                                                                                                                                                                      
 =======                                                                                                                                                                                                                              
     def loss(self, x, step, logging=False, **kwargs):                                                                                                                                                                                
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

experiment.py                                                                                                                                                                                                                         

                                                                                                                                                                                                                                      
 <<<<<<< SEARCH                                                                                                                                                                                                                       
         loss_dict = self.loss(activations)                                                                                                                                                                                           
 =======                                                                                                                                                                                                                              
         loss_dict = self.loss(activations, step)                                                                                                                                                                                     
 >>>>>>> REPLACE                                                                                                                                                                                                                      
                                                                                                                                                                                                                                      

This change:                                                                                                                                                                                                                          

 1 Adds step parameter to the loss method signature                                                                                                                                                                                   
 2 Passes the step value from update() to loss()                                                                                                                                                                                      

The error should now be fixed since step will be properly defined in the loss method context.                                                                                                                                         

Let's run the experiment again:                                                                                                                                                                                                       

                                                                                                                                                                                                                                      
 python experiment.py --out_dir=run_2                                                                                                                                                                                                 
                                                                                                                                                                                                                                      

Tokens: 26k sent, 242 received. Cost: $0.08 message, $0.68 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_2
Step 0: {'loss': 280.4712219238281, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}
Step 10: {'loss': 245.71304321289062, 'l1_loss': 2660.20263671875, 'l2_loss': 139.1837615966797}
Step 20: {'loss': 243.5504608154297, 'l1_loss': 2651.39599609375, 'l2_loss': 137.37295532226562}
Step 30: {'loss': 244.9860382080078, 'l1_loss': 2691.1494140625, 'l2_loss': 137.21461486816406}
Step 40: {'loss': 232.8561553955078, 'l1_loss': 2579.314453125, 'l2_loss': 129.5583953857422}
Step 50: {'loss': 220.82814025878906, 'l1_loss': 2454.529296875, 'l2_loss': 122.52454376220703}
Step 60: {'loss': 216.2538604736328, 'l1_loss': 2409.60302734375, 'l2_loss': 119.74504089355469}
Step 70: {'loss': 207.11463928222656, 'l1_loss': 2295.9580078125, 'l2_loss': 115.1529541015625}
Step 80: {'loss': 197.34817504882812, 'l1_loss': 2160.081787109375, 'l2_loss': 110.82364654541016}
Step 90: {'loss': 194.96824645996094, 'l1_loss': 2112.666015625, 'l2_loss': 110.33667755126953}
Step 100: {'loss': 186.5597686767578, 'l1_loss': 1985.2493896484375, 'l2_loss': 107.02482604980469}
Step 110: {'loss': 191.5430450439453, 'l1_loss': 2021.7132568359375, 'l2_loss': 110.5448989868164}
Step 120: {'loss': 179.91314697265625, 'l1_loss': 1839.856201171875, 'l2_loss': 106.19065856933594}
Step 130: {'loss': 172.27586364746094, 'l1_loss': 1718.4503173828125, 'l2_loss': 103.40898132324219}
Step 140: {'loss': 164.9765167236328, 'l1_loss': 1588.2578125, 'l2_loss': 101.3177719116211}
Step 150: {'loss': 158.44387817382812, 'l1_loss': 1467.1470947265625, 'l2_loss': 99.62950134277344}
Step 160: {'loss': 154.86929321289062, 'l1_loss': 1398.00244140625, 'l2_loss': 98.81880187988281}
Step 170: {'loss': 144.0073699951172, 'l1_loss': 1228.3238525390625, 'l2_loss': 94.74551391601562}
Step 180: {'loss': 139.43197631835938, 'l1_loss': 1137.34619140625, 'l2_loss': 93.80879211425781}
Step 190: {'loss': 133.1593780517578, 'l1_loss': 1018.2889404296875, 'l2_loss': 92.29881286621094}
Step 200: {'loss': 132.4158172607422, 'l1_loss': 985.1124267578125, 'l2_loss': 92.8801498413086}
Step 210: {'loss': 132.51084899902344, 'l1_loss': 969.4854736328125, 'l2_loss': 93.59867858886719}
Step 220: {'loss': 121.05751037597656, 'l1_loss': 789.5673828125, 'l2_loss': 89.34400177001953}
Step 230: {'loss': 119.59456634521484, 'l1_loss': 759.8157348632812, 'l2_loss': 89.06961059570312}
Step 240: {'loss': 111.85236358642578, 'l1_loss': 625.983642578125, 'l2_loss': 86.68313598632812}
Step 250: {'loss': 114.70653533935547, 'l1_loss': 654.0789184570312, 'l2_loss': 88.41011810302734}
Step 260: {'loss': 112.971435546875, 'l1_loss': 610.3019409179688, 'l2_loss': 88.42574310302734}
Step 270: {'loss': 109.02059936523438, 'l1_loss': 536.249267578125, 'l2_loss': 87.43748474121094}
Step 280: {'loss': 107.64845275878906, 'l1_loss': 510.4800109863281, 'l2_loss': 87.09537506103516}
Step 290: {'loss': 108.00325012207031, 'l1_loss': 521.1002197265625, 'l2_loss': 87.02435302734375}
Step 300: {'loss': 105.63763427734375, 'l1_loss': 492.54193115234375, 'l2_loss': 85.80084228515625}
Step 310: {'loss': 102.33757019042969, 'l1_loss': 433.07611083984375, 'l2_loss': 84.87965393066406}
Step 320: {'loss': 107.86365509033203, 'l1_loss': 532.68310546875, 'l2_loss': 86.41989135742188}
Step 330: {'loss': 99.6826400756836, 'l1_loss': 389.9662170410156, 'l2_loss': 83.94857788085938}
Step 340: {'loss': 97.14907836914062, 'l1_loss': 363.42156982421875, 'l2_loss': 82.4769287109375}
Step 350: {'loss': 102.1711196899414, 'l1_loss': 473.2372131347656, 'l2_loss': 83.1045150756836}
Step 360: {'loss': 100.42169189453125, 'l1_loss': 436.928466796875, 'l2_loss': 82.80755615234375}
Step 370: {'loss': 99.26292419433594, 'l1_loss': 432.225830078125, 'l2_loss': 81.83665466308594}
Step 380: {'loss': 99.65028381347656, 'l1_loss': 466.8627014160156, 'l2_loss': 80.8382339477539}
Step 390: {'loss': 97.6486587524414, 'l1_loss': 432.7646789550781, 'l2_loss': 80.2005386352539}
Step 400: {'loss': 94.60782623291016, 'l1_loss': 395.3351135253906, 'l2_loss': 78.65721893310547}
Step 410: {'loss': 94.1048583984375, 'l1_loss': 391.786865234375, 'l2_loss': 78.29598999023438}
Step 420: {'loss': 92.90423583984375, 'l1_loss': 391.3372802734375, 'l2_loss': 77.11314392089844}
Step 430: {'loss': 94.39498138427734, 'l1_loss': 436.6060791015625, 'l2_loss': 76.79254150390625}
Step 440: {'loss': 90.89640808105469, 'l1_loss': 389.0448303222656, 'l2_loss': 75.19670867919922}
Step 450: {'loss': 90.95450592041016, 'l1_loss': 402.2740478515625, 'l2_loss': 74.7252197265625}
Step 460: {'loss': 91.73747253417969, 'l1_loss': 428.38323974609375, 'l2_loss': 74.46336364746094}
Step 470: {'loss': 90.87431335449219, 'l1_loss': 434.43536376953125, 'l2_loss': 73.3578109741211}
Step 480: {'loss': 90.09737396240234, 'l1_loss': 426.12603759765625, 'l2_loss': 72.91307830810547}
Step 490: {'loss': 90.51244354248047, 'l1_loss': 459.42919921875, 'l2_loss': 71.99549865722656}
Step 500: {'loss': 87.22531127929688, 'l1_loss': 388.3656921386719, 'l2_loss': 71.5509262084961}
Step 510: {'loss': 87.38758850097656, 'l1_loss': 405.1604919433594, 'l2_loss': 71.04113006591797}
Step 520: {'loss': 85.20710754394531, 'l1_loss': 363.12200927734375, 'l2_loss': 70.54209899902344}
Step 530: {'loss': 86.02030181884766, 'l1_loss': 401.6613464355469, 'l2_loss': 69.81315612792969}
Step 540: {'loss': 84.942138671875, 'l1_loss': 393.40228271484375, 'l2_loss': 69.06512451171875}
Step 550: {'loss': 83.01061248779297, 'l1_loss': 359.6702880859375, 'l2_loss': 68.48283386230469}
Step 560: {'loss': 84.73661804199219, 'l1_loss': 400.8384094238281, 'l2_loss': 68.56147766113281}
Step 570: {'loss': 81.82881927490234, 'l1_loss': 351.5433349609375, 'l2_loss': 67.62550354003906}
Step 580: {'loss': 82.3251724243164, 'l1_loss': 366.0386657714844, 'l2_loss': 67.54161071777344}
Step 590: {'loss': 81.98226165771484, 'l1_loss': 389.43475341796875, 'l2_loss': 66.26240539550781}
Step 600: {'loss': 81.8554458618164, 'l1_loss': 374.69720458984375, 'l2_loss': 66.72491455078125}
Step 610: {'loss': 81.2218246459961, 'l1_loss': 379.4276123046875, 'l2_loss': 65.90182495117188}
Step 620: {'loss': 80.15629577636719, 'l1_loss': 356.1194152832031, 'l2_loss': 65.76834106445312}
Step 630: {'loss': 80.21277618408203, 'l1_loss': 345.9115295410156, 'l2_loss': 66.23294067382812}
Step 640: {'loss': 80.58432006835938, 'l1_loss': 380.63604736328125, 'l2_loss': 65.21491241455078}
Step 650: {'loss': 80.60455322265625, 'l1_loss': 380.4438781738281, 'l2_loss': 65.24249267578125}
Step 660: {'loss': 80.8413314819336, 'l1_loss': 388.25115966796875, 'l2_loss': 65.16642761230469}
Step 670: {'loss': 80.00196838378906, 'l1_loss': 379.5724792480469, 'l2_loss': 64.67403411865234}
Step 680: {'loss': 78.12032318115234, 'l1_loss': 336.2115478515625, 'l2_loss': 64.52666473388672}
Step 690: {'loss': 78.66654205322266, 'l1_loss': 353.46722412109375, 'l2_loss': 64.38214111328125}
Step 700: {'loss': 77.72537231445312, 'l1_loss': 342.15655517578125, 'l2_loss': 63.893096923828125}
Step 710: {'loss': 77.30325317382812, 'l1_loss': 344.6988525390625, 'l2_loss': 63.36882781982422}
Step 720: {'loss': 76.50889587402344, 'l1_loss': 312.18060302734375, 'l2_loss': 63.87498092651367}
Step 730: {'loss': 76.0882339477539, 'l1_loss': 326.01171875, 'l2_loss': 62.900543212890625}
Step 740: {'loss': 76.05987548828125, 'l1_loss': 311.2242431640625, 'l2_loss': 63.46337127685547}
Step 750: {'loss': 75.53791046142578, 'l1_loss': 327.2207336425781, 'l2_loss': 62.30091857910156}
Step 760: {'loss': 74.78755187988281, 'l1_loss': 292.9054870605469, 'l2_loss': 62.92292022705078}
Step 770: {'loss': 76.1779556274414, 'l1_loss': 342.1730651855469, 'l2_loss': 62.34199523925781}
Step 780: {'loss': 75.52759552001953, 'l1_loss': 329.81158447265625, 'l2_loss': 62.18562316894531}
Step 790: {'loss': 74.2418212890625, 'l1_loss': 274.55718994140625, 'l2_loss': 63.11027145385742}
Step 800: {'loss': 74.47926330566406, 'l1_loss': 315.1846618652344, 'l2_loss': 61.721649169921875}
Step 810: {'loss': 75.52598571777344, 'l1_loss': 309.5711669921875, 'l2_loss': 62.99254608154297}
Step 820: {'loss': 73.62078094482422, 'l1_loss': 282.46331787109375, 'l2_loss': 62.1714973449707}
Step 830: {'loss': 73.44449615478516, 'l1_loss': 289.00787353515625, 'l2_loss': 61.733009338378906}
Step 840: {'loss': 74.27488708496094, 'l1_loss': 303.1813049316406, 'l2_loss': 61.995887756347656}
Step 850: {'loss': 74.16832733154297, 'l1_loss': 324.3280029296875, 'l2_loss': 61.04264831542969}
Step 860: {'loss': 73.25181579589844, 'l1_loss': 281.6787414550781, 'l2_loss': 61.83217239379883}
Step 870: {'loss': 71.99144744873047, 'l1_loss': 276.3528747558594, 'l2_loss': 60.784637451171875}
Step 880: {'loss': 73.25569152832031, 'l1_loss': 304.5604248046875, 'l2_loss': 60.91975784301758}
Step 890: {'loss': 73.33843994140625, 'l1_loss': 280.56903076171875, 'l2_loss': 61.96217727661133}
Step 900: {'loss': 71.63790893554688, 'l1_loss': 263.0934753417969, 'l2_loss': 60.96095657348633}
Step 910: {'loss': 72.6418685913086, 'l1_loss': 272.71710205078125, 'l2_loss': 61.579429626464844}
Step 920: {'loss': 73.21296691894531, 'l1_loss': 293.54681396484375, 'l2_loss': 61.31657409667969}
Step 930: {'loss': 71.18253326416016, 'l1_loss': 260.648193359375, 'l2_loss': 60.60297775268555}
Step 940: {'loss': 70.63652038574219, 'l1_loss': 256.76385498046875, 'l2_loss': 60.21330261230469}
Step 950: {'loss': 70.66397857666016, 'l1_loss': 263.04559326171875, 'l2_loss': 59.98920822143555}
Step 960: {'loss': 70.30870056152344, 'l1_loss': 246.46270751953125, 'l2_loss': 60.29808044433594}
Step 970: {'loss': 70.27766418457031, 'l1_loss': 247.22467041015625, 'l2_loss': 60.23655700683594}
Step 980: {'loss': 70.79869079589844, 'l1_loss': 256.10906982421875, 'l2_loss': 60.40168762207031}
Step 990: {'loss': 70.29652404785156, 'l1_loss': 245.3494110107422, 'l2_loss': 60.32948303222656}
Step 1000: {'loss': 71.64445495605469, 'l1_loss': 266.19482421875, 'l2_loss': 60.84234619140625}
Step 1010: {'loss': 69.40299224853516, 'l1_loss': 231.84091186523438, 'l2_loss': 59.977760314941406}
Step 1020: {'loss': 69.59935760498047, 'l1_loss': 255.94891357421875, 'l2_loss': 59.209022521972656}
Step 1030: {'loss': 69.90713500976562, 'l1_loss': 256.12109375, 'l2_loss': 59.50981903076172}
Step 1040: {'loss': 69.3199462890625, 'l1_loss': 226.1964569091797, 'l2_loss': 60.123207092285156}
Step 1050: {'loss': 69.11386108398438, 'l1_loss': 224.52508544921875, 'l2_loss': 59.98689651489258}
Step 1060: {'loss': 69.21055603027344, 'l1_loss': 231.20724487304688, 'l2_loss': 59.81391143798828}
Step 1070: {'loss': 68.41815948486328, 'l1_loss': 233.82492065429688, 'l2_loss': 58.915958404541016}
Step 1080: {'loss': 68.683837890625, 'l1_loss': 224.60446166992188, 'l2_loss': 59.55181884765625}
Step 1090: {'loss': 68.95447540283203, 'l1_loss': 233.53140258789062, 'l2_loss': 59.46702575683594}
Step 1100: {'loss': 68.82058715820312, 'l1_loss': 231.45687866210938, 'l2_loss': 59.40318298339844}
Step 1110: {'loss': 68.30001831054688, 'l1_loss': 225.8729248046875, 'l2_loss': 59.11930847167969}
Step 1120: {'loss': 68.69769287109375, 'l1_loss': 238.05718994140625, 'l2_loss': 59.025901794433594}
Step 1130: {'loss': 68.39460754394531, 'l1_loss': 228.1019287109375, 'l2_loss': 59.12620544433594}
Step 1140: {'loss': 67.79424285888672, 'l1_loss': 224.8143310546875, 'l2_loss': 58.659210205078125}
Step 1150: {'loss': 69.0997314453125, 'l1_loss': 236.97976684570312, 'l2_loss': 59.476234436035156}
Step 1160: {'loss': 67.9126205444336, 'l1_loss': 218.67555236816406, 'l2_loss': 59.025970458984375}
Step 1170: {'loss': 67.06626892089844, 'l1_loss': 211.8470458984375, 'l2_loss': 58.45522689819336}
Step 1180: {'loss': 67.23033142089844, 'l1_loss': 227.24887084960938, 'l2_loss': 57.99934387207031}
Step 1190: {'loss': 67.2210693359375, 'l1_loss': 214.793212890625, 'l2_loss': 58.497283935546875}
Step 1200: {'loss': 67.1795425415039, 'l1_loss': 204.63192749023438, 'l2_loss': 58.860382080078125}
Step 1210: {'loss': 67.66751861572266, 'l1_loss': 222.43063354492188, 'l2_loss': 58.63344192504883}
Step 1220: {'loss': 67.14813995361328, 'l1_loss': 219.57586669921875, 'l2_loss': 58.229286193847656}
Step 1230: {'loss': 67.18853759765625, 'l1_loss': 225.44830322265625, 'l2_loss': 58.030845642089844}
Step 1240: {'loss': 67.10234832763672, 'l1_loss': 216.92660522460938, 'l2_loss': 58.292572021484375}
Step 1250: {'loss': 67.56326293945312, 'l1_loss': 222.9014129638672, 'l2_loss': 58.51082229614258}
Step 1260: {'loss': 67.69591522216797, 'l1_loss': 226.5474853515625, 'l2_loss': 58.50092697143555}
Step 1270: {'loss': 66.45597076416016, 'l1_loss': 216.1533660888672, 'l2_loss': 57.674560546875}
Step 1280: {'loss': 66.35408020019531, 'l1_loss': 212.60858154296875, 'l2_loss': 57.71637725830078}
Step 1290: {'loss': 67.03975677490234, 'l1_loss': 222.48434448242188, 'l2_loss': 58.00840377807617}
Step 1300: {'loss': 66.724853515625, 'l1_loss': 215.8284454345703, 'l2_loss': 57.955413818359375}
Step 1310: {'loss': 66.67556762695312, 'l1_loss': 203.5839385986328, 'l2_loss': 58.40039825439453}
Step 1320: {'loss': 66.20983123779297, 'l1_loss': 227.40237426757812, 'l2_loss': 56.981788635253906}
Step 1330: {'loss': 66.42860412597656, 'l1_loss': 211.11831665039062, 'l2_loss': 57.85246276855469}
Step 1340: {'loss': 66.2103500366211, 'l1_loss': 218.623291015625, 'l2_loss': 57.33251190185547}
Step 1350: {'loss': 66.24756622314453, 'l1_loss': 211.91098022460938, 'l2_loss': 57.634063720703125}
Step 1360: {'loss': 65.49539947509766, 'l1_loss': 212.287353515625, 'l2_loss': 56.87428665161133}
Step 1370: {'loss': 65.97331237792969, 'l1_loss': 207.48043823242188, 'l2_loss': 57.541141510009766}
Step 1380: {'loss': 66.28313446044922, 'l1_loss': 216.51507568359375, 'l2_loss': 57.48581314086914}
Step 1390: {'loss': 65.54857635498047, 'l1_loss': 204.72547912597656, 'l2_loss': 57.22797393798828}
Step 1400: {'loss': 65.88256072998047, 'l1_loss': 216.20016479492188, 'l2_loss': 57.10547637939453}
Step 1410: {'loss': 65.99486541748047, 'l1_loss': 207.28115844726562, 'l2_loss': 57.5518684387207}
Step 1420: {'loss': 65.07038116455078, 'l1_loss': 213.73716735839844, 'l2_loss': 56.391475677490234}
Step 1430: {'loss': 65.57122802734375, 'l1_loss': 204.39334106445312, 'l2_loss': 57.269020080566406}
Step 1440: {'loss': 65.67262268066406, 'l1_loss': 205.12112426757812, 'l2_loss': 57.335514068603516}
Step 1450: {'loss': 65.47158813476562, 'l1_loss': 206.94442749023438, 'l2_loss': 57.059654235839844}
Step 1460: {'loss': 64.78237915039062, 'l1_loss': 211.5201416015625, 'l2_loss': 56.188438415527344}
Step 1470: {'loss': 65.47730255126953, 'l1_loss': 208.99462890625, 'l2_loss': 56.97969436645508}
Step 1480: {'loss': 65.49954986572266, 'l1_loss': 223.10028076171875, 'l2_loss': 56.407936096191406}
Step 1490: {'loss': 65.32276916503906, 'l1_loss': 214.92335510253906, 'l2_loss': 56.59441375732422}
Step 1500: {'loss': 65.64181518554688, 'l1_loss': 207.57911682128906, 'l2_loss': 57.20943832397461}
Step 1510: {'loss': 64.19800567626953, 'l1_loss': 204.71633911132812, 'l2_loss': 55.876075744628906}
Step 1520: {'loss': 65.33038330078125, 'l1_loss': 224.50828552246094, 'l2_loss': 56.217464447021484}
Step 1530: {'loss': 64.54312133789062, 'l1_loss': 197.3249969482422, 'l2_loss': 56.51551818847656}
Step 1540: {'loss': 64.8169174194336, 'l1_loss': 211.271240234375, 'l2_loss': 56.228118896484375}
Step 1550: {'loss': 64.33872985839844, 'l1_loss': 203.6876220703125, 'l2_loss': 56.053916931152344}
Step 1560: {'loss': 64.59819030761719, 'l1_loss': 199.43418884277344, 'l2_loss': 56.48518753051758}
Step 1570: {'loss': 65.20194244384766, 'l1_loss': 210.40541076660156, 'l2_loss': 56.649139404296875}
Step 1580: {'loss': 64.76951599121094, 'l1_loss': 212.22506713867188, 'l2_loss': 56.1435546875}
Step 1590: {'loss': 64.28791809082031, 'l1_loss': 202.365234375, 'l2_loss': 56.058528900146484}
Step 1600: {'loss': 64.8820571899414, 'l1_loss': 208.5934600830078, 'l2_loss': 56.395084381103516}
Step 1610: {'loss': 63.86184310913086, 'l1_loss': 207.607177734375, 'l2_loss': 55.41936492919922}
Step 1620: {'loss': 65.5725326538086, 'l1_loss': 216.80166625976562, 'l2_loss': 56.765411376953125}
Step 1630: {'loss': 65.23656463623047, 'l1_loss': 221.00119018554688, 'l2_loss': 56.263160705566406}
Step 1640: {'loss': 63.589080810546875, 'l1_loss': 205.34365844726562, 'l2_loss': 55.23862075805664}
Step 1650: {'loss': 64.48033905029297, 'l1_loss': 200.4971923828125, 'l2_loss': 56.32587432861328}
Step 1660: {'loss': 63.82539749145508, 'l1_loss': 207.12791442871094, 'l2_loss': 55.40439224243164}
Step 1670: {'loss': 64.36824798583984, 'l1_loss': 206.82723999023438, 'l2_loss': 55.94670867919922}
Step 1680: {'loss': 64.28643035888672, 'l1_loss': 213.30792236328125, 'l2_loss': 55.614845275878906}
Step 1690: {'loss': 62.744117736816406, 'l1_loss': 198.02420043945312, 'l2_loss': 54.68730926513672}
Step 1700: {'loss': 63.79418182373047, 'l1_loss': 205.3791961669922, 'l2_loss': 55.43975830078125}
Step 1710: {'loss': 64.54639434814453, 'l1_loss': 218.4034423828125, 'l2_loss': 55.675926208496094}
Step 1720: {'loss': 63.937713623046875, 'l1_loss': 214.85031127929688, 'l2_loss': 55.205177307128906}
Step 1730: {'loss': 63.90568542480469, 'l1_loss': 204.3302764892578, 'l2_loss': 55.59696960449219}
Step 1740: {'loss': 64.23306274414062, 'l1_loss': 221.7449951171875, 'l2_loss': 55.227020263671875}
Step 1750: {'loss': 63.705257415771484, 'l1_loss': 197.9569091796875, 'l2_loss': 55.64551544189453}
Step 1760: {'loss': 64.97051239013672, 'l1_loss': 218.24356079101562, 'l2_loss': 56.10527801513672}
Step 1770: {'loss': 64.1229476928711, 'l1_loss': 208.50233459472656, 'l2_loss': 55.64594268798828}
Step 1780: {'loss': 63.782623291015625, 'l1_loss': 208.4413299560547, 'l2_loss': 55.309410095214844}
Step 1790: {'loss': 63.63768768310547, 'l1_loss': 210.42745971679688, 'l2_loss': 55.08499526977539}
Step 1800: {'loss': 63.96034622192383, 'l1_loss': 213.00448608398438, 'l2_loss': 55.305572509765625}
Step 1810: {'loss': 63.63761520385742, 'l1_loss': 205.6670379638672, 'l2_loss': 55.27348327636719}
Step 1820: {'loss': 64.1607894897461, 'l1_loss': 212.62416076660156, 'l2_loss': 55.51008987426758}
Step 1830: {'loss': 63.38996505737305, 'l1_loss': 201.4660186767578, 'l2_loss': 55.19117736816406}
Step 1840: {'loss': 63.3408088684082, 'l1_loss': 204.8934783935547, 'l2_loss': 55.00922775268555}
Step 1850: {'loss': 63.642730712890625, 'l1_loss': 208.6585235595703, 'l2_loss': 55.15947723388672}
Step 1860: {'loss': 63.40415573120117, 'l1_loss': 207.0641632080078, 'l2_loss': 54.97007369995117}
Step 1870: {'loss': 64.1882553100586, 'l1_loss': 213.89639282226562, 'l2_loss': 55.498321533203125}
Step 1880: {'loss': 62.16242599487305, 'l1_loss': 213.37818908691406, 'l2_loss': 53.49114227294922}
Step 1890: {'loss': 63.07548904418945, 'l1_loss': 204.798095703125, 'l2_loss': 54.74457550048828}
Step 1900: {'loss': 63.391109466552734, 'l1_loss': 210.58609008789062, 'l2_loss': 54.826316833496094}
Step 1910: {'loss': 63.43272018432617, 'l1_loss': 211.25096130371094, 'l2_loss': 54.844696044921875}
Step 1920: {'loss': 64.02629089355469, 'l1_loss': 207.90565490722656, 'l2_loss': 55.569786071777344}
Step 1930: {'loss': 63.72547149658203, 'l1_loss': 204.25941467285156, 'l2_loss': 55.417144775390625}
Step 1940: {'loss': 63.09511184692383, 'l1_loss': 211.4562225341797, 'l2_loss': 54.498016357421875}
Step 1950: {'loss': 63.74074172973633, 'l1_loss': 207.01593017578125, 'l2_loss': 55.328407287597656}
Step 1960: {'loss': 63.6540641784668, 'l1_loss': 216.2214813232422, 'l2_loss': 54.861122131347656}
Step 1970: {'loss': 62.57707214355469, 'l1_loss': 211.31961059570312, 'l2_loss': 53.98515319824219}
Step 1980: {'loss': 63.050418853759766, 'l1_loss': 205.39962768554688, 'l2_loss': 54.69476318359375}
Step 1990: {'loss': 63.401432037353516, 'l1_loss': 204.537841796875, 'l2_loss': 55.07893371582031}
Step 2000: {'loss': 63.39004898071289, 'l1_loss': 212.1727294921875, 'l2_loss': 54.76445007324219}
Step 2010: {'loss': 62.98193359375, 'l1_loss': 219.42137145996094, 'l2_loss': 54.058921813964844}
Step 2020: {'loss': 62.75962448120117, 'l1_loss': 205.11367797851562, 'l2_loss': 54.41722869873047}
Step 2030: {'loss': 63.62154769897461, 'l1_loss': 211.3090057373047, 'l2_loss': 55.028560638427734}
Step 2040: {'loss': 63.42687225341797, 'l1_loss': 212.6851043701172, 'l2_loss': 54.77751922607422}
Step 2050: {'loss': 62.0287971496582, 'l1_loss': 203.13510131835938, 'l2_loss': 53.7649040222168}
Step 2060: {'loss': 62.67513656616211, 'l1_loss': 205.12037658691406, 'l2_loss': 54.32881546020508}
Step 2070: {'loss': 62.598548889160156, 'l1_loss': 208.10159301757812, 'l2_loss': 54.137596130371094}
Step 2080: {'loss': 62.99798583984375, 'l1_loss': 211.65188598632812, 'l2_loss': 54.38957977294922}
Step 2090: {'loss': 61.84661102294922, 'l1_loss': 210.4207763671875, 'l2_loss': 53.292476654052734}
Step 2100: {'loss': 62.96026611328125, 'l1_loss': 209.60220336914062, 'l2_loss': 54.44013595581055}
Step 2110: {'loss': 62.323211669921875, 'l1_loss': 207.410888671875, 'l2_loss': 53.8900260925293}
Step 2120: {'loss': 63.16725540161133, 'l1_loss': 212.53579711914062, 'l2_loss': 54.52424621582031}
Step 2130: {'loss': 62.32383346557617, 'l1_loss': 216.9647216796875, 'l2_loss': 53.50222396850586}
Step 2140: {'loss': 62.301734924316406, 'l1_loss': 215.31170654296875, 'l2_loss': 53.54937744140625}
Step 2150: {'loss': 61.81257629394531, 'l1_loss': 210.84701538085938, 'l2_loss': 53.24046325683594}
Step 2160: {'loss': 62.23898696899414, 'l1_loss': 213.7567901611328, 'l2_loss': 53.54878234863281}
Step 2170: {'loss': 61.986480712890625, 'l1_loss': 210.46690368652344, 'l2_loss': 53.424583435058594}
Step 2180: {'loss': 62.97420120239258, 'l1_loss': 212.97735595703125, 'l2_loss': 54.31431579589844}
Step 2190: {'loss': 62.68059539794922, 'l1_loss': 224.0929412841797, 'l2_loss': 53.57440948486328}
Step 2200: {'loss': 62.16696548461914, 'l1_loss': 203.2808380126953, 'l2_loss': 53.88883590698242}
Step 2210: {'loss': 62.46947479248047, 'l1_loss': 217.4451141357422, 'l2_loss': 53.63238525390625}
Step 2220: {'loss': 63.03238296508789, 'l1_loss': 219.47021484375, 'l2_loss': 54.109222412109375}
Step 2230: {'loss': 62.64482116699219, 'l1_loss': 218.62600708007812, 'l2_loss': 53.762481689453125}
Step 2240: {'loss': 62.5172119140625, 'l1_loss': 207.41278076171875, 'l2_loss': 54.07305145263672}
Step 2250: {'loss': 61.79252243041992, 'l1_loss': 208.15635681152344, 'l2_loss': 53.326812744140625}
Step 2260: {'loss': 61.50104522705078, 'l1_loss': 211.62106323242188, 'l2_loss': 52.894744873046875}
Step 2270: {'loss': 61.774024963378906, 'l1_loss': 214.2037353515625, 'l2_loss': 53.06251525878906}
Step 2280: {'loss': 61.3496208190918, 'l1_loss': 212.4106903076172, 'l2_loss': 52.71092987060547}
Step 2290: {'loss': 62.68158721923828, 'l1_loss': 212.53286743164062, 'l2_loss': 54.044681549072266}
Step 2300: {'loss': 61.3097038269043, 'l1_loss': 215.42730712890625, 'l2_loss': 52.553524017333984}
Step 2310: {'loss': 61.93910217285156, 'l1_loss': 211.3292236328125, 'l2_loss': 53.34796142578125}
Step 2320: {'loss': 61.78564453125, 'l1_loss': 207.89251708984375, 'l2_loss': 53.329811096191406}
Step 2330: {'loss': 61.93653869628906, 'l1_loss': 215.97174072265625, 'l2_loss': 53.15495681762695}
Step 2340: {'loss': 61.00575637817383, 'l1_loss': 209.57669067382812, 'l2_loss': 52.47393798828125}
Step 2350: {'loss': 61.83991622924805, 'l1_loss': 210.0505828857422, 'l2_loss': 53.29425811767578}
Step 2360: {'loss': 62.49408721923828, 'l1_loss': 215.64767456054688, 'l2_loss': 53.7232666015625}
Step 2370: {'loss': 62.51968765258789, 'l1_loss': 220.7615509033203, 'l2_loss': 53.54768371582031}
Step 2380: {'loss': 60.86750793457031, 'l1_loss': 216.19497680664062, 'l2_loss': 52.076507568359375}
Step 2390: {'loss': 61.44960403442383, 'l1_loss': 210.67218017578125, 'l2_loss': 52.87677764892578}
Step 2400: {'loss': 61.658241271972656, 'l1_loss': 214.6576385498047, 'l2_loss': 52.93220520019531}
Step 2410: {'loss': 61.715946197509766, 'l1_loss': 208.47076416015625, 'l2_loss': 53.236572265625}
Step 2420: {'loss': 61.35874557495117, 'l1_loss': 217.17572021484375, 'l2_loss': 52.52813720703125}
Step 2430: {'loss': 61.90418243408203, 'l1_loss': 220.34005737304688, 'l2_loss': 52.94962692260742}
Step 2440: {'loss': 62.157005310058594, 'l1_loss': 213.9933319091797, 'l2_loss': 53.45348358154297}
Step 2450: {'loss': 61.63378143310547, 'l1_loss': 214.14340209960938, 'l2_loss': 52.922935485839844}
Step 2460: {'loss': 61.717323303222656, 'l1_loss': 210.68540954589844, 'l2_loss': 53.1490364074707}
Step 2470: {'loss': 62.19927978515625, 'l1_loss': 217.56822204589844, 'l2_loss': 53.35084915161133}
Step 2480: {'loss': 61.93266677856445, 'l1_loss': 215.78346252441406, 'l2_loss': 53.15488815307617}
Step 2490: {'loss': 61.662574768066406, 'l1_loss': 209.75119018554688, 'l2_loss': 53.133758544921875}
Step 2500: {'loss': 61.208946228027344, 'l1_loss': 217.51431274414062, 'l2_loss': 52.36552047729492}
Step 2510: {'loss': 62.03624725341797, 'l1_loss': 223.91563415527344, 'l2_loss': 52.935386657714844}
Step 2520: {'loss': 61.77114486694336, 'l1_loss': 210.65084838867188, 'l2_loss': 53.202423095703125}
Step 2530: {'loss': 60.73757553100586, 'l1_loss': 209.4267578125, 'l2_loss': 52.21141052246094}
Step 2540: {'loss': 61.633155822753906, 'l1_loss': 232.6536865234375, 'l2_loss': 52.18256378173828}
Step 2550: {'loss': 61.194217681884766, 'l1_loss': 210.0791473388672, 'l2_loss': 52.65111541748047}
Step 2560: {'loss': 61.24766159057617, 'l1_loss': 219.2134552001953, 'l2_loss': 52.33396530151367}
Step 2570: {'loss': 61.749267578125, 'l1_loss': 224.2313995361328, 'l2_loss': 52.6364860534668}
Step 2580: {'loss': 61.02321243286133, 'l1_loss': 225.6260528564453, 'l2_loss': 51.858856201171875}
Step 2590: {'loss': 60.830074310302734, 'l1_loss': 214.31939697265625, 'l2_loss': 52.117835998535156}
Step 2600: {'loss': 61.745418548583984, 'l1_loss': 224.95654296875, 'l2_loss': 52.60411071777344}
Step 2610: {'loss': 61.103309631347656, 'l1_loss': 218.92727661132812, 'l2_loss': 52.207061767578125}
Step 2620: {'loss': 61.63043975830078, 'l1_loss': 215.0895233154297, 'l2_loss': 52.88333511352539}
Step 2630: {'loss': 61.147682189941406, 'l1_loss': 218.83938598632812, 'l2_loss': 52.24699020385742}
Step 2640: {'loss': 61.02276611328125, 'l1_loss': 216.15045166015625, 'l2_loss': 52.2315673828125}
Step 2650: {'loss': 61.934024810791016, 'l1_loss': 219.3057861328125, 'l2_loss': 53.02070617675781}
Step 2660: {'loss': 61.225982666015625, 'l1_loss': 212.52194213867188, 'l2_loss': 52.5823860168457}
Step 2670: {'loss': 61.28953552246094, 'l1_loss': 223.85165405273438, 'l2_loss': 52.19331359863281}
Step 2680: {'loss': 60.860225677490234, 'l1_loss': 216.0353240966797, 'l2_loss': 52.07490921020508}
Step 2690: {'loss': 61.55373764038086, 'l1_loss': 214.3555908203125, 'l2_loss': 52.839027404785156}
Step 2700: {'loss': 61.36245346069336, 'l1_loss': 218.98228454589844, 'l2_loss': 52.4632568359375}
Step 2710: {'loss': 60.85559844970703, 'l1_loss': 212.19454956054688, 'l2_loss': 52.22702407836914}
Step 2720: {'loss': 60.766910552978516, 'l1_loss': 219.0129852294922, 'l2_loss': 51.868431091308594}
Step 2730: {'loss': 61.36492156982422, 'l1_loss': 214.09625244140625, 'l2_loss': 52.657447814941406}
Step 2740: {'loss': 60.19276809692383, 'l1_loss': 213.47113037109375, 'l2_loss': 51.510597229003906}
Step 2750: {'loss': 60.89555740356445, 'l1_loss': 216.1214141845703, 'l2_loss': 52.104896545410156}
Step 2760: {'loss': 60.88245391845703, 'l1_loss': 221.95330810546875, 'l2_loss': 51.85645294189453}
Step 2770: {'loss': 61.2081184387207, 'l1_loss': 217.92718505859375, 'l2_loss': 52.34843063354492}
Step 2780: {'loss': 60.35880661010742, 'l1_loss': 211.71359252929688, 'l2_loss': 51.74546432495117}
Step 2790: {'loss': 60.736488342285156, 'l1_loss': 223.2431640625, 'l2_loss': 51.662940979003906}
Step 2800: {'loss': 61.62043762207031, 'l1_loss': 217.99130249023438, 'l2_loss': 52.753082275390625}
Step 2810: {'loss': 61.549461364746094, 'l1_loss': 226.24876403808594, 'l2_loss': 52.35406494140625}
Step 2820: {'loss': 60.29601287841797, 'l1_loss': 217.77542114257812, 'l2_loss': 51.43861770629883}
Step 2830: {'loss': 61.19510269165039, 'l1_loss': 230.10108947753906, 'l2_loss': 51.842369079589844}
Step 2840: {'loss': 60.94028854370117, 'l1_loss': 218.74966430664062, 'l2_loss': 52.04579544067383}
Step 2850: {'loss': 61.6278190612793, 'l1_loss': 224.19149780273438, 'l2_loss': 52.513694763183594}
Step 2860: {'loss': 60.52643585205078, 'l1_loss': 218.4392547607422, 'l2_loss': 51.637939453125}
Step 2870: {'loss': 60.60955047607422, 'l1_loss': 217.87307739257812, 'l2_loss': 51.749755859375}
Step 2880: {'loss': 60.86305236816406, 'l1_loss': 216.91590881347656, 'l2_loss': 52.04176712036133}
Step 2890: {'loss': 60.43730545043945, 'l1_loss': 217.993408203125, 'l2_loss': 51.57243728637695}
Step 2900: {'loss': 60.61632537841797, 'l1_loss': 214.82131958007812, 'l2_loss': 51.877708435058594}
Step 2910: {'loss': 61.12890625, 'l1_loss': 217.1158905029297, 'l2_loss': 52.297584533691406}
Step 2920: {'loss': 60.16506576538086, 'l1_loss': 217.8154296875, 'l2_loss': 51.30849075317383}
Step 2930: {'loss': 60.6362419128418, 'l1_loss': 224.3025360107422, 'l2_loss': 51.51907730102539}
Step 2940: {'loss': 59.93790054321289, 'l1_loss': 218.5471954345703, 'l2_loss': 51.05540466308594}
Step 2950: {'loss': 60.79631805419922, 'l1_loss': 215.6688232421875, 'l2_loss': 52.02978515625}
Step 2960: {'loss': 59.78338623046875, 'l1_loss': 213.56146240234375, 'l2_loss': 51.097877502441406}
Step 2970: {'loss': 60.33940887451172, 'l1_loss': 223.1915283203125, 'l2_loss': 51.26856231689453}
Step 2980: {'loss': 61.430110931396484, 'l1_loss': 223.7609100341797, 'l2_loss': 52.336669921875}
Step 2990: {'loss': 61.52676010131836, 'l1_loss': 225.81478881835938, 'l2_loss': 52.3510856628418}
Step 3000: {'loss': 60.485382080078125, 'l1_loss': 225.52529907226562, 'l2_loss': 51.32184600830078}
Step 3010: {'loss': 60.30447769165039, 'l1_loss': 221.45660400390625, 'l2_loss': 51.29884338378906}
Step 3020: {'loss': 61.53081512451172, 'l1_loss': 226.25225830078125, 'l2_loss': 52.33155059814453}
Step 3030: {'loss': 60.034847259521484, 'l1_loss': 225.45425415039062, 'l2_loss': 50.86872863769531}
Step 3040: {'loss': 60.39578628540039, 'l1_loss': 223.26861572265625, 'l2_loss': 51.32072067260742}
Step 3050: {'loss': 60.72863006591797, 'l1_loss': 224.15670776367188, 'l2_loss': 51.61732864379883}
Step 3060: {'loss': 59.78424072265625, 'l1_loss': 222.0836181640625, 'l2_loss': 50.75579833984375}
Step 3070: {'loss': 59.79821014404297, 'l1_loss': 215.492431640625, 'l2_loss': 51.02920913696289}
Step 3080: {'loss': 60.67922592163086, 'l1_loss': 225.163818359375, 'l2_loss': 51.523345947265625}
Step 3090: {'loss': 60.863338470458984, 'l1_loss': 220.73101806640625, 'l2_loss': 51.891441345214844}
Step 3100: {'loss': 60.35929489135742, 'l1_loss': 224.28518676757812, 'l2_loss': 51.24211120605469}
Step 3110: {'loss': 60.66031265258789, 'l1_loss': 221.71640014648438, 'l2_loss': 51.6466064453125}
Step 3120: {'loss': 60.409061431884766, 'l1_loss': 224.7354736328125, 'l2_loss': 51.27518844604492}
Step 3130: {'loss': 60.2481689453125, 'l1_loss': 222.03543090820312, 'l2_loss': 51.21604919433594}
Step 3140: {'loss': 59.602970123291016, 'l1_loss': 220.74749755859375, 'l2_loss': 50.62802505493164}
Step 3150: {'loss': 59.906211853027344, 'l1_loss': 227.65579223632812, 'l2_loss': 50.654930114746094}
Step 3160: {'loss': 60.017208099365234, 'l1_loss': 221.64370727539062, 'l2_loss': 51.00864791870117}
Step 3170: {'loss': 60.642303466796875, 'l1_loss': 225.33261108398438, 'l2_loss': 51.485084533691406}
Step 3180: {'loss': 59.837440490722656, 'l1_loss': 217.6117401123047, 'l2_loss': 50.990318298339844}
Step 3190: {'loss': 60.177974700927734, 'l1_loss': 222.13258361816406, 'l2_loss': 51.14943313598633}
Step 3200: {'loss': 60.148921966552734, 'l1_loss': 222.81704711914062, 'l2_loss': 51.09343719482422}
Step 3210: {'loss': 60.79543685913086, 'l1_loss': 229.2692108154297, 'l2_loss': 51.478729248046875}
Step 3220: {'loss': 60.22342300415039, 'l1_loss': 219.00851440429688, 'l2_loss': 51.32044982910156}
Step 3230: {'loss': 60.734676361083984, 'l1_loss': 219.84072875976562, 'l2_loss': 51.79710006713867}
Step 3240: {'loss': 59.60377883911133, 'l1_loss': 222.92417907714844, 'l2_loss': 50.542816162109375}
Step 3250: {'loss': 60.130088806152344, 'l1_loss': 222.24465942382812, 'l2_loss': 51.09151840209961}
Step 3260: {'loss': 59.77985763549805, 'l1_loss': 223.47988891601562, 'l2_loss': 50.6955680847168}
Step 3270: {'loss': 60.27246856689453, 'l1_loss': 219.55531311035156, 'l2_loss': 51.342979431152344}
Step 3280: {'loss': 60.223087310791016, 'l1_loss': 223.05784606933594, 'l2_loss': 51.15658187866211}
Step 3290: {'loss': 59.9383544921875, 'l1_loss': 220.63279724121094, 'l2_loss': 50.962989807128906}
Step 3300: {'loss': 60.471412658691406, 'l1_loss': 220.99310302734375, 'l2_loss': 51.48875045776367}
Step 3310: {'loss': 59.600093841552734, 'l1_loss': 216.9562530517578, 'l2_loss': 50.78004837036133}
Step 3320: {'loss': 59.93735122680664, 'l1_loss': 219.89495849609375, 'l2_loss': 50.9943733215332}