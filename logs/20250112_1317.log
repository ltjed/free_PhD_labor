Using GPUs: [0, 1]
Using Anthropic API with model claude-3-5-sonnet-20241022.
Loaded existing ideas:
{'Name': 'adaptive_sparse_autoencoders', 'Title': 'Adaptive Computation in Sparse Autoencoders', 'Experiment': '1. Implement Feature Choice and Mutual Choice sparsifying activation functions\n2. Add aux_zipf_loss and aux_k_loss auxiliary losses\n3. Train SAEs with new activation functions and losses on GPT-2 sized residual stream activations\n4. Compare performance (sparsity, reconstruction error, model loss) and feature utilization against baseline TopK SAEs\n5. Analyze distribution of features per token and feature densities\n6. Implement phased training with Mutual Choice followed by Feature Choice', 'Technical_Details': 'The paper proposes two novel sparse autoencoder (SAE) variants: Feature Choice (FC) and Mutual Choice (MC). These allow for variable numbers of active features per token, framing the token-feature matching as a resource allocation problem with a total sparsity upper bound. The FC approach allows each feature to select m tokens to process, where m = M/F (M is total matches, F is number of features). MC combines aspects of FC and token choice. A new aux_zipf_loss is introduced to encourage feature densities to follow a Zipf distribution, mitigating feature under-utilization. The paper also suggests a phased training approach, starting with MC and transitioning to FC.', 'Research_Impact': 'A key challenge in mechanistic interpretability is extracting meaningful, interpretable features from neural networks while maintaining computational efficiency. This research addresses this by introducing adaptive computation in SAEs, allowing more features and computation for difficult-to-reconstruct tokens. The proposed methods achieve higher reconstruction accuracy with fewer dead features compared to standard approaches, potentially leading to more robust and interpretable feature extraction in large language models.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8, 'novel': False}
{'Name': 'gated_sparse_autoencoder', 'Title': '©2024 Google DeepMind. All rights reservedarXiv:2404.16014v2  [cs.LG]  30 Apr 2024', 'Experiment': '1. Implement Gated SAE architecture with separate gating and magnitude components\n2. Modify loss function to include L1 penalty on gating activations and auxiliary reconstruction task\n3. Train Gated SAEs on activations from GELU-1L, Pythia-2.8B, and Gemma-7B models\n4. Evaluate performance using loss recovered vs. L0 sparsity metrics\n5. Compare against baseline SAEs using Pareto frontier analysis\n6. Conduct shrinkage analysis and human interpretability study', 'Technical_Details': 'The Gated SAE architecture separates feature detection and magnitude estimation by using two sets of weights: w_gate for determining active features and w_mag for estimating magnitudes. The encoder output is computed as h = ReLU(w_gate * x + b_gate) * (w_mag * x + b_mag). The loss function includes an L1 penalty on ReLU(w_gate * x + b_gate) to encourage sparsity, and an auxiliary reconstruction task using these gating activations. Weight tying is employed between encoder and decoder. The architecture can be interpreted as a single-layer encoder with a parameterized JumpReLU activation function.', 'Research_Impact': 'A key challenge in mechanistic interpretability is finding sparse, interpretable features in language model activations. Gated SAEs address this by improving dictionary learning, achieving better reconstruction fidelity at given sparsity levels compared to standard SAEs. They also mitigate the shrinkage problem inherent in L1 regularization, potentially leading to more accurate feature representations. This could enable more reliable extraction of interpretable features across different model sizes and activation sites, advancing our ability to understand the internal workings of large language models.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7, 'novel': False}
{'Name': 'batchtopk_sae', 'Title': 'Information Processing Systems (NeurIPS 2024).arXiv:2412.06410v1  [cs.LG]  9 Dec 2024', 'Experiment': '1. Implement BatchTopK function to replace sample-level TopK operation.2. Modify SAE training procedure to use BatchTopK for sparsity constraint.3. Implement threshold estimation method for inference.4. Train SAEs on GPT-2 Small and Gemma 2 2B activations using both TopK and BatchTopK.5. Compare reconstruction quality (normalized MSE) and impact on language modeling (cross-entropy degradation).6. Evaluate performance across different dictionary sizes (3072, 6144, 12288, 24576).7. Analyze latent activation patterns and flexibility in latent allocation.', 'Technical_Details': 'BatchTopK is a novel training method for Sparse Autoencoders (SAEs) that replaces the sample-level TopK operation with a batch-level constraint. Instead of enforcing a fixed number of active latents per sample, BatchTopK selects the top activations across the entire batch. This allows for variable sparsity per sample, with some samples using more latents and others using fewer. The method introduces a batch dependency during training, which is addressed during inference by estimating a global threshold parameter. This threshold is calculated as the average of minimum positive activation values across multiple batches. The SAE is trained on language model activations (e.g., from the residual stream) using a large text corpus. The loss function combines L2 reconstruction error with an L0 sparsity penalty. Experiments were conducted on GPT-2 Small and Gemma 2 2B models, using dictionary sizes of 3072, 6144, 12288, and 24576, with a bandwidth parameter of 0.001 and the Adam optimizer (learning rate 3e-4).', 'Research_Impact': 'A significant challenge in mechanistic interpretability is developing methods to effectively analyze and interpret the internal representations of large language models. BatchTopK SAEs address this challenge by providing a more flexible and efficient way to compress and represent model activations. By allowing variable sparsity per sample, BatchTopK can potentially capture more nuanced and diverse activation patterns compared to fixed-sparsity methods. This improvement in reconstruction quality, as demonstrated by lower normalized MSE and reduced cross-entropy degradation, could lead to more accurate and insightful interpretations of model behavior. Furthermore, the ability to adaptively allocate latents based on sample complexity aligns well with the varying information content in natural language, potentially revealing more about how language models process different types of inputs.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6, 'novel': False}
{'Name': 'jumprelu_sae', 'Title': '©2024 Google DeepMind. All rights reservedarXiv:2407.14435v3  [cs.LG]  1 Aug 2024', 'Experiment': 'Implement JumpReLU activation function for sparse autoencoders. Modify existing SAE architecture to use JumpReLU instead of ReLU. Train JumpReLU SAEs on language model activations (e.g. Gemma 2 9B). Compare reconstruction fidelity and sparsity trade-offs against Gated and TopK SAEs. Conduct manual and automated interpretability studies on learned features.', 'Technical_Details': 'JumpReLU SAE introduces a threshold parameter τ for each feature. The activation function zeroes out pre-activations below τ. Loss function combines L2 reconstruction error and L0 sparsity penalty. Straight-through estimators are used to estimate gradients of the expected loss. Pseudo-derivatives provide gradient signals within a small window around the threshold. Training involves computing gradients over batches and using batch-wise mean for parameter updates.', 'Research_Impact': 'Addresses the challenge of balancing reconstruction fidelity and interpretability in sparse representations of language model activations. JumpReLU SAEs achieve state-of-the-art reconstruction fidelity at given sparsity levels without sacrificing interpretability. This improves upon existing methods like Gated and TopK SAEs, potentially enabling more accurate identification of computational subgraphs and causal mechanisms in language models.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6, 'novel': False}
{'Name': 'clustered_sparse_autoencoders', 'Title': 'Clustered Sparse Autoencoders for Efficient Interpretability of Large Language Models', 'Experiment': '1. Implement clustering algorithm to group model layers2. Train single SAE for each cluster instead of per-layer3. Evaluate reconstruction performance and sparsity metrics4. Assess downstream task performance using faithfulness/completeness5. Analyze feature interpretability across clustered layers6. Compare computational efficiency to baseline per-layer approach', 'Technical_Details': 'The approach clusters contiguous layers in a large language model and trains a single sparse autoencoder (SAE) per cluster, rather than per individual layer. This reduces the number of SAEs by a factor k, where k is the number of clusters. The method uses the JumpReLU activation function and optimizes an objective combining L2 reconstruction loss and L0 sparsity. Evaluation metrics include L2 loss, R2 score, L0 sparsity, as well as faithfulness and completeness on downstream tasks. The approach is tested with varying k values from 1 to 5, excluding the final layer of the model.', 'Research_Impact': 'This research addresses the computational challenge of training sparse autoencoders for very large language models, which has become a bottleneck in mechanistic interpretability research. By reducing the number of required SAEs, it enables more efficient analysis of state-of-the-art models with billions of parameters. This approach could accelerate progress in understanding the inner workings of advanced AI systems, addressing a key challenge in the field of AI interpretability and safety.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 6, 'novel': False}
{'Name': 'mutual_feature_regularization', 'Title': '1', 'Experiment': '1. Implement MFR technique for SAEs:        - Add reinitializing of SAE weights when too many inactive features detected        - Implement auxiliary penalty to incentivize features present in other SAE decoders    2. Train SAEs with and without MFR on:        - Synthetic dataset with known features        - GPT-2 Small first layer MLP outputs        - EEG data from TUH EEG Corpus    3. Compare reconstruction loss and feature recovery between baseline and MFR SAEs    4. Analyze L2 distance between decoder matrices to assess feature similarity', 'Technical_Details': 'Mutual Feature Regularization (MFR) consists of two main components:    1. Reinitialization: Check for inactive features in SAE hidden state. If too many are detected, reinitialize the weights of the affected SAE.    2. Auxiliary penalty: Add a penalty term to the loss function that encourages features to be present in decoders of other SAEs trained on the same data.        Implementation details:    - Use TopK activation function for sparsity in SAE hidden state    - Train with AdamW optimizer    - Apply cosine warmup for 100 training steps on the auxiliary penalty    - Set auxiliary penalty coefficient to make initial reconstruction loss and penalty equivalent    - Evaluate using reconstruction loss (Euclidean distance between input and output) and feature recovery metrics', 'Research_Impact': 'MFR addresses the challenge of SAEs learning features that are not actually present in the input data, which limits their interpretability. This is a significant issue in mechanistic interpretability, where the goal is to understand the true features and computations of neural networks. By encouraging feature consistency across multiple SAEs, MFR increases the likelihood that learned features correspond to actual input features. This improvement could lead to more reliable and meaningful interpretations of neural network activations, advancing our understanding of how these models process information.', 'Interestingness': 7, 'Feasibility': 8, 'Novelty': 6, 'novel': False}
{'Name': 'switch_sparse_autoencoder', 'Title': 'Switch Sparse Autoencoders', 'Experiment': '1. Implement Switch layer architecture for sparse autoencoders (SAEs)\n2. Modify existing SAE training pipeline to incorporate Switch layer\n3. Train Switch SAEs on GPT-2 small residual stream activations\n4. Compare performance against TopK, ReLU, and Gated SAEs using metrics like reconstruction MSE, cross-entropy loss recovery, and feature interpretability\n5. Analyze scaling laws by training models with varying numbers of experts (16, 32, 64, 128)\n6. Evaluate expert specialization using nearest neighbor cosine similarity\n7. Perform t-SNE projections to visualize feature clustering\n8. Assess true positive and true negative rates for feature detection', 'Technical_Details': 'The Switch Sparse Autoencoder (Switch SAE) combines the Switch layer architecture with TopK SAEs. It consists of multiple expert SAEs and a trainable routing network. The router computes a probability distribution over experts and routes input activations to the expert with the highest probability. This approach reduces computational costs by avoiding dense matrix multiplications. The model is trained on residual stream activations of GPT-2 small, optimizing for reconstruction MSE. The architecture allows for scaling to a large number of features (up to 34 million in this study) while maintaining computational efficiency. The Switch SAE demonstrates improved performance in terms of reconstruction error vs. sparsity trade-off compared to other SAE variants.', 'Research_Impact': 'A key challenge in mechanistic interpretability is scaling sparse autoencoders to very high widths to identify all features represented in frontier models. This research directly addresses this challenge by introducing the Switch SAE architecture, which reduces the compute cost of training wide SAEs. By leveraging conditional computation, the Switch SAE enables scaling to millions of features while maintaining computational tractability. This advancement allows researchers to probe deeper into the internal representations of large language models, potentially uncovering a more comprehensive set of interpretable features and advancing our understanding of model behavior.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8, 'novel': False}
{'Name': 'sparse_autoencoder_improvements', 'Title': 'Sparse Autoencoder Viewer', 'Experiment': '1. Implement TopK activation function and compare with ReLU and ProLU.\n2. Develop Multi-TopK loss function.\n3. Train autoencoders on GPT-2 small and GPT-4 family models with varying latent sizes.\n4. Evaluate using new metrics: downstream loss, probe loss, explainability, and ablation sparsity.\n5. Analyze scaling laws for MSE, compute, and model size.\n6. Implement and test AuxK loss for reducing dead latents.\n7. Conduct ablation studies on different positions and layers of the models.', 'Technical_Details': 'The paper introduces several technical improvements for training sparse autoencoders:\n1. TopK activation function: Only keeps the k largest latents, zeroing the rest.\n2. Multi-TopK loss: Sums multiple TopK losses with different k values to improve generalization.\n3. AuxK loss: An auxiliary loss that models reconstruction error using top-k dead latents.\n4. Optimization techniques: Using Adam optimizer with specific beta values, EMA of weights, and gradient projection.\n5. Scaling laws: Empirical observations on how MSE scales with compute, number of latents, and sparsity level.\n6. Evaluation metrics: New metrics including downstream loss, probe loss, explainability, and ablation sparsity to better quantify autoencoder quality.\n7. Parallelization strategies: Utilizing data parallel and tensor sharding techniques to handle large models.', 'Research_Impact': 'This research addresses the challenge of training extremely wide and sparse autoencoders, which has been a limiting factor in extracting interpretable features from large language models. Specifically:\n1. It improves the reconstruction-sparsity trade-off, allowing for better feature extraction with fewer active latents.\n2. The introduction of TopK and Multi-TopK activation functions mitigates the overfitting problem observed in ReLU-based autoencoders.\n3. The new evaluation metrics provide a more comprehensive assessment of autoencoder quality, moving beyond simple reconstruction error.\n4. The scaling laws discovered offer insights into the relationship between model size, compute, and autoencoder performance, which can guide future research in this area.\n5. The techniques for reducing dead latents (like AuxK loss) address a common problem in sparse autoencoder training, potentially leading to more efficient feature extraction.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 7, 'novel': False}
{'Name': 'causal_sparse_autoencoder', 'Title': 'Causal Sparse Autoencoders: Learning Causally Verified Features in Language Models', 'Experiment': '1. Implement efficient batch-wise causal intervention system\n2. Define normalized causal effect score (NCES) metric\n3. Train CSAEs on residual stream activations from GPT-2 and Gemma\n4. Compare against baselines using NCES and standard metrics\n5. Validate causal consistency through targeted probing tasks\n6. Analyze computational overhead of causal verification\n7. Perform ablation studies on intervention types', 'Technical_Details': "The Causal Sparse Autoencoder (CSAE) extends standard SAEs with an efficient causal verification mechanism. For each batch, we create intervention pairs (x, x') by applying structured perturbations to residual stream activations. Interventions include: (1) zeroing specific activation directions, (2) adding calibrated gaussian noise, and (3) gradient-guided perturbations. The loss function is L = L_recon + λL_causal, where L_causal measures the correlation between feature activation differences and a normalized causal effect score (NCES). NCES is computed as the ratio of KL divergence between perturbed and original model outputs to the L2 norm of the intervention. Training uses standard optimizers with gradient accumulation to amortize intervention costs.", 'Research_Impact': 'A critical challenge in mechanistic interpretability is verifying that extracted features represent genuine computational elements rather than analysis artifacts. CSAEs address this by efficiently learning features with consistent causal effects on model behavior. The introduction of the NCES metric provides a quantitative way to assess feature causality, while the batch-wise intervention system makes the approach computationally tractable. This enables more reliable feature discovery in large language models, potentially accelerating progress in understanding model internals and enabling targeted model editing.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8, 'novel': True}
{'Name': 'hierarchical_sparse_autoencoder', 'Title': 'Hierarchical Sparse Autoencoders: Learning Structured Feature Representations in Language Models', 'Experiment': '1. Implement fixed tree structure with learnable feature projections\n2. Design level-wise sparsity constraints with increasing sparsity at higher levels\n3. Implement hierarchical consistency loss using KL divergence and L2 regularization\n4. Train HSAEs on GPT-2 and Gemma activations using 3-level hierarchy\n5. Evaluate using new metrics: hierarchical consistency score and level-wise interpretability\n6. Compare against flat SAEs on reconstruction and interpretability tasks\n7. Analyze feature compositions through path-based attribution analysis', 'Technical_Details': 'HSAE uses a fixed tree structure with L levels (default 3) and branching factor B (default 4). Each level l has n_l = n_0/B^l features, where n_0 is base feature count. The encoder E_l for level l produces sparse activations h_l with sparsity s_l = s_0 * α^l (α>1). Hierarchical consistency loss L_h combines: (1) KL(P(h_l)||P(W_l h_{l-1})) for distribution matching between levels, (2) ||h_l - W_l h_{l-1}||_2 for feature embedding consistency. W_l are learnable projection matrices. Total loss: L = L_recon + Σ_l (λ1_l L_sparse_l + λ2_l L_h_l). Implementation uses sparse tensors and custom CUDA kernels for efficient hierarchy traversal.', 'Research_Impact': "A major challenge in mechanistic interpretability is understanding how simple features combine to represent complex concepts in language models. Current SAEs learn flat feature sets that don't capture these relationships. HSAEs address this by explicitly modeling feature hierarchies with level-wise sparsity and consistency constraints. The fixed tree structure with learnable projections provides a practical framework for analyzing feature composition while maintaining interpretability. This enables systematic investigation of how models build complex representations from simple features, potentially revealing new insights about model behavior and failure modes.", 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8, 'novel': True}
{'Name': 'temporal_relational_sae', 'Title': 'Temporal-Relational Sparse Autoencoders: Learning Position-Aware Features in Language Models', 'Experiment': '1. Implement sparse position embeddings for features\n2. Add efficient linear attention with local windows\n3. Modify encoder/decoder for selective temporal processing\n4. Implement curriculum learning for context window\n5. Train on GPT-2 and Gemma activations\n6. Compare against baselines with emphasis on syntax/discourse tasks\n7. Analyze computational overhead and memory usage\n8. Evaluate feature interpretability using automated metrics\n9. Perform ablation studies on temporal components', 'Technical_Details': "TR-SAE uses an efficient temporal modeling approach. Each feature f_i has a sparse position embedding p_i trained with L1 regularization. The encoder computes h_t = ReLU(W_enc x_t + b_enc). Temporal processing uses linear attention only on top-k features: h'_t = h_t + LinearAttn(Q=TopK(h_t), K=h_{t-w:t+w}, V=h_{t-w:t+w}), with window size w increased during training. Position modulation uses h''_t = h'_t * ReLU(p_t) for interpretability. Training combines standard L1/TopK sparsity with three auxiliary losses: (1) position embedding sparsity, (2) feature position specialization, and (3) syntactic boundary prediction. Gradient clipping and separate learning rates ensure stable training. The linear attention uses 32-dim projections with stride-2 local windows.", 'Research_Impact': 'A significant challenge in mechanistic interpretability is identifying features that capture position-sensitive computations while maintaining interpretability and computational efficiency. Current SAEs either ignore temporal dependencies or handle them in ways that obscure feature interpretation. TR-SAE addresses this by combining sparse position embeddings with efficient temporal modeling, enabling discovery of interpretable position-dependent features while keeping computation tractable. The curriculum learning approach and specialized regularization ensure stable training and clear feature representations, advancing our ability to understand how language models process sequential information.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8, 'novel': True}
{'Name': 'contrastive_disentangled_sae', 'Title': 'Contrastive Disentangled Sparse Autoencoders for Improved Feature Separation', 'Experiment': '1. Implement efficient mini-batch co-activation sampling\n2. Add adaptive per-feature contrastive strength adjustment\n3. Implement dynamic loss balancing mechanism\n4. Train on GPT-2 and Gemma activations with varying hyperparameters\n5. Evaluate using new disentanglement metrics:\n   - Feature correlation matrix condition number\n   - Mutual information between feature pairs\n   - Interpretability scores via GPT-4\n6. Compare against baseline SAEs on reconstruction and interpretability\n7. Analyze impact on downstream task performance\n8. Conduct ablation studies on sampling and adaptation mechanisms', 'Technical_Details': "The architecture uses stochastic mini-batch estimation of feature co-activations. For each batch B, randomly sample k feature pairs and compute C_ij = E[h_i * h_j | i,j ∈ S_k] where S_k is the sampled pair set. The contrastive loss uses adaptive weights: L_contrast = sum_ij(α_ij * C_ij * cos_sim(W_dec_i, W_dec_j)), where α_ij = sigmoid(log(C_ij + ε) - μ_C)/τ adjusts contrast strength based on co-activation statistics μ_C. Dynamic loss balancing: L = L_recon + λ_1 * L_sparse + λ_2 * L_contrast, where λ_2 = β * (L_recon/L_contrast).detach() maintains stable gradients. Implementation uses reservoir sampling for pair selection and maintains running statistics with Welford's online algorithm. A feature is considered disentangled when its maximum correlation with other features falls below threshold γ.", 'Research_Impact': 'A critical challenge in mechanistic interpretability is that features learned by SAEs often represent mixtures of multiple distinct computational components, making interpretation difficult. This research introduces an efficient and principled approach to disentanglement through adaptive contrastive learning. The stochastic sampling and dynamic balancing mechanisms make the method practical for large models, while the adaptive contrast strength ensures robust feature separation. The addition of specific disentanglement metrics provides quantitative validation of feature quality. This could significantly improve our ability to identify and understand distinct computational circuits in language models, advancing interpretability research.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 8, 'novel': False}
{'Name': 'global_local_sae', 'Title': 'Global-Local Sparse Autoencoders: Capturing Multi-Scale Features in Language Models', 'Experiment': '1. Implement parallel local and global pathways in encoder\n2. Design memory bank mechanism for global features\n3. Implement simple routing mechanism with binary gates\n4. Implement curriculum learning for global pathway\n5. Train on GPT-2 and Gemma activations\n6. Evaluate using:\n   - Reconstruction quality\n   - Feature interpretability\n   - Global coherence tasks (coreference, topic consistency)\n   - Path attribution analysis\n7. Analyze computational overhead\n8. Perform ablation studies on memory bank size and routing mechanism', 'Technical_Details': 'GL-SAE architecture consists of two parallel pathways:\n1. Local pathway: Standard sparse encoding h_l = TopK(W_l x + b_l)\n2. Global pathway: Memory-based processing h_g = TopK(M W_g x) where M is a learned memory bank of size k×d\n3. Binary routing: h = g ⊙ h_l + (1-g) ⊙ h_g where g = step(W_r[h_l; h_g])\n\nCurriculum learning:\n- Phase 1: Train local pathway only\n- Phase 2: Introduce memory bank with increasing size\n- Phase 3: Enable routing mechanism\n\nLoss function:\nL = L_recon + λ1(L_sparse_local + L_sparse_global) + λ2L_memory\nwhere L_memory encourages diversity in memory bank usage.\n\nMemory bank updated using exponential moving average of most activated feature directions. Implementation uses sparse matrix operations throughout.', 'Research_Impact': 'A major challenge in mechanistic interpretability is identifying features that capture both local patterns and long-range dependencies while maintaining interpretability. Current SAEs tend to focus on local patterns, missing important global structures in language model computations. GL-SAE addresses this through a simplified yet powerful architecture that explicitly separates local and global feature extraction. The memory bank mechanism provides a computationally efficient way to capture global patterns, while the binary routing maintains clear interpretability of feature origins. The curriculum learning approach ensures stable training and clear feature separation. This design could significantly improve our ability to understand how language models integrate information across different scales while keeping the analysis tractable and interpretable.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8, 'novel': True}
{'Name': 'weighted_reconstruction_sae', 'Title': 'Weighted Reconstruction Sparse Autoencoders: Learning Functionally Important Features', 'Experiment': '1. Implement variance-based importance initialization\n2. Add learned per-dimension scaling factors\n3. Implement KL regularization between weights and prior\n4. Train on GPT-2 and Gemma activations with varying batch sizes\n5. Compare against baseline SAEs using:\n   - Reconstruction quality\n   - Feature interpretability metrics\n   - Activation variance correlation\n6. Analyze weight convergence patterns\n7. Perform ablation studies on initialization and regularization\n8. Validate with causal intervention experiments', 'Technical_Details': 'WR-SAE uses two-stage importance weight learning:\n1. Prior weights w_prior = softmax(log(var(x) + ε)) computed over large batch\n2. Learned scales α trained to minimize L_recon = ||exp(α) ⊙ (x - x̂)||₂²\nFinal weights w = softmax(log(w_prior) + α). Regularization uses KL(w||w_prior). Training alternates between:\na) Standard SAE updates with fixed w\nb) α updates with fixed SAE parameters\nBatch size increased gradually during training. Implementation uses online variance calculation and efficient sparse operations. Optional momentum on α updates for stability.', 'Research_Impact': "A key challenge in mechanistic interpretability is identifying which features represent genuine computational components versus incidental correlations. Current SAEs may waste capacity learning features that aren't functionally important. WR-SAE addresses this through a principled two-stage approach combining variance-based priors with learned adjustments. This provides a more theoretically grounded way to focus feature extraction on functionally important dimensions while maintaining computational tractability. The explicit connection between activation variance and importance weights provides additional interpretability insights about feature relevance.", 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7, 'novel': True}
{'Name': 'downstream_guided_sae', 'Title': 'Downstream-Guided Sparse Autoencoders: Learning Behaviorally Relevant Features', 'Experiment': '1. Implement normalized maximum covariance estimator\n2. Add next-layer activation tracking\n3. Design adaptive feature regularization\n4. Implement importance-based early stopping\n5. Train on GPT-2 and Gemma activations with varying hyperparameters\n6. Compare against baseline SAEs using:\n   - Standard reconstruction metrics\n   - Behavioral importance scores\n   - Interpretability metrics\n7. Analyze feature stability and redundancy\n8. Validate with causal tracing experiments\n9. Study convergence patterns', 'Technical_Details': 'DG-SAE computes importance scores I_f using normalized maximum covariance: I_f = max_d |cov(h_f, h_next_d)|/sqrt(var(h_f)var(h_next_d)) where h_next are next-layer activations. The reconstruction loss combines standard L2 loss with importance weighting: L = (1-α(t))L_recon + α(t)L_weighted where α(t) follows curriculum schedule. Adaptive regularization added: L_reg = sum_{i,j} max(0, cos_sim(f_i, f_j) - β) * exp(-(I_i - I_j)²/τ) where β controls maximum allowed similarity and τ scales importance difference sensitivity. Training uses:\n1. Online importance estimation with momentum\n2. Curriculum schedule with importance score stability checks\n3. Early stopping when importance scores stabilize\n4. Optional importance score clustering for feature analysis', 'Research_Impact': "A fundamental challenge in mechanistic interpretability is ensuring that extracted features correspond to behaviorally relevant computations rather than just statistical patterns in activations. Current SAEs may learn features that reconstruct the input well but don't meaningfully impact the model's behavior. DG-SAE addresses this by efficiently incorporating downstream behavioral information through normalized maximum covariance, providing a computationally tractable way to guide feature learning toward behaviorally relevant patterns. The adaptive regularization prevents redundant features while respecting natural feature similarities, and the stability-based training ensures robust convergence to meaningful features.", 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8, 'novel': True}
{'Name': 'graph_guided_sae', 'Title': 'Graph-Guided Sparse Autoencoders: Learning Feature Relationships for Enhanced Interpretability', 'Experiment': '1. Implement efficient block-sparse attention\n2. Add momentum-based graph structure learning\n3. Design circuit scoring and pruning module\n4. Implement temperature annealing schedule\n5. Train on GPT-2 and Gemma activations\n6. Evaluate using:\n   - Reconstruction quality\n   - Feature interpretability\n   - Circuit detection metrics:\n     * Circuit consistency score\n     * Causal influence measure\n     * Circuit interpretability rating\n7. Compare against baselines\n8. Analyze computational scaling\n9. Perform ablation studies\n10. Validate with intervention experiments', 'Technical_Details': "GG-SAE uses efficient block-sparse attention:\n1. Initial encoding: h = TopK(W_enc x + b_enc)\n2. Candidate selection: C_ij = ReLU(cos_sim(W_dec_i, W_dec_j) - θ)\n3. Sparse attention: h' = h + α * BlockSpAttn(Q=h, K=h, V=h, mask=C)\n\nGraph structure learning:\n1. Momentum updates: m_ij = βm_ij + (1-β)grad_corr_ij\n2. Temperature annealing: s_ij = m_ij/max(T, T_min)\nwhere T follows exponential decay schedule.\n\nCircuit scoring:\nS_circuit = Σ_ij (s_ij * causal_effect_ij)/|circuit|\n\nLoss function:\nL = L_recon + λ1 L_sparse + λ2 L_graph + λ3 L_circuit\nwhere L_circuit encourages high-scoring circuits.\n\nImplementation uses custom CUDA kernels for block-sparse operations and maintains circuit score cache for efficient updates.", 'Research_Impact': 'A key challenge in mechanistic interpretability is understanding how different computational features interact and combine in language models. Current approaches tend to analyze features in isolation, missing important relational information. GG-SAE addresses this by efficiently learning and extracting interpretable feature circuits through sparse graph attention. The addition of circuit scoring and pruning mechanisms helps identify the most behaviorally relevant feature combinations, while the improved sparse attention implementation makes the approach practical for large models. This could significantly advance our ability to understand computational pathways in language models by providing direct evidence of feature interactions backed by causal validation.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8, 'novel': False}
{'Name': 'ensemble_consistent_sae', 'Title': 'Ensemble-Consistent Sparse Autoencoders: Learning Stable Features Through Multi-Instance Training', 'Experiment': '1. Implement memory-efficient parallel training:\n   - Use gradient accumulation for large ensembles\n   - Implement streaming feature comparison\n2. Add temporal stability tracking:\n   - Maintain EMA of feature activations\n   - Track feature drift metrics\n3. Design improved consistency mechanisms:\n   - Activation-based adaptive thresholds\n   - Efficient negative sampling\n4. Train on GPT-2 and Gemma activations with:\n   - Ensemble sizes: 4, 8, 16\n   - Batch sizes: 512, 1024, 2048\n5. Evaluate using:\n   - Feature stability (cosine similarity over time)\n   - Cross-instance agreement scores\n   - Activation pattern consistency\n   - Interpretability metrics via GPT-4\n6. Compare against:\n   - Independent SAE training\n   - Checkpoint ensembles\n   - Feature distillation\n7. Analyze computational overhead\n8. Perform ablation studies', 'Technical_Details': 'EC-SAE trains N parallel SAE instances with shared architecture but different initialization. Key components:\n1. Memory-efficient feature comparison:\n   - Streaming computation of A_ij = cos_sim(f_i^n, f_j^m)\n   - Maintain sparse adjacency matrix of aligned features\n2. Temporal stability tracking:\n   - s_i(t) = EMA(||f_i(t) - f_i(t-1)||) for feature drift\n   - v_i(t) = EMA(var(activations_i)) for activation stability\n3. Adaptive thresholds:\n   T_nm = β_1 * median(A_ij) + β_2 * sqrt(v_i * v_j)\n4. Enhanced consistency loss:\n   L_consist = sum_{n,m} sum_{i,j} w_ij * max(0, T_nm - A_ij) * M_ij\n   where w_ij = exp(-(s_i + s_j)/τ)\n5. Efficient implementation:\n   - Shared memory buffer for feature statistics\n   - Sparse operations for feature comparisons\n   - Gradient accumulation for large ensembles\n6. Total loss: L = sum_n L_recon^n + λ1 sum_n L_sparse^n + λ2 L_consist', 'Research_Impact': 'A significant challenge in mechanistic interpretability is ensuring that extracted features represent genuine computational components rather than training artifacts. Current approaches can produce unstable or inconsistent features across different runs, making it difficult to reliably identify and study specific circuits. EC-SAE addresses this through principled multi-instance training with both cross-instance and temporal stability enforcement. The improved adaptive thresholds and efficient implementation make the approach practical for large models, while the temporal stability tracking provides additional validation of feature robustness. This could significantly improve the reliability of mechanistic interpretability analyses by providing stronger guarantees about feature stability and meaningfulness, backed by multiple forms of consistency verification.', 'Interestingness': 8, 'Feasibility': 7, 'Novelty': 8, 'novel': False}
{'Name': 'progressive_sae', 'Title': 'Progressive Sparse Autoencoders: Learning Feature Evolution Across Model Layers', 'Experiment': '1. Implement progressive training scheduler:\n   - Layer-wise training order determination\n   - Feature composition tracking via attention maps\n   - Dynamic sparsity adjustment\n2. Add cross-layer feature mapping:\n   - Sparse composition matrix computation\n   - Hierarchical feature clustering\n   - Evolution path tracking\n3. Design theoretically-grounded sparsity mechanism:\n   - Information-theoretic complexity measures\n   - Layer-wise capacity estimation\n4. Train on GPT-2 and Gemma activations:\n   - Progressive vs independent training comparison\n   - Feature evolution analysis\n   - Path importance measurement\n5. Evaluate using:\n   - Layer-wise reconstruction quality\n   - Feature composition clarity\n   - Evolution path consistency\n   - Interpretability metrics\n6. Analyze computational scaling\n7. Perform targeted ablation studies', 'Technical_Details': "Progressive-SAE components:\n1. Feature Composition Tracking:\n   - Compute attention-based alignment scores: A_l[i,j] = softmax(f_i^l · f_j^{l+1})\n   - Update composition matrix: C_l = EMA(A_l)\n   - Maintain sparse top-k connections per feature\n2. Information-theoretic Sparsity:\n   - Estimate layer complexity: H_l = -Σp(f_i^l)log p(f_i^l)\n   - Set target sparsity: s_l = s_0 * exp(α * H_l/H_0)\n   - Adjust dynamically: s'_l = s_l * (1 + β * ΔH_l)\n3. Directional Consistency:\n   L_consist = ||C_l W_dec^{l+1} - W_dec^l||_F + γ * KL(p(f^{l+1})||p(C_l f^l))\n4. Evolution Path Tracking:\n   - Maintain feature evolution graph G_l\n   - Score paths using influence propagation\n   - Prune low-importance connections\n5. Implementation:\n   - Block-sparse operations for composition tracking\n   - Streaming entropy estimation\n   - Efficient path importance computation\n   - Layer-wise gradient accumulation with checkpointing", 'Research_Impact': 'A fundamental challenge in mechanistic interpretability is understanding how neural networks progressively construct complex features from simpler ones across layers. Current approaches that treat each layer independently miss crucial information about feature evolution and transformation. Progressive-SAE addresses this through three key innovations: (1) An efficient and theoretically grounded feature composition tracking mechanism that reveals how features combine and evolve, (2) An information-theoretic approach to adaptive sparsity that better captures the natural complexity hierarchy across layers, and (3) A directional consistency loss that ensures meaningful feature progression. These improvements enable more reliable analysis of feature evolution in large language models, potentially revealing new insights about how these models build sophisticated representations from simple components.', 'Interestingness': 9, 'Feasibility': 7, 'Novelty': 8, 'novel': True}
{'Name': 'compositional_sae', 'Title': 'Compositional Sparse Autoencoders: Learning Primitive Features and Composition Rules', 'Experiment': '1. Implement basis function learning:\n   - Initialize using PCA with interpretability constraints\n   - Add automated interpretability scoring\n   - Track basis function stability\n2. Add causal validation:\n   - Implement targeted intervention experiments\n   - Measure direct causal effects on model outputs\n   - Track feature independence scores\n3. Implement two-level composition:\n   - Design efficient sparse tensor operations\n   - Add information bottleneck constraints\n4. Train on GPT-2 and Gemma activations\n5. Evaluate using:\n   - Primitive feature interpretability\n   - Composition rule clarity\n   - Feature reuse metrics\n   - Causal independence scores\n6. Compare against baseline SAEs\n7. Analyze computational overhead\n8. Perform ablation studies', 'Technical_Details': 'Architecture consists of three components:\n1. Basis Learning:\n   - Learn set B of k primitive basis functions b_i\n   - Add interpretability regularization: L_interp = -log(interp_score(b_i))\n   - Update basis functions using gradient descent with stability constraints\n2. Causal Validation:\n   - Compute direct effects: E_i = E[KL(p(y|do(b_i=0))||p(y))]\n   - Update independence scores: S_ij = max(0, cos_sim(E_i, E_j) - gamma)\n3. Two-Level Composition:\n   - Level 1: h_1 = TopK(sum_i c_i b_i)\n   - Level 2: h_2 = TopK(sum_{i,j} A[i,j] h_{1,i} h_{1,j})\n   where A is sparse composition matrix\n\nLoss combines:\n- Reconstruction: L_recon = ||x - decode(h_2)||^2\n- Sparsity: L_sparse = ||h_1||_1 + ||h_2||_1\n- Interpretability: L_interp for basis functions\n- Independence: L_indep = sum_{i,j} S_ij\n- Information bottleneck: L_IB = I(h_1; x) - beta * I(h_2; y)\n\nImplementation uses efficient sparse operations and maintains feature statistics in streaming fashion.', 'Research_Impact': "A fundamental challenge in mechanistic interpretability is distinguishing genuine computational primitives from complex features that arise from frequent co-activation patterns. Current approaches struggle to disentangle these, leading to features that don't represent true computational units. The refined Compositional SAE addresses this through three key improvements: (1) Explicit interpretability constraints on primitive features, ensuring they remain human-understandable, (2) Direct causal validation of feature independence through intervention experiments, and (3) Information bottleneck theory to guide feature composition. These improvements make the approach more practical while maintaining its ability to identify fundamental computational building blocks. The two-level architecture with clearer theoretical grounding could significantly advance our ability to reverse engineer neural computations.", 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9, 'novel': True}
{'Name': 'counterfactual_sae', 'Title': 'Counterfactual Sparse Autoencoders: Learning Features Through Behavioral Validation', 'Experiment': '1. Implement efficient feature ablation mechanism:\n   - Importance-guided feature sampling\n   - Structured batch composition\n   - Interaction-aware selection\n2. Add counterfactual loss computation:\n   - Direct and indirect effect tracking\n   - Multi-scale consistency scoring\n   - Feature interaction analysis\n3. Design feature modification framework:\n   - Controlled perturbation generation\n   - Impact measurement with interaction effects\n   - Stability tracking with collapse prevention\n4. Train on GPT-2 and Gemma activations\n5. Evaluate using:\n   - Standard reconstruction metrics\n   - Direct and indirect effect sizes\n   - Feature stability measures\n   - Interaction strength metrics\n6. Compare against baseline SAEs\n7. Analyze computational overhead\n8. Validate with targeted experiments', 'Technical_Details': "Architecture introduces structured counterfactual validation:\n1. Feature Importance Estimation:\n   - Primary scores: s_i = ||∂L/∂f_i||\n   - Interaction scores: s_ij = ||∂²L/∂f_i∂f_j||\n   - Combined importance: I_i = β₁ * s_i + β₂ * Σ_j s_ij\n2. Structured Counterfactual Generation:\n   - Stratified sampling based on I_i\n   - Batch composition: {high, medium, low} importance groups\n   - Interaction-based pairing within batch\n3. Multi-scale Behavioral Impact:\n   - Direct effects: δy_i = KL(p(y|x)||p(y|x'_i))\n   - Interaction effects: δy_ij = KL(p(y|x'_i)||p(y|x'_i,j))\n   - Consistency: C_i = α * corr(δy_i(t), δy_i(t-1)) + (1-α) * mean_j(corr(δy_ij(t), δy_ij(t-1)))\n4. Enhanced Loss Function:\n   L = L_recon + λ₁ * L_sparse + λ₂ * L_direct + λ₃ * L_interact\n   where:\n   - L_direct = -Σ_i (I_i * log(δy_i) * C_i)\n   - L_interact = -Σ_i,j (I_i * I_j * log(δy_ij) * C_i * C_j)\n\nImplementation uses:\n- Efficient batch computation with importance-based sampling\n- Streaming statistics for importance and interaction scores\n- Dynamic threshold adjustment for feature selection\n- Gradient accumulation for memory efficiency", 'Research_Impact': 'A critical challenge in mechanistic interpretability is verifying that extracted features represent genuine computational components while understanding their interactions. Current approaches either focus on individual feature importance or treat interactions as secondary effects. The refined CF-SAE addresses this through structured counterfactual validation that explicitly considers both direct and interaction effects during training. The improved sampling strategy and multi-scale behavioral impact measurement make it more computationally efficient while providing stronger guarantees about feature importance and interactions. This could significantly advance our understanding of how features combine to form computational circuits in language models by providing direct evidence of both individual and joint effects through principled counterfactual analysis.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8, 'novel': True}
{'Name': 'curriculum_guided_sae', 'Title': 'Curriculum-Guided Sparse Autoencoders: Learning Features Through Dynamic Training Schedules', 'Experiment': '1. Implement feature-wise learning rate scheduler:\n   - Track per-feature causal effects\n   - Compute feature stability metrics\n   - Adjust learning rates using information theory\n2. Add enhanced importance estimation:\n   - Implement causal intervention sampling\n   - Track feature learning trajectories\n   - Compute temporal stability scores\n3. Design concrete curriculum mechanism:\n   - Phase transitions based on entropy measures\n   - Information-theoretic resource allocation\n   - Adaptive sparsity adjustment\n4. Train on GPT-2 and Gemma activations:\n   - Multiple initialization strategies\n   - Hyperparameter sensitivity analysis\n   - Ablation studies on components\n5. Evaluate using:\n   - Causal importance metrics\n   - Feature learning trajectories\n   - Final feature quality measures\n   - Interpretability scores\n6. Compare against baseline SAEs\n7. Analyze convergence patterns', 'Technical_Details': "CG-SAE components:\n1. Causal Importance Tracking:\n   - Direct effects: DE_i(t) = KL(p(y|x)||p(y|do(f_i=0)))\n   - Stability score: S_i(t) = exp(-var(f_i(t-k:t))/\\tau)\n   - Information gain: IG_i(t) = I(f_i(t); y) - I(f_i(t-1); y)\n   - Combined importance: I_i(t) = \\alpha * DE_i(t) + \\beta * S_i(t) + \\gamma * IG_i(t)\n2. Information-Theoretic Learning Rate:\n   - Base rate: \\eta_i(t) = \\eta_0 * exp(-H(f_i|y)/H(f_i))\n   - Momentum: m_i(t) = \\lambda * m_i(t-1) + (1-\\lambda) * grad_i(t)\n   - Final rate: \\eta'_i(t) = \\eta_i(t) * (1 + \\delta * S_i(t))\n3. Curriculum Phases:\n   - Phase 1 (t < t1): High learning rates, focus on high DE_i features\n   - Phase 2 (t1 ≤ t < t2): Moderate rates, balance DE_i and S_i\n   - Phase 3 (t ≥ t2): Low rates, emphasize stability\n   where t1, t2 determined by entropy stabilization\n4. Implementation Details:\n   - Initialization: W_enc ~ N(0, 1/sqrt(d_in * k))\n   - Batch size: min(512, 8 * d_in)\n   - Momentum decay: \\lambda = 0.9\n   - Stability window: k = 100 steps\n   - Entropy estimation: running average with window 1000", 'Research_Impact': 'A significant challenge in mechanistic interpretability is ensuring that learned features represent fundamental computational components rather than artifacts of simultaneous optimization. Current approaches use uniform training schedules that may lead to suboptimal feature sets or premature convergence. CG-SAE addresses this through three key innovations: (1) Causal importance metrics that better identify genuine computational components, (2) Information-theoretic learning rate adjustment that provides principled resource allocation, and (3) Concrete phase transition criteria based on entropy measures. These improvements make the approach more practical and theoretically grounded while maintaining its ability to learn features in a natural order reflecting their computational importance.', 'Interestingness': 8, 'Feasibility': 8, 'Novelty': 7, 'novel': True}
{'Name': 'attention_guided_sae', 'Title': 'Attention-Guided Sparse Autoencoders: Learning Interpretable Dynamic Feature Compositions', 'Experiment': '1. Implement hierarchical sparse attention:\n   - Multi-level feature clustering\n   - Logarithmic complexity attention\n   - Gradient-based importance tracking\n2. Add enhanced feature attribution:\n   - Path-based attribution scores\n   - Composition pattern mining\n   - Stability metrics\n3. Implement curriculum learning:\n   - Progressive window size increase\n   - Adaptive sparsity constraints\n   - Feature importance thresholds\n4. Train on GPT-2 and Gemma activations\n5. Evaluate using:\n   - Standard reconstruction metrics\n   - Path attribution clarity\n   - Attention pattern interpretability\n   - Composition stability measures\n   - Causal tracing experiments\n6. Compare against baseline SAEs\n7. Analyze computational scaling\n8. Perform ablation studies', 'Technical_Details': "Enhanced AG-SAE architecture:\n1. Feature Extraction:\n   h = TopK(W_enc x + b_enc)\n2. Hierarchical Attention:\n   - Level-1: Local clustering h_1 = cluster(h, k=sqrt(n))\n   - Level-2: Global attention A = ReLU(TopK(q k^T / sqrt(d)))\n   - Composition: h' = A_local h + A_global h_1\n3. Feature Importance:\n   - Gradient-based scores: s_i = ||∂L/∂h_i||\n   - Path attribution: p_ij = s_i * A_ij * s_j\n   - Stability: v_i = var(s_i) over time\n4. Curriculum Schedule:\n   - Window size: w(t) = min(w_max, w_0 * log(1 + t/τ))\n   - Sparsity: k(t) = k_max * (1 - exp(-t/τ))\n\nLoss function:\nL = L_recon + λ₁L_sparse + λ₂L_attn + λ₃L_attr + λ₄L_stab\nwhere:\n- L_stab = sum_i v_i * ||h_i||\n- Other terms as before\n\nImplementation uses:\n- Efficient hierarchical attention\n- Streaming importance estimation\n- Dynamic sparsity adjustment\n- Memory-efficient gradient tracking", 'Research_Impact': 'A fundamental challenge in mechanistic interpretability is understanding how language models dynamically combine features during processing while maintaining computational efficiency. Current SAEs either ignore dynamic interactions or become computationally intractable for longer sequences. The enhanced AG-SAE addresses this through hierarchical attention that scales logarithmically with sequence length while maintaining interpretability through explicit path-based attribution. The addition of curriculum learning and stability metrics makes the approach more practical for large-scale analysis. This could significantly advance our understanding of dynamic feature composition in language models by providing efficient, interpretable evidence of how features interact during processing.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 8, 'novel': False}
{'Name': 'context_separating_sae', 'Title': 'Context-Separating Sparse Autoencoders: Memory-Efficient Feature Disentanglement Through Factorized Expert Mixture', 'Experiment': '1. Implement factorized expert architecture:\n   - Design low-rank expert parameterization\n   - Add shared component computation\n   - Implement efficient memory management\n2. Add robust initialization and training:\n   - Implement activation clustering initialization\n   - Design expert introduction curriculum\n   - Add stability-based pruning\n3. Enhance context separation validation:\n   - Implement causal intervention framework\n   - Add targeted feature modifications\n   - Track intervention effects\n4. Train on GPT-2 and Gemma activations\n5. Evaluate using:\n   - Information bottleneck metrics\n   - Causal validation scores\n   - Memory efficiency measures\n   - Context separation quality\n6. Compare against baseline SAEs\n7. Analyze expert dynamics\n8. Perform targeted ablations', 'Technical_Details': "Refined architecture:\n1. Factorized Experts:\n   - Shared components: B = UV^T (low-rank)\n   - Expert-specific: W_i = B + α_i p_i q_i^T\n   where p_i, q_i are low-dimensional vectors\n2. Information-Theoretic Routing:\n   - Context embedding: c = f_θ(h)\n   - Expert assignment: P(e|c) = softmax(g_φ(c))\n   - IB objective: min I(X;C) - βI(C;E)\n3. Robust Training:\n   - Init clusters: {c_i} = kmeans(h, k)\n   - Expert schedule: n(t) = min(n_max, ⌈n_0(1 + log(1 + t/τ))⌉)\n   - Stability score: s_i = exp(-var(P(e_i|c))/σ)\n4. Causal Validation:\n   - Feature modification: h'_i = h_i + δ_j\n   - Effect size: Δ_ij = KL(P(y|h)||P(y|h'_i))_j\n   - Context specificity: CS_i = var_j(Δ_ij)\n5. Loss function:\n   L = L_rec + λ₁L_sparse + λ₂L_IB + λ₃L_stable\n   where L_stable = -∑_i s_i log P(e_i|c)\n\nImplementation uses:\n- Memory-efficient tensor operations\n- Streaming statistics computation\n- Gradient checkpointing\n- Efficient causal intervention batching", 'Research_Impact': 'A fundamental challenge in mechanistic interpretability is polysemanticity, where individual features respond to multiple distinct contexts, making their interpretation difficult. Current approaches either ignore this issue or attempt to address it through computationally expensive mechanisms. The refined CS-SAE addresses this through a memory-efficient factorized expert architecture and information-theoretic routing, making it both more scalable and theoretically grounded. The addition of causal validation and robust training procedures provides strong evidence for context separation, while the improved memory efficiency makes the approach practical for large-scale models. This could significantly advance our ability to understand and interpret individual features by revealing their distinct activation patterns in different contexts, backed by both theoretical guarantees and empirical validation through causal interventions.', 'Interestingness': 9, 'Feasibility': 8, 'Novelty': 9, 'novel': False}
Skipping seed idea 0
Skipping seed idea 1
Skipping seed idea 2
Skipping seed idea 3
Skipping seed idea 4
Skipping seed idea 5
Skipping seed idea 6
Skipping seed idea 7
Skipping idea 8, already checked.
Skipping idea 9, already checked.
Skipping idea 10, already checked.
Skipping idea 11, already checked.
Skipping idea 12, already checked.
Skipping idea 13, already checked.
Skipping idea 14, already checked.
Skipping idea 15, already checked.
Skipping idea 16, already checked.
Skipping idea 17, already checked.
Skipping idea 18, already checked.
Skipping idea 19, already checked.
Skipping idea 20, already checked.
Skipping idea 21, already checked.
Skipping idea 22, already checked.
Processing idea: causal_sparse_autoencoder
2025-01-12 13:18:03
*Starting idea: 20250112_131803_causal_sparse_autoencoder*
2025-01-12 13:18:03
*Starting Experiments*
Starting experiment with prompt for coder: Your goal is to implement the following idea: Causal Sparse Autoencoders: Learning Causally Verified Features in Language Models.
The proposed experiment is as follows: 1. Implement efficient batch-wise causal intervention system
2. Define normalized causal effect score (NCES) metric
3. Train CSAEs on residual stream activations from GPT-2 and Gemma
4. Compare against baselines using NCES and standard metrics
5. Validate causal consistency through targeted probing tasks
6. Analyze computational overhead of causal verification
7. Perform ablation studies on intervention types.
You are given a total of up to 5 runs to complete the necessary experiments. You do not need to use all 5.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

{'shakespeare_char': {'means': {'final_train_loss_mean': 0.8083514968554179, 'best_val_loss_mean': 1.4693279266357422, 'total_train_time_mean': 301.409769932429, 'avg_inference_tokens_per_second_mean': 476.76571942025066}, 'stderrs': {'final_train_loss_stderr': 0.004606851662603869, 'best_val_loss_stderr': 0.002318185346862607, 'total_train_time_stderr': 3.7604327053095585, 'avg_inference_tokens_per_second_stderr': 2.2846284985581478}, 'final_info_dict': {'final_train_loss': [0.8238834142684937, 0.790310263633728, 0.810860812664032], 'best_val_loss': [1.4679977893829346, 1.478432297706604, 1.461553692817688], 'total_train_time': [317.36346793174744, 293.538613319397, 293.3272285461426], 'avg_inference_tokens_per_second': [467.22402076323704, 480.0598800084391, 483.01325748907584]}}, 'enwik8': {'means': {'final_train_loss_mean': 0.9414697289466858, 'best_val_loss_mean': 1.0054575204849243, 'total_train_time_mean': 2371.9740512371063, 'avg_inference_tokens_per_second_mean': 481.0998539249739}, 'stderrs': {'final_train_loss_stderr': 0.0, 'best_val_loss_stderr': 0.0, 'total_train_time_stderr': 0.0, 'avg_inference_tokens_per_second_stderr': 0.0}, 'final_info_dict': {'final_train_loss': [0.9414697289466858], 'best_val_loss': [1.0054575204849243], 'total_train_time': [2371.9740512371063], 'avg_inference_tokens_per_second': [481.0998539249739]}}, 'text8': {'means': {'final_train_loss_mean': 0.9939898252487183, 'best_val_loss_mean': 0.9801777005195618, 'total_train_time_mean': 2351.458997964859, 'avg_inference_tokens_per_second_mean': 476.5669066097941}, 'stderrs': {'final_train_loss_stderr': 0.0, 'best_val_loss_stderr': 0.0, 'total_train_time_stderr': 0.0, 'avg_inference_tokens_per_second_stderr': 0.0}, 'final_info_dict': {'final_train_loss': [0.9939898252487183], 'best_val_loss': [0.9801777005195618], 'total_train_time': [2351.458997964859], 'avg_inference_tokens_per_second': [476.5669066097941]}}, 'training_steps': 0, 'final_loss': None, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04, 'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': -0.5279503105590062, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 15.375}, 'model_performance_preservation': {'ce_loss_score': -0.5855263157894737, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 18.0, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': -0.78515625, 'mse': 47.25, 'cossim': nan}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 0.0, 'l2_ratio': 0.0, 'relative_reconstruction_bias': nan}, 'sparsity': {'l0': 0.0, 'l1': 0.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
Let me plan the experiments for implementing Causal Sparse Autoencoders (CSAE). We'll need to run several experiments to validate the causal intervention approach:        

Run 1: Implement basic CSAE with causal intervention                                                                                                                       

 • Modify SAE to include causal intervention mechanism                                                                                                                     
 • Add NCES (Normalized Causal Effect Score) metric                                                                                                                        
 • Test on base configuration                                                                                                                                              

Run 2: Ablation study on intervention types                                                                                                                                

 • Test different intervention strategies:                                                                                                                                 
    • Zero-ablation                                                                                                                                                        
    • Random noise                                                                                                                                                         
    • Mean substitution                                                                                                                                                    
 • Compare their effects on feature learning                                                                                                                               

Run 3: Hyperparameter optimization                                                                                                                                         

 • Test different causal penalty weights (λ_causal): [0.01, 0.1, 0.5]                                                                                                      
 • Balance between reconstruction and causal objectives                                                                                                                    

Run 4: Validation with targeted probing                                                                                                                                    

 • Implement probing tasks for causal consistency                                                                                                                          
 • Compare against baseline features                                                                                                                                       

Let's start with Run 1 by implementing the basic CSAE architecture. I'll modify the code to:                                                                               

 1 Add causal intervention system                                                                                                                                          
 2 Implement NCES metric                                                                                                                                                   
 3 Modify training loop to include causal objectives                                                                                                                       

Here are the changes:                                                                                                                                                      

plot.py                                                                                                                                                                    

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 # write your code here                                                                                                                                                    
 =======                                                                                                                                                                   
 import matplotlib.pyplot as plt                                                                                                                                           
 import numpy as np                                                                                                                                                        
                                                                                                                                                                           
 def plot_nces_distribution(nces_scores, save_path):                                                                                                                       
     plt.figure(figsize=(10, 6))                                                                                                                                           
     plt.hist(nces_scores, bins=50, density=True)                                                                                                                          
     plt.xlabel('Normalized Causal Effect Score (NCES)')                                                                                                                   
     plt.ylabel('Density')                                                                                                                                                 
     plt.title('Distribution of NCES Across Features')                                                                                                                     
     plt.savefig(save_path)                                                                                                                                                
     plt.close()                                                                                                                                                           
                                                                                                                                                                           
 def plot_intervention_effects(original, intervened, reconstructed, save_path):                                                                                            
     plt.figure(figsize=(15, 5))                                                                                                                                           
                                                                                                                                                                           
     plt.subplot(131)                                                                                                                                                      
     plt.imshow(original.T, aspect='auto')                                                                                                                                 
     plt.title('Original Activations')                                                                                                                                     
                                                                                                                                                                           
     plt.subplot(132)                                                                                                                                                      
     plt.imshow(intervened.T, aspect='auto')                                                                                                                               
     plt.title('Intervened Activations')                                                                                                                                   
                                                                                                                                                                           
     plt.subplot(133)                                                                                                                                                      
     plt.imshow(reconstructed.T, aspect='auto')                                                                                                                            
     plt.title('Reconstructed Activations')                                                                                                                                
                                                                                                                                                                           
     plt.tight_layout()                                                                                                                                                    
     plt.savefig(save_path)                                                                                                                                                
     plt.close()                                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 class CustomSAE(nn.Module):                                                                                                                                               
     """Implementation of a Custom Sparse Autoencoder."""                                                                                                                  
 =======                                                                                                                                                                   
 class CausalSAE(nn.Module):                                                                                                                                               
     """Implementation of a Causal Sparse Autoencoder with intervention mechanisms."""                                                                                     
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def __init__(                                                                                                                                                         
         self,                                                                                                                                                             
         d_in: int,                                                                                                                                                        
         d_sae: int,                                                                                                                                                       
         hook_layer: int,                                                                                                                                                  
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                                
         hook_name: Optional[str] = None,                                                                                                                                  
     ):                                                                                                                                                                    
 =======                                                                                                                                                                   
     def __init__(                                                                                                                                                         
         self,                                                                                                                                                             
         d_in: int,                                                                                                                                                        
         d_sae: int,                                                                                                                                                       
         hook_layer: int,                                                                                                                                                  
         model_name: str = "EleutherAI/pythia-70m-deduped",                                                                                                                
         hook_name: Optional[str] = None,                                                                                                                                  
         causal_penalty: float = 0.1,                                                                                                                                      
     ):                                                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         self.cfg = CustomSAEConfig(                                                                                                                                       
 =======                                                                                                                                                                   
         self.causal_penalty = causal_penalty                                                                                                                              
         self.cfg = CustomSAEConfig(                                                                                                                                       
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def encode(self, input_acts):                                                                                                                                         
         pre_acts = (input_acts - self.b_dec) @ self.W_enc + self.b_enc                                                                                                    
         acts = torch.relu(pre_acts)                                                                                                                                       
         return acts                                                                                                                                                       
 =======                                                                                                                                                                   
     def encode(self, input_acts, apply_intervention=False):                                                                                                               
         pre_acts = (input_acts - self.b_dec) @ self.W_enc + self.b_enc                                                                                                    
         acts = torch.relu(pre_acts)                                                                                                                                       
                                                                                                                                                                           
         if apply_intervention:                                                                                                                                            
             # Apply zero-ablation intervention                                                                                                                            
             batch_size = acts.shape[0]                                                                                                                                    
             feature_mask = torch.zeros_like(acts)                                                                                                                         
             # Randomly select features to intervene on                                                                                                                    
             for i in range(batch_size):                                                                                                                                   
                 n_features = torch.randint(1, acts.shape[1]//10, (1,))                                                                                                    
                 feature_indices = torch.randperm(acts.shape[1])[:n_features]                                                                                              
                 feature_mask[i, feature_indices] = 1                                                                                                                      
                                                                                                                                                                           
             # Store original and intervened activations for NCES computation                                                                                              
             self.original_acts = acts.clone()                                                                                                                             
             acts = acts * (1 - feature_mask)                                                                                                                              
             self.intervened_acts = acts.clone()                                                                                                                           
                                                                                                                                                                           
         return acts                                                                                                                                                       
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def forward(self, acts, output_features=False):                                                                                                                       
         encoded = self.encode(acts)                                                                                                                                       
         decoded = self.decode(encoded)                                                                                                                                    
         if output_features:                                                                                                                                               
             return decoded, encoded                                                                                                                                       
         return decoded                                                                                                                                                    
 =======                                                                                                                                                                   
     def compute_nces(self):                                                                                                                                               
         """Compute Normalized Causal Effect Score"""                                                                                                                      
         if not hasattr(self, 'original_acts') or not hasattr(self, 'intervened_acts'):                                                                                    
             return 0.0                                                                                                                                                    
                                                                                                                                                                           
         orig_decoded = self.decode(self.original_acts)                                                                                                                    
         int_decoded = self.decode(self.intervened_acts)                                                                                                                   
                                                                                                                                                                           
         # Compute effect size per feature                                                                                                                                 
         effect_sizes = torch.norm(orig_decoded - int_decoded, dim=1)                                                                                                      
         # Normalize by original activation magnitude                                                                                                                      
         orig_magnitudes = torch.norm(self.original_acts, dim=1)                                                                                                           
         nces = effect_sizes / (orig_magnitudes + 1e-8)                                                                                                                    
                                                                                                                                                                           
         return nces.mean()                                                                                                                                                
                                                                                                                                                                           
     def forward(self, acts, output_features=False, apply_intervention=False):                                                                                             
         encoded = self.encode(acts, apply_intervention)                                                                                                                   
         decoded = self.decode(encoded)                                                                                                                                    
                                                                                                                                                                           
         if output_features:                                                                                                                                               
             return decoded, encoded                                                                                                                                       
         return decoded                                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 class CustomTrainer(SAETrainer):                                                                                                                                          
     """Trainer for Custom Sparse Autoencoder using L1 regularization."""                                                                                                  
 =======                                                                                                                                                                   
 class CausalTrainer(SAETrainer):                                                                                                                                          
     """Trainer for Causal Sparse Autoencoder using L1 regularization and causal interventions."""                                                                         
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         # Initialize autoencoder                                                                                                                                          
         self.ae = CustomSAE(d_in=activation_dim, d_sae=dict_size, hook_layer=layer, model_name=lm_name)                                                                   
 =======                                                                                                                                                                   
         # Initialize autoencoder                                                                                                                                          
         self.ae = CausalSAE(d_in=activation_dim, d_sae=dict_size, hook_layer=layer, model_name=lm_name)                                                                   
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def loss(self, x, logging=False, **kwargs):                                                                                                                           
         x_hat, f = self.ae(x, output_features=True)                                                                                                                       
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                             
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                              
                                                                                                                                                                           
         if self.steps_since_active is not None:                                                                                                                           
             # Update steps_since_active                                                                                                                                   
             deads = (f == 0).all(dim=0)                                                                                                                                   
             self.steps_since_active[deads] += 1                                                                                                                           
             self.steps_since_active[~deads] = 0                                                                                                                           
                                                                                                                                                                           
         loss = l2_loss + self.l1_penalty * l1_loss                                                                                                                        
 =======                                                                                                                                                                   
     def loss(self, x, logging=False, **kwargs):                                                                                                                           
         # Regular forward pass                                                                                                                                            
         x_hat, f = self.ae(x, output_features=True)                                                                                                                       
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                             
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                              
                                                                                                                                                                           
         # Causal intervention pass                                                                                                                                        
         _, _ = self.ae(x, output_features=True, apply_intervention=True)                                                                                                  
         causal_loss = self.ae.compute_nces()                                                                                                                              
                                                                                                                                                                           
         if self.steps_since_active is not None:                                                                                                                           
             deads = (f == 0).all(dim=0)                                                                                                                                   
             self.steps_since_active[deads] += 1                                                                                                                           
             self.steps_since_active[~deads] = 0                                                                                                                           
                                                                                                                                                                           
         loss = l2_loss + self.l1_penalty * l1_loss + self.ae.causal_penalty * causal_loss                                                                                 
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This implements the basic CSAE with:                                                                                                                                       

 1 Causal intervention mechanism using zero-ablation                                                                                                                       
 2 NCES metric computation                                                                                                                                                 
 3 Modified loss function incorporating causal objectives                                                                                                                  
 4 Visualization utilities for analysis                                                                                                                                    

The changes rename CustomSAE to CausalSAE and add causal intervention capabilities while maintaining compatibility with the existing evaluation framework.                 

For Run 1, we can execute:                                                                                                                                                 

                                                                                                                                                                           
 python experiment.py --out_dir=run_1                                                                                                                                      
                                                                                                                                                                           

This will train the CSAE with default causal penalty (0.1) and zero-ablation intervention strategy. After seeing these results, we can proceed with the ablation studies in
Run 2.                                                                                                                                                                     

Tokens: 13k sent, 2.4k received. Cost: $0.08 message, $0.08 session.
Applied edit to plot.py
Applied edit to experiment.py

# Fix any errors below, if possible.

## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated 
experiment.py

experiment.py:420:15: F821 undefined name 'CustomTrainer'
    trainer = CustomTrainer(
              ^

## See relevant line below marked with █.

experiment.py:
...⋮...
360│def run_sae_training(
361│    layer: int,
362│    dict_size: int,
363│    num_tokens: int,
364│    out_dir: str,  # Changed from save_dir to out_dir for consistency
365│    device: str,
366│    model_name: str = "google/gemma-2b",
367│    context_length: int = 128,
368│    buffer_size: int = 2048,
369│    llm_batch_size: int = 24,
...⋮...
380│    out_dir = os.path.abspath(out_dir)
...⋮...
417│    )
418│
419│    # Initialize trainer  
420█    trainer = CustomTrainer(
421│        activation_dim=activation_dim,
422│        dict_size=dict_size,
423│        lr=learning_rate,
424│        l1_penalty=sparsity_penalty,
425│        warmup_steps=warmup_steps,
426│        seed=seed,
427│        device=device,
428│        layer=layer,
429│        lm_name=model_name,
...⋮...


I'll fix the undefined name error by updating the trainer initialization to use CausalTrainer instead of CustomTrainer.                                                    

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     # Initialize trainer                                                                                                                                                  
     trainer = CustomTrainer(                                                                                                                                              
 =======                                                                                                                                                                   
     # Initialize trainer                                                                                                                                                  
     trainer = CausalTrainer(                                                                                                                                              
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This change matches the earlier renaming of CustomTrainer to CausalTrainer in the class definition. The trainer will now be properly initialized with the causal           
intervention capabilities we added.                                                                                                                                        

Tokens: 16k sent, 120 received. Cost: $0.05 message, $0.12 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
