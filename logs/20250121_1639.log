Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.

Generating idea 1/1
Iteration 1/10
{'Name': 'causal_discovery_sae', 'Title': 'Causal Discovery in Sparse Autoencoders for Targeted Knowledge Unlearning', 'Experiment': '1. Implement causal discovery module using neural networks to model feature-outcome relationships\n2. Modify SAE training to include interventional phases where features are systematically ablated\n3. Add auxiliary loss terms based on causal impact scores\n4. Train on Gemma-2B model focusing on biology knowledge unlearning\n5. Compare unlearning effectiveness against baseline approaches\n6. Analyze causal graphs and feature importance scores\n7. Evaluate preservation of unrelated capabilities', 'Technical_Details': 'The architecture consists of three components: (1) Standard SAE encoder/decoder, (2) Causal discovery network that takes latent activations and predicts task outcomes, (3) Intervention module that systematically zeros out features. Training alternates between normal reconstruction phases and interventional phases. During interventional phases, features are ablated according to a scheduling algorithm that ensures coverage while maintaining computational efficiency. The causal discovery network is trained to predict task-specific outcomes (e.g. biology question accuracy) from latent features. Causal impact scores are computed using an attribution method similar to Integrated Gradients. These scores inform both feature selection for unlearning and an auxiliary loss term that encourages learning of causally relevant features. The loss function combines standard reconstruction loss with the auxiliary causal impact loss: L = L_recon + λ*L_causal where λ is dynamically adjusted based on validation performance.', 'Research_Impact': "A key challenge in knowledge unlearning is identifying which features to target while minimizing collateral damage to other capabilities. Current sparsity-based approaches can be unreliable as they don't directly consider causal relationships. This research addresses this by explicitly modeling causal connections between features and outcomes, enabling more precise and reliable unlearning interventions. The interventional training approach helps distinguish truly causal features from mere correlations, potentially leading to better preservation of desired capabilities.", 'Implementation_Plan': '1. Add CausalDiscoveryNetwork class implementing the prediction network\n2. Modify CustomSAE to include intervention logic and causal discovery components\n3. Update loss function to include causal impact term\n4. Implement intervention scheduling algorithm\n5. Add evaluation metrics for causal impact scores\n6. Modify training loop to alternate between normal and interventional phases\n7. Add visualization functions for causal graphs', 'Interestingness_Evaluation': 'The integration of causal discovery with sparse autoencoders represents a novel approach to addressing a fundamental challenge in knowledge unlearning, with potential implications for AI safety.', 'Interestingness': 9, 'Feasibility_Evaluation': 'While the core SAE modifications are straightforward, the causal discovery components add significant complexity and computational overhead through the interventional training process, making it challenging to complete within the 30-minute runtime constraint on an H100.', 'Feasibility': 5, 'Novelty_Evaluation': 'While causal discovery has been applied in other ML contexts, its integration with sparse autoencoders for knowledge unlearning represents a novel technical contribution.', 'Novelty': 8, 'Overall_Score': 6.7}
Iteration 2/10
{'Name': 'causal_discovery_sae', 'Title': 'Efficient Causal Feature Discovery in Sparse Autoencoders for Knowledge Unlearning', 'Experiment': '1. Implement lightweight feature attribution network\n2. Add auxiliary loss based on integrated gradients\n3. Train on Gemma-2B model with biology knowledge unlearning task\n4. Compare feature importance scores against baseline methods\n5. Evaluate unlearning performance on WMDP-bio dataset\n6. Analyze feature attribution patterns\n7. Measure impact on unrelated MMLU tasks', 'Technical_Details': "The architecture adds a simple MLP-based feature attribution network to the standard SAE. This network maps latent features to task-specific outcomes using a 2-layer architecture with dimensions [d_sae, d_hidden, 1]. Feature importance scores are computed using integrated gradients with 20 interpolation steps. The loss function combines reconstruction loss with a feature-importance weighted sparsity penalty: L = L_recon + λ*Σ(importance_i * |h_i|) where h_i are latent activations and importance_i are the attribution scores. Importance scores are computed every 100 training steps using a batch of 256 examples. The MLP is trained using binary cross-entropy loss on the unlearning task labels. Implementation uses PyTorch's built-in autograd for efficient gradient computation.", 'Research_Impact': 'Current approaches to knowledge unlearning rely on simple sparsity metrics to select features, which can lead to suboptimal trade-offs between unlearning and capability preservation. This research introduces an efficient way to identify causally relevant features through gradient-based attribution, enabling more targeted unlearning interventions. The lightweight design makes it practical to apply to large language models while providing better feature selection than existing methods.', 'Implementation_Plan': '1. Add FeatureAttributionMLP class implementing the 2-layer network\n2. Modify CustomSAE to include attribution component\n3. Implement integrated gradients computation\n4. Add importance-weighted sparsity loss\n5. Update training loop to periodically compute importance scores\n6. Add evaluation metrics for feature importance\n7. Implement visualization tools for attribution patterns', 'Interestingness_Evaluation': 'The combination of gradient-based feature attribution with sparse autoencoders provides a novel and practical approach to improving knowledge unlearning in large language models.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The simplified design using a lightweight MLP and efficient gradient-based attribution should be implementable within a month and run within 30 minutes on an H100, while the technical complexity remains manageable for a junior PhD student.', 'Feasibility': 8, 'Novelty_Evaluation': 'While both feature attribution and sparse autoencoders are known techniques, their combination for targeted knowledge unlearning represents a novel contribution.', 'Novelty': 8, 'Overall_Score': 8.3}
Iteration 3/10
{'Name': 'causal_discovery_sae', 'Title': 'Direct Gradient Attribution in Sparse Autoencoders for Efficient Knowledge Unlearning', 'Experiment': '1. Implement direct gradient attribution through SAE decoder\n2. Add importance-weighted sparsity loss\n3. Train on Gemma-2B focusing on WMDP-bio unlearning\n4. Compare against baseline using retain_threshold sweep\n5. Evaluate unlearning on both WMDP-bio and MMLU\n6. Analyze feature importance distributions\n7. Measure computational overhead', 'Technical_Details': "The method computes feature importance scores directly through the SAE decoder weights: importance_i = |W_dec[:,i] @ grad_task|, where grad_task is the gradient of the task loss with respect to the decoder output. This eliminates the need for a separate attribution network. The loss function uses an importance-weighted L1 penalty: L = L_recon + λ*Σ(importance_i * |h_i|). Importance scores are computed every 50 steps using a batch of 128 examples from both WMDP-bio (target) and MMLU (control) datasets. The implementation uses efficient batch processing and PyTorch's autograd. Hyperparameters: λ=0.01, learning_rate=3e-4, warmup_steps=1000. Features are selected for unlearning using a threshold on the ratio of importance scores between target and control tasks.", 'Research_Impact': 'Current unlearning methods struggle to efficiently identify which features to target, often requiring extensive hyperparameter tuning of retain_threshold values. This research introduces a simple yet effective way to automatically identify relevant features through direct gradient attribution, enabling more efficient and reliable unlearning. The method requires minimal additional computation while providing better feature selection than sparsity-based approaches.', 'Implementation_Plan': '1. Add importance score computation to CustomSAE forward pass\n2. Implement importance-weighted sparsity loss\n3. Modify training loop to compute gradients on task loss\n4. Add feature selection based on importance ratios\n5. Update evaluation metrics for unlearning benchmark\n6. Add visualization functions for importance scores\n7. Implement efficient batch processing', 'Interestingness_Evaluation': 'The direct use of decoder gradients for feature attribution provides an elegant and efficient solution to the feature selection problem in knowledge unlearning.', 'Interestingness': 9, 'Feasibility_Evaluation': 'The simplified design using direct gradient computation eliminates architectural complexity, making it straightforward to implement within a month and guaranteeing runtime under 30 minutes on an H100, with clear implementation steps for a junior PhD student.', 'Feasibility': 9, 'Novelty_Evaluation': 'While gradient-based attribution is known, its direct application through SAE decoders for knowledge unlearning represents a novel and practical contribution.', 'Novelty': 9, 'Overall_Score': 9.0}