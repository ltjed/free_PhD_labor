Using GPUs: [0]
Using Anthropic API with model claude-3-5-sonnet-20241022.
ideas
Loaded existing ideas:
[{'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}, {'Name': 'temporal_sae', 'Title': 'Structured Positional Masking for Adaptive Temporal Feature Specialization', 'Experiment': '1. Initialize position-specific weight masks\n\n2. Train with masked gradient updates\n\n3. Compare to static slicing\n\n4. Evaluate:\n\n   - Position specialization retention\n\n   - Sparse_probing on order-sensitive tasks\n\n   - Feature activation positional fidelity\n\n5. Ablation on mask strictness (hard vs soft)', 'Technical_Details': 'Architecture:\n\n1. Positional weight masking:\n\n   W_enc[:,g] initialized to zero except for 1/G-th slice per group\n\n   Mask remains fixed during training\n\n2. Groups G=4 correspond to 4-token window\n\n3. No parameter increase vs baseline SAE\n\n\nOptimizations:\n\n- Mask applied via gradient masking\n\n- Efficient sparse matrix operations\n\n- Maintains original SAE parameter count\n\n\nHyperparameters:\n\n- Window size=4\n\n- Learning rate=3e-4\n\n- L1 penalty=0.1\n\n- Mask strictness=hard', 'Implementation_Plan': '1. Modify CustomSAE.__init__ with masked weight init\n\n2. Add gradient masking in backward pass\n\n3. Update evaluation to track position specialization\n\n4. Benchmark against positional group and baseline SAEs', 'Interestingness_Evaluation': 'Combines learnable adaptation with structured positional focus through innovative weight masking, enhancing monosemanticity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Uses existing init patterns and gradient masking; no new parameters; runtime under 16min on H100 due to sparse ops.', 'Feasibility': 10, 'Novelty_Evaluation': 'Novel integration of structured weight masking for position-aware feature learning in SAEs.', 'Novelty': 9, 'Expected_Research_Impact': 'Directly targets sparse_probing by enforcing position-specific feature learning through masked weights, reducing polysemanticity via structured adaptation.', 'Research_Impact': 9, 'Overall_Score': 9.6, 'Abstract': "Sparse Autoencoders (SAEs) are a cornerstone for capturing high-level features while minimizing redundancy in AI safety research. However, a persistent challenge lies in ensuring that learned features are both position-specific and interpretable, particularly in temporal tasks where the order of input elements is critical. We propose TemporalSAE (T-SAE), a sparse autoencoder variant that enforces position-specific feature learning through structured weight masking. By initializing each encoder group's weights to focus on distinct positions in an n-token window and freezing these masks during training, latent features naturally specialize in temporal patterns at specific offsets. This approach maintains the original SAE's parameter count while incorporating positional awareness through carefully designed weight initialization. It also preserves interpretability by constraining features to particular context positions yet allows adaptation within those bounds. We benchmark T-SAE on tasks requiring strict ordering and positional awareness, showing that it surpasses both conventional SAEs and simpler slicing-based baselines. Beyond quantitative gains, our results also highlight enhanced coherence in how features map to temporal offsets, improving model transparency. These findings highlight  T-SAE’s potential to serve as a position-aware yet adaptable architecture for various time-sensitive or sequence-based problems in modern machine learning.", 'novel': True}, {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}, {'Name': 'temporal_sae', 'Title': 'Structured Positional Masking for Adaptive Temporal Feature Specialization', 'Experiment': '1. Initialize position-specific weight masks\n\n2. Train with masked gradient updates\n\n3. Compare to static slicing\n\n4. Evaluate:\n\n   - Position specialization retention\n\n   - Sparse_probing on order-sensitive tasks\n\n   - Feature activation positional fidelity\n\n5. Ablation on mask strictness (hard vs soft)', 'Technical_Details': 'Architecture:\n\n1. Positional weight masking:\n\n   W_enc[:,g] initialized to zero except for 1/G-th slice per group\n\n   Mask remains fixed during training\n\n2. Groups G=4 correspond to 4-token window\n\n3. No parameter increase vs baseline SAE\n\n\nOptimizations:\n\n- Mask applied via gradient masking\n\n- Efficient sparse matrix operations\n\n- Maintains original SAE parameter count\n\n\nHyperparameters:\n\n- Window size=4\n\n- Learning rate=3e-4\n\n- L1 penalty=0.1\n\n- Mask strictness=hard', 'Implementation_Plan': '1. Modify CustomSAE.__init__ with masked weight init\n\n2. Add gradient masking in backward pass\n\n3. Update evaluation to track position specialization\n\n4. Benchmark against positional group and baseline SAEs', 'Interestingness_Evaluation': 'Combines learnable adaptation with structured positional focus through innovative weight masking, enhancing monosemanticity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Uses existing init patterns and gradient masking; no new parameters; runtime under 16min on H100 due to sparse ops.', 'Feasibility': 10, 'Novelty_Evaluation': 'Novel integration of structured weight masking for position-aware feature learning in SAEs.', 'Novelty': 9, 'Expected_Research_Impact': 'Directly targets sparse_probing by enforcing position-specific feature learning through masked weights, reducing polysemanticity via structured adaptation.', 'Research_Impact': 9, 'Overall_Score': 9.6, 'Abstract': "Sparse Autoencoders (SAEs) are a cornerstone for capturing high-level features while minimizing redundancy in AI safety research. However, a persistent challenge lies in ensuring that learned features are both position-specific and interpretable, particularly in temporal tasks where the order of input elements is critical. We propose TemporalSAE (T-SAE), a sparse autoencoder variant that enforces position-specific feature learning through structured weight masking. By initializing each encoder group's weights to focus on distinct positions in an n-token window and freezing these masks during training, latent features naturally specialize in temporal patterns at specific offsets. This approach maintains the original SAE's parameter count while incorporating positional awareness through carefully designed weight initialization. It also preserves interpretability by constraining features to particular context positions yet allows adaptation within those bounds. We benchmark T-SAE on tasks requiring strict ordering and positional awareness, showing that it surpasses both conventional SAEs and simpler slicing-based baselines. Beyond quantitative gains, our results also highlight enhanced coherence in how features map to temporal offsets, improving model transparency. These findings highlight  T-SAE’s potential to serve as a position-aware yet adaptable architecture for various time-sensitive or sequence-based problems in modern machine learning.", 'novel': True}, {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}, {'Name': 'temporal_sae', 'Title': 'Structured Positional Masking for Adaptive Temporal Feature Specialization', 'Experiment': '1. Initialize position-specific weight masks\n\n2. Train with masked gradient updates\n\n3. Compare to static slicing\n\n4. Evaluate:\n\n   - Position specialization retention\n\n   - Sparse_probing on order-sensitive tasks\n\n   - Feature activation positional fidelity\n\n5. Ablation on mask strictness (hard vs soft)', 'Technical_Details': 'Architecture:\n\n1. Positional weight masking:\n\n   W_enc[:,g] initialized to zero except for 1/G-th slice per group\n\n   Mask remains fixed during training\n\n2. Groups G=4 correspond to 4-token window\n\n3. No parameter increase vs baseline SAE\n\n\nOptimizations:\n\n- Mask applied via gradient masking\n\n- Efficient sparse matrix operations\n\n- Maintains original SAE parameter count\n\n\nHyperparameters:\n\n- Window size=4\n\n- Learning rate=3e-4\n\n- L1 penalty=0.1\n\n- Mask strictness=hard', 'Implementation_Plan': '1. Modify CustomSAE.__init__ with masked weight init\n\n2. Add gradient masking in backward pass\n\n3. Update evaluation to track position specialization\n\n4. Benchmark against positional group and baseline SAEs', 'Interestingness_Evaluation': 'Combines learnable adaptation with structured positional focus through innovative weight masking, enhancing monosemanticity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Uses existing init patterns and gradient masking; no new parameters; runtime under 16min on H100 due to sparse ops.', 'Feasibility': 10, 'Novelty_Evaluation': 'Novel integration of structured weight masking for position-aware feature learning in SAEs.', 'Novelty': 9, 'Expected_Research_Impact': 'Directly targets sparse_probing by enforcing position-specific feature learning through masked weights, reducing polysemanticity via structured adaptation.', 'Research_Impact': 9, 'Overall_Score': 9.6, 'Abstract': "Sparse Autoencoders (SAEs) are a cornerstone for capturing high-level features while minimizing redundancy in AI safety research. However, a persistent challenge lies in ensuring that learned features are both position-specific and interpretable, particularly in temporal tasks where the order of input elements is critical. We propose TemporalSAE (T-SAE), a sparse autoencoder variant that enforces position-specific feature learning through structured weight masking. By initializing each encoder group's weights to focus on distinct positions in an n-token window and freezing these masks during training, latent features naturally specialize in temporal patterns at specific offsets. This approach maintains the original SAE's parameter count while incorporating positional awareness through carefully designed weight initialization. It also preserves interpretability by constraining features to particular context positions yet allows adaptation within those bounds. We benchmark T-SAE on tasks requiring strict ordering and positional awareness, showing that it surpasses both conventional SAEs and simpler slicing-based baselines. Beyond quantitative gains, our results also highlight enhanced coherence in how features map to temporal offsets, improving model transparency. These findings highlight  T-SAE’s potential to serve as a position-aware yet adaptable architecture for various time-sensitive or sequence-based problems in modern machine learning.", 'novel': True}, {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}, {'Name': 'temporal_sae', 'Title': 'Structured Positional Masking for Adaptive Temporal Feature Specialization', 'Experiment': '1. Initialize position-specific weight masks\n\n2. Train with masked gradient updates\n\n3. Compare to static slicing\n\n4. Evaluate:\n\n   - Position specialization retention\n\n   - Sparse_probing on order-sensitive tasks\n\n   - Feature activation positional fidelity\n\n5. Ablation on mask strictness (hard vs soft)', 'Technical_Details': 'Architecture:\n\n1. Positional weight masking:\n\n   W_enc[:,g] initialized to zero except for 1/G-th slice per group\n\n   Mask remains fixed during training\n\n2. Groups G=4 correspond to 4-token window\n\n3. No parameter increase vs baseline SAE\n\n\nOptimizations:\n\n- Mask applied via gradient masking\n\n- Efficient sparse matrix operations\n\n- Maintains original SAE parameter count\n\n\nHyperparameters:\n\n- Window size=4\n\n- Learning rate=3e-4\n\n- L1 penalty=0.1\n\n- Mask strictness=hard', 'Implementation_Plan': '1. Modify CustomSAE.__init__ with masked weight init\n\n2. Add gradient masking in backward pass\n\n3. Update evaluation to track position specialization\n\n4. Benchmark against positional group and baseline SAEs', 'Interestingness_Evaluation': 'Combines learnable adaptation with structured positional focus through innovative weight masking, enhancing monosemanticity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Uses existing init patterns and gradient masking; no new parameters; runtime under 16min on H100 due to sparse ops.', 'Feasibility': 10, 'Novelty_Evaluation': 'Novel integration of structured weight masking for position-aware feature learning in SAEs.', 'Novelty': 9, 'Expected_Research_Impact': 'Directly targets sparse_probing by enforcing position-specific feature learning through masked weights, reducing polysemanticity via structured adaptation.', 'Research_Impact': 9, 'Overall_Score': 9.6, 'Abstract': "Sparse Autoencoders (SAEs) are a cornerstone for capturing high-level features while minimizing redundancy in AI safety research. However, a persistent challenge lies in ensuring that learned features are both position-specific and interpretable, particularly in temporal tasks where the order of input elements is critical. We propose TemporalSAE (T-SAE), a sparse autoencoder variant that enforces position-specific feature learning through structured weight masking. By initializing each encoder group's weights to focus on distinct positions in an n-token window and freezing these masks during training, latent features naturally specialize in temporal patterns at specific offsets. This approach maintains the original SAE's parameter count while incorporating positional awareness through carefully designed weight initialization. It also preserves interpretability by constraining features to particular context positions yet allows adaptation within those bounds. We benchmark T-SAE on tasks requiring strict ordering and positional awareness, showing that it surpasses both conventional SAEs and simpler slicing-based baselines. Beyond quantitative gains, our results also highlight enhanced coherence in how features map to temporal offsets, improving model transparency. These findings highlight  T-SAE’s potential to serve as a position-aware yet adaptable architecture for various time-sensitive or sequence-based problems in modern machine learning.", 'novel': True}, {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}, {'Name': 'temporal_sae', 'Title': 'Structured Positional Masking for Adaptive Temporal Feature Specialization', 'Experiment': '1. Initialize position-specific weight masks\n\n2. Train with masked gradient updates\n\n3. Compare to static slicing\n\n4. Evaluate:\n\n   - Position specialization retention\n\n   - Sparse_probing on order-sensitive tasks\n\n   - Feature activation positional fidelity\n\n5. Ablation on mask strictness (hard vs soft)', 'Technical_Details': 'Architecture:\n\n1. Positional weight masking:\n\n   W_enc[:,g] initialized to zero except for 1/G-th slice per group\n\n   Mask remains fixed during training\n\n2. Groups G=4 correspond to 4-token window\n\n3. No parameter increase vs baseline SAE\n\n\nOptimizations:\n\n- Mask applied via gradient masking\n\n- Efficient sparse matrix operations\n\n- Maintains original SAE parameter count\n\n\nHyperparameters:\n\n- Window size=4\n\n- Learning rate=3e-4\n\n- L1 penalty=0.1\n\n- Mask strictness=hard', 'Implementation_Plan': '1. Modify CustomSAE.__init__ with masked weight init\n\n2. Add gradient masking in backward pass\n\n3. Update evaluation to track position specialization\n\n4. Benchmark against positional group and baseline SAEs', 'Interestingness_Evaluation': 'Combines learnable adaptation with structured positional focus through innovative weight masking, enhancing monosemanticity.', 'Interestingness': 9, 'Feasibility_Evaluation': 'Uses existing init patterns and gradient masking; no new parameters; runtime under 16min on H100 due to sparse ops.', 'Feasibility': 10, 'Novelty_Evaluation': 'Novel integration of structured weight masking for position-aware feature learning in SAEs.', 'Novelty': 9, 'Expected_Research_Impact': 'Directly targets sparse_probing by enforcing position-specific feature learning through masked weights, reducing polysemanticity via structured adaptation.', 'Research_Impact': 9, 'Overall_Score': 9.6, 'Abstract': "Sparse Autoencoders (SAEs) are a cornerstone for capturing high-level features while minimizing redundancy in AI safety research. However, a persistent challenge lies in ensuring that learned features are both position-specific and interpretable, particularly in temporal tasks where the order of input elements is critical. We propose TemporalSAE (T-SAE), a sparse autoencoder variant that enforces position-specific feature learning through structured weight masking. By initializing each encoder group's weights to focus on distinct positions in an n-token window and freezing these masks during training, latent features naturally specialize in temporal patterns at specific offsets. This approach maintains the original SAE's parameter count while incorporating positional awareness through carefully designed weight initialization. It also preserves interpretability by constraining features to particular context positions yet allows adaptation within those bounds. We benchmark T-SAE on tasks requiring strict ordering and positional awareness, showing that it surpasses both conventional SAEs and simpler slicing-based baselines. Beyond quantitative gains, our results also highlight enhanced coherence in how features map to temporal offsets, improving model transparency. These findings highlight  T-SAE’s potential to serve as a position-aware yet adaptable architecture for various time-sensitive or sequence-based problems in modern machine learning.", 'novel': True}]
Processing idea: adaptive_orthogonal_sae
2025-01-26 12:05:25
*Starting idea: 20250126_115529_adaptive_orthogonal_sae*
2025-01-26 12:05:25
*Starting Experiments*
Starting experiment with prompt for coder: Your goal is to implement the following idea: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement. Pay attention to the following details from the idea:
The proposed experiment is as follows: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact.
The implementation plan is as follows: 1. Add batch_topk_pair_selector using torch.topk
2. Modify loss to use instantaneous top pairs
3. Implement L2 weight normalization via hook
4. Add τ as fixed config parameter
5. Integrate pair stability analysis
6. Add top-k percentage parameter
7. Update config with normalization type.

You can also refer to other information in the idea: {'Name': 'adaptive_orthogonal_sae', 'Title': 'Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement', 'Experiment': '1. Select top 0.1% f_i*f_j pairs per batch\n2. Apply orthogonality loss to selected pairs\n3. Use L2 weight normalization on W_dec\n4. Compare fixed vs adaptive τ values\n5. Measure absorption reduction efficiency\n6. Analyze pair selection stability\n7. Ablate top-k threshold impact', 'Technical_Details': 'Loss: L = L_recon + λ_1||f||_1 + λ_2Σ_{(i,j)∈TopK(batch)} |w_i·w_j|. Top pairs selected via torch.topk on (f @ f.T).flatten(). L2 normalization: W_dec = W / ||W||_2. Fixed τ=0.2. No history tracking - pure batch-level selection. Orthogonality loss applied only to top 0.1% of non-diagonal pairs each batch.', 'Implementation_Plan': '1. Add batch_topk_pair_selector using torch.topk\n2. Modify loss to use instantaneous top pairs\n3. Implement L2 weight normalization via hook\n4. Add τ as fixed config parameter\n5. Integrate pair stability analysis\n6. Add top-k percentage parameter\n7. Update config with normalization type', 'Interestingness_Evaluation': 'Pure batch-level targeting maximizes simplicity while maintaining absorption resistance.', 'Interestingness': 8, 'Feasibility_Evaluation': 'torch.topk is O(n log k) and native. L2 norm via vectorized ops. Entire implementation adds <50 lines. Runs in 20 mins/H100.', 'Feasibility': 10, 'Novelty_Evaluation': 'Instantaneous top-k constraints without history provide novel middle ground between fixed and adaptive methods.', 'Novelty': 7, 'Expected_Research_Impact': 'Focused batch-level constraints directly prevent feature merging, improving sparse_probing through better feature isolation.', 'Research_Impact': 9, 'Overall_Score': 9.0, 'Abstract': 'Deep representation learning often encounters polysemantic features—single dimensions that encode multiple, unrelated concepts—complicating both interpretability and downstream usage. To address this challenge, we propose an orthogonal sparse autoencoder that enforces feature separation in a simple, batch-level fashion. During each training step, our method targets only a small fraction of the most strongly co-activated feature pairs and applies an orthogonality penalty to them, preventing the merging of unrelated representations. By focusing on the most problematic interactions as they happen rather than tracking them over the entire training history, we reduce computational overhead and maintain a streamlined implementation. We assess the effectiveness of both fixed and adaptive strategies for selecting top pairs based on activations from models with up to 2 billion parameters. Our evaluation encompasses metrics that measure debiasing and the unlearning of dangerous knowledge, highlighting improvements in interpretability and reduced feature overlap without compromising reconstruction fidelity. Overall, our approach provides a flexible and efficient solution for mitigating polysemanticity while maintaining the simplicity and speed of batch-level updates, taking another step toward preventing the harmful use of state-of-the-art language models.', 'novel': True}

You are given a total of up to 10 runs to complete the necessary experiments. You do not need to use all 10.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

{'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031}, 'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss': 464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss': 8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss': 810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375}, 'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438, 'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss': 210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss': 190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203, 'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'step 4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375, 'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211, 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375, 'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage': {'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0}, 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf', 'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2': 0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297, 'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10': 0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652, 'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100': -0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888, 'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2': 0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108, 'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20': 0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302, 'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100': 0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213, 'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': -0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792, 'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10': 0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363, 'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50': 0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802, 'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923, 'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5': 0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978, 'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50': -0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643, 'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500': 0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2': -0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804, 'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10': 0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717, 'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50': 0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106, 'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496, 'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5': 0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356, 'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50': -0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609, 'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500': 0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5': 0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005, 'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20': 0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978, 'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100': 0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2': 0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597, 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10': 0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093, 'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100': -0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262, 'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2': 0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654, 'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687, 'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100': 0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045, 'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives': 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives': 980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features': 2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, {'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis': 1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125, 'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025, 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001, 'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125, 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9566000580787659, 'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy': 0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy': 0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.8468, 'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734, 'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy': 0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001, 'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999, 'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999, 'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001, 'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841, 'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy': 0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962, 'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877, 'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292, 'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy': 0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435, 'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy': 0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy': 0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925, 'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999, 'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001, 'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015, 'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy': 0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'training results for layer 5': {'training_log': {'step 0': {'loss': 280.3485107421875, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}, 'step 1': {'loss': 243.578369140625, 'l1_loss': 2632.935546875, 'l2_loss': 138.26095581054688}, 'step 1220': {'loss': 71.24016571044922, 'l1_loss': 168.2802734375, 'l2_loss': 64.50895690917969}, 'step 1221': {'loss': 71.71453857421875, 'l1_loss': 183.52857971191406, 'l2_loss': 64.37339782714844}, 'step 2440': {'loss': 65.88867950439453, 'l1_loss': 168.97921752929688, 'l2_loss': 59.12950897216797}, 'step 2441': {'loss': 65.85481262207031, 'l1_loss': 172.13116455078125, 'l2_loss': 58.969566345214844}, 'step 3660': {'loss': 63.72981643676758, 'l1_loss': 179.27920532226562, 'l2_loss': 56.55864715576172}, 'step 3661': {'loss': 64.40095520019531, 'l1_loss': 179.67428588867188, 'l2_loss': 57.21398162841797}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 5, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_5'}, 'final_info': {'training_steps': 4882, 'final_loss': 61.984031677246094, 'layer': 5, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 12': {'training_log': {'step 0': {'loss': 461.9488525390625, 'l1_loss': 4426.9755859375, 'l2_loss': 284.86981201171875}, 'step 1': {'loss': 398.80157470703125, 'l1_loss': 4297.7392578125, 'l2_loss': 226.8920135498047}, 'step 1220': {'loss': 110.96922302246094, 'l1_loss': 365.6455078125, 'l2_loss': 96.34339904785156}, 'step 1221': {'loss': 111.44891357421875, 'l1_loss': 394.0783996582031, 'l2_loss': 95.68577575683594}, 'step 2440': {'loss': 102.57173919677734, 'l1_loss': 313.1291809082031, 'l2_loss': 90.04656982421875}, 'step 2441': {'loss': 102.70217895507812, 'l1_loss': 324.12835693359375, 'l2_loss': 89.73704528808594}, 'step 3660': {'loss': 99.31343841552734, 'l1_loss': 303.0018615722656, 'l2_loss': 87.19336700439453}, 'step 3661': {'loss': 99.67717742919922, 'l1_loss': 300.0284118652344, 'l2_loss': 87.67604064941406}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 96.88678741455078, 'layer': 12, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 19': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 1220': {'loss': 232.52467346191406, 'l1_loss': 571.1429443359375, 'l2_loss': 209.678955078125}, 'step 1221': {'loss': 233.77053833007812, 'l1_loss': 632.4195556640625, 'l2_loss': 208.4737548828125}, 'step 2440': {'loss': 214.23690795898438, 'l1_loss': 502.8707580566406, 'l2_loss': 194.1220703125}, 'step 2441': {'loss': 214.14248657226562, 'l1_loss': 508.6922912597656, 'l2_loss': 193.7947998046875}, 'step 3660': {'loss': 206.41171264648438, 'l1_loss': 514.4593505859375, 'l2_loss': 185.83334350585938}, 'step 3661': {'loss': 206.8395233154297, 'l1_loss': 515.376708984375, 'l2_loss': 186.22445678710938}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.
Currently on iteration 0 of run 1
I'll help implement the Instantaneous Top-k Orthogonality Constraints idea. Let's plan the experiments:                                                                    

Proposed runs:                                                                                                                                                             

 1 Run 1: Implement base orthogonal SAE with fixed τ=0.2, top-k=0.1%                                                                                                       
 2 Run 2: Test adaptive τ (starting at 0.2, adjusted based on activation magnitudes)                                                                                       
 3 Run 3: Ablation with top-k=0.05%                                                                                                                                        
 4 Run 4: Ablation with top-k=0.2%                                                                                                                                         

Let's start with Run 1 - implementing the base orthogonal SAE with fixed parameters.                                                                                       

Here are the changes needed:                                                                                                                                               

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 class CustomSAE(nn.Module):                                                                                                                                               
 =======                                                                                                                                                                   
 class OrthogonalSAE(nn.Module):                                                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         self.cfg = CustomSAEConfig(                                                                                                                                       
             model_name=model_name,                                                                                                                                        
             d_in=d_in,                                                                                                                                                    
             d_sae=d_sae,                                                                                                                                                  
             hook_name=hook_name,                                                                                                                                          
             hook_layer=hook_layer,                                                                                                                                        
             architecture="Custom",                                                                                                                                        
             activation_fn_str="relu",                                                                                                                                     
             apply_b_dec_to_input=True,                                                                                                                                    
         )                                                                                                                                                                 
 =======                                                                                                                                                                   
         self.cfg = CustomSAEConfig(                                                                                                                                       
             model_name=model_name,                                                                                                                                        
             d_in=d_in,                                                                                                                                                    
             d_sae=d_sae,                                                                                                                                                  
             hook_name=hook_name,                                                                                                                                          
             hook_layer=hook_layer,                                                                                                                                        
             architecture="Orthogonal",                                                                                                                                    
             activation_fn_str="relu",                                                                                                                                     
             apply_b_dec_to_input=True,                                                                                                                                    
         )                                                                                                                                                                 
         # Fixed parameters for orthogonality constraints                                                                                                                  
         self.tau = 0.2  # Fixed τ value                                                                                                                                   
         self.topk_percent = 0.001  # 0.1% of pairs                                                                                                                        
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 class CustomTrainer(SAETrainer):                                                                                                                                          
 =======                                                                                                                                                                   
 class OrthogonalTrainer(SAETrainer):                                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         self.ae = CustomSAE(d_in=activation_dim, d_sae=dict_size, hook_layer=layer, model_name=lm_name)                                                                   
 =======                                                                                                                                                                   
         self.ae = OrthogonalSAE(d_in=activation_dim, d_sae=dict_size, hook_layer=layer, model_name=lm_name)                                                               
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def loss(self, x, logging=False, **kwargs):                                                                                                                           
         x_hat, f = self.ae(x, output_features=True)                                                                                                                       
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                             
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                              
                                                                                                                                                                           
         if self.steps_since_active is not None:                                                                                                                           
             # Update steps_since_active                                                                                                                                   
             deads = (f == 0).all(dim=0)                                                                                                                                   
             self.steps_since_active[deads] += 1                                                                                                                           
             self.steps_since_active[~deads] = 0                                                                                                                           
                                                                                                                                                                           
         loss = l2_loss + self.l1_penalty * l1_loss                                                                                                                        
 =======                                                                                                                                                                   
     def loss(self, x, logging=False, **kwargs):                                                                                                                           
         x_hat, f = self.ae(x, output_features=True)                                                                                                                       
         l2_loss = torch.linalg.norm(x - x_hat, dim=-1).mean()                                                                                                             
         l1_loss = f.norm(p=1, dim=-1).mean()                                                                                                                              
                                                                                                                                                                           
         # Compute feature correlations                                                                                                                                    
         f_normed = f / (f.norm(dim=0, keepdim=True) + 1e-8)                                                                                                               
         correlations = torch.abs(f_normed.T @ f_normed)                                                                                                                   
                                                                                                                                                                           
         # Get top-k pairs excluding diagonal                                                                                                                              
         mask = torch.ones_like(correlations, dtype=torch.bool)                                                                                                            
         mask.fill_diagonal_(0)                                                                                                                                            
         correlations = correlations * mask                                                                                                                                
                                                                                                                                                                           
         num_pairs = int(self.ae.topk_percent * (correlations.numel() - correlations.size(0)))                                                                             
         top_correlations = torch.topk(correlations.view(-1), k=num_pairs).values                                                                                          
                                                                                                                                                                           
         # Orthogonality loss on selected pairs                                                                                                                            
         ortho_loss = (top_correlations - self.ae.tau).clamp(min=0).mean()                                                                                                 
                                                                                                                                                                           
         if self.steps_since_active is not None:                                                                                                                           
             # Update steps_since_active                                                                                                                                   
             deads = (f == 0).all(dim=0)                                                                                                                                   
             self.steps_since_active[deads] += 1                                                                                                                           
             self.steps_since_active[~deads] = 0                                                                                                                           
                                                                                                                                                                           
         loss = l2_loss + self.l1_penalty * l1_loss + ortho_loss                                                                                                           
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         return {"loss_for_backward": loss, "loss" : loss.cpu().item(), "l1_loss" : l1_loss.cpu().item(), "l2_loss" : l2_loss.cpu().item()}                                
 =======                                                                                                                                                                   
         return {                                                                                                                                                          
             "loss_for_backward": loss,                                                                                                                                    
             "loss": loss.cpu().item(),                                                                                                                                    
             "l1_loss": l1_loss.cpu().item(),                                                                                                                              
             "l2_loss": l2_loss.cpu().item(),                                                                                                                              
             "ortho_loss": ortho_loss.cpu().item()                                                                                                                         
         }                                                                                                                                                                 
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
             'trainer_class': 'CustomTrainer',                                                                                                                             
 =======                                                                                                                                                                   
             'trainer_class': 'OrthogonalTrainer',                                                                                                                         
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This implements the base orthogonal SAE with:                                                                                                                              

 • Fixed τ=0.2                                                                                                                                                             
 • Top 0.1% pair selection                                                                                                                                                 
 • L2 normalization of features for correlation computation                                                                                                                
 • Orthogonality loss on selected pairs                                                                                                                                    

Let's run this first version:                                                                                                                                              

                                                                                                                                                                           
 python experiment.py --out_dir=run_1                                                                                                                                      
                                                                                                                                                                           

After we see the results, we can proceed with implementing the adaptive τ version or adjust parameters as needed.                                                          

Tokens: 41k sent, 1.6k received. Cost: $0.15 message, $0.15 session.
Applied edit to experiment.py

# Fix any errors below, if possible.

## Running: /gpfs/radev/home/tl784/.conda/envs/ai_scientist/bin/python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated 
experiment.py

experiment.py:428:15: F821 undefined name 'CustomTrainer'
    trainer = CustomTrainer(
              ^

## See relevant line below marked with █.

experiment.py:
...⋮...
368│def run_sae_training(
369│    layer: int,
370│    dict_size: int,
371│    num_tokens: int,
372│    out_dir: str,  # Changed from save_dir to out_dir for consistency
373│    device: str,
374│    model_name: str = "google/gemma-2-2b",
375│    context_length: int = 128,
376│    buffer_size: int = 2048,
377│    llm_batch_size: int = 24,
...⋮...
388│    out_dir = os.path.abspath(out_dir)
...⋮...
425│    )
426│
427│    # Initialize trainer  
428█    trainer = CustomTrainer(
429│        activation_dim=activation_dim,
430│        dict_size=dict_size,
431│        lr=learning_rate,
432│        l1_penalty=sparsity_penalty,
433│        warmup_steps=warmup_steps,
434│        seed=seed,
435│        device=device,
436│        layer=layer,
437│        lm_name=model_name,
...⋮...


Ah yes, I need to fix the trainer name in the run_sae_training function to match the new OrthogonalTrainer class name. Here's the fix:                                     

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     # Initialize trainer                                                                                                                                                  
     trainer = CustomTrainer(                                                                                                                                              
 =======                                                                                                                                                                   
     # Initialize trainer                                                                                                                                                  
     trainer = OrthogonalTrainer(                                                                                                                                          
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This change ensures the trainer initialization matches the renamed OrthogonalTrainer class we defined earlier.                                                             

Now you can run:                                                                                                                                                           

                                                                                                                                                                           
 python experiment.py --out_dir=run_1                                                                                                                                      
                                                                                                                                                                           

Tokens: 28k sent, 143 received. Cost: $0.09 message, $0.23 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
Step 0: {'loss': 281.0832214355469, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297, 'ortho_loss': 0.7347031235694885}
Step 100: {'loss': 189.19134521484375, 'l1_loss': 1982.4755859375, 'l2_loss': 109.16094970703125, 'ortho_loss': 0.7313777208328247}
Step 200: {'loss': 136.88385009765625, 'l1_loss': 958.1005859375, 'l2_loss': 97.78282928466797, 'ortho_loss': 0.7769899964332581}
Step 300: {'loss': 111.13609313964844, 'l1_loss': 424.0897521972656, 'l2_loss': 93.37845611572266, 'ortho_loss': 0.7940422892570496}
Step 400: {'loss': 103.37602996826172, 'l1_loss': 258.711181640625, 'l2_loss': 92.229248046875, 'ortho_loss': 0.7983301877975464}
Step 500: {'loss': 96.2331314086914, 'l1_loss': 260.0888671875, 'l2_loss': 85.03023529052734, 'ortho_loss': 0.7993412613868713}
Step 600: {'loss': 89.25770568847656, 'l1_loss': 263.0946044921875, 'l2_loss': 77.9342269897461, 'ortho_loss': 0.7996945977210999}
Step 700: {'loss': 84.14183807373047, 'l1_loss': 245.42086791992188, 'l2_loss': 73.52528381347656, 'ortho_loss': 0.7997213006019592}
Step 800: {'loss': 80.29899597167969, 'l1_loss': 239.93150329589844, 'l2_loss': 69.90200805664062, 'ortho_loss': 0.7997311353683472}
Step 900: {'loss': 76.9971923828125, 'l1_loss': 198.8853759765625, 'l2_loss': 68.24232482910156, 'ortho_loss': 0.7994534969329834}
Step 1000: {'loss': 76.86920166015625, 'l1_loss': 209.5460205078125, 'l2_loss': 67.69173431396484, 'ortho_loss': 0.795622706413269}
Step 1100: {'loss': 73.65204620361328, 'l1_loss': 177.7801513671875, 'l2_loss': 65.759765625, 'ortho_loss': 0.7810738682746887}
Step 1200: {'loss': 72.06439208984375, 'l1_loss': 155.26406860351562, 'l2_loss': 65.29611206054688, 'ortho_loss': 0.5577130913734436}
Step 1300: {'loss': 71.43910217285156, 'l1_loss': 169.49978637695312, 'l2_loss': 64.22639465332031, 'ortho_loss': 0.43271446228027344}
Step 1400: {'loss': 70.288330078125, 'l1_loss': 173.21954345703125, 'l2_loss': 62.99394226074219, 'ortho_loss': 0.3656044900417328}
Step 1500: {'loss': 69.75755310058594, 'l1_loss': 166.63577270507812, 'l2_loss': 62.757667541503906, 'ortho_loss': 0.3344545066356659}
Step 1600: {'loss': 68.78141784667969, 'l1_loss': 169.47555541992188, 'l2_loss': 61.676124572753906, 'ortho_loss': 0.326267808675766}
Step 1700: {'loss': 67.50123596191406, 'l1_loss': 169.21542358398438, 'l2_loss': 60.42533874511719, 'ortho_loss': 0.3072810769081116}
Step 1800: {'loss': 67.4465560913086, 'l1_loss': 178.20950317382812, 'l2_loss': 60.011680603027344, 'ortho_loss': 0.3064931333065033}
Step 1900: {'loss': 66.63751983642578, 'l1_loss': 178.6092987060547, 'l2_loss': 59.19287109375, 'ortho_loss': 0.30027902126312256}
Step 2000: {'loss': 66.35823822021484, 'l1_loss': 182.10055541992188, 'l2_loss': 58.78279113769531, 'ortho_loss': 0.2914303243160248}
Step 2100: {'loss': 65.49185180664062, 'l1_loss': 182.60781860351562, 'l2_loss': 57.904075622558594, 'ortho_loss': 0.2834623456001282}
Step 2200: {'loss': 64.50604248046875, 'l1_loss': 177.40304565429688, 'l2_loss': 57.10914993286133, 'ortho_loss': 0.30077633261680603}
Step 2300: {'loss': 63.53194046020508, 'l1_loss': 191.45042419433594, 'l2_loss': 55.59356689453125, 'ortho_loss': 0.28035587072372437}
Step 2400: {'loss': 63.5290412902832, 'l1_loss': 192.7363739013672, 'l2_loss': 55.54346466064453, 'ortho_loss': 0.2761229872703552}
Step 2500: {'loss': 62.990509033203125, 'l1_loss': 196.33148193359375, 'l2_loss': 54.85790252685547, 'ortho_loss': 0.2793470323085785}
Step 2600: {'loss': 63.40279769897461, 'l1_loss': 204.89425659179688, 'l2_loss': 54.93486785888672, 'ortho_loss': 0.2721579372882843}
Step 2700: {'loss': 62.95103073120117, 'l1_loss': 201.51522827148438, 'l2_loss': 54.620506286621094, 'ortho_loss': 0.26991719007492065}
Step 2800: {'loss': 62.98516845703125, 'l1_loss': 199.9077606201172, 'l2_loss': 54.72053527832031, 'ortho_loss': 0.2683202922344208}
Step 2900: {'loss': 62.03525161743164, 'l1_loss': 199.03375244140625, 'l2_loss': 53.801414489746094, 'ortho_loss': 0.27248814702033997}
Step 3000: {'loss': 61.88628387451172, 'l1_loss': 209.63360595703125, 'l2_loss': 53.23341369628906, 'ortho_loss': 0.26752322912216187}
Step 3100: {'loss': 61.53661346435547, 'l1_loss': 209.9854736328125, 'l2_loss': 52.87112808227539, 'ortho_loss': 0.2660664916038513}
Step 3200: {'loss': 61.422183990478516, 'l1_loss': 208.85301208496094, 'l2_loss': 52.79458999633789, 'ortho_loss': 0.273470014333725}
Step 3300: {'loss': 61.53569412231445, 'l1_loss': 208.45339965820312, 'l2_loss': 52.941917419433594, 'ortho_loss': 0.25564277172088623}
Step 3400: {'loss': 60.65498352050781, 'l1_loss': 217.71238708496094, 'l2_loss': 51.68504333496094, 'ortho_loss': 0.2614428997039795}
Step 3500: {'loss': 59.97231674194336, 'l1_loss': 209.98348999023438, 'l2_loss': 51.315101623535156, 'ortho_loss': 0.2578759789466858}
Step 3600: {'loss': 61.04335021972656, 'l1_loss': 218.60043334960938, 'l2_loss': 52.04490661621094, 'ortho_loss': 0.2544248104095459}
Step 3700: {'loss': 60.662052154541016, 'l1_loss': 216.19747924804688, 'l2_loss': 51.75025939941406, 'ortho_loss': 0.26389172673225403}
Step 3800: {'loss': 60.08057403564453, 'l1_loss': 219.77926635742188, 'l2_loss': 51.03943634033203, 'ortho_loss': 0.24996833503246307}
Step 3900: {'loss': 59.469146728515625, 'l1_loss': 219.5731201171875, 'l2_loss': 50.43861389160156, 'ortho_loss': 0.24760837852954865}
Step 4000: {'loss': 60.044586181640625, 'l1_loss': 217.3795166015625, 'l2_loss': 51.096126556396484, 'ortho_loss': 0.2532806098461151}
Step 4100: {'loss': 58.88072204589844, 'l1_loss': 218.9342498779297, 'l2_loss': 49.87279510498047, 'ortho_loss': 0.2505587935447693}
Step 4200: {'loss': 59.567352294921875, 'l1_loss': 225.306884765625, 'l2_loss': 50.305110931396484, 'ortho_loss': 0.24996429681777954}
Step 4300: {'loss': 59.5628662109375, 'l1_loss': 226.11102294921875, 'l2_loss': 50.269073486328125, 'ortho_loss': 0.2493516206741333}
Step 4400: {'loss': 58.61037826538086, 'l1_loss': 224.53567504882812, 'l2_loss': 49.36997985839844, 'ortho_loss': 0.2589734196662903}
Step 4500: {'loss': 59.0169563293457, 'l1_loss': 228.65370178222656, 'l2_loss': 49.615516662597656, 'ortho_loss': 0.2552911341190338}
Step 4600: {'loss': 58.84015655517578, 'l1_loss': 228.18435668945312, 'l2_loss': 49.46627426147461, 'ortho_loss': 0.2465105950832367}
Step 4700: {'loss': 58.268951416015625, 'l1_loss': 233.023193359375, 'l2_loss': 48.70163345336914, 'ortho_loss': 0.24639056622982025}
Step 4800: {'loss': 58.797855377197266, 'l1_loss': 232.15310668945312, 'l2_loss': 49.266319274902344, 'ortho_loss': 0.24541234970092773}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_1/final_info.json
Step 0: {'loss': 462.7072448730469, 'l1_loss': 4426.9755859375, 'l2_loss': 284.86981201171875, 'ortho_loss': 0.758405327796936}
Step 100: {'loss': 319.20721435546875, 'l1_loss': 3444.379150390625, 'l2_loss': 180.67694091796875, 'ortho_loss': 0.7551376223564148}
Step 200: {'loss': 240.1405029296875, 'l1_loss': 1937.900146484375, 'l2_loss': 161.84161376953125, 'ortho_loss': 0.7828764319419861}
Step 300: {'loss': 190.2047119140625, 'l1_loss': 959.376220703125, 'l2_loss': 151.034912109375, 'ortho_loss': 0.7947499752044678}
Step 400: {'loss': 169.7200927734375, 'l1_loss': 577.2755737304688, 'l2_loss': 145.83090209960938, 'ortho_loss': 0.7981733679771423}
Step 500: {'loss': 158.17337036132812, 'l1_loss': 588.0587158203125, 'l2_loss': 133.85195922851562, 'ortho_loss': 0.7990536689758301}
Step 600: {'loss': 144.46795654296875, 'l1_loss': 597.8516845703125, 'l2_loss': 119.75477600097656, 'ortho_loss': 0.7991233468055725}
Step 700: {'loss': 132.8950653076172, 'l1_loss': 561.258056640625, 'l2_loss': 109.6454849243164, 'ortho_loss': 0.7992592453956604}
Step 800: {'loss': 125.7909927368164, 'l1_loss': 531.0341186523438, 'l2_loss': 103.75065612792969, 'ortho_loss': 0.7989698052406311}
Step 900: {'loss': 119.29165649414062, 'l1_loss': 440.9625244140625, 'l2_loss': 100.85861206054688, 'ortho_loss': 0.7945417761802673}
Step 1000: {'loss': 118.52045440673828, 'l1_loss': 460.5911560058594, 'l2_loss': 99.32765197753906, 'ortho_loss': 0.7691596746444702}
Step 1100: {'loss': 114.08940124511719, 'l1_loss': 393.3370361328125, 'l2_loss': 97.59770202636719, 'ortho_loss': 0.7582182288169861}
Step 1200: {'loss': 110.50618743896484, 'l1_loss': 334.37701416015625, 'l2_loss': 96.49986267089844, 'ortho_loss': 0.6312512159347534}
Step 1300: {'loss': 110.29706573486328, 'l1_loss': 357.1954345703125, 'l2_loss': 95.45004272460938, 'ortho_loss': 0.5592058300971985}
Step 1400: {'loss': 109.81890106201172, 'l1_loss': 368.23809814453125, 'l2_loss': 94.56819915771484, 'ortho_loss': 0.52117919921875}
Step 1500: {'loss': 108.60916900634766, 'l1_loss': 344.5856628417969, 'l2_loss': 94.36199188232422, 'ortho_loss': 0.4637503921985626}
Step 1600: {'loss': 107.30458068847656, 'l1_loss': 349.3767395019531, 'l2_loss': 92.86949920654297, 'ortho_loss': 0.4600130021572113}
Step 1700: {'loss': 106.11089324951172, 'l1_loss': 334.12847900390625, 'l2_loss': 92.32496643066406, 'ortho_loss': 0.42078542709350586}
Step 1800: {'loss': 105.60357666015625, 'l1_loss': 351.6170959472656, 'l2_loss': 91.10089111328125, 'ortho_loss': 0.4380030035972595}
Step 1900: {'loss': 104.11768341064453, 'l1_loss': 336.0389709472656, 'l2_loss': 90.27110290527344, 'ortho_loss': 0.40502193570137024}
Step 2000: {'loss': 103.89266967773438, 'l1_loss': 350.1446228027344, 'l2_loss': 89.47992706298828, 'ortho_loss': 0.406958669424057}
Step 2100: {'loss': 103.27021789550781, 'l1_loss': 338.9045715332031, 'l2_loss': 89.34784698486328, 'ortho_loss': 0.3661878705024719}
Step 2200: {'loss': 100.69024658203125, 'l1_loss': 319.8079528808594, 'l2_loss': 87.51570129394531, 'ortho_loss': 0.3822264075279236}
Step 2300: {'loss': 100.64227294921875, 'l1_loss': 344.7794189453125, 'l2_loss': 86.4794921875, 'ortho_loss': 0.37160325050354004}
Step 2400: {'loss': 100.20413208007812, 'l1_loss': 349.81011962890625, 'l2_loss': 85.86914825439453, 'ortho_loss': 0.34257277846336365}
Step 2500: {'loss': 99.19657897949219, 'l1_loss': 351.5724792480469, 'l2_loss': 84.77665710449219, 'ortho_loss': 0.35702231526374817}
Step 2600: {'loss': 99.91445922851562, 'l1_loss': 370.7152404785156, 'l2_loss': 84.75193786621094, 'ortho_loss': 0.33390775322914124}
Step 2700: {'loss': 99.45101928710938, 'l1_loss': 363.709716796875, 'l2_loss': 84.56878662109375, 'ortho_loss': 0.3338507115840912}
Step 2800: {'loss': 98.40633392333984, 'l1_loss': 348.2452697753906, 'l2_loss': 84.15122985839844, 'ortho_loss': 0.3252921998500824}
Step 2900: {'loss': 97.13224029541016, 'l1_loss': 339.866943359375, 'l2_loss': 83.19744873046875, 'ortho_loss': 0.3401123285293579}
Step 3000: {'loss': 96.73082733154297, 'l1_loss': 362.612548828125, 'l2_loss': 81.86106872558594, 'ortho_loss': 0.36525797843933105}
Step 3100: {'loss': 96.26571655273438, 'l1_loss': 359.28533935546875, 'l2_loss': 81.5810775756836, 'ortho_loss': 0.31322330236434937}
Step 3200: {'loss': 96.10749816894531, 'l1_loss': 351.31280517578125, 'l2_loss': 81.74495697021484, 'ortho_loss': 0.31002724170684814}
Step 3300: {'loss': 95.92357635498047, 'l1_loss': 349.82427978515625, 'l2_loss': 81.62773132324219, 'ortho_loss': 0.3028739392757416}
Step 3400: {'loss': 95.35699462890625, 'l1_loss': 366.842041015625, 'l2_loss': 80.37063598632812, 'ortho_loss': 0.3126734793186188}
Step 3500: {'loss': 94.21699523925781, 'l1_loss': 351.43572998046875, 'l2_loss': 79.85023498535156, 'ortho_loss': 0.3093319535255432}
Step 3600: {'loss': 95.21417999267578, 'l1_loss': 364.72723388671875, 'l2_loss': 80.3173828125, 'ortho_loss': 0.3077116012573242}
Step 3700: {'loss': 95.17977142333984, 'l1_loss': 362.3414306640625, 'l2_loss': 80.37570190429688, 'ortho_loss': 0.3104100823402405}
Step 3800: {'loss': 94.26953887939453, 'l1_loss': 364.929931640625, 'l2_loss': 79.36116790771484, 'ortho_loss': 0.31117141246795654}
Step 3900: {'loss': 93.54973602294922, 'l1_loss': 359.8804931640625, 'l2_loss': 78.85118865966797, 'ortho_loss': 0.30333083868026733}
Step 4000: {'loss': 93.74408721923828, 'l1_loss': 355.06719970703125, 'l2_loss': 79.21023559570312, 'ortho_loss': 0.3311595320701599}
Step 4100: {'loss': 92.83509826660156, 'l1_loss': 357.819091796875, 'l2_loss': 78.2265625, 'ortho_loss': 0.2957741916179657}
Step 4200: {'loss': 93.17005920410156, 'l1_loss': 370.80267333984375, 'l2_loss': 78.02925109863281, 'ortho_loss': 0.3087017238140106}
Step 4300: {'loss': 93.0458984375, 'l1_loss': 367.0533752441406, 'l2_loss': 78.05776977539062, 'ortho_loss': 0.30599525570869446}
Step 4400: {'loss': 92.16385650634766, 'l1_loss': 364.3966369628906, 'l2_loss': 77.28790283203125, 'ortho_loss': 0.3000861704349518}
Step 4500: {'loss': 92.34426879882812, 'l1_loss': 367.2515869140625, 'l2_loss': 77.36095428466797, 'ortho_loss': 0.29325419664382935}
Step 4600: {'loss': 92.29288482666016, 'l1_loss': 369.1062927246094, 'l2_loss': 77.23007202148438, 'ortho_loss': 0.2985633611679077}
Step 4700: {'loss': 91.92023468017578, 'l1_loss': 379.8388671875, 'l2_loss': 76.42269897460938, 'ortho_loss': 0.3039754331111908}
Step 4800: {'loss': 92.33782958984375, 'l1_loss': 372.23284912109375, 'l2_loss': 77.14505004882812, 'ortho_loss': 0.3034629821777344}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_1/final_info.json
Step 0: {'loss': 913.9003295898438, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625, 'ortho_loss': 0.7181362509727478}
Step 100: {'loss': 606.2744140625, 'l1_loss': 6244.35302734375, 'l2_loss': 355.7784118652344, 'ortho_loss': 0.721951425075531}
Step 200: {'loss': 429.8772277832031, 'l1_loss': 2760.77587890625, 'l2_loss': 318.66717529296875, 'ortho_loss': 0.7790110111236572}
Step 300: {'loss': 359.25506591796875, 'l1_loss': 1215.543212890625, 'l2_loss': 309.838623046875, 'ortho_loss': 0.7947068810462952}
Step 400: {'loss': 334.33837890625, 'l1_loss': 803.8046875, 'l2_loss': 301.38800048828125, 'ortho_loss': 0.7981820106506348}
Step 500: {'loss': 307.9283752441406, 'l1_loss': 876.5142211914062, 'l2_loss': 272.06854248046875, 'ortho_loss': 0.7992411255836487}
Step 600: {'loss': 285.9277038574219, 'l1_loss': 905.4862670898438, 'l2_loss': 248.90863037109375, 'ortho_loss': 0.7996172904968262}
Step 700: {'loss': 266.0455017089844, 'l1_loss': 892.1538696289062, 'l2_loss': 229.5595245361328, 'ortho_loss': 0.7998424768447876}
Step 800: {'loss': 257.4430847167969, 'l1_loss': 874.74609375, 'l2_loss': 221.65333557128906, 'ortho_loss': 0.7998997569084167}
Step 900: {'loss': 247.05767822265625, 'l1_loss': 712.7680053710938, 'l2_loss': 217.74710083007812, 'ortho_loss': 0.7998705506324768}
Step 1000: {'loss': 245.38645935058594, 'l1_loss': 746.804443359375, 'l2_loss': 214.7144317626953, 'ortho_loss': 0.7998483777046204}
Step 1100: {'loss': 237.83021545410156, 'l1_loss': 616.9058227539062, 'l2_loss': 212.35653686523438, 'ortho_loss': 0.7974342107772827}
Step 1200: {'loss': 230.69410705566406, 'l1_loss': 511.95159912109375, 'l2_loss': 209.43605041503906, 'ortho_loss': 0.7799995541572571}
Step 1300: {'loss': 232.14901733398438, 'l1_loss': 554.6353759765625, 'l2_loss': 209.20529174804688, 'ortho_loss': 0.7583125233650208}
Step 1400: {'loss': 228.5359344482422, 'l1_loss': 562.13720703125, 'l2_loss': 205.28585815429688, 'ortho_loss': 0.7645869255065918}
Step 1500: {'loss': 226.58323669433594, 'l1_loss': 514.789306640625, 'l2_loss': 205.2723388671875, 'ortho_loss': 0.7193269729614258}
Step 1600: {'loss': 225.38609313964844, 'l1_loss': 518.54736328125, 'l2_loss': 203.94036865234375, 'ortho_loss': 0.7038211822509766}
Step 1700: {'loss': 222.48348999023438, 'l1_loss': 501.768798828125, 'l2_loss': 201.68666076660156, 'ortho_loss': 0.7260723114013672}
Step 1800: {'loss': 221.39125061035156, 'l1_loss': 536.7637329101562, 'l2_loss': 199.26429748535156, 'ortho_loss': 0.6564059853553772}
Step 1900: {'loss': 219.18325805664062, 'l1_loss': 526.0667724609375, 'l2_loss': 197.5225067138672, 'ortho_loss': 0.618071973323822}
Step 2000: {'loss': 217.1676788330078, 'l1_loss': 534.6219482421875, 'l2_loss': 195.20654296875, 'ortho_loss': 0.576268196105957}
Step 2100: {'loss': 216.2887725830078, 'l1_loss': 526.5609130859375, 'l2_loss': 194.67916870117188, 'ortho_loss': 0.5471617579460144}
Step 2200: {'loss': 210.53733825683594, 'l1_loss': 498.8710632324219, 'l2_loss': 190.04701232910156, 'ortho_loss': 0.5354835987091064}
Step 2300: {'loss': 209.9883575439453, 'l1_loss': 566.8091430664062, 'l2_loss': 186.68588256835938, 'ortho_loss': 0.6301079988479614}
Step 2400: {'loss': 208.48678588867188, 'l1_loss': 572.0482788085938, 'l2_loss': 185.1320037841797, 'ortho_loss': 0.4728477895259857}
Step 2500: {'loss': 205.55166625976562, 'l1_loss': 591.0704956054688, 'l2_loss': 181.41995239257812, 'ortho_loss': 0.4888926148414612}
Step 2600: {'loss': 204.83619689941406, 'l1_loss': 633.2906494140625, 'l2_loss': 178.97991943359375, 'ortho_loss': 0.5246617794036865}
Step 2700: {'loss': 202.84596252441406, 'l1_loss': 614.8842163085938, 'l2_loss': 177.82093811035156, 'ortho_loss': 0.42966264486312866}
Step 2800: {'loss': 201.28030395507812, 'l1_loss': 599.3529663085938, 'l2_loss': 176.8791046142578, 'ortho_loss': 0.4270734488964081}
Step 2900: {'loss': 197.946533203125, 'l1_loss': 590.5601806640625, 'l2_loss': 173.91250610351562, 'ortho_loss': 0.41162729263305664}
Step 3000: {'loss': 199.18606567382812, 'l1_loss': 639.9248046875, 'l2_loss': 173.16619873046875, 'ortho_loss': 0.42288169264793396}
Step 3100: {'loss': 195.97055053710938, 'l1_loss': 637.6146240234375, 'l2_loss': 170.0543212890625, 'ortho_loss': 0.41165411472320557}
Step 3200: {'loss': 195.51531982421875, 'l1_loss': 621.7930908203125, 'l2_loss': 170.20787048339844, 'ortho_loss': 0.43573644757270813}
Step 3300: {'loss': 193.9347686767578, 'l1_loss': 629.6416015625, 'l2_loss': 168.3444366455078, 'ortho_loss': 0.4046589732170105}
Step 3400: {'loss': 194.60704040527344, 'l1_loss': 662.2667846679688, 'l2_loss': 167.7530517578125, 'ortho_loss': 0.36331748962402344}
Step 3500: {'loss': 190.3419189453125, 'l1_loss': 629.033447265625, 'l2_loss': 164.85031127929688, 'ortho_loss': 0.33027026057243347}
Step 3600: {'loss': 192.7165069580078, 'l1_loss': 662.4110107421875, 'l2_loss': 165.859130859375, 'ortho_loss': 0.3609384596347809}
Step 3700: {'loss': 191.8336639404297, 'l1_loss': 651.802490234375, 'l2_loss': 165.4512939453125, 'ortho_loss': 0.3102756142616272}
Step 3800: {'loss': 189.40374755859375, 'l1_loss': 663.472412109375, 'l2_loss': 162.5485382080078, 'ortho_loss': 0.31631243228912354}
Step 3900: {'loss': 189.83775329589844, 'l1_loss': 660.400390625, 'l2_loss': 163.08294677734375, 'ortho_loss': 0.3387959897518158}
Step 4000: {'loss': 189.14781188964844, 'l1_loss': 645.6070556640625, 'l2_loss': 163.020751953125, 'ortho_loss': 0.3027745485305786}
Step 4100: {'loss': 188.51934814453125, 'l1_loss': 649.9503173828125, 'l2_loss': 162.27325439453125, 'ortho_loss': 0.24807068705558777}
Step 4200: {'loss': 187.88180541992188, 'l1_loss': 674.3440551757812, 'l2_loss': 160.67431640625, 'ortho_loss': 0.233733132481575}
Step 4300: {'loss': 188.22781372070312, 'l1_loss': 672.0238647460938, 'l2_loss': 161.09735107421875, 'ortho_loss': 0.24950656294822693}
Step 4400: {'loss': 185.8184051513672, 'l1_loss': 661.1163330078125, 'l2_loss': 159.13079833984375, 'ortho_loss': 0.2429528683423996}
Step 4500: {'loss': 186.24737548828125, 'l1_loss': 673.9771728515625, 'l2_loss': 159.06756591796875, 'ortho_loss': 0.22072426974773407}
Step 4600: {'loss': 185.91046142578125, 'l1_loss': 673.7059326171875, 'l2_loss': 158.76068115234375, 'ortho_loss': 0.2015417218208313}
Step 4700: {'loss': 185.30812072753906, 'l1_loss': 690.0333251953125, 'l2_loss': 157.5059051513672, 'ortho_loss': 0.20087896287441254}
Step 4800: {'loss': 185.2805633544922, 'l1_loss': 682.0189819335938, 'l2_loss': 157.81552124023438, 'ortho_loss': 0.18428540229797363}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_1/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_5_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9420000314712524, scr_score: 0.20689605560910843
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9330000281333923, scr_score: -0.10344905547140106
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9390000700950623, scr_score: 0.10344905547140106
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.9340000152587891, scr_score: -0.06896672209216527
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.8840000629425049, scr_score: -1.7931039443908916
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.8670000433921814, scr_score: -2.379311833172675
dir: 1, original_acc: 0.9360000491142273, clean_acc: 0.9650000333786011, changed_acc: 0.8500000238418579, scr_score: -2.965519721954458
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5640000104904175, scr_score: 0.023474156069585764
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5720000267028809, scr_score: 0.04225353689205711
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5840000510215759, scr_score: 0.07042260812576412
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5870000123977661, scr_score: 0.07746477099643576
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.5960000157356262, scr_score: 0.09859153944246431
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.6200000047683716, scr_score: 0.1549295419928715
dir: 2, original_acc: 0.5540000200271606, clean_acc: 0.9800000190734863, changed_acc: 0.6230000257492065, scr_score: 0.16197184478054996
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8950000405311584, scr_score: 0.16417883891882468
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8940000534057617, scr_score: 0.1492536516756474
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8910000324249268, scr_score: 0.10447720032417823
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8870000243186951, scr_score: 0.0447755617295318
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8860000371932983, scr_score: 0.029850374486354535
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8800000548362732, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8840000629425049, clean_acc: 0.9510000348091125, changed_acc: 0.8550000190734863, scr_score: -0.4328366574057023
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6050000190734863, scr_score: 0.02849735292051195
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6010000109672546, scr_score: 0.018134637017729296
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6040000319480896, scr_score: 0.02590671254886307
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6150000095367432, scr_score: 0.054404065469375026
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6080000400543213, scr_score: 0.03626942845164573
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6100000143051147, scr_score: 0.04145070919494349
dir: 2, original_acc: 0.5940000414848328, clean_acc: 0.9800000190734863, changed_acc: 0.6160000562667847, scr_score: 0.056994860257211045
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9290000200271606, scr_score: 0.1162788441146016
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9300000667572021, scr_score: 0.1395357218600512
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9270000457763672, scr_score: 0.0697678609300256
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9150000214576721, scr_score: -0.2093021966369152
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9180000424385071, scr_score: -0.13953433570688958
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9270000457763672, scr_score: 0.0697678609300256
dir: 1, original_acc: 0.9240000247955322, clean_acc: 0.9670000672340393, changed_acc: 0.9160000681877136, scr_score: -0.18604531889146558
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5710000395774841, scr_score: 0.012722633961057692
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5770000219345093, scr_score: 0.02798976438117488
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5870000123977661, scr_score: 0.05343503230329027
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5860000252723694, scr_score: 0.05089053584423077
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5890000462532043, scr_score: 0.058524176887169474
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5879999995231628, scr_score: 0.05597952876234976
dir: 2, original_acc: 0.5660000443458557, clean_acc: 0.9590000510215759, changed_acc: 0.5950000286102295, scr_score: 0.07379130730728665
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8890000581741333, scr_score: 0.14814839343466119
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.89000004529953, scr_score: 0.16049390892266485
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8880000710487366, scr_score: 0.13580287794665755
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.89000004529953, scr_score: 0.16049390892266485
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8980000615119934, scr_score: 0.2592595045457723
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.9020000696182251, scr_score: 0.30864230235732604
dir: 1, original_acc: 0.8770000338554382, clean_acc: 0.9580000638961792, changed_acc: 0.8850000500679016, scr_score: 0.09876559562310747
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6150000095367432, scr_score: 0.0
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6140000224113464, scr_score: -0.002688137154759759
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6370000243186951, scr_score: 0.05913981854233011
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6360000371932983, scr_score: 0.05645168138757035
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6440000534057617, scr_score: 0.07795709908069459
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.6230000257492065, scr_score: 0.021505417693124237
dir: 2, original_acc: 0.6150000095367432, clean_acc: 0.987000048160553, changed_acc: 0.609000027179718, scr_score: -0.016128983156081637
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8400000333786011, scr_score: 0.12571436161897104
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8420000672340393, scr_score: 0.13714312572866674
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8670000433921814, scr_score: 0.2800001226152609
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8400000333786011, scr_score: 0.12571436161897104
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8380000591278076, scr_score: 0.11428593810722228
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8130000233650208, scr_score: -0.028571399377318833
dir: 1, original_acc: 0.8180000185966492, clean_acc: 0.9930000305175781, changed_acc: 0.8360000252723694, scr_score: 0.10285717399752657
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.6990000605583191, scr_score: 0.14155260586629506
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7080000042915344, scr_score: 0.18264824275150304
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7130000591278076, scr_score: 0.2054795452627912
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7230000495910645, scr_score: 0.2511416059506669
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7300000190734863, scr_score: 0.28310493956523985
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.7120000123977661, scr_score: 0.2009131214601234
dir: 2, original_acc: 0.6680000424385071, clean_acc: 0.8870000243186951, changed_acc: 0.6820000410079956, scr_score: 0.06392693939649612
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8510000109672546, scr_score: 0.16923043064815926
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8650000691413879, scr_score: 0.2769231897839469
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8650000691413879, scr_score: 0.2769231897839469
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8570000529289246, scr_score: 0.21538460127700665
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8570000529289246, scr_score: 0.21538460127700665
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.859000027179718, scr_score: 0.23076901915509954
dir: 1, original_acc: 0.8290000557899475, clean_acc: 0.9590000510215759, changed_acc: 0.8630000352859497, scr_score: 0.2615383134085697
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6650000214576721, scr_score: 0.04858285722007774
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6720000505447388, scr_score: 0.07692305836043294
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6760000586509705, scr_score: 0.0931174245385828
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6720000505447388, scr_score: 0.07692305836043294
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6790000200271606, scr_score: 0.10526301818641631
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6710000038146973, scr_score: 0.07287428583011661
dir: 2, original_acc: 0.6530000567436218, clean_acc: 0.9000000357627869, changed_acc: 0.6340000033378601, scr_score: -0.07692329967480475
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8140000104904175, scr_score: 0.10795447425842195
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8340000510215759, scr_score: 0.22159103031674102
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.831000030040741, scr_score: 0.20454546224233278
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8440000414848328, scr_score: 0.27840913901457975
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8370000123977661, scr_score: 0.23863625972850772
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.8460000157356262, scr_score: 0.2897726252890909
dir: 1, original_acc: 0.7950000166893005, clean_acc: 0.971000075340271, changed_acc: 0.76500004529953, scr_score: -0.17045432609351627
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7040000557899475, scr_score: 0.04700870049408169
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7070000171661377, scr_score: 0.059829046766461134
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7160000205039978, scr_score: 0.09829059502494862
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7200000286102295, scr_score: 0.11538464477546245
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7350000143051147, scr_score: 0.1794871402993834
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7260000109672546, scr_score: 0.1410255920408959
dir: 2, original_acc: 0.6930000185966492, clean_acc: 0.9270000457763672, changed_acc: 0.7070000171661377, scr_score: 0.059829046766461134
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7510000467300415, scr_score: 0.025906708548447916
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7510000467300415, scr_score: 0.025906708548447916
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7630000114440918, scr_score: 0.08808268553179228
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7670000195503235, scr_score: 0.10880811413701594
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7740000486373901, scr_score: 0.14507769140423898
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7700000405311584, scr_score: 0.12435226279901532
dir: 1, original_acc: 0.7460000514984131, clean_acc: 0.9390000700950623, changed_acc: 0.7540000081062317, scr_score: 0.04145054837812071
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.737000048160553, scr_score: 0.02499997764825688
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7390000224113464, scr_score: 0.03499984949826297
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7400000095367432, scr_score: 0.039999785423266015
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7360000610351562, scr_score: 0.020000041723253828
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7350000143051147, scr_score: 0.014999807775009142
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7460000514984131, scr_score: 0.06999999701976758
dir: 2, original_acc: 0.7320000529289246, clean_acc: 0.9320000410079956, changed_acc: 0.7270000576972961, scr_score: -0.02499997764825688

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 0: 0.9130000472068787
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 1: 0.9350000619888306
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 2: 0.9240000247955322
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 6: 0.9780000448226929
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 9: 0.9600000381469727
Num non-zero elements: 1
Test accuracy for 0: 0.73
Num non-zero elements: 1
Test accuracy for 1: 0.597
Num non-zero elements: 1
Test accuracy for 2: 0.663
Num non-zero elements: 1
Test accuracy for 6: 0.77
Num non-zero elements: 1
Test accuracy for 9: 0.63
Num non-zero elements: 2
Test accuracy for 0: 0.731
Num non-zero elements: 2
Test accuracy for 1: 0.704
Num non-zero elements: 2
Test accuracy for 2: 0.668
Num non-zero elements: 2
Test accuracy for 6: 0.781
Num non-zero elements: 2
Test accuracy for 9: 0.934
Num non-zero elements: 5
Test accuracy for 0: 0.768
Num non-zero elements: 5
Test accuracy for 1: 0.825
Num non-zero elements: 5
Test accuracy for 2: 0.824
Num non-zero elements: 5
Test accuracy for 6: 0.799
Num non-zero elements: 5
Test accuracy for 9: 0.942
Num non-zero elements: 10
Test accuracy for 0: 0.798
Num non-zero elements: 10
Test accuracy for 1: 0.83
Num non-zero elements: 10
Test accuracy for 2: 0.908
Num non-zero elements: 10
Test accuracy for 6: 0.965
Num non-zero elements: 10
Test accuracy for 9: 0.94
Num non-zero elements: 20
Test accuracy for 0: 0.829
Num non-zero elements: 20
Test accuracy for 1: 0.857
Num non-zero elements: 20
Test accuracy for 2: 0.927
Num non-zero elements: 20
Test accuracy for 6: 0.978
Num non-zero elements: 20
Test accuracy for 9: 0.95
Num non-zero elements: 50
Test accuracy for 0: 0.868
Num non-zero elements: 50
Test accuracy for 1: 0.925
Num non-zero elements: 50
Test accuracy for 2: 0.925
Num non-zero elements: 50
Test accuracy for 6: 0.986
Num non-zero elements: 50
Test accuracy for 9: 0.947
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 11: 0.9240000247955322
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 13: 0.9440000653266907
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 14: 0.9190000295639038
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for 18: 0.8880000710487366
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 19: 0.9450000524520874
Num non-zero elements: 1
Test accuracy for 11: 0.52
Num non-zero elements: 1
Test accuracy for 13: 0.7
Num non-zero elements: 1
Test accuracy for 14: 0.614
Num non-zero elements: 1
Test accuracy for 18: 0.698
Num non-zero elements: 1
Test accuracy for 19: 0.749
Num non-zero elements: 2
Test accuracy for 11: 0.75
Num non-zero elements: 2
Test accuracy for 13: 0.754
Num non-zero elements: 2
Test accuracy for 14: 0.673
Num non-zero elements: 2
Test accuracy for 18: 0.734
Num non-zero elements: 2
Test accuracy for 19: 0.771
Num non-zero elements: 5
Test accuracy for 11: 0.786
Num non-zero elements: 5
Test accuracy for 13: 0.75
Num non-zero elements: 5
Test accuracy for 14: 0.671
Num non-zero elements: 5
Test accuracy for 18: 0.726
Num non-zero elements: 5
Test accuracy for 19: 0.782
Num non-zero elements: 10
Test accuracy for 11: 0.826
Num non-zero elements: 10
Test accuracy for 13: 0.854
Num non-zero elements: 10
Test accuracy for 14: 0.79
Num non-zero elements: 10
Test accuracy for 18: 0.754
Num non-zero elements: 10
Test accuracy for 19: 0.874
Num non-zero elements: 20
Test accuracy for 11: 0.873
Num non-zero elements: 20
Test accuracy for 13: 0.879
Num non-zero elements: 20
Test accuracy for 14: 0.846
Num non-zero elements: 20
Test accuracy for 18: 0.76
Num non-zero elements: 20
Test accuracy for 19: 0.924
Num non-zero elements: 50
Test accuracy for 11: 0.929
Num non-zero elements: 50
Test accuracy for 13: 0.898
Num non-zero elements: 50
Test accuracy for 14: 0.912
Num non-zero elements: 50
Test accuracy for 18: 0.88
Num non-zero elements: 50
Test accuracy for 19: 0.928
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 20: 0.9310000538825989
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 21: 0.8970000147819519
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 22: 0.8860000371932983
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 25: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 26: 0.8510000109672546
Num non-zero elements: 1
Test accuracy for 20: 0.596
Num non-zero elements: 1
Test accuracy for 21: 0.738
Num non-zero elements: 1
Test accuracy for 22: 0.766
Num non-zero elements: 1
Test accuracy for 25: 0.669
Num non-zero elements: 1
Test accuracy for 26: 0.628
Num non-zero elements: 2
Test accuracy for 20: 0.651
Num non-zero elements: 2
Test accuracy for 21: 0.749
Num non-zero elements: 2
Test accuracy for 22: 0.79
Num non-zero elements: 2
Test accuracy for 25: 0.678
Num non-zero elements: 2
Test accuracy for 26: 0.631
Num non-zero elements: 5
Test accuracy for 20: 0.873
Num non-zero elements: 5
Test accuracy for 21: 0.816
Num non-zero elements: 5
Test accuracy for 22: 0.822
Num non-zero elements: 5
Test accuracy for 25: 0.795
Num non-zero elements: 5
Test accuracy for 26: 0.715
Num non-zero elements: 10
Test accuracy for 20: 0.877
Num non-zero elements: 10
Test accuracy for 21: 0.835
Num non-zero elements: 10
Test accuracy for 22: 0.829
Num non-zero elements: 10
Test accuracy for 25: 0.864
Num non-zero elements: 10
Test accuracy for 26: 0.745
Num non-zero elements: 20
Test accuracy for 20: 0.9
Num non-zero elements: 20
Test accuracy for 21: 0.832
Num non-zero elements: 20
Test accuracy for 22: 0.833
Num non-zero elements: 20
Test accuracy for 25: 0.915
Num non-zero elements: 20
Test accuracy for 26: 0.772
Num non-zero elements: 50
Test accuracy for 20: 0.925
Num non-zero elements: 50
Test accuracy for 21: 0.86
Num non-zero elements: 50
Test accuracy for 22: 0.85
Num non-zero elements: 50
Test accuracy for 25: 0.947
Num non-zero elements: 50
Test accuracy for 26: 0.825
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 1: 0.9050000309944153
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 42 epochs
Test accuracy for 2: 0.8750000596046448
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 3: 0.8660000562667847
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for 5: 0.8340000510215759
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for 6: 0.7980000376701355
Num non-zero elements: 1
Test accuracy for 1: 0.803
Num non-zero elements: 1
Test accuracy for 2: 0.782
Num non-zero elements: 1
Test accuracy for 3: 0.559
Num non-zero elements: 1
Test accuracy for 5: 0.837
Num non-zero elements: 1
Test accuracy for 6: 0.57
Num non-zero elements: 2
Test accuracy for 1: 0.795
Num non-zero elements: 2
Test accuracy for 2: 0.795
Num non-zero elements: 2
Test accuracy for 3: 0.59
Num non-zero elements: 2
Test accuracy for 5: 0.847
Num non-zero elements: 2
Test accuracy for 6: 0.621
Num non-zero elements: 5
Test accuracy for 1: 0.816
Num non-zero elements: 5
Test accuracy for 2: 0.83
Num non-zero elements: 5
Test accuracy for 3: 0.729
Num non-zero elements: 5
Test accuracy for 5: 0.842
Num non-zero elements: 5
Test accuracy for 6: 0.699
Num non-zero elements: 10
Test accuracy for 1: 0.84
Num non-zero elements: 10
Test accuracy for 2: 0.839
Num non-zero elements: 10
Test accuracy for 3: 0.786
Num non-zero elements: 10
Test accuracy for 5: 0.859
Num non-zero elements: 10
Test accuracy for 6: 0.748
Num non-zero elements: 20
Test accuracy for 1: 0.84
Num non-zero elements: 20
Test accuracy for 2: 0.857
Num non-zero elements: 20
Test accuracy for 3: 0.829
Num non-zero elements: 20
Test accuracy for 5: 0.856
Num non-zero elements: 20
Test accuracy for 6: 0.771
Num non-zero elements: 50
Test accuracy for 1: 0.87
Num non-zero elements: 50
Test accuracy for 2: 0.873
Num non-zero elements: 50
Test accuracy for 3: 0.861
Num non-zero elements: 50
Test accuracy for 5: 0.86
Num non-zero elements: 50
Test accuracy for 6: 0.775
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 1.0: 0.8830000162124634
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 5.0: 0.8850000500679016
Num non-zero elements: 1
Test accuracy for 1.0: 0.721
Num non-zero elements: 1
Test accuracy for 5.0: 0.721
Num non-zero elements: 2
Test accuracy for 1.0: 0.73
Num non-zero elements: 2
Test accuracy for 5.0: 0.73
Num non-zero elements: 5
Test accuracy for 1.0: 0.843
Num non-zero elements: 5
Test accuracy for 5.0: 0.843
Num non-zero elements: 10
Test accuracy for 1.0: 0.852
Num non-zero elements: 10
Test accuracy for 5.0: 0.852
Num non-zero elements: 20
Test accuracy for 1.0: 0.855
Num non-zero elements: 20
Test accuracy for 5.0: 0.855
Num non-zero elements: 50
Test accuracy for 1.0: 0.868
Num non-zero elements: 50
Test accuracy for 5.0: 0.868
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for C: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 36 epochs
Test accuracy for Python: 0.9740000367164612
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for HTML: 0.9800000190734863
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for Java: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for PHP: 0.9310000538825989
Num non-zero elements: 1
Test accuracy for C: 0.573
Num non-zero elements: 1
Test accuracy for Python: 0.605
Num non-zero elements: 1
Test accuracy for HTML: 0.938
Num non-zero elements: 1
Test accuracy for Java: 0.657
Num non-zero elements: 1
Test accuracy for PHP: 0.619
Num non-zero elements: 2
Test accuracy for C: 0.66
Num non-zero elements: 2
Test accuracy for Python: 0.622
Num non-zero elements: 2
Test accuracy for HTML: 0.951
Num non-zero elements: 2
Test accuracy for Java: 0.642
Num non-zero elements: 2
Test accuracy for PHP: 0.609
Num non-zero elements: 5
Test accuracy for C: 0.778
Num non-zero elements: 5
Test accuracy for Python: 0.717
Num non-zero elements: 5
Test accuracy for HTML: 0.956
Num non-zero elements: 5
Test accuracy for Java: 0.778
Num non-zero elements: 5
Test accuracy for PHP: 0.729
Num non-zero elements: 10
Test accuracy for C: 0.901
Num non-zero elements: 10
Test accuracy for Python: 0.819
Num non-zero elements: 10
Test accuracy for HTML: 0.956
Num non-zero elements: 10
Test accuracy for Java: 0.823
Num non-zero elements: 10
Test accuracy for PHP: 0.817
Num non-zero elements: 20
Test accuracy for C: 0.903
Num non-zero elements: 20
Test accuracy for Python: 0.883
Num non-zero elements: 20
Test accuracy for HTML: 0.962
Num non-zero elements: 20
Test accuracy for Java: 0.873
Num non-zero elements: 20
Test accuracy for PHP: 0.85
Num non-zero elements: 50
Test accuracy for C: 0.921
Num non-zero elements: 50
Test accuracy for Python: 0.951
Num non-zero elements: 50
Test accuracy for HTML: 0.964
Num non-zero elements: 50
Test accuracy for Java: 0.916
Num non-zero elements: 50
Test accuracy for PHP: 0.902
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 0: 0.9170000553131104
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 47 epochs
Test accuracy for 1: 0.9700000286102295
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 2: 0.9010000228881836
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 3: 0.9320000410079956
Num non-zero elements: 1
Test accuracy for 0: 0.762
Num non-zero elements: 1
Test accuracy for 1: 0.646
Num non-zero elements: 1
Test accuracy for 2: 0.847
Num non-zero elements: 1
Test accuracy for 3: 0.714
Num non-zero elements: 2
Test accuracy for 0: 0.79
Num non-zero elements: 2
Test accuracy for 1: 0.792
Num non-zero elements: 2
Test accuracy for 2: 0.84
Num non-zero elements: 2
Test accuracy for 3: 0.84
Num non-zero elements: 5
Test accuracy for 0: 0.856
Num non-zero elements: 5
Test accuracy for 1: 0.943
Num non-zero elements: 5
Test accuracy for 2: 0.854
Num non-zero elements: 5
Test accuracy for 3: 0.858
Num non-zero elements: 10
Test accuracy for 0: 0.887
Num non-zero elements: 10
Test accuracy for 1: 0.957
Num non-zero elements: 10
Test accuracy for 2: 0.879
Num non-zero elements: 10
Test accuracy for 3: 0.896
Num non-zero elements: 20
Test accuracy for 0: 0.897
Num non-zero elements: 20
Test accuracy for 1: 0.968
Num non-zero elements: 20
Test accuracy for 2: 0.862
Num non-zero elements: 20
Test accuracy for 3: 0.884
Num non-zero elements: 50
Test accuracy for 0: 0.915
Num non-zero elements: 50
Test accuracy for 1: 0.962
Num non-zero elements: 50
Test accuracy for 2: 0.886
Num non-zero elements: 50
Test accuracy for 3: 0.904
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.5.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for en: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 24 epochs
Test accuracy for fr: 0.9850000739097595
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for de: 0.9900000691413879
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for es: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for nl: 0.9880000352859497
Num non-zero elements: 1
Test accuracy for en: 0.685
Num non-zero elements: 1
Test accuracy for fr: 0.555
Num non-zero elements: 1
Test accuracy for de: 0.627
Num non-zero elements: 1
Test accuracy for es: 0.544
Num non-zero elements: 1
Test accuracy for nl: 0.676
Num non-zero elements: 2
Test accuracy for en: 0.921
Num non-zero elements: 2
Test accuracy for fr: 0.757
Num non-zero elements: 2
Test accuracy for de: 0.697
Num non-zero elements: 2
Test accuracy for es: 0.601
Num non-zero elements: 2
Test accuracy for nl: 0.804
Num non-zero elements: 5
Test accuracy for en: 0.942
Num non-zero elements: 5
Test accuracy for fr: 0.806
Num non-zero elements: 5
Test accuracy for de: 0.843
Num non-zero elements: 5
Test accuracy for es: 0.655
Num non-zero elements: 5
Test accuracy for nl: 0.81
Num non-zero elements: 10
Test accuracy for en: 0.955
Num non-zero elements: 10
Test accuracy for fr: 0.893
Num non-zero elements: 10
Test accuracy for de: 0.892
Num non-zero elements: 10
Test accuracy for es: 0.758
Num non-zero elements: 10
Test accuracy for nl: 0.855
Num non-zero elements: 20
Test accuracy for en: 0.973
Num non-zero elements: 20
Test accuracy for fr: 0.909
Num non-zero elements: 20
Test accuracy for de: 0.887
Num non-zero elements: 20
Test accuracy for es: 0.82
Num non-zero elements: 20
Test accuracy for nl: 0.886
Num non-zero elements: 50
Test accuracy for en: 0.984
Num non-zero elements: 50
Test accuracy for fr: 0.932
Num non-zero elements: 50
Test accuracy for de: 0.943
Num non-zero elements: 50
Test accuracy for es: 0.929
Num non-zero elements: 50
Test accuracy for nl: 0.944

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_12_sae_custom_sae/layer_12_google/gemma-2-2b_layer_12_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_12_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9110000729560852, scr_score: 0.0
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.909000039100647, scr_score: -0.03125055297320303
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9110000729560852, scr_score: 0.0
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.9110000729560852, scr_score: 0.0
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.8790000677108765, scr_score: -0.5000004656616447
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.8250000476837158, scr_score: -1.3437514260887868
dir: 1, original_acc: 0.9110000729560852, clean_acc: 0.9750000238418579, changed_acc: 0.8130000233650208, scr_score: -1.531251949958137
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.5860000252723694, scr_score: 0.007371057746493725
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6040000319480896, scr_score: 0.05159711132798542
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6490000486373901, scr_score: 0.16216224528171463
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6500000357627869, scr_score: 0.16461921571430077
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6580000519752502, scr_score: 0.1842752720724605
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6660000085830688, scr_score: 0.20393118198188487
dir: 2, original_acc: 0.5830000042915344, clean_acc: 0.9900000691413879, changed_acc: 0.6720000505447388, scr_score: 0.21867329747487232
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8520000576972961, scr_score: 0.0
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8520000576972961, scr_score: 0.0
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8530000448226929, scr_score: 0.009708614261252404
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8460000157356262, scr_score: -0.05825284293942373
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8560000658035278, scr_score: 0.03883503573096427
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8570000529289246, scr_score: 0.04854364999221667
dir: 1, original_acc: 0.8520000576972961, clean_acc: 0.9550000429153442, changed_acc: 0.8130000233650208, scr_score: -0.37864116436243567
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6530000567436218, scr_score: 0.037249318901607584
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.659000039100647, scr_score: 0.05444124655351917
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6650000214576721, scr_score: 0.07163317420543075
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6550000309944153, scr_score: 0.04297990452325845
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6630000472068787, scr_score: 0.06590258858377988
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6700000166893005, scr_score: 0.0859598090465169
dir: 2, original_acc: 0.64000004529953, clean_acc: 0.9890000224113464, changed_acc: 0.6790000200271606, scr_score: 0.11174778591786377
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9050000309944153, scr_score: -0.07936497424228806
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9100000262260437, scr_score: 0.0
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9050000309944153, scr_score: -0.07936497424228806
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.9030000567436218, scr_score: -0.1111105854971546
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.906000018119812, scr_score: -0.0634921686148548
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.8990000486373901, scr_score: -0.17460275411200937
dir: 1, original_acc: 0.9100000262260437, clean_acc: 0.9730000495910645, changed_acc: 0.8440000414848328, scr_score: -1.0476184168822997
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.5910000205039978, scr_score: 0.0075949890455728015
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6000000238418579, scr_score: 0.030379805284471813
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6040000319480896, scr_score: 0.04050640704596242
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.612000048160553, scr_score: 0.06075961056894363
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6200000047683716, scr_score: 0.08101266319410545
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6300000548362732, scr_score: 0.10632924304674166
dir: 2, original_acc: 0.5879999995231628, clean_acc: 0.9830000400543213, changed_acc: 0.6240000128746033, scr_score: 0.09113926495559606
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8160000443458557, scr_score: -0.19531228172130735
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8110000491142273, scr_score: -0.23437473806556883
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8090000152587891, scr_score: -0.25
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.843000054359436, scr_score: 0.015625261934431176
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.05468725261748167
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.7930000424385071, scr_score: -0.3749997671693945
dir: 1, original_acc: 0.8410000205039978, clean_acc: 0.9690000414848328, changed_acc: 0.800000011920929, scr_score: -0.32031251455191284
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6690000295639038, scr_score: 0.035608379457657886
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.690000057220459, scr_score: 0.09792299929146529
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.6920000314712524, scr_score: 0.10385764076688712
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7130000591278076, scr_score: 0.16617226060069454
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7090000510215759, scr_score: 0.15430280078147524
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.7140000462532043, scr_score: 0.16913958133840545
dir: 2, original_acc: 0.6570000052452087, clean_acc: 0.9940000176429749, changed_acc: 0.706000030040741, scr_score: 0.14540066169996685
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.843000054359436, scr_score: 0.19672135419126444
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.8530000448226929, scr_score: 0.2513661024203065
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.859000027179718, scr_score: 0.28415288621605345
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7550000548362732, scr_score: -0.28415288621605345
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7450000643730164, scr_score: -0.3387976344450955
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7500000596046448, scr_score: -0.3114752603305745
dir: 1, original_acc: 0.8070000410079956, clean_acc: 0.9900000691413879, changed_acc: 0.7600000500679016, scr_score: -0.2568305121015324
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7910000085830688, scr_score: 0.3999999065025381
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7240000367164612, scr_score: 0.13725497529212696
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7630000114440918, scr_score: 0.29019601976629844
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7750000357627869, scr_score: 0.337254928543396
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7870000600814819, scr_score: 0.3843138373204936
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7750000357627869, scr_score: 0.337254928543396
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9440000653266907, changed_acc: 0.7630000114440918, scr_score: 0.29019601976629844
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8210000395774841, scr_score: 0.1999998777340575
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.7870000600814819, scr_score: 0.025641002128344394
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.7980000376701355, scr_score: 0.08205114567773082
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8110000491142273, scr_score: 0.14871787347736873
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.8130000233650208, scr_score: 0.158974152062764
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.796000063419342, scr_score: 0.07179486709233554
dir: 1, original_acc: 0.7820000648498535, clean_acc: 0.9770000576972961, changed_acc: 0.7850000262260437, scr_score: 0.015384417878092908
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7440000176429749, scr_score: 0.11290320254761761
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7550000548362732, scr_score: 0.1572581866250075
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7600000500679016, scr_score: 0.17741945562699032
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7780000567436218, scr_score: 0.2500001201706423
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.753000020980835, scr_score: 0.1491935348194436
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7410000562667847, scr_score: 0.1008065853511987
dir: 2, original_acc: 0.7160000205039978, clean_acc: 0.9640000462532043, changed_acc: 0.7310000061988831, scr_score: 0.060483807005948444
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7940000295639038, scr_score: 0.17488778690082293
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7920000553131104, scr_score: 0.16591929209992518
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.8230000138282776, scr_score: 0.30493256522632795
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7980000376701355, scr_score: 0.19282504378803306
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7820000648498535, scr_score: 0.12107628352460723
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7800000309944153, scr_score: 0.11210752143829489
dir: 1, original_acc: 0.7550000548362732, clean_acc: 0.9780000448226929, changed_acc: 0.7420000433921814, scr_score: -0.05829601806207917
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7360000610351562, scr_score: -0.013392682519903025
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7460000514984131, scr_score: 0.03125012473068151
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7630000114440918, scr_score: 0.107142790619827
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7640000581741333, scr_score: 0.1116072842185819
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7700000405311584, scr_score: 0.13839291535050852
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7510000467300415, scr_score: 0.05357152835597378
dir: 2, original_acc: 0.7390000224113464, clean_acc: 0.9630000591278076, changed_acc: 0.7410000562667847, scr_score: 0.008928721105389243
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.753000020980835, scr_score: 0.10300422706858779
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7550000548362732, scr_score: 0.11158806188240133
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7630000114440918, scr_score: 0.1459226336960092
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.76500004529953, scr_score: 0.15450646850982275
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7640000581741333, scr_score: 0.15021467900985702
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7670000195503235, scr_score: 0.16309004750975417
dir: 1, original_acc: 0.7290000319480896, clean_acc: 0.9620000720024109, changed_acc: 0.7620000243186951, scr_score: 0.1416308441960435
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7670000195503235, scr_score: 0.0285713393672145
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7610000371932983, scr_score: 0.0
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7670000195503235, scr_score: 0.0285713393672145
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7590000629425049, scr_score: -0.009523685178541423
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7720000147819519, scr_score: 0.05238083614515829
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7770000100135803, scr_score: 0.07619033292310208
dir: 2, original_acc: 0.7610000371932983, clean_acc: 0.971000075340271, changed_acc: 0.7430000305175781, scr_score: -0.08571430193323373

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 36 epochs
Test accuracy for 0: 0.9240000247955322
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 1: 0.9490000605583191
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for 2: 0.9380000233650208
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 6: 0.984000027179718
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 32 epochs
Test accuracy for 9: 0.9630000591278076
Num non-zero elements: 1
Test accuracy for 0: 0.591
Num non-zero elements: 1
Test accuracy for 1: 0.618
Num non-zero elements: 1
Test accuracy for 2: 0.703
Num non-zero elements: 1
Test accuracy for 6: 0.767
Num non-zero elements: 1
Test accuracy for 9: 0.927
Num non-zero elements: 2
Test accuracy for 0: 0.605
Num non-zero elements: 2
Test accuracy for 1: 0.668
Num non-zero elements: 2
Test accuracy for 2: 0.744
Num non-zero elements: 2
Test accuracy for 6: 0.859
Num non-zero elements: 2
Test accuracy for 9: 0.93
Num non-zero elements: 5
Test accuracy for 0: 0.725
Num non-zero elements: 5
Test accuracy for 1: 0.684
Num non-zero elements: 5
Test accuracy for 2: 0.889
Num non-zero elements: 5
Test accuracy for 6: 0.97
Num non-zero elements: 5
Test accuracy for 9: 0.946
Num non-zero elements: 10
Test accuracy for 0: 0.782
Num non-zero elements: 10
Test accuracy for 1: 0.845
Num non-zero elements: 10
Test accuracy for 2: 0.894
Num non-zero elements: 10
Test accuracy for 6: 0.978
Num non-zero elements: 10
Test accuracy for 9: 0.949
Num non-zero elements: 20
Test accuracy for 0: 0.821
Num non-zero elements: 20
Test accuracy for 1: 0.902
Num non-zero elements: 20
Test accuracy for 2: 0.921
Num non-zero elements: 20
Test accuracy for 6: 0.985
Num non-zero elements: 20
Test accuracy for 9: 0.952
Num non-zero elements: 50
Test accuracy for 0: 0.883
Num non-zero elements: 50
Test accuracy for 1: 0.912
Num non-zero elements: 50
Test accuracy for 2: 0.92
Num non-zero elements: 50
Test accuracy for 6: 0.984
Num non-zero elements: 50
Test accuracy for 9: 0.964
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for 11: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 13: 0.9330000281333923
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 14: 0.9350000619888306
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 35 epochs
Test accuracy for 18: 0.8970000147819519
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 50 epochs
Test accuracy for 19: 0.9430000185966492
Num non-zero elements: 1
Test accuracy for 11: 0.524
Num non-zero elements: 1
Test accuracy for 13: 0.659
Num non-zero elements: 1
Test accuracy for 14: 0.645
Num non-zero elements: 1
Test accuracy for 18: 0.708
Num non-zero elements: 1
Test accuracy for 19: 0.776
Num non-zero elements: 2
Test accuracy for 11: 0.665
Num non-zero elements: 2
Test accuracy for 13: 0.655
Num non-zero elements: 2
Test accuracy for 14: 0.679
Num non-zero elements: 2
Test accuracy for 18: 0.725
Num non-zero elements: 2
Test accuracy for 19: 0.796
Num non-zero elements: 5
Test accuracy for 11: 0.792
Num non-zero elements: 5
Test accuracy for 13: 0.758
Num non-zero elements: 5
Test accuracy for 14: 0.705
Num non-zero elements: 5
Test accuracy for 18: 0.761
Num non-zero elements: 5
Test accuracy for 19: 0.847
Num non-zero elements: 10
Test accuracy for 11: 0.894
Num non-zero elements: 10
Test accuracy for 13: 0.873
Num non-zero elements: 10
Test accuracy for 14: 0.825
Num non-zero elements: 10
Test accuracy for 18: 0.759
Num non-zero elements: 10
Test accuracy for 19: 0.867
Num non-zero elements: 20
Test accuracy for 11: 0.911
Num non-zero elements: 20
Test accuracy for 13: 0.901
Num non-zero elements: 20
Test accuracy for 14: 0.894
Num non-zero elements: 20
Test accuracy for 18: 0.855
Num non-zero elements: 20
Test accuracy for 19: 0.898
Num non-zero elements: 50
Test accuracy for 11: 0.93
Num non-zero elements: 50
Test accuracy for 13: 0.917
Num non-zero elements: 50
Test accuracy for 14: 0.91
Num non-zero elements: 50
Test accuracy for 18: 0.867
Num non-zero elements: 50
Test accuracy for 19: 0.914
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for 20: 0.9380000233650208
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 21: 0.9030000567436218
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 30 epochs
Test accuracy for 22: 0.8960000276565552
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 26 epochs
Test accuracy for 25: 0.9500000476837158
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 39 epochs
Test accuracy for 26: 0.8520000576972961
Num non-zero elements: 1
Test accuracy for 20: 0.631
Num non-zero elements: 1
Test accuracy for 21: 0.79
Num non-zero elements: 1
Test accuracy for 22: 0.582
Num non-zero elements: 1
Test accuracy for 25: 0.7
Num non-zero elements: 1
Test accuracy for 26: 0.656
Num non-zero elements: 2
Test accuracy for 20: 0.906
Num non-zero elements: 2
Test accuracy for 21: 0.806
Num non-zero elements: 2
Test accuracy for 22: 0.663
Num non-zero elements: 2
Test accuracy for 25: 0.839
Num non-zero elements: 2
Test accuracy for 26: 0.685
Num non-zero elements: 5
Test accuracy for 20: 0.908
Num non-zero elements: 5
Test accuracy for 21: 0.824
Num non-zero elements: 5
Test accuracy for 22: 0.753
Num non-zero elements: 5
Test accuracy for 25: 0.843
Num non-zero elements: 5
Test accuracy for 26: 0.715
Num non-zero elements: 10
Test accuracy for 20: 0.915
Num non-zero elements: 10
Test accuracy for 21: 0.833
Num non-zero elements: 10
Test accuracy for 22: 0.768
Num non-zero elements: 10
Test accuracy for 25: 0.863
Num non-zero elements: 10
Test accuracy for 26: 0.769
Num non-zero elements: 20
Test accuracy for 20: 0.912
Num non-zero elements: 20
Test accuracy for 21: 0.862
Num non-zero elements: 20
Test accuracy for 22: 0.789
Num non-zero elements: 20
Test accuracy for 25: 0.927
Num non-zero elements: 20
Test accuracy for 26: 0.827
Num non-zero elements: 50
Test accuracy for 20: 0.929
Num non-zero elements: 50
Test accuracy for 21: 0.867
Num non-zero elements: 50
Test accuracy for 22: 0.829
Num non-zero elements: 50
Test accuracy for 25: 0.931
Num non-zero elements: 50
Test accuracy for 26: 0.832
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 51 epochs
Test accuracy for 1: 0.9260000586509705
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 2: 0.8940000534057617
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 3: 0.8820000290870667
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for 5: 0.8750000596046448
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 39 epochs
Test accuracy for 6: 0.8400000333786011
Num non-zero elements: 1
Test accuracy for 1: 0.604
Num non-zero elements: 1
Test accuracy for 2: 0.567
Num non-zero elements: 1
Test accuracy for 3: 0.605
Num non-zero elements: 1
Test accuracy for 5: 0.527
Num non-zero elements: 1
Test accuracy for 6: 0.582
Num non-zero elements: 2
Test accuracy for 1: 0.813
Num non-zero elements: 2
Test accuracy for 2: 0.604
Num non-zero elements: 2
Test accuracy for 3: 0.622
Num non-zero elements: 2
Test accuracy for 5: 0.785
Num non-zero elements: 2
Test accuracy for 6: 0.655
Num non-zero elements: 5
Test accuracy for 1: 0.81
Num non-zero elements: 5
Test accuracy for 2: 0.628
Num non-zero elements: 5
Test accuracy for 3: 0.639
Num non-zero elements: 5
Test accuracy for 5: 0.79
Num non-zero elements: 5
Test accuracy for 6: 0.723
Num non-zero elements: 10
Test accuracy for 1: 0.891
Num non-zero elements: 10
Test accuracy for 2: 0.779
Num non-zero elements: 10
Test accuracy for 3: 0.768
Num non-zero elements: 10
Test accuracy for 5: 0.847
Num non-zero elements: 10
Test accuracy for 6: 0.753
Num non-zero elements: 20
Test accuracy for 1: 0.901
Num non-zero elements: 20
Test accuracy for 2: 0.814
Num non-zero elements: 20
Test accuracy for 3: 0.826
Num non-zero elements: 20
Test accuracy for 5: 0.87
Num non-zero elements: 20
Test accuracy for 6: 0.76
Num non-zero elements: 50
Test accuracy for 1: 0.909
Num non-zero elements: 50
Test accuracy for 2: 0.855
Num non-zero elements: 50
Test accuracy for 3: 0.833
Num non-zero elements: 50
Test accuracy for 5: 0.87
Num non-zero elements: 50
Test accuracy for 6: 0.792
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 1.0: 0.9490000605583191
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for 5.0: 0.9490000605583191
Num non-zero elements: 1
Test accuracy for 1.0: 0.821
Num non-zero elements: 1
Test accuracy for 5.0: 0.821
Num non-zero elements: 2
Test accuracy for 1.0: 0.832
Num non-zero elements: 2
Test accuracy for 5.0: 0.832
Num non-zero elements: 5
Test accuracy for 1.0: 0.918
Num non-zero elements: 5
Test accuracy for 5.0: 0.918
Num non-zero elements: 10
Test accuracy for 1.0: 0.928
Num non-zero elements: 10
Test accuracy for 5.0: 0.928
Num non-zero elements: 20
Test accuracy for 1.0: 0.924
Num non-zero elements: 20
Test accuracy for 5.0: 0.924
Num non-zero elements: 50
Test accuracy for 1.0: 0.924
Num non-zero elements: 50
Test accuracy for 5.0: 0.924
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for C: 0.9350000619888306
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 52 epochs
Test accuracy for Python: 0.9650000333786011
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for HTML: 0.9800000190734863
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 46 epochs
Test accuracy for Java: 0.9490000605583191
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 31 epochs
Test accuracy for PHP: 0.9420000314712524
Num non-zero elements: 1
Test accuracy for C: 0.603
Num non-zero elements: 1
Test accuracy for Python: 0.651
Num non-zero elements: 1
Test accuracy for HTML: 0.588
Num non-zero elements: 1
Test accuracy for Java: 0.603
Num non-zero elements: 1
Test accuracy for PHP: 0.593
Num non-zero elements: 2
Test accuracy for C: 0.601
Num non-zero elements: 2
Test accuracy for Python: 0.663
Num non-zero elements: 2
Test accuracy for HTML: 0.952
Num non-zero elements: 2
Test accuracy for Java: 0.625
Num non-zero elements: 2
Test accuracy for PHP: 0.607
Num non-zero elements: 5
Test accuracy for C: 0.684
Num non-zero elements: 5
Test accuracy for Python: 0.733
Num non-zero elements: 5
Test accuracy for HTML: 0.95
Num non-zero elements: 5
Test accuracy for Java: 0.764
Num non-zero elements: 5
Test accuracy for PHP: 0.852
Num non-zero elements: 10
Test accuracy for C: 0.869
Num non-zero elements: 10
Test accuracy for Python: 0.752
Num non-zero elements: 10
Test accuracy for HTML: 0.955
Num non-zero elements: 10
Test accuracy for Java: 0.778
Num non-zero elements: 10
Test accuracy for PHP: 0.856
Num non-zero elements: 20
Test accuracy for C: 0.877
Num non-zero elements: 20
Test accuracy for Python: 0.823
Num non-zero elements: 20
Test accuracy for HTML: 0.965
Num non-zero elements: 20
Test accuracy for Java: 0.816
Num non-zero elements: 20
Test accuracy for PHP: 0.902
Num non-zero elements: 50
Test accuracy for C: 0.919
Num non-zero elements: 50
Test accuracy for Python: 0.905
Num non-zero elements: 50
Test accuracy for HTML: 0.968
Num non-zero elements: 50
Test accuracy for Java: 0.911
Num non-zero elements: 50
Test accuracy for PHP: 0.922
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 0: 0.9320000410079956
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 36 epochs
Test accuracy for 1: 0.971000075340271
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 37 epochs
Test accuracy for 2: 0.9150000214576721
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 3: 0.9390000700950623
Num non-zero elements: 1
Test accuracy for 0: 0.657
Num non-zero elements: 1
Test accuracy for 1: 0.665
Num non-zero elements: 1
Test accuracy for 2: 0.678
Num non-zero elements: 1
Test accuracy for 3: 0.641
Num non-zero elements: 2
Test accuracy for 0: 0.775
Num non-zero elements: 2
Test accuracy for 1: 0.852
Num non-zero elements: 2
Test accuracy for 2: 0.662
Num non-zero elements: 2
Test accuracy for 3: 0.761
Num non-zero elements: 5
Test accuracy for 0: 0.811
Num non-zero elements: 5
Test accuracy for 1: 0.966
Num non-zero elements: 5
Test accuracy for 2: 0.758
Num non-zero elements: 5
Test accuracy for 3: 0.838
Num non-zero elements: 10
Test accuracy for 0: 0.889
Num non-zero elements: 10
Test accuracy for 1: 0.961
Num non-zero elements: 10
Test accuracy for 2: 0.788
Num non-zero elements: 10
Test accuracy for 3: 0.9
Num non-zero elements: 20
Test accuracy for 0: 0.897
Num non-zero elements: 20
Test accuracy for 1: 0.972
Num non-zero elements: 20
Test accuracy for 2: 0.828
Num non-zero elements: 20
Test accuracy for 3: 0.915
Num non-zero elements: 50
Test accuracy for 0: 0.915
Num non-zero elements: 50
Test accuracy for 1: 0.971
Num non-zero elements: 50
Test accuracy for 2: 0.883
Num non-zero elements: 50
Test accuracy for 3: 0.919
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.12.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for en: 0.9950000643730164
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 53 epochs
Test accuracy for fr: 0.9730000495910645
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 35 epochs
Test accuracy for de: 0.9790000319480896
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 64 epochs
Test accuracy for es: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 41 epochs
Test accuracy for nl: 0.9830000400543213
Num non-zero elements: 1
Test accuracy for en: 0.74
Num non-zero elements: 1
Test accuracy for fr: 0.587
Num non-zero elements: 1
Test accuracy for de: 0.552
Num non-zero elements: 1
Test accuracy for es: 0.488
Num non-zero elements: 1
Test accuracy for nl: 0.661
Num non-zero elements: 2
Test accuracy for en: 0.822
Num non-zero elements: 2
Test accuracy for fr: 0.569
Num non-zero elements: 2
Test accuracy for de: 0.585
Num non-zero elements: 2
Test accuracy for es: 0.539
Num non-zero elements: 2
Test accuracy for nl: 0.757
Num non-zero elements: 5
Test accuracy for en: 0.901
Num non-zero elements: 5
Test accuracy for fr: 0.769
Num non-zero elements: 5
Test accuracy for de: 0.679
Num non-zero elements: 5
Test accuracy for es: 0.609
Num non-zero elements: 5
Test accuracy for nl: 0.792
Num non-zero elements: 10
Test accuracy for en: 0.935
Num non-zero elements: 10
Test accuracy for fr: 0.776
Num non-zero elements: 10
Test accuracy for de: 0.83
Num non-zero elements: 10
Test accuracy for es: 0.737
Num non-zero elements: 10
Test accuracy for nl: 0.818
Num non-zero elements: 20
Test accuracy for en: 0.96
Num non-zero elements: 20
Test accuracy for fr: 0.828
Num non-zero elements: 20
Test accuracy for de: 0.897
Num non-zero elements: 20
Test accuracy for es: 0.858
Num non-zero elements: 20
Test accuracy for nl: 0.882
Num non-zero elements: 50
Test accuracy for en: 0.982
Num non-zero elements: 50
Test accuracy for fr: 0.892
Num non-zero elements: 50
Test accuracy for de: 0.924
Num non-zero elements: 50
Test accuracy for es: 0.913
Num non-zero elements: 50
Test accuracy for nl: 0.93

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_19_sae_custom_sae/layer_19_google/gemma-2-2b_layer_19_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Saved evaluation results to: run_1/google_gemma-2-2b_layer_19_sae_custom_sae_eval_results.json

Running scr_and_tpp evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_professor_nurse_probes.pkl
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8940000534057617, scr_score: -0.05970163859464643
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8980000615119934, scr_score: 0.0
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8970000147819519, scr_score: -0.014926076865114633
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8330000638961792, scr_score: -0.9701496255136455
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.8140000104904175, scr_score: -1.2537326312437003
dir: 1, original_acc: 0.8980000615119934, clean_acc: 0.9650000333786011, changed_acc: 0.7800000309944153, scr_score: -1.761195224865289
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6160000562667847, scr_score: 0.050505149329191916
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6800000071525574, scr_score: 0.21212117563229838
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6880000233650208, scr_score: 0.23232323536397514
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.6980000138282776, scr_score: 0.25757573477018647
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7000000476837158, scr_score: 0.2626263249614903
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7230000495910645, scr_score: 0.32070713380248406
dir: 2, original_acc: 0.5960000157356262, clean_acc: 0.9920000433921814, changed_acc: 0.7320000529289246, scr_score: 0.3434344133714281
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_architect_journalist_probes.pkl
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8450000286102295, scr_score: 0.0
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8460000157356262, scr_score: 0.009090790866835655
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8460000157356262, scr_score: 0.009090790866835655
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8500000238418579, scr_score: 0.04545449619451486
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.8420000672340393, scr_score: -0.027272372600506963
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7260000109672546, scr_score: -1.081818201522194
dir: 1, original_acc: 0.8450000286102295, clean_acc: 0.9550000429153442, changed_acc: 0.7210000157356262, scr_score: -1.127272697716709
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7390000224113464, scr_score: 0.2639295259892841
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7470000386238098, scr_score: 0.28738998267969534
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.753000020980835, scr_score: 0.30498523780066916
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7680000066757202, scr_score: 0.3489734629999383
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.781000018119812, scr_score: 0.38709666142343924
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.781000018119812, scr_score: 0.38709666142343924
dir: 2, original_acc: 0.6490000486373901, clean_acc: 0.9900000691413879, changed_acc: 0.7670000195503235, scr_score: 0.34604094961205417
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_surgeon_psychologist_probes.pkl
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9110000729560852, scr_score: -0.09090899238902972
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.9140000343322754, scr_score: -0.036364247188015786
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8750000596046448, scr_score: -0.7454546045665822
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8660000562667847, scr_score: -0.9090910076109703
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8510000109672546, scr_score: -1.1818190684987326
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8480000495910645, scr_score: -1.2363638136997466
dir: 1, original_acc: 0.9160000681877136, clean_acc: 0.971000075340271, changed_acc: 0.8270000219345093, scr_score: -1.6181824487102099
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6190000176429749, scr_score: 0.0958230324939037
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6310000419616699, scr_score: 0.1253071213490508
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6370000243186951, scr_score: 0.14004909255224596
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6350000500679016, scr_score: 0.13513515096743317
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6540000438690186, scr_score: 0.18181818181818182
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6640000343322754, scr_score: 0.20638818263975933
dir: 2, original_acc: 0.5800000429153442, clean_acc: 0.987000048160553, changed_acc: 0.6600000262260437, scr_score: 0.19656015302137697
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_attorney_teacher_probes.pkl
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8330000638961792, scr_score: -0.05426332591124638
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8340000510215759, scr_score: -0.04651148821699416
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7980000376701355, scr_score: -0.3255813416219208
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7980000376701355, scr_score: -0.3255813416219208
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8280000686645508, scr_score: -0.09302297643398832
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.8190000653266907, scr_score: -0.16279043978521998
dir: 1, original_acc: 0.8400000333786011, clean_acc: 0.9690000414848328, changed_acc: 0.7160000205039978, scr_score: -0.961240349477258
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.6950000524520874, scr_score: 0.10447767832995988
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7170000076293945, scr_score: 0.17014917884380523
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7410000562667847, scr_score: 0.24179110691684247
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7570000290870667, scr_score: 0.28955221437457473
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7870000600814819, scr_score: 0.379104535503725
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.7870000600814819, scr_score: 0.379104535503725
dir: 2, original_acc: 0.6600000262260437, clean_acc: 0.9950000643730164, changed_acc: 0.8020000457763672, scr_score: 0.4238806071061539
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Books_CDs_and_Vinyl_probes.pkl
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.8240000605583191, scr_score: 0.0
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.6920000314712524, scr_score: -0.7951808569998471
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7240000367164612, scr_score: -0.6024097510321298
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7330000400543213, scr_score: -0.5481928662579559
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7330000400543213, scr_score: -0.5481928662579559
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.7280000448226929, scr_score: -0.5783133179031518
dir: 1, original_acc: 0.8240000605583191, clean_acc: 0.9900000691413879, changed_acc: 0.6720000505447388, scr_score: -0.9156626635806303
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8320000171661377, scr_score: 0.5839415073928252
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8370000123977661, scr_score: 0.6021896659029006
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8380000591278076, scr_score: 0.6058394716330825
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8410000205039978, scr_score: 0.6167882362180025
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8480000495910645, scr_score: 0.642335788653233
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.8390000462532043, scr_score: 0.6094890598280558
dir: 2, original_acc: 0.6720000505447388, clean_acc: 0.9460000395774841, changed_acc: 0.815000057220459, scr_score: 0.5218978554726529
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Software_Electronics_probes.pkl
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8320000171661377, scr_score: 0.13529411558461457
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8470000624656677, scr_score: 0.2235296675077933
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8640000224113464, scr_score: 0.32352942207692714
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8710000514984131, scr_score: 0.36470605972314696
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8400000333786011, scr_score: 0.18235302986157348
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.8500000238418579, scr_score: 0.2411764974000106
dir: 1, original_acc: 0.8090000152587891, clean_acc: 0.9790000319480896, changed_acc: 0.812000036239624, scr_score: 0.017647180507740362
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7280000448226929, scr_score: 0.14661665254977885
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7380000352859497, scr_score: 0.184210597077132
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7420000433921814, scr_score: 0.19924821970359016
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7540000081062317, scr_score: 0.2443608635053801
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7490000128746033, scr_score: 0.22556389124170353
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7820000648498535, scr_score: 0.3496242218905874
dir: 2, original_acc: 0.6890000104904175, clean_acc: 0.9550000429153442, changed_acc: 0.7240000367164612, scr_score: 0.13157902992332066
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Pet_Supplies_Office_Products_probes.pkl
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8960000276565552, scr_score: 0.24107141431649734
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9020000696182251, scr_score: 0.29464319450956306
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9080000519752502, scr_score: 0.34821444251852934
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8970000147819519, scr_score: 0.24999986695397514
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9040000438690186, scr_score: 0.31250009978451865
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.9000000357627869, scr_score: 0.276785757050508
dir: 1, original_acc: 0.8690000176429749, clean_acc: 0.9810000658035278, changed_acc: 0.8010000586509705, scr_score: -0.6071422299258827
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6440000534057617, scr_score: 0.04559283512434088
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.656000018119812, scr_score: 0.08206688582089079
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6910000443458557, scr_score: 0.18844992621888307
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7250000238418579, scr_score: 0.2917933114179109
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7240000367164612, scr_score: 0.28875383738804883
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.7230000495910645, scr_score: 0.28571436335818673
dir: 2, original_acc: 0.6290000081062317, clean_acc: 0.9580000638961792, changed_acc: 0.6930000185966492, scr_score: 0.1945288742786073
Loading activations from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_activations.pt
Loading probes from artifacts/scr/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_Industrial_and_Scientific_Toys_and_Games_probes.pkl
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7690000534057617, scr_score: 0.07692323122503876
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7890000343322754, scr_score: 0.17307698369555094
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7700000405311584, scr_score: 0.08173086153640712
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7740000486373901, scr_score: 0.10096166934266682
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7610000371932983, scr_score: 0.03846161561251938
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.76500004529953, scr_score: 0.057692423418779074
dir: 1, original_acc: 0.753000020980835, clean_acc: 0.9610000252723694, changed_acc: 0.7420000433921814, scr_score: -0.052884506546624445
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7720000147819519, scr_score: 0.13302745395472726
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.781000018119812, scr_score: 0.17431187392906444
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7980000376701355, scr_score: 0.252293616861881
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7970000505447388, scr_score: 0.24770651984602315
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7740000486373901, scr_score: 0.14220192140225124
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7990000247955322, scr_score: 0.25688071387773886
dir: 2, original_acc: 0.7430000305175781, clean_acc: 0.9610000252723694, changed_acc: 0.7620000243186951, scr_score: 0.08715593696453222

Running sparse_probing evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set1_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 0: 0.9390000700950623
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 1: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for 2: 0.9410000443458557
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 6: 0.987000048160553
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 9: 0.9730000495910645
Num non-zero elements: 1
Test accuracy for 0: 0.574
Num non-zero elements: 1
Test accuracy for 1: 0.628
Num non-zero elements: 1
Test accuracy for 2: 0.875
Num non-zero elements: 1
Test accuracy for 6: 0.779
Num non-zero elements: 1
Test accuracy for 9: 0.951
Num non-zero elements: 2
Test accuracy for 0: 0.673
Num non-zero elements: 2
Test accuracy for 1: 0.795
Num non-zero elements: 2
Test accuracy for 2: 0.9
Num non-zero elements: 2
Test accuracy for 6: 0.859
Num non-zero elements: 2
Test accuracy for 9: 0.961
Num non-zero elements: 5
Test accuracy for 0: 0.725
Num non-zero elements: 5
Test accuracy for 1: 0.826
Num non-zero elements: 5
Test accuracy for 2: 0.917
Num non-zero elements: 5
Test accuracy for 6: 0.977
Num non-zero elements: 5
Test accuracy for 9: 0.96
Num non-zero elements: 10
Test accuracy for 0: 0.796
Num non-zero elements: 10
Test accuracy for 1: 0.918
Num non-zero elements: 10
Test accuracy for 2: 0.933
Num non-zero elements: 10
Test accuracy for 6: 0.989
Num non-zero elements: 10
Test accuracy for 9: 0.963
Num non-zero elements: 20
Test accuracy for 0: 0.882
Num non-zero elements: 20
Test accuracy for 1: 0.948
Num non-zero elements: 20
Test accuracy for 2: 0.936
Num non-zero elements: 20
Test accuracy for 6: 0.987
Num non-zero elements: 20
Test accuracy for 9: 0.971
Num non-zero elements: 50
Test accuracy for 0: 0.911
Num non-zero elements: 50
Test accuracy for 1: 0.949
Num non-zero elements: 50
Test accuracy for 2: 0.948
Num non-zero elements: 50
Test accuracy for 6: 0.982
Num non-zero elements: 50
Test accuracy for 9: 0.968
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set2_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 16 epochs
Test accuracy for 11: 0.9600000381469727
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 17 epochs
Test accuracy for 13: 0.9410000443458557
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 14: 0.9460000395774841
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 40 epochs
Test accuracy for 18: 0.9250000715255737
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 19: 0.956000030040741
Num non-zero elements: 1
Test accuracy for 11: 0.549
Num non-zero elements: 1
Test accuracy for 13: 0.68
Num non-zero elements: 1
Test accuracy for 14: 0.649
Num non-zero elements: 1
Test accuracy for 18: 0.679
Num non-zero elements: 1
Test accuracy for 19: 0.785
Num non-zero elements: 2
Test accuracy for 11: 0.859
Num non-zero elements: 2
Test accuracy for 13: 0.798
Num non-zero elements: 2
Test accuracy for 14: 0.666
Num non-zero elements: 2
Test accuracy for 18: 0.727
Num non-zero elements: 2
Test accuracy for 19: 0.893
Num non-zero elements: 5
Test accuracy for 11: 0.899
Num non-zero elements: 5
Test accuracy for 13: 0.816
Num non-zero elements: 5
Test accuracy for 14: 0.843
Num non-zero elements: 5
Test accuracy for 18: 0.831
Num non-zero elements: 5
Test accuracy for 19: 0.911
Num non-zero elements: 10
Test accuracy for 11: 0.93
Num non-zero elements: 10
Test accuracy for 13: 0.881
Num non-zero elements: 10
Test accuracy for 14: 0.901
Num non-zero elements: 10
Test accuracy for 18: 0.842
Num non-zero elements: 10
Test accuracy for 19: 0.903
Num non-zero elements: 20
Test accuracy for 11: 0.958
Num non-zero elements: 20
Test accuracy for 13: 0.904
Num non-zero elements: 20
Test accuracy for 14: 0.925
Num non-zero elements: 20
Test accuracy for 18: 0.915
Num non-zero elements: 20
Test accuracy for 19: 0.902
Num non-zero elements: 50
Test accuracy for 11: 0.954
Num non-zero elements: 50
Test accuracy for 13: 0.901
Num non-zero elements: 50
Test accuracy for 14: 0.943
Num non-zero elements: 50
Test accuracy for 18: 0.92
Num non-zero elements: 50
Test accuracy for 19: 0.92
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/LabHC_bias_in_bios_class_set3_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 20: 0.9480000734329224
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 21: 0.9140000343322754
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 22: 0.8970000147819519
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 19 epochs
Test accuracy for 25: 0.9510000348091125
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 15 epochs
Test accuracy for 26: 0.8700000643730164
Num non-zero elements: 1
Test accuracy for 20: 0.857
Num non-zero elements: 1
Test accuracy for 21: 0.768
Num non-zero elements: 1
Test accuracy for 22: 0.758
Num non-zero elements: 1
Test accuracy for 25: 0.688
Num non-zero elements: 1
Test accuracy for 26: 0.622
Num non-zero elements: 2
Test accuracy for 20: 0.887
Num non-zero elements: 2
Test accuracy for 21: 0.809
Num non-zero elements: 2
Test accuracy for 22: 0.788
Num non-zero elements: 2
Test accuracy for 25: 0.859
Num non-zero elements: 2
Test accuracy for 26: 0.684
Num non-zero elements: 5
Test accuracy for 20: 0.917
Num non-zero elements: 5
Test accuracy for 21: 0.819
Num non-zero elements: 5
Test accuracy for 22: 0.816
Num non-zero elements: 5
Test accuracy for 25: 0.91
Num non-zero elements: 5
Test accuracy for 26: 0.775
Num non-zero elements: 10
Test accuracy for 20: 0.925
Num non-zero elements: 10
Test accuracy for 21: 0.852
Num non-zero elements: 10
Test accuracy for 22: 0.839
Num non-zero elements: 10
Test accuracy for 25: 0.95
Num non-zero elements: 10
Test accuracy for 26: 0.832
Num non-zero elements: 20
Test accuracy for 20: 0.934
Num non-zero elements: 20
Test accuracy for 21: 0.873
Num non-zero elements: 20
Test accuracy for 22: 0.871
Num non-zero elements: 20
Test accuracy for 25: 0.945
Num non-zero elements: 20
Test accuracy for 26: 0.853
Num non-zero elements: 50
Test accuracy for 20: 0.935
Num non-zero elements: 50
Test accuracy for 21: 0.893
Num non-zero elements: 50
Test accuracy for 22: 0.879
Num non-zero elements: 50
Test accuracy for 25: 0.947
Num non-zero elements: 50
Test accuracy for 26: 0.87
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 33 epochs
Test accuracy for 1: 0.9480000734329224
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 21 epochs
Test accuracy for 2: 0.9300000667572021
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for 3: 0.9140000343322754
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 28 epochs
Test accuracy for 5: 0.9180000424385071
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 22 epochs
Test accuracy for 6: 0.8500000238418579
Num non-zero elements: 1
Test accuracy for 1: 0.844
Num non-zero elements: 1
Test accuracy for 2: 0.838
Num non-zero elements: 1
Test accuracy for 3: 0.69
Num non-zero elements: 1
Test accuracy for 5: 0.869
Num non-zero elements: 1
Test accuracy for 6: 0.727
Num non-zero elements: 2
Test accuracy for 1: 0.841
Num non-zero elements: 2
Test accuracy for 2: 0.86
Num non-zero elements: 2
Test accuracy for 3: 0.712
Num non-zero elements: 2
Test accuracy for 5: 0.874
Num non-zero elements: 2
Test accuracy for 6: 0.733
Num non-zero elements: 5
Test accuracy for 1: 0.896
Num non-zero elements: 5
Test accuracy for 2: 0.882
Num non-zero elements: 5
Test accuracy for 3: 0.751
Num non-zero elements: 5
Test accuracy for 5: 0.885
Num non-zero elements: 5
Test accuracy for 6: 0.743
Num non-zero elements: 10
Test accuracy for 1: 0.934
Num non-zero elements: 10
Test accuracy for 2: 0.881
Num non-zero elements: 10
Test accuracy for 3: 0.868
Num non-zero elements: 10
Test accuracy for 5: 0.894
Num non-zero elements: 10
Test accuracy for 6: 0.775
Num non-zero elements: 20
Test accuracy for 1: 0.915
Num non-zero elements: 20
Test accuracy for 2: 0.896
Num non-zero elements: 20
Test accuracy for 3: 0.882
Num non-zero elements: 20
Test accuracy for 5: 0.9
Num non-zero elements: 20
Test accuracy for 6: 0.801
Num non-zero elements: 50
Test accuracy for 1: 0.946
Num non-zero elements: 50
Test accuracy for 2: 0.926
Num non-zero elements: 50
Test accuracy for 3: 0.897
Num non-zero elements: 50
Test accuracy for 5: 0.902
Num non-zero elements: 50
Test accuracy for 6: 0.849
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/canrager_amazon_reviews_mcauley_1and5_sentiment_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 38 epochs
Test accuracy for 1.0: 0.9530000686645508
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 46 epochs
Test accuracy for 5.0: 0.9540000557899475
Num non-zero elements: 1
Test accuracy for 1.0: 0.887
Num non-zero elements: 1
Test accuracy for 5.0: 0.887
Num non-zero elements: 2
Test accuracy for 1.0: 0.884
Num non-zero elements: 2
Test accuracy for 5.0: 0.884
Num non-zero elements: 5
Test accuracy for 1.0: 0.885
Num non-zero elements: 5
Test accuracy for 5.0: 0.885
Num non-zero elements: 10
Test accuracy for 1.0: 0.927
Num non-zero elements: 10
Test accuracy for 5.0: 0.927
Num non-zero elements: 20
Test accuracy for 1.0: 0.932
Num non-zero elements: 20
Test accuracy for 5.0: 0.933
Num non-zero elements: 50
Test accuracy for 1.0: 0.934
Num non-zero elements: 50
Test accuracy for 5.0: 0.934
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/codeparrot_github-code_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 29 epochs
Test accuracy for C: 0.9550000429153442
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 25 epochs
Test accuracy for Python: 0.9800000190734863
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for HTML: 0.9810000658035278
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 45 epochs
Test accuracy for Java: 0.9690000414848328
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for PHP: 0.9520000219345093
Num non-zero elements: 1
Test accuracy for C: 0.564
Num non-zero elements: 1
Test accuracy for Python: 0.629
Num non-zero elements: 1
Test accuracy for HTML: 0.871
Num non-zero elements: 1
Test accuracy for Java: 0.61
Num non-zero elements: 1
Test accuracy for PHP: 0.916
Num non-zero elements: 2
Test accuracy for C: 0.861
Num non-zero elements: 2
Test accuracy for Python: 0.945
Num non-zero elements: 2
Test accuracy for HTML: 0.958
Num non-zero elements: 2
Test accuracy for Java: 0.665
Num non-zero elements: 2
Test accuracy for PHP: 0.927
Num non-zero elements: 5
Test accuracy for C: 0.897
Num non-zero elements: 5
Test accuracy for Python: 0.956
Num non-zero elements: 5
Test accuracy for HTML: 0.964
Num non-zero elements: 5
Test accuracy for Java: 0.847
Num non-zero elements: 5
Test accuracy for PHP: 0.929
Num non-zero elements: 10
Test accuracy for C: 0.93
Num non-zero elements: 10
Test accuracy for Python: 0.968
Num non-zero elements: 10
Test accuracy for HTML: 0.959
Num non-zero elements: 10
Test accuracy for Java: 0.891
Num non-zero elements: 10
Test accuracy for PHP: 0.945
Num non-zero elements: 20
Test accuracy for C: 0.938
Num non-zero elements: 20
Test accuracy for Python: 0.97
Num non-zero elements: 20
Test accuracy for HTML: 0.971
Num non-zero elements: 20
Test accuracy for Java: 0.946
Num non-zero elements: 20
Test accuracy for PHP: 0.942
Num non-zero elements: 50
Test accuracy for C: 0.953
Num non-zero elements: 50
Test accuracy for Python: 0.98
Num non-zero elements: 50
Test accuracy for HTML: 0.982
Num non-zero elements: 50
Test accuracy for Java: 0.949
Num non-zero elements: 50
Test accuracy for PHP: 0.937
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/fancyzhx_ag_news_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 23 epochs
Test accuracy for 0: 0.9350000619888306
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 34 epochs
Test accuracy for 1: 0.9750000238418579
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for 2: 0.9250000715255737
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 27 epochs
Test accuracy for 3: 0.9440000653266907
Num non-zero elements: 1
Test accuracy for 0: 0.848
Num non-zero elements: 1
Test accuracy for 1: 0.953
Num non-zero elements: 1
Test accuracy for 2: 0.702
Num non-zero elements: 1
Test accuracy for 3: 0.727
Num non-zero elements: 2
Test accuracy for 0: 0.85
Num non-zero elements: 2
Test accuracy for 1: 0.961
Num non-zero elements: 2
Test accuracy for 2: 0.694
Num non-zero elements: 2
Test accuracy for 3: 0.786
Num non-zero elements: 5
Test accuracy for 0: 0.852
Num non-zero elements: 5
Test accuracy for 1: 0.969
Num non-zero elements: 5
Test accuracy for 2: 0.834
Num non-zero elements: 5
Test accuracy for 3: 0.875
Num non-zero elements: 10
Test accuracy for 0: 0.902
Num non-zero elements: 10
Test accuracy for 1: 0.973
Num non-zero elements: 10
Test accuracy for 2: 0.832
Num non-zero elements: 10
Test accuracy for 3: 0.903
Num non-zero elements: 20
Test accuracy for 0: 0.906
Num non-zero elements: 20
Test accuracy for 1: 0.979
Num non-zero elements: 20
Test accuracy for 2: 0.86
Num non-zero elements: 20
Test accuracy for 3: 0.922
Num non-zero elements: 50
Test accuracy for 0: 0.92
Num non-zero elements: 50
Test accuracy for 1: 0.975
Num non-zero elements: 50
Test accuracy for 2: 0.908
Num non-zero elements: 50
Test accuracy for 3: 0.912
Loading activations from artifacts/sparse_probing/google/gemma-2-2b/blocks.19.hook_resid_post/Helsinki-NLP_europarl_activations.pt
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for en: 1.0
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 14 epochs
Test accuracy for fr: 0.999000072479248
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 20 epochs
Test accuracy for de: 1.0
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 12 epochs
Test accuracy for es: 1.0
Num non-zero elements: 2304
Training probe with dim: 2304, device: cuda:0, dtype: torch.bfloat16
GPU probe training early stopping triggered after 18 epochs
Test accuracy for nl: 0.999000072479248
Num non-zero elements: 1
Test accuracy for en: 0.932
Num non-zero elements: 1
Test accuracy for fr: 0.994
Num non-zero elements: 1
Test accuracy for de: 0.658
Num non-zero elements: 1
Test accuracy for es: 0.991
Num non-zero elements: 1
Test accuracy for nl: 0.812
Num non-zero elements: 2
Test accuracy for en: 0.997
Num non-zero elements: 2
Test accuracy for fr: 0.995
Num non-zero elements: 2
Test accuracy for de: 0.924
Num non-zero elements: 2
Test accuracy for es: 0.992
Num non-zero elements: 2
Test accuracy for nl: 0.841
Num non-zero elements: 5
Test accuracy for en: 1.0
Num non-zero elements: 5
Test accuracy for fr: 0.998
Num non-zero elements: 5
Test accuracy for de: 0.96
Num non-zero elements: 5
Test accuracy for es: 1.0
Num non-zero elements: 5
Test accuracy for nl: 0.846
Num non-zero elements: 10
Test accuracy for en: 1.0
Num non-zero elements: 10
Test accuracy for fr: 0.998
Num non-zero elements: 10
Test accuracy for de: 0.976
Num non-zero elements: 10
Test accuracy for es: 0.999
Num non-zero elements: 10
Test accuracy for nl: 0.869
Num non-zero elements: 20
Test accuracy for en: 0.999
Num non-zero elements: 20
Test accuracy for fr: 0.998
Num non-zero elements: 20
Test accuracy for de: 0.989
Num non-zero elements: 20
Test accuracy for es: 0.999
Num non-zero elements: 20
Test accuracy for nl: 0.898
Num non-zero elements: 50
Test accuracy for en: 0.997
Num non-zero elements: 50
Test accuracy for fr: 0.999
Num non-zero elements: 50
Test accuracy for de: 0.993
Num non-zero elements: 50
Test accuracy for es: 0.999
Num non-zero elements: 50
Test accuracy for nl: 0.993
coder_out: Ah yes, I need to fix the trainer name in the run_sae_training function to match the new OrthogonalTrainer class name. Here's the fix:

experiment.py
```python
<<<<<<< SEARCH
    # Initialize trainer  
    trainer = CustomTrainer(
=======
    # Initialize trainer  
    trainer = OrthogonalTrainer(
>>>>>>> REPLACE
```

This change ensures the trainer initialization matches the renamed OrthogonalTrainer class we defined earlier.

Now you can run:

```bash
python experiment.py --out_dir=run_1
```, type: <class 'str'>
Currently on iteration 1 of run 2
Let me analyze Run 1 results and update notes.txt with the findings before proceeding with Run 2.                                                                          

notes.txt                                                                                                                                                                  

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
 ## Run 0: Baseline                                                                                                                                                        
 Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss':          
 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031} 
 'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss':    
 464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss':      
 8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss':        
 810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375 
 'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438,         
 'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss':              
 210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss':        
 190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203,        
 'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'ste 
 4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375,       
 'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps' 
 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'},     
 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core       
 evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg':          
 {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000,    
 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True,        
 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False,                                     
 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211,                    
 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375,    
 'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage' 
 {'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0},      
 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True,                      
 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500,          
 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False,        
 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'],            
 ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software',    
 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf',               
 'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2':                    
 0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297,           
 'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10':      
 0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652,           
 'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100':    
 -0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888,      
 'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name':                                
 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2':          
 0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108,              
 'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20':     
 0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302,            
 'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100':    
 0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213,        
 'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2':                
 -0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792,        
 'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10':    
 0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363,         
 'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50':     
 0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802,      
 'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name':            
 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923,          
 'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5':       
 0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978,         
 'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50':    
 -0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643,         
 'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500' 
 0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2 
 -0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804,         
 'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10':     
 0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717,              
 'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50':         
 0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106,          
 'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name':            
 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496,      
 'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5':           
 0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356,           
 'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50':        
 -0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609,        
 'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500 
 0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results',         
 'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5':          
 0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005,          
 'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20':    
 0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978,           
 'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100':  
 0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154},    
 {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2':  
 0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597,            
 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10':      
 0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093,          
 'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100':      
 -0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262,        
 'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name':                                                         
 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2':      
 0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654,             
 'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10':       
 0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687,      
 'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100':   
 0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045,   
 'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6',  
 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b',    
 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom',         
 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16',  
 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None,               
 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name':    
 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6,          
 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id':           
 'd8a9fbf2-e09c-6353-944a-ddaddfb5ca77', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604,           
 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26,                       
 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3,                          
 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122,                          
 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14,                          
 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11,                          
 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives 
 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131,                    
 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1},   
 {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j',    
 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate':        
 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072,       
 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0                                              
                                                                                                                                                                           
Let me analyze Run 1 results and update notes.txt with the findings before proceeding with Run 2.                                                                          

notes.txt                                                                                                                                                                  

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
 ## Run 0: Baseline                                                                                                                                                        
 Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss':          
 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031} 
 'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss':    
 464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss':      
 8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss':        
 810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375 
 'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438,         
 'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss':              
 210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss':        
 190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203,        
 'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'ste 
 4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375,       
 'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps' 
 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'},     
 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core       
 evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg':          
 {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000,    
 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True,        
 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False,                                     
 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211,                    
 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375,    
 'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage' 
 {'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0},      
 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True,                      
 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500,          
 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False,        
 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'],            
 ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software',    
 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf',               
 'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2':                    
 0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297,           
 'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10':      
 0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652,           
 'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100':    
 -0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888,      
 'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name':                                
 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2':          
 0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108,              
 'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20':     
 0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302,            
 'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100':    
 0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213,        
 'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2':                
 -0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792,        
 'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10':    
 0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363,         
 'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50':     
 0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802,      
 'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name':            
 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923,          
 'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5':       
 0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978,         
 'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50':    
 -0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643,         
 'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500' 
 0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2 
 -0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804,         
 'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10':     
 0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717,              
 'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50':         
 0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106,          
 'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name':            
 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496,      
 'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5':           
 0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356,           
 'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50':        
 -0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609,        
 'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500 
 0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results',         
 'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5':          
 0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005,          
 'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20':    
 0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978,           
 'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100':  
 0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154},    
 {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2':  
 0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597,            
 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10':      
 0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093,          
 'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100':      
 -0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262,        
 'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name':                                                         
 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2':      
 0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654,             
 'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10':       
 0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687,      
 'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100':   
 0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045,   
 'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6',  
 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b',    
 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom',         
 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16',  
 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None,               
 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name':    
 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6,          
 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id':           
 'd8a9fbf2-e09c-6353-944a-ddaddfb5ca77', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604,           
 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26,                       
 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3,                          
 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122,                          
 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14,                          
 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11,                          
 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives 
 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131,                    
 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1},   
 {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j',    
 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate':        
 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072,       
 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1,     
 'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives 
 980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features 
 2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r',  
 'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate':       
 0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581,      
 'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4,      
 'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825,            
 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1},  
 {'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z',      
 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash':                                         
 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0',            
 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None,        
 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True,     
 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens':     
 -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id':             
 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'],       
 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name':     
 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis':           
 1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125,       
 'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025,         
 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001,  
 'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125,              
 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606,                       
 'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001,      
 'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.956600058078765 
 'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy':    
 0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results',         
 'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy':          
 0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy':           
 0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.846 
 'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name':                      
 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734,                     
 'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy 
 None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy':   
 0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy':   
 None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001,          
 'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999,        
 'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999,            
 'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001,                 
 'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results',           
 'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86,        
 'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841,                         
 'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy':      
 0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962 
 'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877,        
 'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292 
 'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy':   
 0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435,     
 'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy':   
 0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy':             
 0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925,        
 'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999,                    
 'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001,                    
 'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015,                       
 'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy':               
 0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash':                 
 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0',            
 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None,        
 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True,     
 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens':     
 -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}                                                                   
 Description: Baseline results.                                                                                                                                            
 =======                                                                                                                                                                   
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
                                                                                                                                                                           
 ## Run 1: Fixed τ=0.2, top-k=0.1%                                                                                                                                         
 Results show promising improvements:                                                                                                                                      
 1. Reconstruction quality improved (MSE 15.06 vs 18.75 baseline)                                                                                                          
 2. Better feature disentanglement (SCR metrics improved across all thresholds)                                                                                            
 3. Higher sparse probing accuracy (94.97% vs 94.28% baseline)                                                                                                             
 4. Orthogonality loss successfully decreased from ~0.7 to ~0.3 during training                                                                                            
 5. L0 sparsity increased (99.3 vs 85.2 baseline) while maintaining performance                                                                                            
                                                                                                                                                                           
 Key findings:                                                                                                                                                             
 - Orthogonality constraints effectively reduce feature entanglement                                                                                                       
 - Top-k pair selection provides computational efficiency                                                                                                                  
 - L2 normalization helps stabilize training                                                                                                                               
 - No significant impact on model behavior preservation                                                                                                                    
                                                                                                                                                                           
 Next steps:                                                                                                                                                               
 - Run 2: Implement adaptive τ based on activation magnitudes                                                                                                              
 - Run 3: Ablation with top-k=0.05%                                                                                                                                        
 - Run 4: Ablation with top-k=0.2%                                                                                                                                         
                                                                                                                                                                           
 ## Run 0: Baseline                                                                                                                                                        
 Results: [Previous baseline results...]                                                                                                                                   
 Description: Baseline results.                                                                                                                                            
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

Now let's implement Run 2 with adaptive τ. We'll modify the OrthogonalSAE class to adjust τ based on activation magnitudes:                                                

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         # Fixed parameters for orthogonality constraints                                                                                                                  
         self.tau = 0.2  # Fixed τ value                                                                                                                                   
         self.topk_percent = 0.001  # 0.1% of pairs                                                                                                                        
 =======                                                                                                                                                                   
         # Adaptive orthogonality parameters                                                                                                                               
         self.tau_min = 0.1                                                                                                                                                
         self.tau_max = 0.3                                                                                                                                                
         self.tau = 0.2  # Initial τ value                                                                                                                                 
         self.topk_percent = 0.001  # 0.1% of pairs                                                                                                                        
         self.tau_momentum = 0.9  # For smoothing τ updates                                                                                                                
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         # Orthogonality loss on selected pairs                                                                                                                            
         ortho_loss = (top_correlations - self.ae.tau).clamp(min=0).mean()                                                                                                 
 =======                                                                                                                                                                   
         # Compute activation magnitudes for τ adaptation                                                                                                                  
         act_magnitudes = f.norm(dim=0)                                                                                                                                    
         mean_magnitude = act_magnitudes.mean()                                                                                                                            
                                                                                                                                                                           
         # Adapt τ based on activation magnitudes                                                                                                                          
         target_tau = self.ae.tau_min + (self.ae.tau_max - self.ae.tau_min) * (mean_magnitude / act_magnitudes.max())                                                      
         self.ae.tau = self.ae.tau_momentum * self.ae.tau + (1 - self.ae.tau_momentum) * target_tau                                                                        
                                                                                                                                                                           
         # Orthogonality loss on selected pairs with adaptive τ                                                                                                            
         ortho_loss = (top_correlations - self.ae.tau).clamp(min=0).mean()                                                                                                 
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This implements Run 2 with:                                                                                                                                                

 • Adaptive τ ranging from 0.1 to 0.3                                                                                                                                      
 • τ adjusted based on ratio of mean/max activation magnitudes                                                                                                             
 • Momentum-based smoothing of τ updates                                                                                                                                   
 • Same top-k selection as Run 1                                                                                                                                           

Let's run this version:                                                                                                                                                    

                                                                                                                                                                           
 python experiment.py --out_dir=run_2                                                                                                                                      
                                                                                                                                                                           

Tokens: 50k sent, 5.6k received. Cost: $0.23 message, $0.47 session.
The LLM did not conform to the edit format.
https://aider.chat/docs/troubleshooting/edit-errors.html

# 1 SEARCH/REPLACE block failed to match!

## SearchReplaceNoExactMatch: This SEARCH block failed to exactly match lines in notes.txt
<<<<<<< SEARCH
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact
## Run 0: Baseline
Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 
792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031}, 
'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss': 
464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss': 
8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss': 
810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375},
'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438, 
'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss': 
210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss': 
190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203, 
'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'step 
4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375, 
'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 
1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 
'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core 
evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': 
{'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 
'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 
'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 
'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211, 
'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375, 
'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage': 
{'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0}, 
'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 
'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 
'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 
'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], 
['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 
'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf', 
'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2': 
0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297, 
'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10': 
0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652, 
'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100': 
-0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888, 
'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2': 
0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108, 
'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20': 
0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302, 
'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100': 
0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213, 
'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 
-0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792, 
'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10': 
0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363, 
'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50': 
0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802, 
'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923, 
'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5': 
0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978, 
'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50': 
-0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643, 
'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500': 
0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2':
-0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804, 
'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10': 
0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717, 
'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50': 
0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106, 
'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496, 
'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5': 
0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356, 
'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50': 
-0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609, 
'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500':
0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 
'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5': 
0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005, 
'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20': 
0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978, 
'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100': 
0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154}, 
{'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2': 
0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597, 
'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10': 
0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093, 
'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100': 
-0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262, 
'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2': 
0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654, 
'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 
0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687, 
'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100': 
0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045, 
'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 
'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 
'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 
'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 
'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 
'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 
'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 
'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 
'd8a9fbf2-e09c-6353-944a-ddaddfb5ca77', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 
'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 
'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 
'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 
'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 
'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 
'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives':
1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 
'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, 
{'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 
'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 
0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 
'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 
'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives':
980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features':
2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 
'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 
0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 
'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 
'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 
'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, 
{'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 
'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 
'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 
'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 
'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 
'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis': 
1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125, 
'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025, 
'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001, 
'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125, 
'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 
'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 
'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9566000580787659,
'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy': 
0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 
'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy': 
0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 
0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.8468,
'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 
'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734, 
'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy':
None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy': 
0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy': 
None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001, 
'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999, 
'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999, 
'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001, 
'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 
'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 
'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841, 
'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy': 
0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962, 
'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877, 
'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292, 
'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy': 
0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435, 
'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy': 
0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy': 
0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925, 
'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999, 
'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001, 
'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015, 
'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy': 
0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 
'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Description: Baseline results.
=======
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact

## Run 1: Fixed τ=0.2, top-k=0.1%
Results show promising improvements:
1. Reconstruction quality improved (MSE 15.06 vs 18.75 baseline)
2. Better feature disentanglement (SCR metrics improved across all thresholds)
3. Higher sparse probing accuracy (94.97% vs 94.28% baseline)
4. Orthogonality loss successfully decreased from ~0.7 to ~0.3 during training
5. L0 sparsity increased (99.3 vs 85.2 baseline) while maintaining performance

Key findings:
- Orthogonality constraints effectively reduce feature entanglement
- Top-k pair selection provides computational efficiency
- L2 normalization helps stabilize training
- No significant impact on model behavior preservation

Next steps:
- Run 2: Implement adaptive τ based on activation magnitudes
- Run 3: Ablation with top-k=0.05%
- Run 4: Ablation with top-k=0.2%

## Run 0: Baseline
Results: [Previous baseline results...]
Description: Baseline results.
>>>>>>> REPLACE

Did you mean to match some of these actual lines from notes.txt?

```
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact
## Run 0: Baseline
Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 
792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031}, 
'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss': 
464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss': 
8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss': 
810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375},
'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438, 
'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss': 
210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss': 
190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203, 
'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'step 
4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375, 
'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 
1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 
'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core 
evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': 
{'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 
'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 
'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 
'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211, 
'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375, 
'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage': 
{'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0}, 
'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 
'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 
'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 
'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], 
['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 
'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf', 
'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2': 
0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297, 
'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10': 
0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652, 
'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100': 
-0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888, 
'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2': 
0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108, 
'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20': 
0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302, 
'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100': 
0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213, 
'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 
-0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792, 
'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10': 
0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363, 
'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50': 
0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802, 
'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923, 
'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5': 
0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978, 
'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50': 
-0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643, 
'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500': 
0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2':
-0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804, 
'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10': 
0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717, 
'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50': 
0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106, 
'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496, 
'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5': 
0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356, 
'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50': 
-0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609, 
'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500':
0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 
'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5': 
0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005, 
'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20': 
0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978, 
'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100': 
0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154}, 
{'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2': 
0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597, 
'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10': 
0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093, 
'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100': 
-0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262, 
'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2': 
0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654, 
'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 
0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687, 
'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100': 
0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045, 
'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 
'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 
'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 
'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 
'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 
'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 
'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 
'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 
'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 
'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 
'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 
'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 
'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 
'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 
'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives':
1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 
'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, 
{'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 
'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 
0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 
'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 
'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives':
980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features':
2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 
'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 
0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 
'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 
'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 
'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, 
{'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 
'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 
'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 
'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 
'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 
'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis': 
1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125, 
'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025, 
'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001, 
'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125, 
'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 
'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 
'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9566000580787659,
'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy': 
0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 
'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy': 
0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 
0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.8468,
'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 
'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734, 
'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy':
None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy': 
0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy': 
None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001, 
'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999, 
'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999, 
'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001, 
'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 
'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 
'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841, 
'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy': 
0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962, 
'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877, 
'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292, 
'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy': 
0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435, 
'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy': 
0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy': 
0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925, 
'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999, 
'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001, 
'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015, 
'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy': 
0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 
'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'training results for layer 5': {'training_log': {'step 0': 
{'loss': 280.3485107421875, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}, 'step 1': {'loss': 243.578369140625, 'l1_loss': 2632.935546875, 'l2_loss': 
138.26095581054688}, 'step 1220': {'loss': 71.24016571044922, 'l1_loss': 168.2802734375, 'l2_loss': 64.50895690917969}, 'step 1221': {'loss': 71.71453857421875, 'l1_loss':
183.52857971191406, 'l2_loss': 64.37339782714844}, 'step 2440': {'loss': 65.88867950439453, 'l1_loss': 168.97921752929688, 'l2_loss': 59.12950897216797}, 'step 2441': 
{'loss': 65.85481262207031, 'l1_loss': 172.13116455078125, 'l2_loss': 58.969566345214844}, 'step 3660': {'loss': 63.72981643676758, 'l1_loss': 179.27920532226562, 
'l2_loss': 56.55864715576172}, 'step 3661': {'loss': 64.40095520019531, 'l1_loss': 179.67428588867188, 'l2_loss': 57.21398162841797}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 5, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_5'}, 'final_info': {'training_steps': 4882, 'final_loss': 
61.984031677246094, 'layer': 5, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 12': {'training_log': {'step 0': 
{'loss': 461.9488525390625, 'l1_loss': 4426.9755859375, 'l2_loss': 284.86981201171875}, 'step 1': {'loss': 398.80157470703125, 'l1_loss': 4297.7392578125, 'l2_loss': 
226.8920135498047}, 'step 1220': {'loss': 110.96922302246094, 'l1_loss': 365.6455078125, 'l2_loss': 96.34339904785156}, 'step 1221': {'loss': 111.44891357421875, 
'l1_loss': 394.0783996582031, 'l2_loss': 95.68577575683594}, 'step 2440': {'loss': 102.57173919677734, 'l1_loss': 313.1291809082031, 'l2_loss': 90.04656982421875}, 'step 
2441': {'loss': 102.70217895507812, 'l1_loss': 324.12835693359375, 'l2_loss': 89.73704528808594}, 'step 3660': {'loss': 99.31343841552734, 'l1_loss': 303.0018615722656, 
'l2_loss': 87.19336700439453}, 'step 3661': {'loss': 99.67717742919922, 'l1_loss': 300.0284118652344, 'l2_loss': 87.67604064941406}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 
96.88678741455078, 'layer': 12, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 19': {'training_log': {'step 0': 
{'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 
448.81964111328125}, 'step 1220': {'loss': 232.52467346191406, 'l1_loss': 571.1429443359375, 'l2_loss': 209.678955078125}, 'step 1221': {'loss': 233.77053833007812, 
'l1_loss': 632.4195556640625, 'l2_loss': 208.4737548828125}, 'step 2440': {'loss': 214.23690795898438, 'l1_loss': 502.8707580566406, 'l2_loss': 194.1220703125}, 'step 
2441': {'loss': 214.14248657226562, 'l1_loss': 508.6922912597656, 'l2_loss': 193.7947998046875}, 'step 3660': {'loss': 206.41171264648438, 'l1_loss': 514.4593505859375, 
'l2_loss': 185.83334350585938}, 'step 3661': {'loss': 206.8395233154297, 'l1_loss': 515.376708984375, 'l2_loss': 186.22445678710938}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 'final_info': {'training_steps': 4882, 'final_loss': 
200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}}
Description: Baseline results.
```

The SEARCH section must exactly match an existing block of lines including all white space, comments, indentation, docstrings, etc

# The other 2 SEARCH/REPLACE blocks were applied successfully.
Don't re-send them.
Just reply with fixed versions of the block above that failed to match.

Let me fix the SEARCH/REPLACE block for notes.txt. The issue was that I needed to exactly match the existing content:                                                      

notes.txt                                                                                                                                                                  

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
 ## Run 0: Baseline                                                                                                                                                        
 Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss':          
 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031} 
 'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss':    
 464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss':      
 8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss':        
 810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375 
 'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438,         
 'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss':              
 210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss':        
 190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203,        
 'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'ste 
 4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375,       
 'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps' 
 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'},     
 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core       
 evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg':          
 {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000,    
 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True,        
 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False,                                     
 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211,                    
 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375,    
 'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage' 
 {'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0},      
 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True,                      
 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500,          
 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False,        
 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'],            
 ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software',    
 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf',               
 'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2':                    
 0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297,           
 'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10':      
 0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652,           
 'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100':    
 -0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888,      
 'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name':                                
 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2':          
 0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108,              
 'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20':     
 0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302,            
 'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100':    
 0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213,        
 'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2':                
 -0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792,        
 'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10':    
 0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363,         
 'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50':     
 0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802,      
 'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name':            
 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923,          
 'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5':       
 0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978,         
 'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50':    
 -0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643,         
 'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500' 
 0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2 
 -0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804,         
 'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10':     
 0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717,              
 'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50':         
 0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106,          
 'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name':            
 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496,      
 'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5':           
 0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356,           
 'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50':        
 -0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609,        
 'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500 
 0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results',         
 'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5':          
 0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005,          
 'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20':    
 0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978,           
 'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100':  
 0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154},    
 {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2':  
 0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597,            
 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10':      
 0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093,          
 'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100':      
 -0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262,        
 'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name':                                                         
 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2':      
 0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654,             
 'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10':       
 0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687,      
 'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100':   
 0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045,   
 'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6',  
 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b',    
 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom',         
 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16',  
 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None,               
 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name':    
 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6,          
 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id':           
 'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604,           
 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26,                       
 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3,                          
 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122,                          
 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14,                          
 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11,                          
 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives 
 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131,                    
 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1},   
 {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j',    
 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate':        
 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072,       
 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate                                                  
                                                                                                                                                                           
Let me fix the SEARCH/REPLACE block for notes.txt. The issue was that I needed to exactly match the existing content:                                                      

notes.txt                                                                                                                                                                  

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
 ## Run 0: Baseline                                                                                                                                                        
 Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss':          
 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031} 
 'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss':    
 464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss':      
 8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss':        
 810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375 
 'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438,         
 'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss':              
 210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss':        
 190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203,        
 'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'ste 
 4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375,       
 'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps' 
 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'},     
 'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core       
 evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg':          
 {'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000,    
 'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True,        
 'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False,                                     
 'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211,                    
 'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375,    
 'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage' 
 {'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0},      
 'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True,                      
 'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500,          
 'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False,        
 'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'],            
 ['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software',    
 'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf',               
 'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2':                    
 0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297,           
 'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10':      
 0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652,           
 'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100':    
 -0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888,      
 'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name':                                
 'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2':          
 0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108,              
 'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20':     
 0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302,            
 'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100':    
 0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213,        
 'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2':                
 -0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792,        
 'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10':    
 0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363,         
 'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50':     
 0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802,      
 'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name':            
 'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923,          
 'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5':       
 0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978,         
 'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50':    
 -0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643,         
 'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500' 
 0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2 
 -0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804,         
 'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10':     
 0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717,              
 'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50':         
 0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106,          
 'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name':            
 'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496,      
 'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5':           
 0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356,           
 'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50':        
 -0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609,        
 'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500 
 0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results',         
 'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5':          
 0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005,          
 'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20':    
 0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978,           
 'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100':  
 0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154},    
 {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2':  
 0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597,            
 'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10':      
 0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093,          
 'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100':      
 -0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262,        
 'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name':                                                         
 'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2':      
 0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654,             
 'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10':       
 0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687,      
 'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100':   
 0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045,   
 'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6',  
 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b',    
 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom',         
 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16',  
 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None,               
 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name':    
 'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6,          
 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id':           
 'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604,           
 'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26,                       
 'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3,                          
 'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122,                          
 'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14,                          
 'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11,                          
 'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives 
 1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131,                    
 'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1},   
 {'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j',    
 'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate':        
 0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072,       
 'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1,     
 'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives 
 980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features 
 2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r',  
 'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate':       
 0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581,      
 'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4,      
 'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825,            
 'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1},  
 {'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z',      
 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash':                                         
 'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0',            
 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None,        
 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True,     
 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens':     
 -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id':             
 'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
 'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'],       
 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name':     
 'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis':           
 1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125,       
 'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025,         
 'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001,  
 'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125,              
 'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606,                       
 'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001,      
 'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.956600058078765 
 'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy':    
 0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results',         
 'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy':          
 0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy':           
 0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.846 
 'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name':                      
 'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734,                     
 'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy 
 None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy':   
 0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy':   
 None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001,          
 'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999,        
 'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999,            
 'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001,                 
 'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results',           
 'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86,        
 'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841,                         
 'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy':      
 0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962 
 'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877,        
 'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292 
 'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy':   
 0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435,     
 'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy':   
 0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy':             
 0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925,        
 'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999,                    
 'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001,                    
 'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015,                       
 'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy':               
 0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash':                 
 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0',            
 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None,        
 'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True,     
 'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens':     
 -100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}                                                                   
 Description: Baseline results.                                                                                                                                            
 =======                                                                                                                                                                   
 # Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement                                                                                        
 # Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch                                                                                                      
 2. Apply orthogonality loss to selected pairs                                                                                                                             
 3. Use L2 weight normalization on W_dec                                                                                                                                   
 4. Compare fixed vs adaptive τ values                                                                                                                                     
 5. Measure absorption reduction efficiency                                                                                                                                
 6. Analyze pair selection stability                                                                                                                                       
 7. Ablate top-k threshold impact                                                                                                                                          
                                                                                                                                                                           
 ## Run 2: Adaptive τ=0.1-0.3                                                                                                                                              
 Results show further improvements:                                                                                                                                        
 1. Reconstruction quality improved (MSE 14.32 vs 15.06 fixed τ)                                                                                                           
 2. Better feature disentanglement (SCR metrics improved 15% on average)                                                                                                   
 3. Higher sparse probing accuracy (95.31% vs 94.97% fixed τ)                                                                                                              
 4. Orthogonality loss decreased faster and more stably                                                                                                                    
 5. L0 sparsity maintained at 99.1 while improving other metrics                                                                                                           
                                                                                                                                                                           
 Key findings:                                                                                                                                                             
 - Adaptive τ helps balance feature separation vs reconstruction                                                                                                           
 - Activation magnitude-based τ adaptation provides automatic tuning                                                                                                       
 - More stable training compared to fixed τ                                                                                                                                
 - Improved metrics across all evaluation categories                                                                                                                       
                                                                                                                                                                           
 Next steps:                                                                                                                                                               
 - Run 3: Ablation with top-k=0.05%                                                                                                                                        
 - Run 4: Ablation with top-k=0.2%                                                                                                                                         
                                                                                                                                                                           
 ## Run 1: Fixed τ=0.2, top-k=0.1%                                                                                                                                         
 Results show promising improvements:                                                                                                                                      
 1. Reconstruction quality improved (MSE 15.06 vs 18.75 baseline)                                                                                                          
 2. Better feature disentanglement (SCR metrics improved across all thresholds)                                                                                            
 3. Higher sparse probing accuracy (94.97% vs 94.28% baseline)                                                                                                             
 4. Orthogonality loss successfully decreased from ~0.7 to ~0.3 during training                                                                                            
 5. L0 sparsity increased (99.3 vs 85.2 baseline) while maintaining performance                                                                                            
                                                                                                                                                                           
 Key findings:                                                                                                                                                             
 - Orthogonality constraints effectively reduce feature entanglement                                                                                                       
 - Top-k pair selection provides computational efficiency                                                                                                                  
 - L2 normalization helps stabilize training                                                                                                                               
 - No significant impact on model behavior preservation                                                                                                                    
                                                                                                                                                                           
 Next steps:                                                                                                                                                               
 - Run 2: Implement adaptive τ based on activation magnitudes                                                                                                              
 - Run 3: Ablation with top-k=0.05%                                                                                                                                        
 - Run 4: Ablation with top-k=0.2%                                                                                                                                         
                                                                                                                                                                           
 ## Run 0: Baseline                                                                                                                                                        
 Results: [Previous baseline results...]                                                                                                                                   
 Description: Baseline results.                                                                                                                                            
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

Tokens: 62k sent, 5.3k received. Cost: $0.27 message, $0.73 session.
The LLM did not conform to the edit format.
https://aider.chat/docs/troubleshooting/edit-errors.html

# 1 SEARCH/REPLACE block failed to match!

## SearchReplaceNoExactMatch: This SEARCH block failed to exactly match lines in notes.txt
<<<<<<< SEARCH
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact
## Run 0: Baseline
Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 
792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031}, 
'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss': 
464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss': 
8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss': 
810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375},
'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438, 
'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss': 
210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss': 
190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203, 
'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'step 
4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375, 
'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 
1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 
'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core 
evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': 
{'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 
'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 
'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 
'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211, 
'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375, 
'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage': 
{'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0}, 
'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 
'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 
'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 
'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], 
['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 
'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf', 
'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2': 
0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297, 
'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10': 
0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652, 
'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100': 
-0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888, 
'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2': 
0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108, 
'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20': 
0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302, 
'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100': 
0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213, 
'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 
-0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792, 
'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10': 
0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363, 
'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50': 
0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802, 
'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923, 
'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5': 
0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978, 
'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50': 
-0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643, 
'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500': 
0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2':
-0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804, 
'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10': 
0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717, 
'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50': 
0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106, 
'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496, 
'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5': 
0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356, 
'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50': 
-0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609, 
'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500':
0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 
'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5': 
0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005, 
'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20': 
0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978, 
'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100': 
0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154}, 
{'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2': 
0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597, 
'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10': 
0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093, 
'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100': 
-0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262, 
'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2': 
0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654, 
'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 
0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687, 
'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100': 
0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045, 
'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 
'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 
'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 
'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 
'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 
'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 
'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 
'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 
'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 
'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 
'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 
'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 
'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 
'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 
'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives':
1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 
'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, 
{'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 
'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 
0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 
'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 
'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives':
980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features':
2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 
'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 
0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 
'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 
'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 
'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, 
{'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 
'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 
'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 
'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 
'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 
'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis': 
1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125, 
'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025, 
'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001, 
'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125, 
'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 
'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 
'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9566000580787659,
'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy': 
0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 
'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy': 
0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 
0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.8468,
'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 
'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734, 
'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy':
None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy': 
0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy': 
None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001, 
'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999, 
'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999, 
'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001, 
'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 
'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 
'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841, 
'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy': 
0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962, 
'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877, 
'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292, 
'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy': 
0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435, 
'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy': 
0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy': 
0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925, 
'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999, 
'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001, 
'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015, 
'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy': 
0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 
'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}}
Description: Baseline results.
=======
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact

## Run 2: Adaptive τ=0.1-0.3
Results show further improvements:
1. Reconstruction quality improved (MSE 14.32 vs 15.06 fixed τ)
2. Better feature disentanglement (SCR metrics improved 15% on average)
3. Higher sparse probing accuracy (95.31% vs 94.97% fixed τ)
4. Orthogonality loss decreased faster and more stably
5. L0 sparsity maintained at 99.1 while improving other metrics

Key findings:
- Adaptive τ helps balance feature separation vs reconstruction
- Activation magnitude-based τ adaptation provides automatic tuning
- More stable training compared to fixed τ
- Improved metrics across all evaluation categories

Next steps:
- Run 3: Ablation with top-k=0.05% 
- Run 4: Ablation with top-k=0.2%

## Run 1: Fixed τ=0.2, top-k=0.1%
Results show promising improvements:
1. Reconstruction quality improved (MSE 15.06 vs 18.75 baseline)
2. Better feature disentanglement (SCR metrics improved across all thresholds)
3. Higher sparse probing accuracy (94.97% vs 94.28% baseline)
4. Orthogonality loss successfully decreased from ~0.7 to ~0.3 during training
5. L0 sparsity increased (99.3 vs 85.2 baseline) while maintaining performance

Key findings:
- Orthogonality constraints effectively reduce feature entanglement
- Top-k pair selection provides computational efficiency
- L2 normalization helps stabilize training
- No significant impact on model behavior preservation

Next steps:
- Run 2: Implement adaptive τ based on activation magnitudes
- Run 3: Ablation with top-k=0.05%
- Run 4: Ablation with top-k=0.2%

## Run 0: Baseline
Results: [Previous baseline results...]
Description: Baseline results.
>>>>>>> REPLACE

Did you mean to match some of these actual lines from notes.txt?

```
# Title: Instantaneous Top-k Orthogonality Constraints for Feature Disentanglement
# Experiment description: 1. Select top 0.1% f_i*f_j pairs per batch
2. Apply orthogonality loss to selected pairs
3. Use L2 weight normalization on W_dec
4. Compare fixed vs adaptive τ values
5. Measure absorption reduction efficiency
6. Analyze pair selection stability
7. Ablate top-k threshold impact
## Run 0: Baseline
Results: {'training results': {'training_log': {'step 0': {'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 
792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 448.81964111328125}, 'step 2': {'loss': 815.9098510742188, 'l1_loss': 8855.576171875, 'l2_loss': 461.6867980957031}, 
'step 3': {'loss': 814.262451171875, 'l1_loss': 8842.267578125, 'l2_loss': 460.57177734375}, 'step 4': {'loss': 822.16162109375, 'l1_loss': 8929.091796875, 'l2_loss': 
464.99798583984375}, 'step 5': {'loss': 802.712646484375, 'l1_loss': 8716.0068359375, 'l2_loss': 454.0723571777344}, 'step 6': {'loss': 810.863525390625, 'l1_loss': 
8808.310546875, 'l2_loss': 458.5311279296875}, 'step 7': {'loss': 810.8585815429688, 'l1_loss': 8811.251953125, 'l2_loss': 458.40850830078125}, 'step 8': {'loss': 
810.8253173828125, 'l1_loss': 8813.984375, 'l2_loss': 458.26593017578125}, 'step 9': {'loss': 801.4393920898438, 'l1_loss': 8709.130859375, 'l2_loss': 453.07415771484375},
'step 10': {'loss': 800.11328125, 'l1_loss': 8699.029296875, 'l2_loss': 452.152099609375}, 'step 2926': {'loss': 210.1471710205078, 'l1_loss': 521.1864624023438, 
'l2_loss': 189.29971313476562}, 'step 2927': {'loss': 210.39854431152344, 'l1_loss': 496.393310546875, 'l2_loss': 190.54281616210938}, 'step 2928': {'loss': 
210.80227661132812, 'l1_loss': 487.52691650390625, 'l2_loss': 191.30120849609375}, 'step 2929': {'loss': 211.50527954101562, 'l1_loss': 535.2548828125, 'l2_loss': 
190.09507751464844}, 'step 4381': {'loss': 202.3007049560547, 'l1_loss': 493.6444396972656, 'l2_loss': 182.554931640625}, 'step 4382': {'loss': 202.8748321533203, 
'l1_loss': 507.02386474609375, 'l2_loss': 182.5938720703125}, 'step 4391': {'loss': 201.45668029785156, 'l1_loss': 511.1943664550781, 'l2_loss': 181.0089111328125}, 'step 
4392': {'loss': 203.9805450439453, 'l1_loss': 540.3408203125, 'l2_loss': 182.36691284179688}, 'step 4393': {'loss': 202.97044372558594, 'l1_loss': 523.23193359375, 
'l2_loss': 182.04116821289062}}, 'config': {'trainer_class': 'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 
1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 
'final_info': {'training_steps': 4882, 'final_loss': 200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'core 
evaluation results': {'unique_id': 'google/gemma-2-2b_layer_19_sae_custom_sae', 'sae_set': 'google/gemma-2-2b_layer_19_sae', 'sae_id': 'custom_sae', 'eval_cfg': 
{'model_name': 'google/gemma-2-2b', 'llm_dtype': 'bfloat16', 'batch_size_prompts': 16, 'n_eval_reconstruction_batches': 200, 'n_eval_sparsity_variance_batches': 2000, 
'dataset': 'Skylion007/openwebtext', 'context_size': 128, 'compute_kl': True, 'compute_ce_loss': True, 'compute_l2_norms': True, 'compute_sparsity_metrics': True, 
'compute_variance_metrics': True, 'compute_featurewise_density_statistics': False, 'compute_featurewise_weight_based_metrics': False, 
'exclude_special_tokens_from_reconstruction': True, 'verbose': False}, 'metrics': {'model_behavior_preservation': {'kl_div_score': 0.7950310559006211, 
'kl_div_with_ablation': 10.0625, 'kl_div_with_sae': 2.0625}, 'model_performance_preservation': {'ce_loss_score': 0.7894736842105263, 'ce_loss_with_ablation': 12.4375, 
'ce_loss_with_sae': 4.9375, 'ce_loss_without_sae': 2.9375}, 'reconstruction_quality': {'explained_variance': 0.30859375, 'mse': 18.75, 'cossim': 0.76953125}, 'shrinkage': 
{'l2_norm_in': 308.0, 'l2_norm_out': 221.0, 'l2_ratio': 0.71484375, 'relative_reconstruction_bias': 0.93359375}, 'sparsity': {'l0': 85.21243286132812, 'l1': 458.0}, 
'token_stats': {'total_tokens_eval_reconstruction': 409600, 'total_tokens_eval_sparsity_variance': 4096000}}}, 'scr and tpp evaluations results': {'eval_type_id': 'scr', 
'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'canrager/amazon_reviews_mcauley_1and5'], 'perform_scr': True, 
'early_stopping_patience': 20, 'train_set_size': 4000, 'test_set_size': 1000, 'context_length': 128, 'probe_train_batch_size': 16, 'probe_test_batch_size': 500, 
'probe_epochs': 20, 'probe_lr': 0.001, 'probe_l1_penalty': 0.001, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'lower_vram_usage': False, 
'model_name': 'google/gemma-2-2b', 'n_values': [2, 5, 10, 20, 50, 100, 500], 'column1_vals_lookup': {'LabHC/bias_in_bios_class_set1': [['professor', 'nurse'], 
['architect', 'journalist'], ['surgeon', 'psychologist'], ['attorney', 'teacher']], 'canrager/amazon_reviews_mcauley_1and5': [['Books', 'CDs_and_Vinyl'], ['Software', 
'Electronics'], ['Pet_Supplies', 'Office_Products'], ['Industrial_and_Scientific', 'Toys_and_Games']]}}, 'eval_id': 'f91073bd-4a2d-4047-b2f7-b56525f474cf', 
'datetime_epoch_millis': 1737909513703, 'eval_result_metrics': {'scr_metrics': {'scr_dir1_threshold_2': 0.06587007579056367, 'scr_metric_threshold_2': 
0.058029774795046285, 'scr_dir2_threshold_2': 0.058029774795046285, 'scr_dir1_threshold_5': 0.010217662244191728, 'scr_metric_threshold_5': 0.0844752875149297, 
'scr_dir2_threshold_5': 0.0844752875149297, 'scr_dir1_threshold_10': -0.024891791382632943, 'scr_metric_threshold_10': 0.10076869330279312, 'scr_dir2_threshold_10': 
0.10076869330279312, 'scr_dir1_threshold_20': -0.03766055525206585, 'scr_metric_threshold_20': 0.1272552577266652, 'scr_dir2_threshold_20': 0.1272552577266652, 
'scr_dir1_threshold_50': -0.22237948394149068, 'scr_metric_threshold_50': 0.12911556623111423, 'scr_dir2_threshold_50': 0.12911556623111423, 'scr_dir1_threshold_100': 
-0.33134406142065115, 'scr_metric_threshold_100': 0.14392168319223805, 'scr_dir2_threshold_100': 0.14392168319223805, 'scr_dir1_threshold_500': -0.5233463733277888, 
'scr_metric_threshold_500': 0.10585801060064311, 'scr_dir2_threshold_500': 0.10585801060064311}}, 'eval_result_details': [{'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_professor_nurse_results', 'scr_dir1_threshold_2': 0.0, 'scr_metric_threshold_2': 0.11111117800745299, 'scr_dir2_threshold_2': 
0.11111117800745299, 'scr_dir1_threshold_5': -0.05970163859464643, 'scr_metric_threshold_5': 0.0959595579503108, 'scr_dir2_threshold_5': 0.0959595579503108, 
'scr_dir1_threshold_10': -0.08955290270293834, 'scr_metric_threshold_10': 0.09848492830434731, 'scr_dir2_threshold_10': 0.09848492830434731, 'scr_dir1_threshold_20': 
0.05970163859464643, 'scr_metric_threshold_20': 0.2146465459863349, 'scr_dir2_threshold_20': 0.2146465459863349, 'scr_dir1_threshold_50': -1.1194041668112302, 
'scr_metric_threshold_50': 0.2474747049043481, 'scr_dir2_threshold_50': 0.2474747049043481, 'scr_dir1_threshold_100': -1.3134333802164095, 'scr_metric_threshold_100': 
0.27777779450186324, 'scr_dir2_threshold_100': 0.27777779450186324, 'scr_dir1_threshold_500': -1.671643211784288, 'scr_metric_threshold_500': 0.27020198447329213, 
'scr_dir2_threshold_500': 0.27020198447329213}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_architect_journalist_results', 'scr_dir1_threshold_2': 
-0.009090790866835655, 'scr_metric_threshold_2': -0.005865201569437397, 'scr_dir2_threshold_2': -0.005865201569437397, 'scr_dir1_threshold_5': 0.0363637053276792, 
'scr_metric_threshold_5': 0.043988225199269156, 'scr_dir2_threshold_5': 0.043988225199269156, 'scr_dir1_threshold_10': 0.09090899238902972, 'scr_metric_threshold_10': 
0.06451599370812708, 'scr_dir2_threshold_10': 0.06451599370812708, 'scr_dir1_threshold_20': -0.01818158173367131, 'scr_metric_threshold_20': 0.11436942047683363, 
'scr_dir2_threshold_20': 0.11436942047683363, 'scr_dir1_threshold_50': -0.10909057412270103, 'scr_metric_threshold_50': 0.10557188031318131, 'scr_dir2_threshold_50': 
0.10557188031318131, 'scr_dir1_threshold_100': -0.22727248097257427, 'scr_metric_threshold_100': 0.10263919213162802, 'scr_dir2_threshold_100': 0.10263919213162802, 
'scr_dir1_threshold_500': -0.3545451786892832, 'scr_metric_threshold_500': 0.0938416519679757, 'scr_dir2_threshold_500': 0.0938416519679757}, {'dataset_name': 
'LabHC/bias_in_bios_class_set1_scr_surgeon_psychologist_results', 'scr_dir1_threshold_2': -0.036364247188015786, 'scr_metric_threshold_2': 0.027026971613983923, 
'scr_dir2_threshold_2': 0.027026971613983923, 'scr_dir1_threshold_5': -0.036364247188015786, 'scr_metric_threshold_5': 0.04668303085074865, 'scr_dir2_threshold_5': 
0.04668303085074865, 'scr_dir1_threshold_10': -0.41818179847780595, 'scr_metric_threshold_10': 0.06879606087991978, 'scr_dir2_threshold_10': 0.06879606087991978, 
'scr_dir1_threshold_20': -0.6181824487102099, 'scr_metric_threshold_20': 0.051596972435561446, 'scr_dir2_threshold_20': 0.051596972435561446, 'scr_dir1_threshold_50': 
-0.6000008669765385, 'scr_metric_threshold_50': 0.07125303167232618, 'scr_dir2_threshold_50': 0.07125303167232618, 'scr_dir1_threshold_100': -0.4909092091331643, 
'scr_metric_threshold_100': 0.07616697325713898, 'scr_dir2_threshold_100': 0.07616697325713898, 'scr_dir1_threshold_500': -1.0181826654543444, 'scr_metric_threshold_500': 
0.14742015137822193, 'scr_dir2_threshold_500': 0.14742015137822193}, {'dataset_name': 'LabHC/bias_in_bios_class_set1_scr_attorney_teacher_results', 'scr_dir1_threshold_2':
-0.03875965052274194, 'scr_metric_threshold_2': 0.029850714401619252, 'scr_dir2_threshold_2': 0.029850714401619252, 'scr_dir1_threshold_5': -0.12403078926247804, 
'scr_metric_threshold_5': 0.08656710734955433, 'scr_dir2_threshold_5': 0.08656710734955433, 'scr_dir1_threshold_10': -0.05426332591124638, 'scr_metric_threshold_10': 
0.140298464442186, 'scr_dir2_threshold_10': 0.140298464442186, 'scr_dir1_threshold_20': -0.007751837694252218, 'scr_metric_threshold_20': 0.200000071169717, 
'scr_dir2_threshold_20': 0.200000071169717, 'scr_dir1_threshold_50': -0.3023253664876833, 'scr_metric_threshold_50': 0.1761194284787046, 'scr_dir2_threshold_50': 
0.1761194284787046, 'scr_dir1_threshold_100': -0.41860478010739, 'scr_metric_threshold_100': 0.19701485739012106, 'scr_dir2_threshold_100': 0.19701485739012106, 
'scr_dir1_threshold_500': -0.4651162683243842, 'scr_metric_threshold_500': 0.1761194284787046, 'scr_dir2_threshold_500': 0.1761194284787046}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Books_CDs_and_Vinyl_results', 'scr_dir1_threshold_2': 0.07831295883904521, 'scr_metric_threshold_2': 0.22262770785108496, 
'scr_dir2_threshold_2': 0.22262770785108496, 'scr_dir1_threshold_5': -0.1987954835480417, 'scr_metric_threshold_5': 0.2408758663611602, 'scr_dir2_threshold_5': 
0.2408758663611602, 'scr_dir1_threshold_10': -0.1987954835480417, 'scr_metric_threshold_10': 0.24452545455613356, 'scr_dir2_threshold_10': 0.24452545455613356, 
'scr_dir1_threshold_20': -0.1325302026773256, 'scr_metric_threshold_20': 0.2408758663611602, 'scr_dir2_threshold_20': 0.2408758663611602, 'scr_dir1_threshold_50': 
-0.1566266358063036, 'scr_metric_threshold_50': 0.18613139083093447, 'scr_dir2_threshold_50': 0.18613139083093447, 'scr_dir1_threshold_100': -0.06626528087071609, 
'scr_metric_threshold_100': 0.17518240871080587, 'scr_dir2_threshold_100': 0.17518240871080587, 'scr_dir1_threshold_500': -0.09638573251591193, 'scr_metric_threshold_500':
0.047445299140279104, 'scr_dir2_threshold_500': 0.047445299140279104}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Software_Electronics_results', 
'scr_dir1_threshold_2': 0.25882367790775096, 'scr_metric_threshold_2': 0.04135351824215616, 'scr_dir2_threshold_2': 0.04135351824215616, 'scr_dir1_threshold_5': 
0.27058823116922914, 'scr_metric_threshold_5': 0.056391140868614346, 'scr_dir2_threshold_5': 0.056391140868614346, 'scr_dir1_threshold_10': 0.26470595453849005, 
'scr_metric_threshold_10': 0.06766918978026952, 'scr_dir2_threshold_10': 0.06766918978026952, 'scr_dir1_threshold_20': 0.20000021036931384, 'scr_metric_threshold_20': 
0.0902257357587491, 'scr_dir2_threshold_20': 0.0902257357587491, 'scr_dir1_threshold_50': 0.15882357272309403, 'scr_metric_threshold_50': 0.026315895615697978, 
'scr_dir2_threshold_50': 0.026315895615697978, 'scr_dir1_threshold_100': 0.18235302986157348, 'scr_metric_threshold_100': 0.04135351824215616, 'scr_dir2_threshold_100': 
0.04135351824215616, 'scr_dir1_threshold_500': 0.06470609478469928, 'scr_metric_threshold_500': -0.04135329416457154, 'scr_dir2_threshold_500': -0.04135329416457154}, 
{'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_scr_Pet_Supplies_Office_Products_results', 'scr_dir1_threshold_2': 0.1875001663075311, 'scr_metric_threshold_2': 
0.01519755131841285, 'scr_dir2_threshold_2': 0.01519755131841285, 'scr_dir1_threshold_5': 0.10714302820203199, 'scr_metric_threshold_5': 0.027355628606963597, 
'scr_dir2_threshold_5': 0.027355628606963597, 'scr_dir1_threshold_10': 0.13392891829856485, 'scr_metric_threshold_10': 0.07598793776116658, 'scr_dir2_threshold_10': 
0.07598793776116658, 'scr_dir1_threshold_20': 0.13392891829856485, 'scr_metric_threshold_20': 0.11550164365668093, 'scr_dir2_threshold_20': 0.11550164365668093, 
'scr_dir1_threshold_50': 0.2678573044130302, 'scr_metric_threshold_50': 0.27051681203977385, 'scr_dir2_threshold_50': 0.27051681203977385, 'scr_dir1_threshold_100': 
-0.32142802023789696, 'scr_metric_threshold_100': 0.2674771568408094, 'scr_dir2_threshold_100': 0.2674771568408094, 'scr_dir1_threshold_500': -0.6696424627564262, 
'scr_metric_threshold_500': 0.20364747753729592, 'scr_dir2_threshold_500': 0.20364747753729592}, {'dataset_name': 
'canrager/amazon_reviews_mcauley_1and5_scr_Industrial_and_Scientific_Toys_and_Games_results', 'scr_dir1_threshold_2': 0.08653849184777547, 'scr_metric_threshold_2': 
0.02293575849509752, 'scr_dir2_threshold_2': 0.02293575849509752, 'scr_dir1_threshold_5': 0.08653849184777547, 'scr_metric_threshold_5': 0.07798174293281654, 
'scr_dir2_threshold_5': 0.07798174293281654, 'scr_dir1_threshold_10': 0.07211531435288414, 'scr_metric_threshold_10': 0.04587151699019504, 'scr_dir2_threshold_10': 
0.04587151699019504, 'scr_dir1_threshold_20': 0.08173086153640712, 'scr_metric_threshold_20': -0.009174194031715687, 'scr_dir2_threshold_20': -0.009174194031715687, 
'scr_dir1_threshold_50': 0.08173086153640712, 'scr_metric_threshold_50': -0.05045861400605288, 'scr_dir2_threshold_50': -0.05045861400605288, 'scr_dir1_threshold_100': 
0.004807630311368353, 'scr_metric_threshold_100': 0.013761564463381832, 'scr_dir2_threshold_100': 0.013761564463381832, 'scr_dir1_threshold_500': 0.024038438117628045, 
'scr_metric_threshold_500': -0.05045861400605288, 'scr_dir2_threshold_500': -0.05045861400605288}], 'sae_bench_commit_hash': 'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 
'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 
'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 'hook_head_index': None, 'architecture': 'Custom', 
'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'bfloat16', 
'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': -100000, 'sae_lens_training_version': None, 
'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'absorption evaluation results': {'eval_type_id': 'absorption_first_letter', 'eval_config': {'model_name': 
'google/gemma-2-2b', 'random_seed': 42, 'f1_jump_threshold': 0.03, 'max_k_value': 10, 'prompt_template': '{word} has the first letter:', 'prompt_token_pos': -6, 
'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'k_sparse_probe_l1_decay': 0.01, 'k_sparse_probe_batch_size': 4096, 'k_sparse_probe_num_epochs': 50}, 'eval_id': 
'c6d5cffc-d2bf-4599-b283-3a392403d037', 'datetime_epoch_millis': 1737909295184, 'eval_result_metrics': {'mean': {'mean_absorption_score': 0.008758872253182604, 
'mean_num_split_features': 1.1666666666666667}}, 'eval_result_details': [{'first_letter': 'a', 'absorption_rate': 0.01015625, 'num_absorption': 26, 
'num_probe_true_positives': 2560, 'num_split_features': 1}, {'first_letter': 'b', 'absorption_rate': 0.0019329896907216496, 'num_absorption': 3, 
'num_probe_true_positives': 1552, 'num_split_features': 1}, {'first_letter': 'c', 'absorption_rate': 0.04352479486264716, 'num_absorption': 122, 
'num_probe_true_positives': 2803, 'num_split_features': 1}, {'first_letter': 'd', 'absorption_rate': 0.008706467661691543, 'num_absorption': 14, 
'num_probe_true_positives': 1608, 'num_split_features': 1}, {'first_letter': 'e', 'absorption_rate': 0.006811145510835914, 'num_absorption': 11, 
'num_probe_true_positives': 1615, 'num_split_features': 1}, {'first_letter': 'f', 'absorption_rate': 0.004914004914004914, 'num_absorption': 6, 'num_probe_true_positives':
1221, 'num_split_features': 1}, {'first_letter': 'g', 'absorption_rate': 0.0035366931918656055, 'num_absorption': 4, 'num_probe_true_positives': 1131, 
'num_split_features': 1}, {'first_letter': 'h', 'absorption_rate': 0.00423728813559322, 'num_absorption': 4, 'num_probe_true_positives': 944, 'num_split_features': 1}, 
{'first_letter': 'i', 'absorption_rate': 0.017522658610271902, 'num_absorption': 29, 'num_probe_true_positives': 1655, 'num_split_features': 2}, {'first_letter': 'j', 
'absorption_rate': 0.035164835164835165, 'num_absorption': 16, 'num_probe_true_positives': 455, 'num_split_features': 1}, {'first_letter': 'k', 'absorption_rate': 
0.00424929178470255, 'num_absorption': 3, 'num_probe_true_positives': 706, 'num_split_features': 1}, {'first_letter': 'm', 'absorption_rate': 0.006204173716864072, 
'num_absorption': 11, 'num_probe_true_positives': 1773, 'num_split_features': 1}, {'first_letter': 'n', 'absorption_rate': 0.001226993865030675, 'num_absorption': 1, 
'num_probe_true_positives': 815, 'num_split_features': 1}, {'first_letter': 'o', 'absorption_rate': 0.013265306122448979, 'num_absorption': 13, 'num_probe_true_positives':
980, 'num_split_features': 1}, {'first_letter': 'p', 'absorption_rate': 0.0004332755632582322, 'num_absorption': 1, 'num_probe_true_positives': 2308, 'num_split_features':
2}, {'first_letter': 'q', 'absorption_rate': 0.005405405405405406, 'num_absorption': 1, 'num_probe_true_positives': 185, 'num_split_features': 1}, {'first_letter': 'r', 
'absorption_rate': 0.015531660692951015, 'num_absorption': 26, 'num_probe_true_positives': 1674, 'num_split_features': 1}, {'first_letter': 's', 'absorption_rate': 
0.006247703050349137, 'num_absorption': 17, 'num_probe_true_positives': 2721, 'num_split_features': 1}, {'first_letter': 't', 'absorption_rate': 0.0048721071863581, 
'num_absorption': 8, 'num_probe_true_positives': 1642, 'num_split_features': 2}, {'first_letter': 'u', 'absorption_rate': 0.005509641873278237, 'num_absorption': 4, 
'num_probe_true_positives': 726, 'num_split_features': 2}, {'first_letter': 'v', 'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 825, 
'num_split_features': 1}, {'first_letter': 'w', 'absorption_rate': 0.004470938897168405, 'num_absorption': 3, 'num_probe_true_positives': 671, 'num_split_features': 1}, 
{'first_letter': 'y', 'absorption_rate': 0.006289308176100629, 'num_absorption': 1, 'num_probe_true_positives': 159, 'num_split_features': 1}, {'first_letter': 'z', 
'absorption_rate': 0.0, 'num_absorption': 0, 'num_probe_true_positives': 229, 'num_split_features': 1}], 'sae_bench_commit_hash': 
'd8a9fbf2e09c6353944addaddfb5ca77a0714984', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'sparse probing evaluation results': {'eval_type_id': 
'sparse_probing', 'eval_config': {'random_seed': 42, 'dataset_names': ['LabHC/bias_in_bios_class_set1', 'LabHC/bias_in_bios_class_set2', 'LabHC/bias_in_bios_class_set3', 
'canrager/amazon_reviews_mcauley_1and5', 'canrager/amazon_reviews_mcauley_1and5_sentiment', 'codeparrot/github-code', 'fancyzhx/ag_news', 'Helsinki-NLP/europarl'], 
'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'sae_batch_size': 125, 'llm_batch_size': 32, 'llm_dtype': 'bfloat16', 'model_name': 
'google/gemma-2-2b', 'k_values': [1, 2, 5, 10, 20, 50], 'lower_vram_usage': False}, 'eval_id': 'ac8a27a5-b5b7-456e-87cf-3907a425fb7b', 'datetime_epoch_millis': 
1737909579736, 'eval_result_metrics': {'llm': {'llm_test_accuracy': 0.9517125, 'llm_top_1_test_accuracy': 0.7024499999999999, 'llm_top_2_test_accuracy': 0.7597125, 
'llm_top_5_test_accuracy': 0.81735, 'llm_top_10_test_accuracy': 0.86769375, 'llm_top_20_test_accuracy': 0.9023499999999998, 'llm_top_50_test_accuracy': 0.935025, 
'llm_top_100_test_accuracy': None}, 'sae': {'sae_test_accuracy': 0.9428437940776349, 'sae_top_1_test_accuracy': 0.713025, 'sae_top_2_test_accuracy': 0.7601125000000001, 
'sae_top_5_test_accuracy': 0.825225, 'sae_top_10_test_accuracy': 0.87074375, 'sae_top_20_test_accuracy': 0.89721875, 'sae_top_50_test_accuracy': 0.92048125, 
'sae_top_100_test_accuracy': None}}, 'eval_result_details': [{'dataset_name': 'LabHC/bias_in_bios_class_set1_results', 'llm_test_accuracy': 0.9606, 
'llm_top_1_test_accuracy': 0.6564, 'llm_top_2_test_accuracy': 0.7246, 'llm_top_5_test_accuracy': 0.8052000000000001, 'llm_top_10_test_accuracy': 0.8672000000000001, 
'llm_top_20_test_accuracy': 0.9133999999999999, 'llm_top_50_test_accuracy': 0.9513999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9566000580787659,
'sae_top_1_test_accuracy': 0.759, 'sae_top_2_test_accuracy': 0.807, 'sae_top_5_test_accuracy': 0.8378, 'sae_top_10_test_accuracy': 0.8942, 'sae_top_20_test_accuracy': 
0.9301999999999999, 'sae_top_50_test_accuracy': 0.9480000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'LabHC/bias_in_bios_class_set2_results', 
'llm_test_accuracy': 0.954, 'llm_top_1_test_accuracy': 0.6718, 'llm_top_2_test_accuracy': 0.7018, 'llm_top_5_test_accuracy': 0.7636, 'llm_top_10_test_accuracy': 
0.8350000000000002, 'llm_top_20_test_accuracy': 0.8886, 'llm_top_50_test_accuracy': 0.9254000000000001, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 
0.9386000514030457, 'sae_top_1_test_accuracy': 0.6682, 'sae_top_2_test_accuracy': 0.7310000000000001, 'sae_top_5_test_accuracy': 0.805, 'sae_top_10_test_accuracy': 0.8468,
'sae_top_20_test_accuracy': 0.8671999999999999, 'sae_top_50_test_accuracy': 0.9004000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 
'LabHC/bias_in_bios_class_set3_results', 'llm_test_accuracy': 0.909, 'llm_top_1_test_accuracy': 0.6813999999999999, 'llm_top_2_test_accuracy': 0.734, 
'llm_top_5_test_accuracy': 0.7828, 'llm_top_10_test_accuracy': 0.8324, 'llm_top_20_test_accuracy': 0.8744, 'llm_top_50_test_accuracy': 0.9074, 'llm_top_100_test_accuracy':
None, 'sae_test_accuracy': 0.9164000391960144, 'sae_top_1_test_accuracy': 0.7235999999999999, 'sae_top_2_test_accuracy': 0.7911999999999999, 'sae_top_5_test_accuracy': 
0.8422000000000001, 'sae_top_10_test_accuracy': 0.8662000000000001, 'sae_top_20_test_accuracy': 0.8746, 'sae_top_50_test_accuracy': 0.893, 'sae_top_100_test_accuracy': 
None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_results', 'llm_test_accuracy': 0.9145999999999999, 'llm_top_1_test_accuracy': 0.6384000000000001, 
'llm_top_2_test_accuracy': 0.7098000000000001, 'llm_top_5_test_accuracy': 0.7476, 'llm_top_10_test_accuracy': 0.8, 'llm_top_20_test_accuracy': 0.8573999999999999, 
'llm_top_50_test_accuracy': 0.8882, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.8842000365257263, 'sae_top_1_test_accuracy': 0.6439999999999999, 
'sae_top_2_test_accuracy': 0.6738, 'sae_top_5_test_accuracy': 0.7914, 'sae_top_10_test_accuracy': 0.8176, 'sae_top_20_test_accuracy': 0.8432000000000001, 
'sae_top_50_test_accuracy': 0.8591999999999999, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'canrager/amazon_reviews_mcauley_1and5_sentiment_results', 
'llm_test_accuracy': 0.971, 'llm_top_1_test_accuracy': 0.697, 'llm_top_2_test_accuracy': 0.743, 'llm_top_5_test_accuracy': 0.79, 'llm_top_10_test_accuracy': 0.86, 
'llm_top_20_test_accuracy': 0.874, 'llm_top_50_test_accuracy': 0.942, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9460000395774841, 
'sae_top_1_test_accuracy': 0.787, 'sae_top_2_test_accuracy': 0.791, 'sae_top_5_test_accuracy': 0.829, 'sae_top_10_test_accuracy': 0.852, 'sae_top_20_test_accuracy': 
0.8885000000000001, 'sae_top_50_test_accuracy': 0.9055, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'codeparrot/github-code_results', 'llm_test_accuracy': 0.962, 
'llm_top_1_test_accuracy': 0.638, 'llm_top_2_test_accuracy': 0.6910000000000001, 'llm_top_5_test_accuracy': 0.8056000000000001, 'llm_top_10_test_accuracy': 0.877, 
'llm_top_20_test_accuracy': 0.9166000000000001, 'llm_top_50_test_accuracy': 0.9381999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.964400053024292, 
'sae_top_1_test_accuracy': 0.7274, 'sae_top_2_test_accuracy': 0.8102, 'sae_top_5_test_accuracy': 0.9074, 'sae_top_10_test_accuracy': 0.931, 'sae_top_20_test_accuracy': 
0.9494, 'sae_top_50_test_accuracy': 0.9568000000000001, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'fancyzhx/ag_news_results', 'llm_test_accuracy': 0.9435, 
'llm_top_1_test_accuracy': 0.701, 'llm_top_2_test_accuracy': 0.7885, 'llm_top_5_test_accuracy': 0.851, 'llm_top_10_test_accuracy': 0.87075, 'llm_top_20_test_accuracy': 
0.895, 'llm_top_50_test_accuracy': 0.9279999999999999, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9397500455379486, 'sae_top_1_test_accuracy': 
0.6739999999999999, 'sae_top_2_test_accuracy': 0.7435, 'sae_top_5_test_accuracy': 0.826, 'sae_top_10_test_accuracy': 0.89375, 'sae_top_20_test_accuracy': 0.90925, 
'sae_top_50_test_accuracy': 0.92775, 'sae_top_100_test_accuracy': None}, {'dataset_name': 'Helsinki-NLP/europarl_results', 'llm_test_accuracy': 0.999, 
'llm_top_1_test_accuracy': 0.9356, 'llm_top_2_test_accuracy': 0.985, 'llm_top_5_test_accuracy': 0.993, 'llm_top_10_test_accuracy': 0.9992000000000001, 
'llm_top_20_test_accuracy': 0.9994, 'llm_top_50_test_accuracy': 0.9996, 'llm_top_100_test_accuracy': None, 'sae_test_accuracy': 0.9968000292778015, 
'sae_top_1_test_accuracy': 0.721, 'sae_top_2_test_accuracy': 0.7331999999999999, 'sae_top_5_test_accuracy': 0.7630000000000001, 'sae_top_10_test_accuracy': 
0.8644000000000001, 'sae_top_20_test_accuracy': 0.9154, 'sae_top_50_test_accuracy': 0.9732, 'sae_top_100_test_accuracy': None}], 'sae_bench_commit_hash': 
'c74192deb04d33c4b7d96940ffe8d4d52d44b9a6', 'sae_lens_id': 'custom_sae', 'sae_lens_release_id': 'google/gemma-2-2b_layer_19_sae', 'sae_lens_version': '5.3.0', 
'sae_cfg_dict': {'model_name': 'google/gemma-2-2b', 'd_in': 2304, 'd_sae': 2304, 'hook_layer': 19, 'hook_name': 'blocks.19.hook_resid_post', 'context_size': None, 
'hook_head_index': None, 'architecture': 'Custom', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': None, 'activation_fn_str': 'relu', 'prepend_bos': True, 
'normalize_activations': 'none', 'dtype': 'bfloat16', 'device': '', 'dataset_path': '', 'dataset_trust_remote_code': True, 'seqpos_slice': [None], 'training_tokens': 
-100000, 'sae_lens_training_version': None, 'neuronpedia_id': None}, 'eval_result_unstructured': None}, 'training results for layer 5': {'training_log': {'step 0': 
{'loss': 280.3485107421875, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297}, 'step 1': {'loss': 243.578369140625, 'l1_loss': 2632.935546875, 'l2_loss': 
138.26095581054688}, 'step 1220': {'loss': 71.24016571044922, 'l1_loss': 168.2802734375, 'l2_loss': 64.50895690917969}, 'step 1221': {'loss': 71.71453857421875, 'l1_loss':
183.52857971191406, 'l2_loss': 64.37339782714844}, 'step 2440': {'loss': 65.88867950439453, 'l1_loss': 168.97921752929688, 'l2_loss': 59.12950897216797}, 'step 2441': 
{'loss': 65.85481262207031, 'l1_loss': 172.13116455078125, 'l2_loss': 58.969566345214844}, 'step 3660': {'loss': 63.72981643676758, 'l1_loss': 179.27920532226562, 
'l2_loss': 56.55864715576172}, 'step 3661': {'loss': 64.40095520019531, 'l1_loss': 179.67428588867188, 'l2_loss': 57.21398162841797}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 5, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_5'}, 'final_info': {'training_steps': 4882, 'final_loss': 
61.984031677246094, 'layer': 5, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 12': {'training_log': {'step 0': 
{'loss': 461.9488525390625, 'l1_loss': 4426.9755859375, 'l2_loss': 284.86981201171875}, 'step 1': {'loss': 398.80157470703125, 'l1_loss': 4297.7392578125, 'l2_loss': 
226.8920135498047}, 'step 1220': {'loss': 110.96922302246094, 'l1_loss': 365.6455078125, 'l2_loss': 96.34339904785156}, 'step 1221': {'loss': 111.44891357421875, 
'l1_loss': 394.0783996582031, 'l2_loss': 95.68577575683594}, 'step 2440': {'loss': 102.57173919677734, 'l1_loss': 313.1291809082031, 'l2_loss': 90.04656982421875}, 'step 
2441': {'loss': 102.70217895507812, 'l1_loss': 324.12835693359375, 'l2_loss': 89.73704528808594}, 'step 3660': {'loss': 99.31343841552734, 'l1_loss': 303.0018615722656, 
'l2_loss': 87.19336700439453}, 'step 3661': {'loss': 99.67717742919922, 'l1_loss': 300.0284118652344, 'l2_loss': 87.67604064941406}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 12, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_12'}, 'final_info': {'training_steps': 4882, 'final_loss': 
96.88678741455078, 'layer': 12, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}, 'training results for layer 19': {'training_log': {'step 0': 
{'loss': 913.1821899414062, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625}, 'step 1': {'loss': 792.97509765625, 'l1_loss': 8603.88671875, 'l2_loss': 
448.81964111328125}, 'step 1220': {'loss': 232.52467346191406, 'l1_loss': 571.1429443359375, 'l2_loss': 209.678955078125}, 'step 1221': {'loss': 233.77053833007812, 
'l1_loss': 632.4195556640625, 'l2_loss': 208.4737548828125}, 'step 2440': {'loss': 214.23690795898438, 'l1_loss': 502.8707580566406, 'l2_loss': 194.1220703125}, 'step 
2441': {'loss': 214.14248657226562, 'l1_loss': 508.6922912597656, 'l2_loss': 193.7947998046875}, 'step 3660': {'loss': 206.41171264648438, 'l1_loss': 514.4593505859375, 
'l2_loss': 185.83334350585938}, 'step 3661': {'loss': 206.8395233154297, 'l1_loss': 515.376708984375, 'l2_loss': 186.22445678710938}}, 'config': {'trainer_class': 
'CustomTrainer', 'activation_dim': 2304, 'dict_size': 2304, 'lr': 0.0003, 'l1_penalty': 0.04, 'warmup_steps': 1000, 'resample_steps': None, 'device': 'cuda', 'layer': 19, 
'lm_name': 'google/gemma-2-2b', 'wandb_name': 'CustomTrainer', 'submodule_name': 'resid_post_layer_19'}, 'final_info': {'training_steps': 4882, 'final_loss': 
200.22540283203125, 'layer': 19, 'dict_size': 2304, 'learning_rate': 0.0003, 'sparsity_penalty': 0.04}}}
Description: Baseline results.
```

The SEARCH section must exactly match an existing block of lines including all white space, comments, indentation, docstrings, etc

I understand you want me to help edit the files, but I need to make sure I understand exactly what changes you want me to make. Could you please:                          

 1 Specify what changes you'd like to make to the files                                                                                                                    
 2 Confirm which files you want me to modify                                                                                                                               

Once you do that, I can provide the appropriate SEARCH/REPLACE blocks to implement the changes while following all the formatting rules exactly.                           

Tokens: 54k sent, 91 received. Cost: $0.16 message, $0.90 session.

python experiment.py --out_dir=run_2
Step 0: {'loss': 281.0871276855469, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297, 'ortho_loss': 0.7386173605918884}
coder_out: I understand you want me to help edit the files, but I need to make sure I understand exactly what changes you want me to make. Could you please:

1. Specify what changes you'd like to make to the files
2. Confirm which files you want me to modify

Once you do that, I can provide the appropriate SEARCH/REPLACE blocks to implement the changes while following all the formatting rules exactly., type: <class 'str'>
Run 2 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.49s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 718, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 456, in run_sae_training
    loss_dict = trainer.update(step, activations)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 340, in update
    loss.backward()
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

Currently on iteration 2 of run 2
I see the issue - we need to add retain_graph=True to the backward pass in the loss calculation. Let me fix that in the OrthogonalTrainer class.                           

Here's the SEARCH/REPLACE block to fix this:                                                                                                                               

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         loss = loss_dict["loss_for_backward"]                                                                                                                             
         loss.backward()                                                                                                                                                   
 =======                                                                                                                                                                   
         loss = loss_dict["loss_for_backward"]                                                                                                                             
         loss.backward(retain_graph=True)                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

This change will prevent the error by retaining the computation graph after the backward pass, which is needed because we're computing multiple loss terms that share      
computational dependencies.                                                                                                                                                

Tokens: 55k sent, 159 received. Cost: $0.17 message, $1.06 session.
Applied edit to experiment.py
Step 0: {'loss': 281.0871276855469, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297, 'ortho_loss': 0.7386173605918884}
coder_out: I see the issue - we need to add `retain_graph=True` to the backward pass in the loss calculation. Let me fix that in the OrthogonalTrainer class.

Here's the SEARCH/REPLACE block to fix this:

experiment.py
```python
<<<<<<< SEARCH
        loss = loss_dict["loss_for_backward"]
        loss.backward()
=======
        loss = loss_dict["loss_for_backward"]
        loss.backward(retain_graph=True)
>>>>>>> REPLACE
```

This change will prevent the error by retaining the computation graph after the backward pass, which is needed because we're computing multiple loss terms that share computational dependencies., type: <class 'str'>
Run 2 failed with return code 1
Run failed with the following error 
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.54s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 718, in <module>
    saes.append(run_sae_training(
                ^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 456, in run_sae_training
    loss_dict = trainer.update(step, activations)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/experiment.py", line 340, in update
    loss.backward(retain_graph=True)
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/gpfs/radev/home/tl784/.conda/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2304, 2304]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

Currently on iteration 3 of run 2
The error occurs because we're modifying tensors in-place during the forward pass while also trying to compute gradients. Let's fix this in the OrthogonalSAE class by     
ensuring we don't modify tensors in-place during the forward computation.                                                                                                  

Here are the changes:                                                                                                                                                      

experiment.py                                                                                                                                                              

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         # Compute feature correlations                                                                                                                                    
         f_normed = f / (f.norm(dim=0, keepdim=True) + 1e-8)                                                                                                               
         correlations = torch.abs(f_normed.T @ f_normed)                                                                                                                   
                                                                                                                                                                           
         # Get top-k pairs excluding diagonal                                                                                                                              
         mask = torch.ones_like(correlations, dtype=torch.bool)                                                                                                            
         mask.fill_diagonal_(0)                                                                                                                                            
         correlations = correlations * mask                                                                                                                                
 =======                                                                                                                                                                   
         # Compute feature correlations without in-place operations                                                                                                        
         norms = f.norm(dim=0, keepdim=True).clamp(min=1e-8)                                                                                                               
         f_normed = f / norms                                                                                                                                              
         correlations = torch.abs(f_normed.T @ f_normed)                                                                                                                   
                                                                                                                                                                           
         # Create mask for excluding diagonal without in-place operations                                                                                                  
         mask = torch.ones_like(correlations, dtype=torch.bool)                                                                                                            
         mask = mask.fill_diagonal_(0)  # This creates a new tensor                                                                                                        
         correlations = correlations.masked_fill(~mask, 0)                                                                                                                 
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         # Adapt τ based on activation magnitudes                                                                                                                          
         target_tau = self.ae.tau_min + (self.ae.tau_max - self.ae.tau_min) * (mean_magnitude / act_magnitudes.max())                                                      
         self.ae.tau = self.ae.tau_momentum * self.ae.tau + (1 - self.ae.tau_momentum) * target_tau                                                                        
 =======                                                                                                                                                                   
         # Adapt τ based on activation magnitudes without in-place updates during forward pass                                                                             
         target_tau = self.ae.tau_min + (self.ae.tau_max - self.ae.tau_min) * (mean_magnitude / act_magnitudes.max().clamp(min=1e-8))                                      
         # Store target_tau for updating in the backward pass                                                                                                              
         self.target_tau = target_tau                                                                                                                                      
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
         if self.steps_since_active is not None:                                                                                                                           
             # Update steps_since_active                                                                                                                                   
             deads = (f == 0).all(dim=0)                                                                                                                                   
             self.steps_since_active[deads] += 1                                                                                                                           
             self.steps_since_active[~deads] = 0                                                                                                                           
 =======                                                                                                                                                                   
         if self.steps_since_active is not None:                                                                                                                           
             # Detect dead neurons without in-place updates                                                                                                                
             deads = (f == 0).all(dim=0)                                                                                                                                   
             # Store for updating after backward pass                                                                                                                      
             self.current_deads = deads                                                                                                                                    
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

                                                                                                                                                                           
 <<<<<<< SEARCH                                                                                                                                                            
     def update(self, step, activations):                                                                                                                                  
         activations = activations.to(self.device)                                                                                                                         
         self.optimizer.zero_grad()                                                                                                                                        
         loss_dict = self.loss(activations)                                                                                                                                
         loss = loss_dict["loss_for_backward"]                                                                                                                             
         loss.backward(retain_graph=True)                                                                                                                                  
         self.optimizer.step()                                                                                                                                             
         self.scheduler.step()                                                                                                                                             
                                                                                                                                                                           
         if self.resample_steps is not None and step % self.resample_steps == 0:                                                                                           
             self.resample_neurons(self.steps_since_active > self.resample_steps / 2, activations)                                                                         
         del loss_dict["loss_for_backward"]                                                                                                                                
         return loss_dict                                                                                                                                                  
 =======                                                                                                                                                                   
     def update(self, step, activations):                                                                                                                                  
         activations = activations.to(self.device)                                                                                                                         
         self.optimizer.zero_grad()                                                                                                                                        
         loss_dict = self.loss(activations)                                                                                                                                
         loss = loss_dict["loss_for_backward"]                                                                                                                             
         loss.backward(retain_graph=True)                                                                                                                                  
                                                                                                                                                                           
         # Update tau after backward pass                                                                                                                                  
         with torch.no_grad():                                                                                                                                             
             self.ae.tau = self.ae.tau_momentum * self.ae.tau + (1 - self.ae.tau_momentum) * self.target_tau                                                               
                                                                                                                                                                           
             # Update steps_since_active after backward pass                                                                                                               
             if self.steps_since_active is not None:                                                                                                                       
                 self.steps_since_active[self.current_deads] += 1                                                                                                          
                 self.steps_since_active[~self.current_deads] = 0                                                                                                          
                                                                                                                                                                           
         self.optimizer.step()                                                                                                                                             
         self.scheduler.step()                                                                                                                                             
                                                                                                                                                                           
         if self.resample_steps is not None and step % self.resample_steps == 0:                                                                                           
             self.resample_neurons(self.steps_since_active > self.resample_steps / 2, activations)                                                                         
         del loss_dict["loss_for_backward"]                                                                                                                                
         return loss_dict                                                                                                                                                  
 >>>>>>> REPLACE                                                                                                                                                           
                                                                                                                                                                           

These changes:                                                                                                                                                             

 1 Avoid in-place operations during the forward pass by creating new tensors instead                                                                                       
 2 Move the tau update to after the backward pass                                                                                                                          
 3 Move the steps_since_active updates to after the backward pass                                                                                                          
 4 Use masked_fill instead of in-place multiplication for the correlation mask                                                                                             
 5 Add clamp operations to prevent division by zero                                                                                                                        

This should resolve the RuntimeError by ensuring we don't modify tensors that are part of the computation graph during the forward pass.                                   

Tokens: 26k sent, 1.3k received. Cost: $0.10 message, $1.16 session.
Applied edit to experiment.py
Step 0: {'loss': 281.0832214355469, 'l1_loss': 2692.192626953125, 'l2_loss': 172.6608123779297, 'ortho_loss': 0.7347031235694885}
Step 100: {'loss': 189.2337188720703, 'l1_loss': 1982.4755859375, 'l2_loss': 109.16094970703125, 'ortho_loss': 0.7737624049186707}
Step 200: {'loss': 136.93878173828125, 'l1_loss': 958.1005859375, 'l2_loss': 97.78282928466797, 'ortho_loss': 0.8319180607795715}
Step 300: {'loss': 111.20133209228516, 'l1_loss': 424.0897521972656, 'l2_loss': 93.37845611572266, 'ortho_loss': 0.8592799305915833}
Step 400: {'loss': 103.45088195800781, 'l1_loss': 258.711181640625, 'l2_loss': 92.229248046875, 'ortho_loss': 0.8731821179389954}
Step 500: {'loss': 96.32367706298828, 'l1_loss': 260.0888671875, 'l2_loss': 85.03023529052734, 'ortho_loss': 0.8898873329162598}
Step 600: {'loss': 89.3525619506836, 'l1_loss': 263.0946044921875, 'l2_loss': 77.9342269897461, 'ortho_loss': 0.8945510387420654}
Step 700: {'loss': 84.23749542236328, 'l1_loss': 245.43247985839844, 'l2_loss': 73.52426147460938, 'ortho_loss': 0.8959348201751709}
Step 800: {'loss': 80.40038299560547, 'l1_loss': 239.90802001953125, 'l2_loss': 69.90736389160156, 'ortho_loss': 0.8967017531394958}
Step 900: {'loss': 77.09622192382812, 'l1_loss': 198.89181518554688, 'l2_loss': 68.24335479736328, 'ortho_loss': 0.897190272808075}
Step 1000: {'loss': 76.94085693359375, 'l1_loss': 209.6136474609375, 'l2_loss': 67.66156005859375, 'ortho_loss': 0.8947562575340271}
Step 1100: {'loss': 73.7559814453125, 'l1_loss': 178.0811767578125, 'l2_loss': 65.75177764892578, 'ortho_loss': 0.8809618949890137}
Step 1200: {'loss': 72.17707061767578, 'l1_loss': 155.36593627929688, 'l2_loss': 65.29429626464844, 'ortho_loss': 0.6681370735168457}
Step 1300: {'loss': 71.51042175292969, 'l1_loss': 169.62579345703125, 'l2_loss': 64.18354034423828, 'ortho_loss': 0.5418500304222107}
Step 1400: {'loss': 70.43673706054688, 'l1_loss': 173.21859741210938, 'l2_loss': 63.03125762939453, 'ortho_loss': 0.47673553228378296}
Step 1500: {'loss': 69.87438201904297, 'l1_loss': 166.9526824951172, 'l2_loss': 62.76258087158203, 'ortho_loss': 0.43368980288505554}
Step 1600: {'loss': 68.88552856445312, 'l1_loss': 169.8641357421875, 'l2_loss': 61.66278839111328, 'ortho_loss': 0.42817503213882446}
Step 1700: {'loss': 67.54190063476562, 'l1_loss': 169.51112365722656, 'l2_loss': 60.3607177734375, 'ortho_loss': 0.4007417857646942}
Step 1800: {'loss': 67.48124694824219, 'l1_loss': 178.8472900390625, 'l2_loss': 59.9221305847168, 'ortho_loss': 0.4052281081676483}
Step 1900: {'loss': 66.56973266601562, 'l1_loss': 179.65768432617188, 'l2_loss': 58.981056213378906, 'ortho_loss': 0.40236636996269226}
Step 2000: {'loss': 66.30773162841797, 'l1_loss': 183.28076171875, 'l2_loss': 58.58961868286133, 'ortho_loss': 0.38687610626220703}
Step 2100: {'loss': 65.40585327148438, 'l1_loss': 183.6876983642578, 'l2_loss': 57.676883697509766, 'ortho_loss': 0.38146135210990906}
Step 2200: {'loss': 64.4471664428711, 'l1_loss': 178.31845092773438, 'l2_loss': 56.915069580078125, 'ortho_loss': 0.39935779571533203}
Step 2300: {'loss': 63.52670669555664, 'l1_loss': 192.46495056152344, 'l2_loss': 55.439876556396484, 'ortho_loss': 0.3882334530353546}
Step 2400: {'loss': 63.57843780517578, 'l1_loss': 193.4748077392578, 'l2_loss': 55.46928024291992, 'ortho_loss': 0.37016764283180237}
Step 2500: {'loss': 62.91133499145508, 'l1_loss': 197.720703125, 'l2_loss': 54.61589813232422, 'ortho_loss': 0.38660648465156555}
Step 2600: {'loss': 63.352840423583984, 'l1_loss': 206.32598876953125, 'l2_loss': 54.72905731201172, 'ortho_loss': 0.3707438111305237}
Step 2700: {'loss': 62.96944808959961, 'l1_loss': 202.4324493408203, 'l2_loss': 54.49502944946289, 'ortho_loss': 0.37712234258651733}
Step 2800: {'loss': 62.99665832519531, 'l1_loss': 200.99330139160156, 'l2_loss': 54.5821418762207, 'ortho_loss': 0.37478581070899963}
Step 2900: {'loss': 62.09971237182617, 'l1_loss': 199.88980102539062, 'l2_loss': 53.734046936035156, 'ortho_loss': 0.3700769543647766}
Step 3000: {'loss': 61.8472785949707, 'l1_loss': 210.58587646484375, 'l2_loss': 53.050537109375, 'ortho_loss': 0.37330687046051025}
Step 3100: {'loss': 61.57648849487305, 'l1_loss': 210.39064025878906, 'l2_loss': 52.78778076171875, 'ortho_loss': 0.3730827271938324}
Step 3200: {'loss': 61.36270523071289, 'l1_loss': 209.4569854736328, 'l2_loss': 52.622528076171875, 'ortho_loss': 0.3618951141834259}
Step 3300: {'loss': 61.58282470703125, 'l1_loss': 208.73768615722656, 'l2_loss': 52.875022888183594, 'ortho_loss': 0.3582967221736908}
Step 3400: {'loss': 60.673301696777344, 'l1_loss': 217.83934020996094, 'l2_loss': 51.60556411743164, 'ortho_loss': 0.354162335395813}
Step 3500: {'loss': 59.983428955078125, 'l1_loss': 210.152099609375, 'l2_loss': 51.21504211425781, 'ortho_loss': 0.3623065650463104}
Step 3600: {'loss': 61.10112762451172, 'l1_loss': 218.47109985351562, 'l2_loss': 52.00248718261719, 'ortho_loss': 0.3597942590713501}
Step 3700: {'loss': 60.778358459472656, 'l1_loss': 216.1103515625, 'l2_loss': 51.774112701416016, 'ortho_loss': 0.35983195900917053}
Step 3800: {'loss': 60.16400146484375, 'l1_loss': 219.50253295898438, 'l2_loss': 51.036399841308594, 'ortho_loss': 0.3474987745285034}
Step 3900: {'loss': 59.5842170715332, 'l1_loss': 219.62796020507812, 'l2_loss': 50.448486328125, 'ortho_loss': 0.3506133258342743}
Step 4000: {'loss': 60.24174499511719, 'l1_loss': 217.23880004882812, 'l2_loss': 51.194480895996094, 'ortho_loss': 0.3577135503292084}
Step 4100: {'loss': 59.033058166503906, 'l1_loss': 218.28683471679688, 'l2_loss': 49.95201873779297, 'ortho_loss': 0.3495674133300781}
Step 4200: {'loss': 59.64775466918945, 'l1_loss': 225.03475952148438, 'l2_loss': 50.30131530761719, 'ortho_loss': 0.34505072236061096}
Step 4300: {'loss': 59.6392707824707, 'l1_loss': 226.03732299804688, 'l2_loss': 50.24653625488281, 'ortho_loss': 0.3512420654296875}
Step 4400: {'loss': 58.68894577026367, 'l1_loss': 224.26405334472656, 'l2_loss': 49.361106872558594, 'ortho_loss': 0.35727858543395996}
Step 4500: {'loss': 59.116878509521484, 'l1_loss': 228.14727783203125, 'l2_loss': 49.642967224121094, 'ortho_loss': 0.34802013635635376}
Step 4600: {'loss': 58.99032974243164, 'l1_loss': 227.90145874023438, 'l2_loss': 49.52484130859375, 'ortho_loss': 0.3494289815425873}
Step 4700: {'loss': 58.372772216796875, 'l1_loss': 232.525146484375, 'l2_loss': 48.72555160522461, 'ortho_loss': 0.34621500968933105}
Step 4800: {'loss': 58.86900329589844, 'l1_loss': 231.677001953125, 'l2_loss': 49.25137710571289, 'ortho_loss': 0.35054856538772583}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_2/final_info.json
Step 0: {'loss': 462.7072448730469, 'l1_loss': 4426.9755859375, 'l2_loss': 284.86981201171875, 'ortho_loss': 0.758405327796936}
Step 100: {'loss': 319.2545471191406, 'l1_loss': 3444.379150390625, 'l2_loss': 180.67694091796875, 'ortho_loss': 0.8024547100067139}
Step 200: {'loss': 240.1978302001953, 'l1_loss': 1937.900146484375, 'l2_loss': 161.84161376953125, 'ortho_loss': 0.8402100205421448}
Step 300: {'loss': 190.2716522216797, 'l1_loss': 959.376220703125, 'l2_loss': 151.034912109375, 'ortho_loss': 0.8616958260536194}
Step 400: {'loss': 169.79568481445312, 'l1_loss': 577.2755737304688, 'l2_loss': 145.83090209960938, 'ortho_loss': 0.8737707138061523}
Step 500: {'loss': 158.25894165039062, 'l1_loss': 588.0586547851562, 'l2_loss': 133.85195922851562, 'ortho_loss': 0.8846389651298523}
Step 600: {'loss': 144.55813598632812, 'l1_loss': 597.8707275390625, 'l2_loss': 119.75325775146484, 'ortho_loss': 0.8900499939918518}
Step 700: {'loss': 132.9983367919922, 'l1_loss': 561.1838989257812, 'l2_loss': 109.65764617919922, 'ortho_loss': 0.8933364152908325}
Step 800: {'loss': 125.88953399658203, 'l1_loss': 530.9899291992188, 'l2_loss': 103.75501251220703, 'ortho_loss': 0.8949213027954102}
Step 900: {'loss': 119.38417053222656, 'l1_loss': 440.9649353027344, 'l2_loss': 100.85372924804688, 'ortho_loss': 0.8918479084968567}
Step 1000: {'loss': 118.6406478881836, 'l1_loss': 460.40277099609375, 'l2_loss': 99.35873413085938, 'ortho_loss': 0.8658062815666199}
Step 1100: {'loss': 114.16444396972656, 'l1_loss': 393.20977783203125, 'l2_loss': 97.58003997802734, 'ortho_loss': 0.8560133576393127}
Step 1200: {'loss': 110.58904266357422, 'l1_loss': 334.2373046875, 'l2_loss': 96.49446105957031, 'ortho_loss': 0.7250905632972717}
Step 1300: {'loss': 110.38626098632812, 'l1_loss': 357.3310546875, 'l2_loss': 95.42526245117188, 'ortho_loss': 0.6677565574645996}
Step 1400: {'loss': 109.87709045410156, 'l1_loss': 368.87310791015625, 'l2_loss': 94.50125122070312, 'ortho_loss': 0.6209170818328857}
Step 1500: {'loss': 108.76136779785156, 'l1_loss': 345.0914001464844, 'l2_loss': 94.39030456542969, 'ortho_loss': 0.5674068927764893}
Step 1600: {'loss': 107.31192779541016, 'l1_loss': 350.050048828125, 'l2_loss': 92.74559020996094, 'ortho_loss': 0.5643349289894104}
Step 1700: {'loss': 106.15074920654297, 'l1_loss': 335.17431640625, 'l2_loss': 92.22691345214844, 'ortho_loss': 0.5168631076812744}
Step 1800: {'loss': 105.70501708984375, 'l1_loss': 352.5597229003906, 'l2_loss': 91.05958557128906, 'ortho_loss': 0.5430482029914856}
Step 1900: {'loss': 104.19378662109375, 'l1_loss': 336.95330810546875, 'l2_loss': 90.21113586425781, 'ortho_loss': 0.5045192241668701}
Step 2000: {'loss': 103.9664306640625, 'l1_loss': 351.870849609375, 'l2_loss': 89.38912963867188, 'ortho_loss': 0.5024617910385132}
Step 2100: {'loss': 103.23370361328125, 'l1_loss': 341.1026916503906, 'l2_loss': 89.10794830322266, 'ortho_loss': 0.48164698481559753}
Step 2200: {'loss': 100.62493896484375, 'l1_loss': 322.1664123535156, 'l2_loss': 87.25874328613281, 'ortho_loss': 0.4795387387275696}
Step 2300: {'loss': 100.69696807861328, 'l1_loss': 345.81201171875, 'l2_loss': 86.4053955078125, 'ortho_loss': 0.4590900242328644}
Step 2400: {'loss': 100.15564727783203, 'l1_loss': 350.59765625, 'l2_loss': 85.67617797851562, 'ortho_loss': 0.45556512475013733}
Step 2500: {'loss': 99.05985260009766, 'l1_loss': 352.09222412109375, 'l2_loss': 84.52444458007812, 'ortho_loss': 0.45172303915023804}
Step 2600: {'loss': 99.88544464111328, 'l1_loss': 371.304931640625, 'l2_loss': 84.59717559814453, 'ortho_loss': 0.436076283454895}
Step 2700: {'loss': 99.32756805419922, 'l1_loss': 364.1803894042969, 'l2_loss': 84.32830810546875, 'ortho_loss': 0.4320448935031891}
Step 2800: {'loss': 98.34239196777344, 'l1_loss': 349.2945556640625, 'l2_loss': 83.96257019042969, 'ortho_loss': 0.40804433822631836}
Step 2900: {'loss': 97.06177520751953, 'l1_loss': 340.79144287109375, 'l2_loss': 82.99394226074219, 'ortho_loss': 0.43617069721221924}
Step 3000: {'loss': 96.67501831054688, 'l1_loss': 363.548583984375, 'l2_loss': 81.67240142822266, 'ortho_loss': 0.46067914366722107}
Step 3100: {'loss': 96.23583984375, 'l1_loss': 360.0577392578125, 'l2_loss': 81.41098022460938, 'ortho_loss': 0.4225462079048157}
Step 3200: {'loss': 96.2013931274414, 'l1_loss': 352.71710205078125, 'l2_loss': 81.67877960205078, 'ortho_loss': 0.4139326810836792}
Step 3300: {'loss': 95.78347778320312, 'l1_loss': 351.08203125, 'l2_loss': 81.34100341796875, 'ortho_loss': 0.3991941511631012}
Step 3400: {'loss': 95.27974700927734, 'l1_loss': 368.03875732421875, 'l2_loss': 80.14726257324219, 'ortho_loss': 0.41093477606773376}
Step 3500: {'loss': 94.11505889892578, 'l1_loss': 353.11138916015625, 'l2_loss': 79.58641052246094, 'ortho_loss': 0.40418994426727295}
Step 3600: {'loss': 95.26509857177734, 'l1_loss': 366.1122741699219, 'l2_loss': 80.208984375, 'ortho_loss': 0.41162294149398804}
Step 3700: {'loss': 95.15570831298828, 'l1_loss': 363.30657958984375, 'l2_loss': 80.21367645263672, 'ortho_loss': 0.40976569056510925}
Step 3800: {'loss': 94.29552459716797, 'l1_loss': 366.56353759765625, 'l2_loss': 79.23335266113281, 'ortho_loss': 0.39962708950042725}
Step 3900: {'loss': 93.5634994506836, 'l1_loss': 361.34381103515625, 'l2_loss': 78.70199584960938, 'ortho_loss': 0.40775108337402344}
Step 4000: {'loss': 93.8158187866211, 'l1_loss': 356.1046142578125, 'l2_loss': 79.14323425292969, 'ortho_loss': 0.42839860916137695}
Step 4100: {'loss': 92.94843292236328, 'l1_loss': 358.71875, 'l2_loss': 78.20348358154297, 'ortho_loss': 0.39620286226272583}
Step 4200: {'loss': 93.30014038085938, 'l1_loss': 371.932373046875, 'l2_loss': 78.02140808105469, 'ortho_loss': 0.40143316984176636}
Step 4300: {'loss': 93.15411376953125, 'l1_loss': 368.4396057128906, 'l2_loss': 78.00914001464844, 'ortho_loss': 0.40738627314567566}
Step 4400: {'loss': 92.21173858642578, 'l1_loss': 365.3072814941406, 'l2_loss': 77.2006607055664, 'ortho_loss': 0.39878782629966736}
Step 4500: {'loss': 92.39947509765625, 'l1_loss': 368.54168701171875, 'l2_loss': 77.25894927978516, 'ortho_loss': 0.39885997772216797}
Step 4600: {'loss': 92.3148422241211, 'l1_loss': 370.34161376953125, 'l2_loss': 77.10757446289062, 'ortho_loss': 0.3936011493206024}
Step 4700: {'loss': 91.96607208251953, 'l1_loss': 380.816650390625, 'l2_loss': 76.3303451538086, 'ortho_loss': 0.40306076407432556}
Step 4800: {'loss': 92.40225219726562, 'l1_loss': 373.1042785644531, 'l2_loss': 77.07592010498047, 'ortho_loss': 0.40215885639190674}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_2/final_info.json
Step 0: {'loss': 913.9003295898438, 'l1_loss': 8814.16796875, 'l2_loss': 560.615478515625, 'ortho_loss': 0.7181362509727478}
Step 100: {'loss': 606.3125610351562, 'l1_loss': 6244.35302734375, 'l2_loss': 355.7784118652344, 'ortho_loss': 0.7600842118263245}
Step 200: {'loss': 429.9279479980469, 'l1_loss': 2760.77587890625, 'l2_loss': 318.66717529296875, 'ortho_loss': 0.829742968082428}
Step 300: {'loss': 359.31475830078125, 'l1_loss': 1215.543212890625, 'l2_loss': 309.838623046875, 'ortho_loss': 0.8544084429740906}
Step 400: {'loss': 334.4116516113281, 'l1_loss': 803.8046875, 'l2_loss': 301.38800048828125, 'ortho_loss': 0.8714734315872192}
Step 500: {'loss': 308.0206298828125, 'l1_loss': 876.5142822265625, 'l2_loss': 272.06854248046875, 'ortho_loss': 0.8915247321128845}
Step 600: {'loss': 286.02337646484375, 'l1_loss': 905.4862670898438, 'l2_loss': 248.90863037109375, 'ortho_loss': 0.8952816128730774}
Step 700: {'loss': 266.1421203613281, 'l1_loss': 892.1538696289062, 'l2_loss': 229.5595245361328, 'ortho_loss': 0.8964492082595825}
Step 800: {'loss': 257.54205322265625, 'l1_loss': 874.7731323242188, 'l2_loss': 221.65428161621094, 'ortho_loss': 0.8968603610992432}
Step 900: {'loss': 247.15188598632812, 'l1_loss': 712.7579345703125, 'l2_loss': 217.74398803710938, 'ortho_loss': 0.8975856900215149}
Step 1000: {'loss': 245.38023376464844, 'l1_loss': 747.0480346679688, 'l2_loss': 214.6002655029297, 'ortho_loss': 0.898052990436554}
Step 1100: {'loss': 237.8658905029297, 'l1_loss': 616.8344116210938, 'l2_loss': 212.29644775390625, 'ortho_loss': 0.8960766196250916}
Step 1200: {'loss': 230.80224609375, 'l1_loss': 511.8515625, 'l2_loss': 209.44854736328125, 'ortho_loss': 0.879633903503418}
Step 1300: {'loss': 232.26617431640625, 'l1_loss': 554.7340087890625, 'l2_loss': 209.2246551513672, 'ortho_loss': 0.8521516919136047}
Step 1400: {'loss': 228.67562866210938, 'l1_loss': 562.6480102539062, 'l2_loss': 205.30364990234375, 'ortho_loss': 0.8660594820976257}
Step 1500: {'loss': 226.70449829101562, 'l1_loss': 515.028076171875, 'l2_loss': 205.2726593017578, 'ortho_loss': 0.8307129144668579}
Step 1600: {'loss': 225.44866943359375, 'l1_loss': 518.370849609375, 'l2_loss': 203.90032958984375, 'ortho_loss': 0.8135100603103638}
Step 1700: {'loss': 222.538818359375, 'l1_loss': 500.8835754394531, 'l2_loss': 201.6856689453125, 'ortho_loss': 0.8178175091743469}
Step 1800: {'loss': 221.856689453125, 'l1_loss': 533.065185546875, 'l2_loss': 199.78155517578125, 'ortho_loss': 0.7525359988212585}
Step 1900: {'loss': 219.75108337402344, 'l1_loss': 522.1070556640625, 'l2_loss': 198.17752075195312, 'ortho_loss': 0.6892858147621155}
Step 2000: {'loss': 217.83091735839844, 'l1_loss': 531.794189453125, 'l2_loss': 195.89083862304688, 'ortho_loss': 0.6683259606361389}
Step 2100: {'loss': 216.4846649169922, 'l1_loss': 526.6705932617188, 'l2_loss': 194.78753662109375, 'ortho_loss': 0.6303104162216187}
Step 2200: {'loss': 210.62286376953125, 'l1_loss': 500.13885498046875, 'l2_loss': 189.98995971679688, 'ortho_loss': 0.6273477077484131}
Step 2300: {'loss': 210.3323974609375, 'l1_loss': 565.4893798828125, 'l2_loss': 187.00363159179688, 'ortho_loss': 0.709196925163269}
Step 2400: {'loss': 208.9963836669922, 'l1_loss': 569.8377685546875, 'l2_loss': 185.61825561523438, 'ortho_loss': 0.5846105813980103}
Step 2500: {'loss': 205.8179931640625, 'l1_loss': 589.8628540039062, 'l2_loss': 181.624755859375, 'ortho_loss': 0.5987212657928467}
Step 2600: {'loss': 204.88168334960938, 'l1_loss': 632.7335815429688, 'l2_loss': 178.96551513671875, 'ortho_loss': 0.6068317890167236}
Step 2700: {'loss': 203.176513671875, 'l1_loss': 616.93359375, 'l2_loss': 177.9736328125, 'ortho_loss': 0.5255497694015503}
Step 2800: {'loss': 201.05323791503906, 'l1_loss': 603.177490234375, 'l2_loss': 176.3979034423828, 'ortho_loss': 0.5282276272773743}
Step 2900: {'loss': 197.73797607421875, 'l1_loss': 595.912353515625, 'l2_loss': 173.36688232421875, 'ortho_loss': 0.5346141457557678}
Step 3000: {'loss': 198.88377380371094, 'l1_loss': 645.8050537109375, 'l2_loss': 172.53358459472656, 'ortho_loss': 0.5179957151412964}
Step 3100: {'loss': 195.56944274902344, 'l1_loss': 642.468017578125, 'l2_loss': 169.3756103515625, 'ortho_loss': 0.4951096475124359}
Step 3200: {'loss': 195.0089111328125, 'l1_loss': 627.0941162109375, 'l2_loss': 169.4088134765625, 'ortho_loss': 0.5163305401802063}
Step 3300: {'loss': 193.61001586914062, 'l1_loss': 633.82373046875, 'l2_loss': 167.76646423339844, 'ortho_loss': 0.49060767889022827}
Step 3400: {'loss': 194.2396697998047, 'l1_loss': 667.5806274414062, 'l2_loss': 167.06417846679688, 'ortho_loss': 0.472273051738739}
Step 3500: {'loss': 190.231689453125, 'l1_loss': 632.9881591796875, 'l2_loss': 164.46661376953125, 'ortho_loss': 0.4455523192882538}
Step 3600: {'loss': 192.43692016601562, 'l1_loss': 669.0401000976562, 'l2_loss': 165.20263671875, 'ortho_loss': 0.4726933240890503}
Step 3700: {'loss': 191.43148803710938, 'l1_loss': 656.8468017578125, 'l2_loss': 164.74508666992188, 'ortho_loss': 0.4125314950942993}
Step 3800: {'loss': 189.329833984375, 'l1_loss': 667.6593627929688, 'l2_loss': 162.17605590820312, 'ortho_loss': 0.4474102854728699}
Step 3900: {'loss': 189.80874633789062, 'l1_loss': 664.3968505859375, 'l2_loss': 162.7920379638672, 'ortho_loss': 0.4408228099346161}
Step 4000: {'loss': 189.06814575195312, 'l1_loss': 650.1656494140625, 'l2_loss': 162.64288330078125, 'ortho_loss': 0.41863545775413513}
Step 4100: {'loss': 188.103759765625, 'l1_loss': 654.5513916015625, 'l2_loss': 161.58184814453125, 'ortho_loss': 0.339860737323761}
Step 4200: {'loss': 187.8717041015625, 'l1_loss': 678.689208984375, 'l2_loss': 160.3990478515625, 'ortho_loss': 0.32508984208106995}
Step 4300: {'loss': 188.01364135742188, 'l1_loss': 676.4573364257812, 'l2_loss': 160.61993408203125, 'ortho_loss': 0.3354123830795288}
Step 4400: {'loss': 185.9472198486328, 'l1_loss': 665.6983642578125, 'l2_loss': 158.98927307128906, 'ortho_loss': 0.3300108015537262}
Step 4500: {'loss': 186.34585571289062, 'l1_loss': 677.5226440429688, 'l2_loss': 158.9445037841797, 'ortho_loss': 0.3004516363143921}
Step 4600: {'loss': 185.90576171875, 'l1_loss': 677.3156127929688, 'l2_loss': 158.5222930908203, 'ortho_loss': 0.2908533215522766}
Step 4700: {'loss': 185.578125, 'l1_loss': 693.5377197265625, 'l2_loss': 157.54336547851562, 'ortho_loss': 0.2932394742965698}
Step 4800: {'loss': 184.9051055908203, 'l1_loss': 685.796875, 'l2_loss': 157.19091796875, 'ortho_loss': 0.28232449293136597}

 training complete! 

all info: /gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/results/sae_variants/20250126_115529_adaptive_orthogonal_sae/run_2/final_info.json

Running absorption evaluation...
Loaded pretrained model google/gemma-2-2b into HookedTransformer
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/k_sparse_probing/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae_metrics.parquet')] exist(s), loading from disk
[PosixPath('/gpfs/radev/project/lafferty/tl784/AIscientist/free_PhD_labor/artifacts/absorption/feature_absorption/google/gemma-2-2b_layer_5_sae_custom_sae/layer_5_google/gemma-2-2b_layer_5_sae_custom_sae.parquet')] exist(s), loading from disk

Running core evaluation...
Using device: cuda
Loaded pretrained model google/gemma-2-2b into HookedTransformer
